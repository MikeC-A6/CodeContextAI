URL: https://openrouter.ai/openai/o3-mini
---
# OpenAI: o3 Mini

### [openai](https://openrouter.ai/openai)/o3-mini

[Chat](https://openrouter.ai/chat?models=openai/o3-mini)

Created Jan 31, 2025200,000 context

$1.1/M input tokens$4.4/M output tokens

OpenAI o3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding.

This model supports the `reasoning_effort` parameter, which can be set to "high", "medium", or "low" to control the thinking time of the model. The default is "medium". OpenRouter also offers the model slug `openai/o3-mini-high` to default the parameter to "high".

The model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.

The model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.

[Chat with o3 Mini](https://openrouter.ai/chat?models=openai/o3-mini)

Overview

Providers

Apps

Activity

Uptime

API

## Providers for o3 Mini

### OpenRouter [routes requests](https://openrouter.ai/docs/provider-routing) to the best providers that are able to handle your prompt size and parameters, with fallbacks to maximize [uptime](https://openrouter.ai/openai/o3-mini/uptime).

|     |     |
| --- | --- |
| [OpenAI](https://openrouter.ai/provider/openai) | Context<br>200K<br>Max Output<br>100K<br>Input<br>$1.1<br>Output<br>$4.4<br>Latency<br>3.99s<br>Throughput<br>97.59t/s |

### Throughput

### Latency

## Apps using o3 Mini

### Top public apps this week using this model

1\.

![Favicon for https://cline.bot/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://cline.bot/&size=256)

[Cline](https://cline.bot/)

Autonomous coding agent right in your IDE

805M tokens

2\.

![Favicon for https://github.com/RooVetGit/Roo-Cline](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://github.com/RooVetGit/Roo-Cline&size=256)

[Roo Code](https://github.com/RooVetGit/Roo-Cline)

Fork of Cline with some experimental features

216M tokens

3\.

![Favicon for https://litellm.ai/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://litellm.ai/&size=256)

[liteLLM](https://litellm.ai/)

Open-source library to simplify LLM calls

87.8M tokens

4\.

![Favicon for https://aider.chat/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://aider.chat/&size=256)

[Aider](https://aider.chat/)

AI pair programming in your terminal

75.8M tokens

5\.

![Favicon for https://gitdiagram.com/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://gitdiagram.com/&size=256)

[gitdiagram](https://gitdiagram.com/)

new

50.6M tokens

6\.

![Favicon for https://bothub.chat/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://bothub.chat/&size=256)

[bothub.chat](https://bothub.chat/)

new

41.9M tokens

7\.

![Favicon for https://openrouter.ai/chat](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://openrouter.ai/chat&size=256)

[OpenRouter: Chatroom](https://openrouter.ai/chat)

Chat with multiple LLMs at once

39.9M tokens

8\.

![Favicon for https://straico.com/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://straico.com/&size=256)

[Straico](https://straico.com/)

Multi-model AI for content & image generation

38.1M tokens

9\.

![Favicon for https://openwebui.com/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://openwebui.com/&size=256)

[Open WebUI](https://openwebui.com/)

Extensible, self-hosted AI interface

27M tokens

10\.

![Favicon for https://anythingllm.com/](https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://anythingllm.com/&size=256)

[AnythingLLM](https://anythingllm.com/)

new

23M tokens

## Recent activity on o3 Mini

### Tokens processed per day

Jan 31Feb 1Feb 2Feb 3Feb 4Feb 5Feb 6Feb 7Feb 8Feb 9Feb 10Feb 11Feb 12Feb 13Feb 14Feb 15Feb 16Feb 170200M400M600M800M

## Uptime stats for o3 Mini

### Uptime stats for o3 Minion the only provider

Too many requests

# [![Datadog](https://us5.datadoghq.com/static/images/datadog-horizontal-logo.svg)Datadog](https://us5.datadoghq.com/)

![Sad Bits](https://us5.datadoghq.com/static/images/datadog-error-icon.svg)

## 429 - Too many requests!

- Datadog status:

- [Check the Datadog Status Page →](http://status.us5.datadoghq.com/)
- Follow [@datadogops](http://twitter.com/datadogops) for status updates
- Email us at [support@datadoghq.com](mailto:support@datadoghq.com)
- Try Datadog again →

## Scheduled Maintenance

See the [Datadog Status Page](http://status.us5.datadoghq.com/) for more info.

When an error occurs in an upstream provider, we can recover by routing to another healthy provider, if your request filters allow it.

[Learn more](https://openrouter.ai/docs/provider-routing) about our load balancing and customization options.

Too many requests

# [![Datadog](https://us5.datadoghq.com/static/images/datadog-horizontal-logo.svg)Datadog](https://us5.datadoghq.com/)

![Sad Bits](https://us5.datadoghq.com/static/images/datadog-error-icon.svg)

## 429 - Too many requests!

- Datadog status:

- [Check the Datadog Status Page →](http://status.us5.datadoghq.com/)
- Follow [@datadogops](http://twitter.com/datadogops) for status updates
- Email us at [support@datadoghq.com](mailto:support@datadoghq.com)
- Try Datadog again →

## Scheduled Maintenance

See the [Datadog Status Page](http://status.us5.datadoghq.com/) for more info.

## Sample code and API for o3 Mini

### OpenRouter normalizes requests and responses across providers for you.

[Create API key](https://openrouter.ai/settings/keys)

OpenRouter provides an OpenAI-compatible completion API to 300+ models & providers that you can call directly, or using the OpenAI SDK. Additionally, some third-party SDKs are available.

In the examples below, the [OpenRouter-specific headers](https://openrouter.ai/docs/requests#request-headers) are optional. Setting them allows your app to appear on the OpenRouter leaderboards.

openai-pythonpythontypescriptcurl

Copy

```python
from openai import OpenAI

client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key="<OPENROUTER_API_KEY>",
)

completion = client.chat.completions.create(
  extra_headers={
    "HTTP-Referer": "<YOUR_SITE_URL>", # Optional. Site URL for rankings on openrouter.ai.
    "X-Title": "<YOUR_SITE_NAME>", # Optional. Site title for rankings on openrouter.ai.
  },
  extra_body={},
  model="openai/o3-mini",
  messages=[\
    {\
      "role": "user",\
      "content": "What is the meaning of life?"\
    }\
  ]
)
print(completion.choices[0].message.content)
```

## Using third-party SDKs

For information about using third-party SDKs and frameworks with OpenRouter, please see our [frameworks documentation](https://openrouter.ai/docs/frameworks).

See the [Request docs](https://openrouter.ai/docs/api-reference/overview) for all possible fields, and [Parameters](https://openrouter.ai/docs/api-reference/parameters) for explanations of specific sampling parameters.

## Recommended parameters for o3 Mini

Median values from users on OpenRouter

|     |     |
| --- | --- |
| temperature<br>This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.<br>- Optional, **float**, 0.0 to 2.0<br>  <br>- Default: 1.0<br>  <br>- Explainer Video: [Watch](https://youtu.be/ezgqHnWvua8)<br>  <br>p10<br>0<br>p50<br>0.70<br>p90<br>1 |
| top\_p<br>This setting limits the model's choices to a percentage of likely tokens: only the top tokens whose probabilities add up to P. A lower value makes the model's responses more predictable, while the default setting allows for a full range of token choices. Think of it like a dynamic Top-K.<br>- Optional, **float**, 0.0 to 1.0<br>  <br>- Default: 1.0<br>  <br>- Explainer Video: [Watch](https://youtu.be/wQP-im_HInk)<br>  <br>p10<br>1<br>p50<br>1<br>p90<br>1 |
| top\_k<br>This limits the model's choice of tokens at each step, making it choose from a smaller set. A value of 1 means the model will always pick the most likely next token, leading to predictable results. By default this setting is disabled, making the model to consider all choices.<br>- Optional, **integer**, 0 or above<br>  <br>- Default: 0<br>  <br>- Explainer Video: [Watch](https://youtu.be/EbZv6-N8Xlk)<br>  <br>p10<br>0<br>p50<br>0<br>p90<br>0 |
| frequency\_penalty<br>This setting aims to control the repetition of tokens based on how often they appear in the input. It tries to use less frequently those tokens that appear more in the input, proportional to how frequently they occur. Token penalty scales with the number of occurrences. Negative values will encourage token reuse.<br>- Optional, **float**, -2.0 to 2.0<br>  <br>- Default: 0.0<br>  <br>- Explainer Video: [Watch](https://youtu.be/p4gl6fqI0_w)<br>  <br>p10<br>0<br>p50<br>0<br>p90<br>0 |
| presence\_penalty<br>Adjusts how often the model repeats specific tokens already used in the input. Higher values make such repetition less likely, while negative values do the opposite. Token penalty does not scale with the number of occurrences. Negative values will encourage token reuse.<br>- Optional, **float**, -2.0 to 2.0<br>  <br>- Default: 0.0<br>  <br>- Explainer Video: [Watch](https://youtu.be/MwHG5HL-P74)<br>  <br>p10<br>0<br>p50<br>0<br>p90<br>0 |
| repetition\_penalty<br>Helps to reduce the repetition of tokens from the input. A higher value makes the model less likely to repeat tokens, but too high a value can make the output less coherent (often with run-on sentences that lack small words). Token penalty scales based on original token's probability.<br>- Optional, **float**, 0.0 to 2.0<br>  <br>- Default: 1.0<br>  <br>- Explainer Video: [Watch](https://youtu.be/LHjGAnLm3DM)<br>  <br>p10<br>1<br>p50<br>1<br>p90<br>1 |
| min\_p<br>Represents the minimum probability for a token to be<br>considered, relative to the probability of the most likely token. (The value changes depending on the confidence level of the most probable token.) If your Min-P is set to 0.1, that means it will only allow for tokens that are at least 1/10th as probable as the best possible option.<br>- Optional, **float**, 0.0 to 1.0<br>  <br>- Default: 0.0<br>  <br>p10<br>0<br>p50<br>0<br>p90<br>0 |
| top\_a<br>Consider only the top tokens with "sufficiently high" probabilities based on the probability of the most likely token. Think of it like a dynamic Top-P. A lower Top-A value focuses the choices based on the highest probability token but with a narrower scope. A higher Top-A value does not necessarily affect the creativity of the output, but rather refines the filtering process based on the maximum probability.<br>- Optional, **float**, 0.0 to 1.0<br>  <br>- Default: 0.0<br>  <br>p10<br>0<br>p50<br>0<br>p90<br>0 |
| temperature<br>This setting influences the variety in the model's responses. Lower values lead to more predictable and typical responses, while higher values encourage more diverse and less common responses. At 0, the model always gives the same response for a given input.<br>- Optional, **float**, 0.0 to 2.0<br>  <br>- Default: 1.0<br>  <br>- Explainer Video: [Watch](https://youtu.be/ezgqHnWvua8) | p10<br>0<br>p50<br>0.70<br>p90<br>1 |
| top\_p<br>This setting limits the model's choices to a percentage of likely tokens: only the top tokens whose probabilities add up to P. A lower value makes the model's responses more predictable, while the default setting allows for a full range of token choices. Think of it like a dynamic Top-K.<br>- Optional, **float**, 0.0 to 1.0<br>  <br>- Default: 1.0<br>  <br>- Explainer Video: [Watch](https://youtu.be/wQP-im_HInk) | p10<br>1<br>p50<br>1<br>p90<br>1 |
| top\_k<br>This limits the model's choice of tokens at each step, making it choose from a smaller set. A value of 1 means the model will always pick the most likely next token, leading to predictable results. By default this setting is disabled, making the model to consider all choices.<br>- Optional, **integer**, 0 or above<br>  <br>- Default: 0<br>  <br>- Explainer Video: [Watch](https://youtu.be/EbZv6-N8Xlk) | p10<br>0<br>p50<br>0<br>p90<br>0 |
| frequency\_penalty<br>This setting aims to control the repetition of tokens based on how often they appear in the input. It tries to use less frequently those tokens that appear more in the input, proportional to how frequently they occur. Token penalty scales with the number of occurrences. Negative values will encourage token reuse.<br>- Optional, **float**, -2.0 to 2.0<br>  <br>- Default: 0.0<br>  <br>- Explainer Video: [Watch](https://youtu.be/p4gl6fqI0_w) | p10<br>0<br>p50<br>0<br>p90<br>0 |
| presence\_penalty<br>Adjusts how often the model repeats specific tokens already used in the input. Higher values make such repetition less likely, while negative values do the opposite. Token penalty does not scale with the number of occurrences. Negative values will encourage token reuse.<br>- Optional, **float**, -2.0 to 2.0<br>  <br>- Default: 0.0<br>  <br>- Explainer Video: [Watch](https://youtu.be/MwHG5HL-P74) | p10<br>0<br>p50<br>0<br>p90<br>0 |
| repetition\_penalty<br>Helps to reduce the repetition of tokens from the input. A higher value makes the model less likely to repeat tokens, but too high a value can make the output less coherent (often with run-on sentences that lack small words). Token penalty scales based on original token's probability.<br>- Optional, **float**, 0.0 to 2.0<br>  <br>- Default: 1.0<br>  <br>- Explainer Video: [Watch](https://youtu.be/LHjGAnLm3DM) | p10<br>1<br>p50<br>1<br>p90<br>1 |
| min\_p<br>Represents the minimum probability for a token to be<br>considered, relative to the probability of the most likely token. (The value changes depending on the confidence level of the most probable token.) If your Min-P is set to 0.1, that means it will only allow for tokens that are at least 1/10th as probable as the best possible option.<br>- Optional, **float**, 0.0 to 1.0<br>  <br>- Default: 0.0 | p10<br>0<br>p50<br>0<br>p90<br>0 |
| top\_a<br>Consider only the top tokens with "sufficiently high" probabilities based on the probability of the most likely token. Think of it like a dynamic Top-P. A lower Top-A value focuses the choices based on the highest probability token but with a narrower scope. A higher Top-A value does not necessarily affect the creativity of the output, but rather refines the filtering process based on the maximum probability.<br>- Optional, **float**, 0.0 to 1.0<br>  <br>- Default: 0.0 | p10<br>0<br>p50<br>0<br>p90<br>0 |

### Sample code using the median

typescriptpythonshell

Copy

```typescript
fetch("https://openrouter.ai/api/v1/chat/completions", {
  method: "POST",
  headers: {
    "Authorization": "Bearer <OPENROUTER_API_KEY>",
    "HTTP-Referer": "<YOUR_SITE_URL>", // Optional. Site URL for rankings on openrouter.ai.
    "X-Title": "<YOUR_SITE_NAME>", // Optional. Site title for rankings on openrouter.ai.
    "Content-Type": "application/json"
  },
  body: JSON.stringify({
    "model": "openai/o3-mini",
    "messages": [\
      {"role": "user", "content": "What is the meaning of life?"}\
    ],
    "top_p": 1,
    "temperature": 0.7,
    "repetition_penalty": 1
  })
});
```

## More models from [OpenAI](https://openrouter.ai/openai)

[o3 Mini High\\
\\
OpenAI o3-mini-high is the same model as o3-mini with reasoning\_effort set to high.\\
\\
o3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding. The model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.\\
\\
The model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.](https://openrouter.ai/openai/o3-mini-high)

[o1\\
\\
The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding. The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought.\\
\\
The o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the launch announcement.](https://openrouter.ai/openai/o1)

[GPT-4o\\
\\
The 2024-11-20 version of GPT-4o offers a leveled-up creative writing ability with more natural, engaging, and tailored writing to improve relevance & readability. It’s also better at working with uploaded files, providing deeper insights & more thorough responses.\\
\\
GPT-4o ("o" for "omni") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of GPT-4 Turbo while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.](https://openrouter.ai/openai/gpt-4o-2024-11-20)

[o1-mini\\
\\
The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\\
\\
The o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the launch announcement.\\
\\
Note: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.](https://openrouter.ai/openai/o1-mini-2024-09-12)

[o1-preview\\
\\
The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\\
\\
The o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the launch announcement.\\
\\
Note: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.](https://openrouter.ai/openai/o1-preview)

[o1-preview\\
\\
The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\\
\\
The o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the launch announcement.\\
\\
Note: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.](https://openrouter.ai/openai/o1-preview-2024-09-12)

[o1-mini\\
\\
The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\\
\\
The o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the launch announcement.\\
\\
Note: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.](https://openrouter.ai/openai/o1-mini)

[ChatGPT-4o\\
\\
OpenAI ChatGPT 4o is continually updated by OpenAI to point to the current version of GPT-4o used by ChatGPT. It therefore differs slightly from the API version of GPT-4o in that it has additional RLHF. It is intended for research and evaluation.\\
\\
OpenAI notes that this model is not suited for production use-cases as it may be removed or redirected to another model in the future.](https://openrouter.ai/openai/chatgpt-4o-latest)

[GPT-4o\\
\\
The 2024-08-06 version of GPT-4o offers improved performance in structured outputs, with the ability to supply a JSON schema in the respone\_format. Read more here.\\
\\
GPT-4o ("o" for "omni") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of GPT-4 Turbo while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\\
\\
For benchmarking against other models, it was briefly called "im-also-a-good-gpt2-chatbot"](https://openrouter.ai/openai/gpt-4o-2024-08-06)

[GPT-4o-mini\\
\\
GPT-4o mini is OpenAI's newest model after GPT-4 Omni, supporting both text and image inputs with text outputs.\\
\\
As their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than GPT-3.5 Turbo. It maintains SOTA intelligence, while being significantly more cost-effective.\\
\\
GPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences common leaderboards.\\
\\
Check out the launch announcement to learn more.\\
\\
#multimodal](https://openrouter.ai/openai/gpt-4o-mini)

[GPT-4o-mini\\
\\
GPT-4o mini is OpenAI's newest model after GPT-4 Omni, supporting both text and image inputs with text outputs.\\
\\
As their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than GPT-3.5 Turbo. It maintains SOTA intelligence, while being significantly more cost-effective.\\
\\
GPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences common leaderboards.\\
\\
Check out the launch announcement to learn more.\\
\\
#multimodal](https://openrouter.ai/openai/gpt-4o-mini-2024-07-18)

[GPT-4o\\
\\
GPT-4o ("o" for "omni") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of GPT-4 Turbo while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\\
\\
For benchmarking against other models, it was briefly called "im-also-a-good-gpt2-chatbot"\\
\\
#multimodal](https://openrouter.ai/openai/gpt-4o-2024-05-13)

[GPT-4o\\
\\
GPT-4o ("o" for "omni") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of GPT-4 Turbo while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\\
\\
For benchmarking against other models, it was briefly called "im-also-a-good-gpt2-chatbot"\\
\\
#multimodal](https://openrouter.ai/openai/gpt-4o)

[GPT-4 Turbo\\
\\
The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling.\\
\\
Training data: up to December 2023.](https://openrouter.ai/openai/gpt-4-turbo)

[GPT-3.5 Turbo\\
\\
GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural language or code, and is optimized for chat and traditional completion tasks.\\
\\
Training data up to Sep 2021.](https://openrouter.ai/openai/gpt-3.5-turbo-0613)

[GPT-4 Turbo Preview\\
\\
The preview GPT-4 model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Training data: up to Dec 2023.\\
\\
**Note:** heavily rate limited by OpenAI while in preview.](https://openrouter.ai/openai/gpt-4-turbo-preview)

[GPT-4 Vision\\
\\
Ability to understand images, in addition to all other GPT-4 Turbo capabilties. Training data: up to Apr 2023.\\
\\
**Note:** heavily rate limited by OpenAI while in preview.\\
\\
#multimodal](https://openrouter.ai/openai/gpt-4-vision-preview)

[GPT-3.5 Turbo 16k\\
\\
An older GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Training data: up to Sep 2021.](https://openrouter.ai/openai/gpt-3.5-turbo-1106)

[GPT-4 Turbo\\
\\
The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling.\\
\\
Training data: up to April 2023.](https://openrouter.ai/openai/gpt-4-1106-preview)

[GPT-3.5 Turbo Instruct\\
\\
This model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related optimizations. Training data: up to Sep 2021.](https://openrouter.ai/openai/gpt-3.5-turbo-instruct)

[GPT-3.5 Turbo 16k\\
\\
This model offers four times the context length of gpt-3.5-turbo, allowing it to support approximately 20 pages of text in a single request at a higher cost. Training data: up to Sep 2021.](https://openrouter.ai/openai/gpt-3.5-turbo-16k)

[GPT-4 32k\\
\\
GPT-4-32k is an extended version of GPT-4, with the same capabilities but quadrupled context length, allowing for processing up to 40 pages of text in a single pass. This is particularly beneficial for handling longer content like interacting with PDFs without an external vector database. Training data: up to Sep 2021.](https://openrouter.ai/openai/gpt-4-32k)

[GPT-4 32k\\
\\
GPT-4-32k is an extended version of GPT-4, with the same capabilities but quadrupled context length, allowing for processing up to 40 pages of text in a single pass. This is particularly beneficial for handling longer content like interacting with PDFs without an external vector database. Training data: up to Sep 2021.](https://openrouter.ai/openai/gpt-4-32k-0314)

[Shap-e\\
\\
A generative model developed by OpenAI that generates 3D objects conditioned on text, capable of directly generating parameters of implicit functions that can be rendered as textured meshes and neural radiance fields.](https://openrouter.ai/openai/shap-e)

[GPT-3.5 Turbo\\
\\
GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural language or code, and is optimized for chat and traditional completion tasks.\\
\\
Training data up to Sep 2021.](https://openrouter.ai/openai/gpt-3.5-turbo)

[GPT-3.5 Turbo 16k\\
\\
The latest GPT-3.5 Turbo model with improved instruction following, JSON mode, reproducible outputs, parallel function calling, and more. Training data: up to Sep 2021.\\
\\
This version has a higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls.](https://openrouter.ai/openai/gpt-3.5-turbo-0125)

[GPT-3.5 Turbo\\
\\
GPT-3.5 Turbo is OpenAI's fastest model. It can understand and generate natural language or code, and is optimized for chat and traditional completion tasks.\\
\\
Training data up to Sep 2021.](https://openrouter.ai/openai/gpt-3.5-turbo-0301)

[GPT-4\\
\\
OpenAI's flagship model, GPT-4 is a large-scale multimodal language model capable of solving difficult problems with greater accuracy than previous models due to its broader general knowledge and advanced reasoning capabilities. Training data: up to Sep 2021.](https://openrouter.ai/openai/gpt-4)

[GPT-4\\
\\
GPT-4-0314 is the first version of GPT-4 released, with a context length of 8,192 tokens, and was supported until June 14. Training data: up to Sep 2021.](https://openrouter.ai/openai/gpt-4-0314)

Previous slideNext slide

0