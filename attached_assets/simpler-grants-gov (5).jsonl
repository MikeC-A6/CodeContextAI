{"path":".devcontainer/devcontainer.json","language":"json","type":"code","directory":".devcontainer","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.devcontainer/devcontainer.json","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"================================================================================\nFile: .devcontainer/devcontainer.json\nLanguage: json\nType: code\nDirectory: .devcontainer\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.devcontainer/devcontainer.json\nSize: 0.44 KB\nLast Modified: 2025-02-14T17:08:26.402Z"}
{"path":".dockerignore","language":"unknown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.dockerignore","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"\"portsAttributes\": {\n    \"3000\": {\n      \"label\": \"Application\",\n      \"onAutoForward\": \"openPreview\"\n    }\n  },\n  \"forwardPorts\": [3000]\n}"}
{"path":".dockleconfig","language":"unknown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.dockleconfig","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .dockerignore\nLanguage: unknown\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.dockerignore\nSize: 0.03 KB\nLast Modified: 2025-02-14T17:08:26.402Z"}
{"path":".github/CODEOWNERS","language":"unknown","type":"code","directory":".github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/CODEOWNERS","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":""}
{"path":".github/ISSUE_TEMPLATE/1_bug_report.yml","language":"yaml","type":"code","directory":".github/ISSUE_TEMPLATE","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/1_bug_report.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .dockleconfig\nLanguage: unknown\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.dockleconfig\nSize: 0.67 KB\nLast Modified: 2025-02-14T17:08:26.402Z"}
{"path":".github/ISSUE_TEMPLATE/2_feature_request.yml","language":"yaml","type":"code","directory":".github/ISSUE_TEMPLATE","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/2_feature_request.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":""}
{"path":".github/ISSUE_TEMPLATE/3_internal_decision.yml","language":"yaml","type":"code","directory":".github/ISSUE_TEMPLATE","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/3_internal_decision.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .github/CODEOWNERS\nLanguage: unknown\nType: code\nDirectory: .github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/CODEOWNERS\nSize: 0.72 KB\nLast Modified: 2025-02-14T17:08:26.402Z"}
{"path":".github/ISSUE_TEMPLATE/4_internal_deliverable.yml","language":"yaml","type":"code","directory":".github/ISSUE_TEMPLATE","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/4_internal_deliverable.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"* @mdragon # project lead\n\n/documentation/  @acouch @andycochran @btabaska @chouinar @coilysiren @doug-s-nava @mdragon @widal001 @capriiis # everyone\n\n/analytics/               @acouch @coilysiren @widal001\n/documentation/analytics/ @acouch @coilysiren @widal001\n\n/api/               @chouinar @mdragon\n/documentation/api/ @chouinar @mdragon\n\n/frontend/               @acouch @andycochran @btabaska @doug-s-nava\n/documentation/frontend/ @acouch @andycochran @btabaska @doug-s-nava\n\n/infra/               @acouch @coilysiren @mdragon\n/documentation/infra/ @acouch @coilysiren @mdragon\n/.github/             @acouch @coilysiren @mdragon\n/bin/                 @acouch @coilysiren @mdragon"}
{"path":".github/ISSUE_TEMPLATE/5_internal_epic.yml","language":"yaml","type":"code","directory":".github/ISSUE_TEMPLATE","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/5_internal_epic.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .github/ISSUE_TEMPLATE/1_bug_report.yml\nLanguage: yml\nType: code\nDirectory: .github/ISSUE_TEMPLATE\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/1_bug_report.yml\nSize: 1.59 KB\nLast Modified: 2025-02-14T17:08:26.402Z"}
{"path":".github/ISSUE_TEMPLATE/6_internal_task.yml","language":"yaml","type":"code","directory":".github/ISSUE_TEMPLATE","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/6_internal_task.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":""}
{"path":".github/ISSUE_TEMPLATE/config.yml","language":"yaml","type":"code","directory":".github/ISSUE_TEMPLATE","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/config.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .github/ISSUE_TEMPLATE/2_feature_request.yml\nLanguage: yml\nType: code\nDirectory: .github/ISSUE_TEMPLATE\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/2_feature_request.yml\nSize: 1.23 KB\nLast Modified: 2025-02-14T17:08:26.402Z"}
{"path":".github/actionlint.yml","language":"yaml","type":"code","directory":".github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/actionlint.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":""}
{"path":".github/actions/configure-aws-credentials/action.yml","language":"yaml","type":"code","directory":".github/actions/configure-aws-credentials","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/actions/configure-aws-credentials/action.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .github/ISSUE_TEMPLATE/3_internal_decision.yml\nLanguage: yml\nType: code\nDirectory: .github/ISSUE_TEMPLATE\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/3_internal_decision.yml\nSize: 1.88 KB\nLast Modified: 2025-02-14T17:08:26.402Z"}
{"path":".github/actions/setup-terraform/action.yml","language":"yaml","type":"code","directory":".github/actions/setup-terraform","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/actions/setup-terraform/action.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"- type: textarea\n    id: summary\n    attributes:\n      label: Summary\n      description: 1-2 sentence summary of the decision that needs to be made\n    validations:\n      required: true\n\n  - type: textarea\n    id: options\n    attributes:\n      label: Options\n      description: List of options to evaluate\n    validations:\n      required: false\n\n  - type: textarea\n    id: decision_criteria\n    attributes:\n      label: Decision Criteria\n      description: List of decision criteria to evaluate\n    validations:\n      required: false\n\n  - type: textarea\n    id: approvers\n    attributes:\n      label: Approvers\n      description: List individuals or groups that must approve this decision before the ADR is accepted\n    validations:\n      required: false\n\n  - type: checkboxes\n    id: acceptance_criteria\n    attributes:\n      label: Acceptance criteria\n      description: Leave the following acceptance criteria unchecked when the ticket is created then mark them as completed as you meet each criterion with the ADR\n      options:\n        - label: The approvers for this decision have been identified (ideally before work on the ADR starts)\n        - label: The ADR is created and stored in `documentation/wiki/decisions/adr` with status \"Accepted\"\n        - label: The ADR has been reviewed and approved by the approvers listed above\n        - label: The ADR satisfies requirements that are outlined in the ADR template\n        - label: Any follow-up tickets have been created (if necessary)\n    validations:\n      required: false"}
{"path":".github/linters/README.md","language":"markdown","type":"code","directory":".github/linters","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/linters/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .github/ISSUE_TEMPLATE/4_internal_deliverable.yml\nLanguage: yml\nType: code\nDirectory: .github/ISSUE_TEMPLATE\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/4_internal_deliverable.yml\nSize: 1.43 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/linters/queries/getFieldMetadata.graphql","language":"unknown","type":"code","directory":".github/linters/queries","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/linters/queries/getFieldMetadata.graphql","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"- type: textarea\n    id: acceptance_criteria\n    attributes:\n      label: Acceptance criteria\n      description: One to three short acceptance criteria that are clear indicators of whether this deliverable is complete\n    validations:\n      required: true\n\n  - type: textarea\n    id: metrics\n    attributes:\n      label: Metrics\n      description: Two to three metrics relating to customer experience that this deliverable is intended to impact\n    validations:\n      required: true\n\n  - type: textarea\n    id: related_goals\n    attributes:\n      label: Related goals\n      description: Which of the overall project goals will this deliverable advance?\n    validations:\n      required: false\n\n  - type: textarea\n    id: assumptions\n    attributes:\n      label: Assumptions and dependencies\n      description: A bulleted list of assumptions and dependencies\n    validations:\n      required: false\n\n  - type: textarea\n    id: additional_info\n    attributes:\n      label: Additional information\n      description: Any important additional information about this deliverable that is not captured above\n    validations:\n      required: false"}
{"path":".github/linters/queries/getItemMetadata.graphql","language":"unknown","type":"code","directory":".github/linters/queries","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/linters/queries/getItemMetadata.graphql","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .github/ISSUE_TEMPLATE/5_internal_epic.yml\nLanguage: yml\nType: code\nDirectory: .github/ISSUE_TEMPLATE\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/5_internal_epic.yml\nSize: 1.00 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/linters/scripts/check-wiki-pages-linked-to-summary.sh","language":"unknown","type":"code","directory":".github/linters/scripts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/linters/scripts/check-wiki-pages-linked-to-summary.sh","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"- type: textarea\n    id: success_probability\n    attributes:\n      label: Probability of success and key risks\n      description: Estimated probability that this epic succeeds; and a brief summary of key risks\n    validations:\n      required: true\n\n  - type: input\n    id: delivery_date_estimated\n    attributes:\n      label: Date of delivery (estimated)\n      description: Estimated delivery date for this epic\n    validations:\n      required: true\n\n  - type: dropdown\n    id: loe_tshirt\n    attributes:\n      label: Level of effort\n      options:\n        - \"XS\"\n        - \"S\"\n        - \"M\"\n        - \"L\"\n        - \"XL\"\n        - \"XXL\"\n    validations:\n      required: false"}
{"path":".github/linters/scripts/set-points-and-sprint.sh","language":"unknown","type":"code","directory":".github/linters/scripts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/linters/scripts/set-points-and-sprint.sh","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .github/ISSUE_TEMPLATE/6_internal_task.yml\nLanguage: yml\nType: code\nDirectory: .github/ISSUE_TEMPLATE\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/6_internal_task.yml\nSize: 0.53 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/pull_request_template.md","language":"markdown","type":"code","directory":".github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/pull_request_template.md","size":894508,"lastModified":"2025-02-14T17:08:31.120Z","content":""}
{"path":".github/renovate.json","language":"json","type":"code","directory":".github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/renovate.json","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .github/ISSUE_TEMPLATE/config.yml\nLanguage: yml\nType: code\nDirectory: .github/ISSUE_TEMPLATE\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/ISSUE_TEMPLATE/config.yml\nSize: 0.03 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/README.md","language":"markdown","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.120Z","content":""}
{"path":".github/workflows/build-and-publish.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/build-and-publish.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .github/actionlint.yml\nLanguage: yml\nType: code\nDirectory: .github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/actionlint.yml\nSize: 0.16 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/cd-analytics.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/cd-analytics.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":""}
{"path":".github/workflows/cd-api.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/cd-api.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .github/actions/configure-aws-credentials/action.yml\nLanguage: yml\nType: code\nDirectory: .github/actions/configure-aws-credentials\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/actions/configure-aws-credentials/action.yml\nSize: 2.92 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/cd-frontend.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/cd-frontend.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"echo \"::group::AWS account authentication details\"\n\n        terraform -chdir=infra/project-config init > /dev/null\n        terraform -chdir=infra/project-config apply -auto-approve > /dev/null\n        AWS_REGION=$(terraform -chdir=infra/project-config output -raw default_region)\n        echo \"AWS_REGION=$AWS_REGION\"\n        GITHUB_ACTIONS_ROLE_NAME=$(terraform -chdir=infra/project-config output -raw github_actions_role_name)\n        echo \"GITHUB_ACTIONS_ROLE_NAME=$GITHUB_ACTIONS_ROLE_NAME\"\n\n        terraform -chdir=infra/${{ inputs.app_name }}/app-config init > /dev/null\n        terraform -chdir=infra/${{ inputs.app_name }}/app-config apply -auto-approve > /dev/null\n        ACCOUNT_NAME=$(terraform -chdir=infra/${{ inputs.app_name }}/app-config output -json account_names_by_environment | jq -r .${{ inputs.environment }})\n        echo \"ACCOUNT_NAME=$ACCOUNT_NAME\"\n\n        # Get the account id associated with the account name extracting the\n        # ACCOUNT_ID part of the tfbackend file name which looks like\n        # <ACCOUNT_NAME>.<ACCOUNT_ID>.s3.tfbackend.\n        # The cut command splits the string with period as the delimeter and\n        # extracts the second field.\n        ACCOUNT_ID=$(ls infra/accounts/$ACCOUNT_NAME.*.s3.tfbackend | cut -d. -f2)\n        echo \"ACCOUNT_ID=$ACCOUNT_ID\"\n\n        AWS_ROLE_TO_ASSUME=arn:aws:iam::$ACCOUNT_ID:role/$GITHUB_ACTIONS_ROLE_NAME\n        echo \"AWS_ROLE_TO_ASSUME=$AWS_ROLE_TO_ASSUME\"\n\n        echo \"::endgroup::\"\n\n        echo \"Setting env vars AWS_ROLE_TO_ASSUME and AWS_REGION...\"\n        echo \"AWS_ROLE_TO_ASSUME=$AWS_ROLE_TO_ASSUME\" >> \"$GITHUB_ENV\"\n        echo \"AWS_REGION=$AWS_REGION\" >> \"$GITHUB_ENV\"\n      shell: bash\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        role-to-assume: ${{ env.AWS_ROLE_TO_ASSUME }}\n        aws-region: ${{ env.AWS_REGION }}"}
{"path":".github/workflows/cd-metabase.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/cd-metabase.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .github/actions/setup-terraform/action.yml\nLanguage: yml\nType: code\nDirectory: .github/actions/setup-terraform\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/actions/setup-terraform/action.yml\nSize: 0.62 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/cd-storybook.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/cd-storybook.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":""}
{"path":".github/workflows/check-ci-cd-auth.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/check-ci-cd-auth.yml","size":0,"lastModified":"2025-02-14T17:08:31.120Z","content":"File: .github/linters/README.md\nLanguage: md\nType: code\nDirectory: .github/linters\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/linters/README.md\nSize: 3.77 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/ci-analytics-vulnerability-scans.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-analytics-vulnerability-scans.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"## Introduction\n\nThis section of the codebase contains a set of custom linters that we run against our GitHub repo and the associated GitHub projects.\n\n## Project directory structure\n\nOutlines the structure of the linters codebase, relative to the root of the simpler-grants-gov repo.\n\n```text\nroot\nâ”œâ”€â”€ .github\nâ”‚   â””â”€â”€ linters\nâ”‚       â””â”€â”€ queries    Contains graphql queries used by the custom linting scripts\nâ”‚       â””â”€â”€ scripts    Contains scripts that lint the codebase, GitHub repo, or GitHub projects\nâ”‚       â””â”€â”€ tmp        Git ignored directory that can store temporary outputs of scripts\n```\n\n## Usage\n\n### Review automated linters\n\n| Workflow name                                         | Description                                                            | Trigger                        |\n| ----------------------------------------------------- | ---------------------------------------------------------------------- | ------------------------------ |\n| [Lint - Set points and sprint][set-points-and-sprint] | Sets default points and sprint value (if unset) when issues are closed | On issue close                 |\n| [Lint - Check wiki links][check-wiki-links]      | Checks that all wiki markdown files are linked in the SUMMARY.md       | Each PR that modifies the wiki |\n\n### Manually run the linters\n\n> [!NOTE]\n> Only project maintainers can manually run the linters\n\n1. Navigate to the [GitHub actions tab of the repo](https://github.com/HHS/simpler-grants-gov/actions).\n2. Find the name of the workflow in the left hand side of the \"Actions\" menu. It should start with `Lint -`.\n3. Click on the workflow you want to trigger manually.\n4. On the next page, click the dropdown menu that says \"Run workflow\".\n5. Choose the version of the workflow you want to run based on its branch. **Note:** In most cases this will be `Branch: main`.\n6. Finally, click the green \"Run workflow\" button to trigger that linter.\n\n### Add a new linter\n\n1. Create a new linting script in `linters/scripts/`.\n   - **Note:** If you're script requires a long graphql query for the GitHub graphql API, pull that query out into its own `.graphql` file stored in `linters/queries/`.\n   - **Note:** If you're script changes any resources directly in GitHub, make sure you include a dry run option that skips over any write step if the `--dry-run` flag is passed during execution.\n   - For a reference please see [`linters/scripts/set-points-and-sprint.sh`][set-points-and-sprint-script] and its associated query [`linters/queries/get-project-items.graphql`][get-project-items-query]\n2. Update the permissions on your script so it can be executed: `chmod 744 ./scripts/<path-to-script>`\n3. Test your script locally `./scripts/<path-to-script> --dry-run`\n4. Add your script to the [CI checks for the linters](../workflows/ci-project-linters.yml). Make sure you include any environment variables needed by your script and the `--dry-run` flag in the GitHub action `run` statement.\n5. Create a new GitHub action workflow to run your linter.\n   - **Note:** Make sure the name of the yaml file is prefixed with `lint-` (or `ci-` if run on PRs).\n   - **Note:** Make sure the workflow is run from the `linters/` sub-directory.\n   - **Note:** Make sure the workflow has a `workflow_dispatch:` trigger option to allow for manual triggers.\n   - For a reference, please see [`.github/workflows/lint-set-points-and-sprint.yml`][set-points-and-sprint]\n6. Add your new linter to the table in the [\"Review automated linters\"](#review-automated-linters) section above\n\n\n[set-points-and-sprint]: ../workflows/lint-set-points-and-sprint.yml\n[set-points-and-sprint-script]: ./scripts/set-points-and-sprint.sh\n[get-project-items-query]: ./queries/getItemMetadata.graphql\n[check-wiki-links]: ../workflows/ci-wiki-links.yml"}
{"path":".github/workflows/ci-analytics.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-analytics.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/linters/queries/getFieldMetadata.graphql\nLanguage: graphql\nType: code\nDirectory: .github/linters/queries\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/linters/queries/getFieldMetadata.graphql\nSize: 0.55 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/ci-api-vulnerability-scans.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-api-vulnerability-scans.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"fragment numberMetadata on ProjectV2Field {\n  fieldId: id\n}\n\nfragment iterationMetadata on ProjectV2IterationField {\n  fieldId: id\n  configuration {\n    iterations {\n      id\n      startDate\n      duration\n    }\n  }\n}"}
{"path":".github/workflows/ci-api.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-api.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/linters/queries/getItemMetadata.graphql\nLanguage: graphql\nType: code\nDirectory: .github/linters/queries\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/linters/queries/getItemMetadata.graphql\nSize: 1.43 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/ci-cron-vulnerability-scans.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-cron-vulnerability-scans.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"# get list of pull requests that are linked to this issue\n      ...pullRequestMetadata\n\n      # get all of the project items associated with this issue\n      projectItems(first: 10) {\n        nodes {\n          ... on ProjectV2Item {\n            # Get the project ID, number, and owner, as well as itemId\n            ...projectMetadata\n\n            # Get the value of the \"sprint\" field, if set\n            sprint: fieldValueByName(name: $sprintField) {\n              ... on ProjectV2ItemFieldIterationValue {\n                iterationId\n              }\n            }\n\n            # Get the value of the \"points\" field, if set\n            points: fieldValueByName(name: $pointsField) {\n              ... on ProjectV2ItemFieldNumberValue {\n                number\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\nfragment projectMetadata on ProjectV2Item {\n  itemId: id\n  project {\n    projectId: id\n    number\n    owner {\n      ... on Organization {\n        login\n      }\n    }\n  }\n}\n\nfragment pullRequestMetadata on Issue {\n  pullRequests: closedByPullRequestsReferences(\n    first: 10\n    includeClosedPrs: true\n  ) {\n    nodes {\n      ... on PullRequest {\n        author {\n          ... on User {\n            login\n          }\n        }\n      }\n    }\n  }\n}"}
{"path":".github/workflows/ci-frontend-a11y.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-frontend-a11y.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/linters/scripts/check-wiki-pages-linked-to-summary.sh\nLanguage: sh\nType: code\nDirectory: .github/linters/scripts\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/linters/scripts/check-wiki-pages-linked-to-summary.sh\nSize: 1.40 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/ci-frontend-e2e.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-frontend-e2e.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"REPO_ROOT=\"../..\"\nWIKI_DIR=\"${REPO_ROOT}/documentation/wiki\"\nTEMP_DIR=\"./tmp\"\nWIKI_FILES=\"${TEMP_DIR}/wiki-files.txt\"\nSUMMARY_FILES=\"${TEMP_DIR}/summary-files.txt\"\nMISSING_FILES=\"${TEMP_DIR}/missing-from-summary.txt\"\nmkdir -p tmp # create tmp directory\n\n# list all of the markdown files in the wiki directory\nfind \"${WIKI_DIR}\" -name \"*.md\" |\\\n # make file paths relative to the root of the wiki directory\n sed -E \"s|${WIKI_DIR}/(.*)|\\1|\" |\\\n # filter out the SUMMARY.md file\n grep -Ev '(SUMMARY.md)' |\\\n # sort the files alphabetically and write to a temporary file\n sort > $WIKI_FILES\n\n# list all of the markdown files linked in the SUMMARY.md file\ngrep -oE '\\((.*\\.md)\\)' \"${WIKI_DIR}/SUMMARY.md\" |\\\n # remove the extra parantheses around the markdown files\n sed -E \"s|\\((.+)\\)|\\1|\" |\\\n # sort the files alphabetically and write to a temporary file\n sort > $SUMMARY_FILES\n\n# find files that are in the wiki but not in the summary\ncomm -2 -3 $WIKI_FILES $SUMMARY_FILES > $MISSING_FILES\n\n# if there are missing files exit with a non-zero code and print them\nif [[ -z \"$(cat ${MISSING_FILES})\" ]]; then\n    echo \"All files added to summary\"\n    exit 0\nelse\n    echo \"The following files need to be added to documentation/wiki/SUMMARY.md:\"\n    cat $MISSING_FILES\n    exit 1\nfi"}
{"path":".github/workflows/ci-frontend-vulnerability-scans.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-frontend-vulnerability-scans.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/linters/scripts/set-points-and-sprint.sh\nLanguage: sh\nType: code\nDirectory: .github/linters/scripts\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/linters/scripts/set-points-and-sprint.sh\nSize: 6.51 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/ci-frontend.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-frontend.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"# #######################################################\n# Parse command line args with format `--option arg`\n# #######################################################\n\n# see this stack overflow for more details:\n# https://stackoverflow.com/a/14203146/7338319\nwhile [[ $# -gt 0 ]]; do\n  case $1 in\n    --dry-run)\n      echo \"Running in dry run mode\"\n      dry_run=YES\n      shift # past argument\n      ;;\n    --url)\n      issue_url=\"$2\"\n      shift # past argument\n      shift # past value\n      ;;\n    --org)\n      org=\"$2\"\n      shift # past argument\n      shift # past value\n      ;;\n    --project)\n      project=\"$2\"\n      shift # past argument\n      shift # past value\n      ;;\n    --sprint-field)\n      sprint_field=\"$2\"\n      shift # past argument\n      shift # past value\n      ;;\n    --points-field)\n      points_field=\"$2\"\n      shift # past argument\n      shift # past value\n      ;;\n    -*|--*)\n      echo \"Unknown option $1\"\n      exit 1\n      ;;\n    *)\n      POSITIONAL_ARGS+=(\"$1\") # save positional arg\n      shift # past argument\n      ;;\n  esac\ndone\n\n# #######################################################\n# Set script-specific variables\n# #######################################################\n\nmkdir -p tmp\nraw_data_file=\"tmp/closed-issue-data-raw.json\"\nitem_data_file=\"tmp/closed-issue-data.json\"\nfield_data_file=\"tmp/field-data.json\"\nitem_query=$(cat \"queries/getItemMetadata.graphql\")\nfield_query=$(cat \"queries/getFieldMetadata.graphql\")\n\n# #######################################################\n# Fetch issue metadata\n# #######################################################\n\ngh api graphql \\\n --header 'GraphQL-Features:issue_types' \\\n --field url=\"${issue_url}\" \\\n --field sprintField=\"${sprint_field}\" \\\n --field pointsField=\"${points_field}\" \\\n -f query=\"${item_query}\" > $raw_data_file\n\n# #######################################################\n# Isolate the correct project item and issue type\n# #######################################################\n\n# Get project item\njq \".data.resource.projectItems.nodes[] |\n\n # filter for the item in the given project\n select(.project.number == ${project}) |\n\n # filter for items with the sprint or points fields unset\n select (.sprint == null or .points == null or .points.number == 0) |\n\n # format the output\n {\n   itemId,\n   projectId: .project.projectId,\n   sprint: .sprint.iterationId,\n   points: .points.number,\n }\n \" $raw_data_file > $item_data_file  # read from raw and write to item_data_file\n\n# Get issue type\nissue_type=$(jq -r \".data.resource.issueType.name\" $raw_data_file)\n\n# Get PR author (if PR is linked to issue)\npr_author=$(\n  jq -r '.data.resource.pullRequests.nodes |\n  if length > 0 then .[0].author.login else null end' $raw_data_file\n)\n\n# #######################################################\n# Assign PR author to issue, or abort update\n# #######################################################\n\n# If the issue doesn't have a linked PR, print a message and exit\nif [[ $pr_author == 'null' ]]; then\n  echo \"Issue '$issue_url' wasn't closed by a PR. Skipping further action.\"\n  exit 0\n\n# If the output file for issue data is empty, print a message and exit\nelif [[ ! -s $item_data_file ]]; then\n  echo \"Sprint and story points don't need to be updated. Skipping further action.\"\n  exit 0\n\n# Otherwise add the PR author to the list of assignees and proceed with other updates\nelse\n  if [[ $dry_run == \"YES\" ]]; then\n    echo \"Would assign issue ${issue_url} to $pr_author\"\n  else\n    echo \"Assigning issue ${issue_url} to $pr_author\"\n    gh issue edit $issue_url --add-assignee $pr_author\n  fi\nfi\n\n# #######################################################\n# Fetch project metadata\n# #######################################################\n\n# If issue is a Task, Bug, or Enhancement, fetch the project metadata\ncase \"${issue_type}\" in\n\"Task\"|\"Bug\"|\"Enhancement\")\n    gh api graphql \\\n    --field org=\"${org}\" \\\n    --field project=\"${project}\" \\\n    --field sprintField=\"${sprint_field}\" \\\n    --field pointsField=\"${points_field}\" \\\n    -f query=\"${field_query}\" \\\n    --jq \".data.organization.projectV2 |\n\n    # reformat the field metadata\n    {\n      points,\n      sprint: {\n        fieldId: .sprint.fieldId,\n        iterationId: .sprint.configuration.iterations[0].id,\n      }\n    }\" > $field_data_file  # write output to a file\n;;\n\n# If it's some other type, print a message and exit\n*)\n    echo \"Not updating because issue has type: ${issue_type}\"\n    exit 0\n;;\nesac\n\n# get the itemId and the projectId\nitem_id=$(jq -r '.itemId' \"$item_data_file\")\nproject_id=$(jq -r '.projectId' \"$item_data_file\")\n\n# #######################################################\n# Set the points value, if empty\n# #######################################################\n\nif jq -e \".points == null or .points == 0\" $item_data_file > /dev/null; then\n\n    if [[ $dry_run == \"YES\" ]]; then\n      echo \"Would set points field to 1 for issue: ${issue_url}\"\n    else\n      echo \"Setting points field to 1 for issue: ${issue_url}\"\n      # Get fieldId from field data\n      point_field_id=$(jq -r '.points.fieldId' \"$field_data_file\")\n      # Use GitHub CLI to update field\n      gh project item-edit \\\n        --id \"${item_id}\" \\\n        --project-id \"${project_id}\" \\\n        --field-id \"${point_field_id}\" \\\n        --number 1\n    fi\n\nelse\n    echo \"Point value already set for issue: ${issue_url}\"\nfi\n\n# #######################################################\n# Set the sprint value, if empty\n# #######################################################\n\nif jq -e \".sprint == null\" $item_data_file > /dev/null; then\n\n    # Skip actual update in dry-run mode\n    if [[ $dry_run == \"YES\" ]]; then\n      echo \"Would set sprint field to current sprint for issue: ${issue_url}\"\n    else\n      echo \"Setting sprint field to current sprint for issue: ${issue_url}\"\n      # Get fieldId and iterationId from field data\n      sprint_field_id=$(jq -r '.sprint.fieldId' \"$field_data_file\")\n      iteration_id=$(jq -r '.sprint.iterationId' \"$field_data_file\")\n      # Use GitHub CLI to update project field\n      gh project item-edit \\\n        --id \"${item_id}\" \\\n        --project-id \"${project_id}\" \\\n        --field-id \"${sprint_field_id}\" \\\n        --iteration-id \"${iteration_id}\"\n    fi\n\nelse\n    echo \"Sprint value already set for issue: ${issue_url}\"\nfi"}
{"path":".github/workflows/ci-infra-service-api.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-infra-service-api.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/pull_request_template.md\nLanguage: md\nType: code\nDirectory: .github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/pull_request_template.md\nSize: 0.46 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/ci-infra-service-frontend.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-infra-service-frontend.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"### Time to review: __x mins__\n\n## Changes proposed\n> What was added, updated, or removed in this PR.\n\n## Context for reviewers\n> Testing instructions, background context, more in-depth details of the implementation, and anything else you'd like to call out or ask reviewers. Explain how the changes were verified.\n\n## Additional information\n> Screenshots, GIF demos, code examples or output to help show the changes working as expected."}
{"path":".github/workflows/ci-infra.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-infra.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/renovate.json\nLanguage: json\nType: code\nDirectory: .github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/renovate.json\nSize: 3.85 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/ci-openapi.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-openapi.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":""}
{"path":".github/workflows/ci-project-linters.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-project-linters.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/README.md\nLanguage: md\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/README.md\nSize: 1.52 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/ci-wiki-links.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-wiki-links.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"The CI/CD for this project uses [reusable Github Actions workflows](https://docs.github.com/en/actions/using-workflows/reusing-workflows).\n\n## ðŸ§ª CI\n\n### Per app workflows\n\nEach app should have:\n\n- `ci-[app_name]`: must be created; should run linting and testing\n- `ci-[app_name]-vulnerability-scans`: calls `vulnerability-scans`\n  - Based on [ci-app-vulnerability-scans](https://github.com/navapbc/template-infra/blob/main/.github/workflows/ci-app-vulnerability-scans.yml)\n\n### App-agnostic workflows\n\n- [`ci-docs`](./ci-docs.yml): runs markdown linting on all markdown files in the file\n  - Configure in [markdownlint-config.json](./markdownlint-config.json)\n- [`ci-infra`](./ci-infra.yml): run infrastructure CI checks\n\n## ðŸš¢ CD\n\nEach app should have:\n\n- `cd-[app_name]`: deploys an application\n  - Based on [`cd-app`](https://github.com/navapbc/template-infra/blob/main/.github/workflows/cd-app.yml)\n\nThe CD workflow uses these reusable workflows:\n\n- [`deploy`](./deploy.yml): deploys an application\n- [`database-migrations`](./database-migrations.yml): runs database migrations for an application\n- [`build-and-publish`](./build-and-publish.yml): builds a container image for an application and publishes it to an image repository\n\n```mermaid\ngraph TD\n  cd-app\n  deploy\n  database-migrations\n  build-and-publish\n\n  cd-app-->|calls|deploy-->|calls|database-migrations-->|calls|build-and-publish\n```\n\n## â›‘ï¸ Helper workflows\n\n- [`check-ci-cd-auth`](./check-ci-cd-auth.yml): verifes that the project's Github repo is able to connect to AWS"}
{"path":".github/workflows/database-migrations.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/database-migrations.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/build-and-publish.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/build-and-publish.yml\nSize: 2.88 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/deploy-metabase.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/deploy-metabase.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  workflow_call:\n    inputs:\n      app_name:\n        description: \"name of application folder under infra directory\"\n        required: true\n        type: string\n      ref:\n        description: The branch, tag or SHA to checkout. When checking out the repository that triggered a workflow, this defaults to the reference or SHA for that event. Otherwise, use branch or tag that triggered the workflow run.\n        required: true\n        type: string\n    outputs:\n      commit_hash:\n        description: The SHA that was built\n        value: ${{ jobs.get-commit-hash.outputs.commit_hash }}\n  workflow_dispatch:\n    inputs:\n      app_name:\n        description: \"name of application folder under infra directory\"\n        required: true\n        type: string\n      ref:\n        description: The branch, tag or SHA to checkout. When checking out the repository that triggered a workflow, this defaults to the reference or SHA for that event. Otherwise, use branch or tag that triggered the workflow run.\n        required: true\n        type: string\n\njobs:\n  get-commit-hash:\n    name: Get commit hash\n    runs-on: ubuntu-22.04\n    outputs:\n      commit_hash: ${{ steps.get-commit-hash.outputs.commit_hash }}\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ inputs.ref }}\n      - name: Get commit hash\n        id: get-commit-hash\n        run: |\n          COMMIT_HASH=$(git rev-parse ${{ inputs.ref }})\n          echo \"Commit hash: $COMMIT_HASH\"\n          echo \"commit_hash=$COMMIT_HASH\" >> \"$GITHUB_OUTPUT\"\n  build-and-publish:\n    name: Build and publish\n    runs-on: ubuntu-22.04\n    needs: get-commit-hash\n    concurrency: ${{ github.workflow }}-${{ needs.get-commit-hash.outputs.commit_hash }}\n\n    permissions:\n      contents: read\n      id-token: write\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ inputs.ref }}\n\n      - name: Set up Terraform\n        uses: ./.github/actions/setup-terraform\n\n      - name: Configure AWS credentials\n        uses: ./.github/actions/configure-aws-credentials\n        with:\n          app_name: ${{ inputs.app_name }}\n          environment: shared\n\n      - name: Check if image is already published\n        id: check-image-published\n        run: |\n          is_image_published=$(./bin/is-image-published \"${{ inputs.app_name }}\" \"${{ needs.get-commit-hash.outputs.commit_hash }}\")\n          echo \"Is image published: $is_image_published\"\n          echo \"is_image_published=$is_image_published\" >> \"$GITHUB_OUTPUT\"\n\n      - name: Build release\n        if: steps.check-image-published.outputs.is_image_published == 'false'\n        run: make APP_NAME=${{ inputs.app_name }} release-build\n\n      - name: Publish release\n        if: steps.check-image-published.outputs.is_image_published == 'false'\n        run: make APP_NAME=${{ inputs.app_name }} release-publish"}
{"path":".github/workflows/deploy.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/deploy.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/cd-analytics.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/cd-analytics.yml\nSize: 1.59 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/infra-service.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/infra-service.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  push:\n    branches:\n      - \"main\"\n    paths:\n      - \"analytics/**\"\n      - \"infra/analytics/**\"\n  release:\n    types: [published]\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: \"target environment\"\n        required: true\n        default: \"dev\"\n        type: choice\n        options:\n          - dev\n          - staging\n          - prod\n      version:\n        required: true\n        default: \"main\"\n        description: Tag or branch or SHA to deploy\n\njobs:\n  analytics-checks:\n    name: Run Analytics Checks\n    uses: ./.github/workflows/ci-analytics.yml\n    secrets: inherit\n\n  vulnerability-scans:\n    name: Vulnerability Scans\n    uses: ./.github/workflows/vulnerability-scans.yml\n    with:\n      app_name: analytics\n\n  deploy:\n    name: Deploy\n    needs: [analytics-checks, vulnerability-scans]\n    uses: ./.github/workflows/deploy.yml\n    strategy:\n      max-parallel: 1\n      fail-fast: false\n      matrix:\n        envs: ${{ fromJSON(inputs.environment != null && format('[\"{0}\"]', inputs.environment) || github.event_name == 'release' && '[\"prod\"]' || github.ref_name == 'main' && '[\"dev\", \"staging\"]' || '[\"dev\"]')  }}\n    with:\n      app_name: \"analytics\"\n      environment: ${{ matrix.envs }}\n      version: ${{ inputs.version || github.ref }}\n\n  send-slack-notification:\n    if: failure()\n    needs: [analytics-checks, vulnerability-scans, deploy]\n    uses: ./.github/workflows/send-slack-notification.yml\n    secrets: inherit"}
{"path":".github/workflows/lint-set-points-and-sprint.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/lint-set-points-and-sprint.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/cd-api.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/cd-api.yml\nSize: 1.51 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".github/workflows/send-slack-notification.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/send-slack-notification.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  push:\n    branches:\n      - \"main\"\n    paths:\n      - \"api/**\"\n      - \"infra/api/**\"\n  release:\n    types: [published]\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: \"target environment\"\n        required: true\n        default: \"dev\"\n        type: choice\n        options:\n          - dev\n          - staging\n          - prod\n      version:\n        required: true\n        default: \"main\"\n        description: Tag or branch or SHA to deploy\n\njobs:\n  api-checks:\n    name: Run API Checks\n    uses: ./.github/workflows/ci-api.yml\n\n  vulnerability-scans:\n    name: Vulnerability Scans\n    uses: ./.github/workflows/vulnerability-scans.yml\n    with:\n      app_name: api\n\n  deploy:\n    name: Deploy\n    needs: [api-checks, vulnerability-scans]\n    uses: ./.github/workflows/deploy.yml\n    strategy:\n      max-parallel: 1\n      fail-fast: false\n      matrix:\n        envs: ${{ fromJSON(inputs.environment != null && format('[\"{0}\"]', inputs.environment) || github.event_name == 'release' && '[\"prod\"]' || github.ref_name == 'main' && '[\"dev\", \"staging\"]' || '[\"dev\"]')  }}\n    with:\n      app_name: \"api\"\n      environment: ${{ matrix.envs }}\n      version: ${{ inputs.version || github.ref }}\n\n  send-slack-notification:\n    if: failure()\n    needs: [api-checks, vulnerability-scans, deploy]\n    uses: ./.github/workflows/send-slack-notification.yml\n    secrets: inherit"}
{"path":".github/workflows/vulnerability-scans.yml","language":"yaml","type":"code","directory":".github/workflows","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/vulnerability-scans.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/cd-frontend.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/cd-frontend.yml\nSize: 1.56 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".grype.yml","language":"yaml","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.grype.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  push:\n    branches:\n      - \"main\"\n    paths:\n      - \"frontend/**\"\n      - \"infra/frontend/**\"\n  release:\n    types: [published]\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: \"target environment\"\n        required: true\n        default: \"dev\"\n        type: choice\n        options:\n          - dev\n          - staging\n          - prod\n      version:\n        required: true\n        default: \"main\"\n        description: Tag or branch or SHA to deploy\n\njobs:\n  frontend-checks:\n    name: Run Frontend Checks\n    uses: ./.github/workflows/ci-frontend.yml\n\n  vulnerability-scans:\n    name: Vulnerability Scans\n    uses: ./.github/workflows/vulnerability-scans.yml\n    with:\n      app_name: frontend\n\n  deploy:\n    name: Deploy\n    needs: [frontend-checks, vulnerability-scans]\n    uses: ./.github/workflows/deploy.yml\n    strategy:\n      max-parallel: 1\n      fail-fast: false\n      matrix:\n        envs: ${{ fromJSON(inputs.environment != null && format('[\"{0}\"]', inputs.environment) || github.event_name == 'release' && '[\"prod\"]' || github.ref_name == 'main' && '[\"dev\", \"staging\"]' || '[\"dev\"]')  }}\n    with:\n      app_name: \"frontend\"\n      environment: ${{ matrix.envs }}\n      version: ${{ inputs.version || github.ref }}\n\n  send-slack-notification:\n    if: failure()\n    needs: [frontend-checks, vulnerability-scans, deploy]\n    uses: ./.github/workflows/send-slack-notification.yml\n    secrets: inherit"}
{"path":".hadolint.yaml","language":"yaml","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.hadolint.yaml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/cd-metabase.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/cd-metabase.yml\nSize: 0.98 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".pre-commit-config.yaml","language":"yaml","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.pre-commit-config.yaml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  workflow_dispatch:\n    inputs:\n      environment:\n        description: \"target environment\"\n        required: true\n        default: \"dev\"\n        type: choice\n        options:\n          - dev\n          - staging\n          - prod\n      image-tag:\n        description: \"Metabase enterprise image tag to deploy\"\n        required: true\n        type: string\n\njobs:\n  deploy:\n    name: Deploy\n    uses: ./.github/workflows/deploy-metabase.yml\n    strategy:\n      max-parallel: 1\n      fail-fast: false\n      matrix:\n        envs: ${{ fromJSON(format('[\"{0}\"]', inputs.environment))  }}\n    with:\n      app_name: \"metabase\"\n      version: ${{ inputs.image-tag }}\n      environment: ${{ matrix.envs }}\n\n  send-slack-notification:\n    if: failure()\n    needs: [deploy]\n    uses: ./.github/workflows/send-slack-notification.yml\n    secrets: inherit"}
{"path":".template-infra/app-analytics.yml","language":"yaml","type":"code","directory":".template-infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.template-infra/app-analytics.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/cd-storybook.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/cd-storybook.yml\nSize: 1.49 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".template-infra/app-api.yml","language":"yaml","type":"code","directory":".template-infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.template-infra/app-api.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  # Runs on pushes targeting the default branch\n  push:\n    branches: [\"main\"]\n    paths:\n      - frontend/**\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# Sets permissions of the GITHUB_TOKEN to allow access to GitHub Pages\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\n# Cancel any older in-progress runs of this workflow\nconcurrency:\n  group: \"pages\"\n  cancel-in-progress: true\n\njobs:\n  build:\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n      - name: Setup Node\n        uses: actions/setup-node@v4\n        with:\n          node-version: 18\n          cache-dependency-path: ./frontend/package-lock.json # or yarn.lock\n          cache: npm # or yarn\n      - name: Setup Pages\n        uses: actions/configure-pages@v5\n        id: pages_config\n      - name: Install dependencies\n        run: npm ci\n        working-directory: ./frontend\n      - name: Build\n        run: NEXT_PUBLIC_BASE_PATH=${{ steps.pages_config.outputs.base_path }} npm run storybook-build\n        working-directory: ./frontend\n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: ./frontend/storybook-static\n\n  deploy:\n    environment:\n      name: github-pages\n      url: ${{ steps.hosting.outputs.page_url }}\n    runs-on: ubuntu-22.04\n    needs: build\n    steps:\n      - name: Deploy to GitHub Pages\n        id: hosting\n        uses: actions/deploy-pages@v4"}
{"path":".template-infra/app-frontend.yml","language":"yaml","type":"code","directory":".template-infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.template-infra/app-frontend.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/check-ci-cd-auth.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/check-ci-cd-auth.yml\nSize: 0.71 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".template-infra/base.yml","language":"yaml","type":"code","directory":".template-infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.template-infra/base.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  workflow_dispatch:\n    inputs:\n      aws_region:\n        description: AWS region\n        default: us-east-1\n        required: false\n      role_to_assume:\n        description: ARN of IAM role to assume\n        required: true\n\npermissions:\n  contents: read\n  id-token: write\n\njobs:\n  caller-identity:\n    name: Check caller identity\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-region: ${{ inputs.aws_region }}\n          role-to-assume: ${{ inputs.role_to_assume }}\n      - run: aws sts get-caller-identity"}
{"path":".terraform-version","language":"unknown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.terraform-version","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-analytics-vulnerability-scans.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-analytics-vulnerability-scans.yml\nSize: 0.63 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":".trivyignore","language":"unknown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.trivyignore","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"name: CI Analytics Vulnerability Scans\n\non:\n  pull_request:\n    paths:\n      - .grype.yml\n      - .hadolint.yaml\n      - .trivyignore\n      - .github/workflows/ci-analytics-vulnerability-scans.yml\n      - .github/workflows/vulnerability-scans.yml\n      - analytics/Dockerfile\n      - analytics/pyproject.toml\n      - analytics/poetry.lock\n\njobs:\n  vulnerability-scans:\n    name: Vulnerability Scans\n    uses: ./.github/workflows/vulnerability-scans.yml\n    with:\n      app_name: analytics"}
{"path":".vscode/launch.json","language":"json","type":"code","directory":".vscode","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/.vscode/launch.json","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-analytics.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-analytics.yml\nSize: 1.28 KB\nLast Modified: 2025-02-14T17:08:26.403Z"}
{"path":"CODE_OF_CONDUCT.md","language":"markdown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/CODE_OF_CONDUCT.md","size":894508,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  workflow_call:\n  pull_request:\n    paths:\n      - analytics/**\n      - .github/workflows/ci-analytics.yml\n\ndefaults:\n  run:\n    working-directory: ./analytics\n\njobs:\n  lint-test:\n    name: Analytics Lint, Format & Tests\n    runs-on: ubuntu-22.04\n    env:\n      GH_TOKEN: ${{ secrets.GH_TOKEN_PROJECT_ACCESS }}\n      ANALYTICS_SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}\n      ANALYTICS_REPORTING_CHANNEL_ID: ${{ secrets.REPORTING_CHANNEL_ID_TEST }}\n      ACTION: show-results # show results, but don't post them to slack\n    steps:\n      # set up python\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.13\"\n\n      # install poetry\n      - uses: Gr1N/setup-poetry@v9\n\n      - name: Install analytics package using poetry\n        run: make install\n\n      - name: Check formatters\n        run: make format-check\n\n      - name: Run linting\n        run: make lint\n\n      - name: Run database migrations\n        run: docker compose down --volumes && make db-migrate\n\n      - name: Run tests\n        run: make test-audit\n\n      # Both of these tasks are looking for github and slack auth\n      # - name: Export GitHub data\n      #   run: make gh-data-export\n\n      # - name: Run reports\n      #   run: make sprint-reports"}
{"path":"COMMUNITY_GUIDELINES.md","language":"markdown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/COMMUNITY_GUIDELINES.md","size":894508,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-api-vulnerability-scans.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-api-vulnerability-scans.yml\nSize: 0.59 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"CONTRIBUTING.md","language":"markdown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/CONTRIBUTING.md","size":894508,"lastModified":"2025-02-14T17:08:31.121Z","content":"name: CI API Vulnerability Scans\n\non:\n  pull_request:\n    paths:\n      - .grype.yml\n      - .hadolint.yaml\n      - .trivyignore\n      - .github/workflows/ci-api-vulnerability-scans.yml\n      - .github/workflows/vulnerability-scans.yml\n      - api/Dockerfile\n      - api/pyproject.toml\n      - api/poetry.lock\n\njobs:\n  vulnerability-scans:\n    name: Vulnerability Scans\n    uses: ./.github/workflows/vulnerability-scans.yml\n    with:\n      app_name: api"}
{"path":"DEVELOPMENT.md","language":"markdown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/DEVELOPMENT.md","size":894508,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-api.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-api.yml\nSize: 0.67 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"LICENSE.md","language":"markdown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/LICENSE.md","size":894508,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  workflow_call:\n  pull_request:\n    paths:\n      - api/**\n      - .github/workflows/ci-api.yml\n\ndefaults:\n  run:\n    working-directory: ./api\n\njobs:\n  lint-test:\n    name: API Lint, Format & Tests\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Initialize the docker containers\n        run: make init\n\n      - name: Run format check\n        run: make format-check\n\n      - name: Run linting\n        run: make lint\n\n      - name: Check migrations up-to-date\n        run: make db-check-migrations\n\n      - name: Run security linting\n        run: make lint-security\n\n      - name: Start tests\n        run: make test-coverage"}
{"path":"MAINTAINERS.md","language":"markdown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/MAINTAINERS.md","size":894508,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-cron-vulnerability-scans.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-cron-vulnerability-scans.yml\nSize: 0.73 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"Makefile","language":"unknown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/Makefile","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"name: CI Cron Vulnerability Scans\n\non:\n  workflow_dispatch:\n  schedule:\n    # Run every day at (8am ET, 5am PT) before the start of the workday\n    - cron: \"0 12 * * *\"\n\njobs:\n  vulnerability-scans:\n    name: Vulnerability Scans\n    strategy:\n      fail-fast: false\n      matrix:\n        app_name: [\"frontend\", \"api\", \"analytics\"]\n    uses: ./.github/workflows/vulnerability-scans.yml\n    with:\n      app_name: ${{ matrix.app_name }}\n\n  send-slack-notification:\n    if: failure()\n    needs: vulnerability-scans\n    uses: ./.github/workflows/send-slack-notification.yml\n    secrets: inherit"}
{"path":"OPERATIONS.md","language":"markdown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/OPERATIONS.md","size":894508,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-frontend-a11y.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-frontend-a11y.yml\nSize: 1.86 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"README.md","language":"markdown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  pull_request:\n    paths:\n      - frontend/**\n      - .github/workflows/ci-frontend-a11y.yml\n\njobs:\n  build:\n    name: Pa11y-ci tests\n    runs-on: ubuntu-22.04\n    defaults:\n      run:\n        working-directory: ./frontend\n\n    env:\n      NODE_VERSION: 22\n      LOCKFILE_PATH: ./frontend/package-lock.json\n      PACKAGE_MANAGER: npm\n\n    steps:\n      - name: Checkout source\n        uses: actions/checkout@v4\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache-dependency-path: ${{ env.LOCKFILE_PATH }}\n          cache: ${{ env.PACKAGE_MANAGER }}\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Create screenshots directory\n        run: mkdir -p screenshots-output\n\n      - name: Start API Server for search results\n        run: |\n          cd ../api\n          make init db-seed-local start &\n          cd ../frontend\n          # ensure the API wait script is executable\n          chmod +x ../api/bin/wait-for-api.sh\n          ../api/bin/wait-for-api.sh\n        shell: bash\n\n      - name: Build Site\n        run: |\n          cat .env.development >> .env.local\n          npm run build -- --no-lint\n\n      - name: Run Server\n        run: npm run start &\n\n      - name: Wait for frontend to be ready\n        run: |\n          # Ensure the server wait script is executable\n          chmod +x ./bin/wait-for-frontend.sh\n          ./bin/wait-for-frontend.sh\n\n      - name: Run pa11y-ci\n        run: |\n          set -e # Ensure the script fails if any command fails\n          npm run test:pa11y-desktop\n          npm run test:pa11y-mobile\n          echo \"pa11y-ci tests finished.\"\n\n      - name: Upload screenshots to artifacts\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: screenshots\n          path: ./frontend/screenshots-output"}
{"path":"SECURITY.md","language":"markdown","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/SECURITY.md","size":894508,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-frontend-e2e.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-frontend-e2e.yml\nSize: 3.60 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/.vscode/settings.json","language":"json","type":"code","directory":"analytics/.vscode","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/.vscode/settings.json","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"defaults:\n  run:\n    working-directory: ./frontend\n\nenv:\n  NODE_VERSION: 22\n  LOCKFILE_PATH: ./frontend/package-lock.json\n  PACKAGE_MANAGER: npm\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  e2e-tests:\n    name: Run E2E Tests\n    runs-on: ubuntu-22.04\n\n    strategy:\n      matrix:\n        shard: [1, 2, 3]\n        total_shards: [3]\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: ${{ env.PACKAGE_MANAGER }}\n          cache-dependency-path: ${{ env.LOCKFILE_PATH }}\n\n      - run: |\n          npm ci\n          npx playwright install --with-deps\n\n      - name: Start API Server for e2e tests\n        run: |\n          cd ../api\n          make init db-seed-local populate-search-opportunities start\n          echo \"LOGIN_FINAL_DESTINATION=http://localhost:3000/api/auth/callback\" >> override.env\n          echo \"ENABLE_OPPORTUNITY_ATTACHMENT_PIPELINE=false\" >> override.env\n          cd ../frontend\n          # Ensure the API wait script is executable\n          chmod +x ../api/bin/wait-for-api.sh\n          ../api/bin/wait-for-api.sh\n        shell: bash\n\n      - name: Build a prod version of the site\n        run: |\n          sed -En '/API_JWT_PUBLIC_KEY/,/-----END PUBLIC KEY-----/p' ../api/override.env >> .env.local\n          cat .env.development >> .env.local\n          npm run build -- --no-lint\n\n      - name: Run e2e tests (Shard ${{ matrix.shard }}/${{ matrix.total_shards }})\n        env:\n          CI: true\n          TOTAL_SHARDS: ${{ matrix.total_shards }}\n          CURRENT_SHARD: ${{ matrix.shard }}\n        run: npm run test:e2e\n\n      - name: Verify Blob Report Directory\n        run: |\n          echo \"Contents of blob-report directory:\"\n          ls -R blob-report || echo \"blob-report directory not found\"\n\n      - name: Upload Blob Report\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: blob-report-shard-${{ matrix.shard }}\n          path: /home/runner/work/simpler-grants-gov/simpler-grants-gov/frontend/blob-report\n          retention-days: 1\n\n  create-report:\n    name: Create Merged Test Report\n    if: ${{ !cancelled() }}\n    needs: [e2e-tests]\n    runs-on: ubuntu-22.04\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: ${{ env.PACKAGE_MANAGER }}\n          cache-dependency-path: ${{ env.LOCKFILE_PATH }}\n\n      - run: |\n          npm ci\n          npx playwright install --with-deps\n\n      - name: Download All Blob Reports\n        uses: actions/download-artifact@v4\n        with:\n          path: frontend/all-blob-reports\n          pattern: blob-report-shard-*\n          merge-multiple: true\n\n      - name: Verify Downloaded Artifacts\n        run: |\n          echo \"Contents of all-blob-reports after download:\"\n          ls -R all-blob-reports*\n\n      - name: Merge Blob Reports into HTML\n        run: npx playwright merge-reports --reporter html ./all-blob-reports\n\n      - name: Verify Downloaded Artifacts\n        run: |\n          echo \"Contents of all-blob-reports after download:\"\n          ls -R playwright*\n\n      - name: Upload Merged HTML Report\n        uses: actions/upload-artifact@v4\n        with:\n          name: merged-html-report\n          path: frontend/playwright-report\n          retention-days: 30"}
{"path":"analytics/Dockerfile","language":"unknown","type":"code","directory":"analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/Dockerfile","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-frontend-vulnerability-scans.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-frontend-vulnerability-scans.yml\nSize: 0.63 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/Makefile","language":"unknown","type":"code","directory":"analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/Makefile","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"name: CI Frontend Vulnerability Scans\n\non:\n  pull_request:\n    paths:\n      - .grype.yml\n      - .hadolint.yaml\n      - .trivyignore\n      - .github/workflows/ci-frontend-vulnerability-scans.yml\n      - .github/workflows/vulnerability-scans.yml\n      - frontend/Dockerfile\n      - frontendpi/package.json\n      - frontend/package-lock.json\n\njobs:\n  vulnerability-scans:\n    name: Vulnerability Scans\n    uses: ./.github/workflows/vulnerability-scans.yml\n    with:\n      app_name: frontend"}
{"path":"analytics/README.md","language":"markdown","type":"code","directory":"analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-frontend.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-frontend.yml\nSize: 2.95 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/config/github-projects.json","language":"json","type":"code","directory":"analytics/config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/config/github-projects.json","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  workflow_call:\n  pull_request:\n    paths:\n      - frontend/**\n      - .github/workflows/ci-frontend.yml\n\ndefaults:\n  run:\n    working-directory: ./frontend\n\nenv:\n  NODE_VERSION: 22\n  LOCKFILE_PATH: ./frontend/package-lock.json # or yarn.lock\n  PACKAGE_MANAGER: npm # or yarn\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\njobs:\n  tests:\n    name: FE Lint, Type Check, Format & Tests\n    runs-on: ubuntu-22.04\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache-dependency-path: ${{ env.LOCKFILE_PATH }}\n          cache: ${{ env.PACKAGE_MANAGER }}\n      - run: npm ci\n\n      - name: Run lint\n        run: npm run lint\n\n      - name: Run type check\n        run: npm run ts:check\n\n      - name: Run format\n        run: npm run format-check\n\n      - run: npm run test -- --testLocationInResults --json --outputFile=coverage/report.json\n      - uses: ArtiomTr/jest-coverage-report-action@v2\n        with:\n          coverage-file: coverage/report.json\n          test-script: npm test\n          working-directory: ./frontend\n          annotations: ${{ github.event.pull_request.head.repo.full_name == github.event.repository.name && 'failed-tests' || 'none' }}\n          package-manager: npm\n          icons: emoji\n          skip-step: none\n          output: ${{ github.event.pull_request.head.repo.full_name == github.event.repository.name && 'comment' || 'report-markdown' }}\n\n  # Confirms the front end still builds successfully\n  check-frontend-builds:\n    name: FE Build Check\n    runs-on: ubuntu-22.04\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache-dependency-path: ${{ env.LOCKFILE_PATH }}\n          cache: ${{ env.PACKAGE_MANAGER }}\n\n      # https://nextjs.org/docs/advanced-features/ci-build-caching\n      - uses: actions/cache@v4\n        with:\n          path: |\n            ~/.npm\n            ${{ github.workspace }}/frontend/.next/cache\n          # Generate a new cache whenever packages or source files change.\n          key: ${{ runner.os }}-nextjs-${{ hashFiles('**/package-lock.json') }}-${{ hashFiles('**.[jt]s', '**.[jt]sx') }}\n          # If source files changed but packages didn't, rebuild from a prior cache.\n          restore-keys: |\n            ${{ runner.os }}-nextjs-${{ hashFiles('**/package-lock.json') }}-\n\n      - run: npm ci\n      - run: npm run build -- --no-lint\n\n  # Confirms Storybook still builds successfully\n  check-storybook-builds:\n    name: FE Storybook Build Check\n    runs-on: ubuntu-22.04\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache-dependency-path: ${{ env.LOCKFILE_PATH }}\n          cache: ${{ env.PACKAGE_MANAGER }}\n      - run: npm ci\n      - run: npm run storybook-build"}
{"path":"analytics/config.py","language":"python","type":"code","directory":"analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/config.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-infra-service-api.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-infra-service-api.yml\nSize: 0.46 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/docker-compose.yml","language":"yaml","type":"code","directory":"analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/docker-compose.yml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  push:\n    branches:\n      - main\n    paths:\n      - infra/api/service/**\n      - infra/test/**\n      - .github/workflows/ci-infra-service.yml\n  pull_request:\n    paths:\n      - infra/api/service/**\n      - infra/test/**\n      - .github/workflows/ci-infra-service.yml\n  workflow_dispatch:\n\njobs:\n  infra-service-checks:\n    name: Infra Service Checks\n    uses: ./.github/workflows/infra-service.yml\n    with:\n      app_name: api"}
{"path":"analytics/local.env","language":"unknown","type":"code","directory":"analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/local.env","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-infra-service-frontend.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-infra-service-frontend.yml\nSize: 0.48 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/pyproject.toml","language":"unknown","type":"code","directory":"analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/pyproject.toml","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  push:\n    branches:\n      - main\n    paths:\n      - infra/frontend/service/**\n      - infra/test/**\n      - .github/workflows/ci-infra-service.yml\n  pull_request:\n    paths:\n      - infra/frontend/service/**\n      - infra/test/**\n      - .github/workflows/ci-infra-service.yml\n  workflow_dispatch:\n\njobs:\n  infra-service-checks:\n    name: Infra Service Checks\n    uses: ./.github/workflows/infra-service.yml\n    with:\n      app_name: frontend"}
{"path":"analytics/reporting.ipynb","language":"unknown","type":"code","directory":"analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/reporting.ipynb","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-infra.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-infra.yml\nSize: 3.02 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/src/analytics/__init__.py","language":"python","type":"code","directory":"analytics/src/analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  push:\n    branches:\n      - main\n    paths:\n      - bin/**\n      - infra/**\n      - .github/workflows/**\n  pull_request:\n    paths:\n      - bin/**\n      - infra/**\n      - .github/workflows/**\n\njobs:\n  lint-github-actions:\n    # Lint github actions files using https://github.com/rhysd/actionlint\n    # This job configuration is largely copied from https://github.com/rhysd/actionlint/blob/main/docs/usage.md#use-actionlint-on-github-actions\n    name: Lint GitHub Actions workflows\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v4\n      - name: Download actionlint\n        id: get_actionlint\n        run: bash <(curl https://raw.githubusercontent.com/rhysd/actionlint/main/scripts/download-actionlint.bash)\n        shell: bash\n      - name: Check workflow files\n        run: ${{ steps.get_actionlint.outputs.executable }} -color\n        shell: bash\n  lint-scripts:\n    name: Lint scripts\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v4\n      - name: Shellcheck\n        run: make infra-lint-scripts\n  check-terraform-format:\n    name: Check Terraform format\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v4\n      - uses: hashicorp/setup-terraform@v3\n        with:\n          terraform_version: 1.9.7\n          terraform_wrapper: false\n      - name: Run infra-lint-terraform\n        run: |\n          echo \"If this fails, run 'make infra-format'\"\n          make infra-lint-terraform\n  validate-terraform:\n    name: Validate Terraform modules\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v4\n      - uses: hashicorp/setup-terraform@v3\n        with:\n          terraform_version: 1.9.7\n          terraform_wrapper: false\n      - name: Validate\n        run: make infra-validate-modules\n  check-compliance-with-checkov:\n    name: Check compliance with checkov\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.10\"\n      - name: Run Checkov check\n        # Pin to specific checkov version rather than running from checkov@master\n        # since checkov frequently adds new checks that can cause CI checks to fail unpredictably.\n        # There is currently no way to specify the checkov version to pin to (See https://github.com/bridgecrewio/checkov-action/issues/41)\n        # so we need to pin the version of the checkov-action, which indirectly pins the checkov version.\n        # In this case, checkov-action v12.2875.0 is mapped to checkov v3.2.257.\n        uses: bridgecrewio/checkov-action@v12.2875.0\n        with:\n          directory: infra\n          framework: terraform\n          quiet: true # only displays failed checks\n  check-compliance-with-tfsec:\n    name: Check compliance with tfsec\n    runs-on: ubuntu-22.04\n\n    permissions:\n      contents: read\n      pull-requests: write\n\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run tfsec check\n        uses: aquasecurity/tfsec-pr-commenter-action@v1.3.1\n        with:\n          github_token: ${{ github.token }}"}
{"path":"analytics/src/analytics/cli.py","language":"python","type":"code","directory":"analytics/src/analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/cli.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-openapi.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-openapi.yml\nSize: 1.22 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/src/analytics/datasets/__init__.py","language":"python","type":"code","directory":"analytics/src/analytics/datasets","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/datasets/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  pull_request:\n    paths:\n      - api/**\n      - Makefile\n      - .github/workflows/ci-openapi.yml\n\ndefaults:\n  run:\n    working-directory: ./api\n\n# Only trigger run one update of the OpenAPI spec at a time on the branch.\n# If new commits are pushed to the branch, cancel in progress runs and start\n# a new one.\nconcurrency:\n  group: ${{ github.head_ref }}\n  cancel-in-progress: true\n\njobs:\n  update-openapi-docs:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          # Checkout the feature branch associated with the pull request\n          ref: ${{ github.head_ref }}\n\n      - name: Create ERD diagram\n        run: make create-erds\n\n      - name: Update OpenAPI spec\n        run: make openapi-spec\n\n      - name: Push changes\n        run: |\n          git config user.name nava-platform-bot\n          git config user.email platform-admins@navapbc.com\n          git add --all\n          # Commit changes (if no changes then no-op)\n          git diff-index --quiet HEAD || git commit -m \"Create ERD diagram and Update OpenAPI spec\"\n          git push"}
{"path":"analytics/src/analytics/datasets/base.py","language":"python","type":"code","directory":"analytics/src/analytics/datasets","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/datasets/base.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-project-linters.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-project-linters.yml\nSize: 1.56 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/src/analytics/datasets/etl_dataset.py","language":"python","type":"code","directory":"analytics/src/analytics/datasets","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/datasets/etl_dataset.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  workflow_dispatch:\n  pull_request:\n    paths:\n      - .github/linters/**\n      - .github/workflows/ci-project-linters.yml\n\ndefaults:\n  run:\n    working-directory: ./.github/linters # ensures that this job runs from the ./linters sub-directory\n\njobs:\n  dry-run-project-linters:\n    name: Dry run GitHub project linters\n    runs-on: ubuntu-22.04\n    env:\n      GH_TOKEN: ${{ secrets.GH_TOKEN_PROJECT_ACCESS }}\n      # Test issue with points and sprint values unset\n      # to be used with set-points-and-sprint.sh test to demonstrate update\n      UNSET_ISSUE: \"https://github.com/HHS/simpler-grants-gov/issues/1932\"\n      # Test issue with points and sprint values unset, but no linked PR,\n      # to be used with set-points-and-sprint.sh test to demonstrate skipping\n      ISSUE_WITHOUT_PR: \"https://github.com/HHS/grants-product-and-delivery/issues/261\"\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Dry run - Set points and sprint field (update metadata)\n        run: |\n          ./scripts/set-points-and-sprint.sh \\\n            --url \"${UNSET_ISSUE}\" \\\n            --org \"HHS\" \\\n            --project 13 \\\n            --sprint-field \"Sprint\" \\\n            --points-field \"Story Points\" \\\n            --dry-run\n\n      - name: Dry run - Set points and sprint field (skip issue because no PR)\n        run: |\n          ./scripts/set-points-and-sprint.sh \\\n            --url \"${ISSUE_WITHOUT_PR}\" \\\n            --org \"HHS\" \\\n            --project 17 \\\n            --sprint-field \"Sprint\" \\\n            --points-field \"Points\" \\\n            --dry-run"}
{"path":"analytics/src/analytics/datasets/issues.py","language":"python","type":"code","directory":"analytics/src/analytics/datasets","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/datasets/issues.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/ci-wiki-links.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/ci-wiki-links.yml\nSize: 0.68 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/src/analytics/datasets/utils.py","language":"python","type":"code","directory":"analytics/src/analytics/datasets","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/datasets/utils.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  pull_request:\n    paths:\n      - documentation/wiki/**\n      - .github/workflows/ci-wiki-links.yml\n      - .github/linters/scripts/check-wiki-pages-linked-to-summary.sh\n\ndefaults:\n  run:\n    working-directory: ./.github/linters # ensures that this job runs from the ./linters sub-directory\n\njobs:\n  check-wiki-links:\n    name: Check wiki links in SUMMARY.md\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Check all wiki files are linked in SUMMARY.md\n        run: ./scripts/check-wiki-pages-linked-to-summary.sh"}
{"path":"analytics/src/analytics/etl/__init__.py","language":"python","type":"code","directory":"analytics/src/analytics/etl","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/etl/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/database-migrations.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/database-migrations.yml\nSize: 1.58 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/src/analytics/etl/github.py","language":"python","type":"code","directory":"analytics/src/analytics/etl","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/etl/github.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  workflow_call:\n    inputs:\n      app_name:\n        description: \"name of application folder under infra directory\"\n        required: true\n        type: string\n      environment:\n        description: \"the name of the application environment (e.g. dev, staging, prod)\"\n        required: true\n        type: string\n      version:\n        description: \"git reference to deploy (e.g., a branch, tag, or commit SHA)\"\n        required: true\n        type: string\n    outputs:\n      commit_hash:\n        description: The SHA that was used for migrations\n        value: ${{ jobs.build-and-publish.outputs.commit_hash }}\n\nconcurrency: database-migrations-${{ inputs.app_name }}-${{ inputs.environment }}\n\njobs:\n  build-and-publish:\n    name: Build\n    uses: ./.github/workflows/build-and-publish.yml\n    with:\n      app_name: ${{ inputs.app_name }}\n      ref: ${{ inputs.version }}\n  run-migrations:\n    name: Run migrations\n    runs-on: ubuntu-22.04\n    needs: [build-and-publish]\n\n    permissions:\n      contents: read\n      id-token: write\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Terraform\n        uses: ./.github/actions/setup-terraform\n\n      - name: Configure AWS credentials\n        uses: ./.github/actions/configure-aws-credentials\n        with:\n          app_name: ${{ inputs.app_name }}\n          environment: ${{ inputs.environment }}\n\n      - name: Run migrations\n        run: |\n          make release-run-database-migrations APP_NAME=${{ inputs.app_name }} ENVIRONMENT=${{ inputs.environment }} IMAGE_TAG=${{ needs.build-and-publish.outputs.commit_hash }}"}
{"path":"analytics/src/analytics/etl/utils.py","language":"python","type":"code","directory":"analytics/src/analytics/etl","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/etl/utils.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/deploy-metabase.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/deploy-metabase.yml\nSize: 1.11 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/src/analytics/integrations/__init__.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  workflow_call:\n    inputs:\n      app_name:\n        description: \"name of application folder under infra directory\"\n        required: true\n        type: string\n      environment:\n        description: \"the name of the application environment (e.g. dev, staging, prod)\"\n        required: true\n        type: string\n      version:\n        description: \"git reference to deploy (e.g., a branch, tag, or commit SHA)\"\n        required: true\n        type: string\n\nconcurrency: cd-${{inputs.app_name}}-${{ inputs.environment }}\n\njobs:\n  deploy:\n    name: Deploy\n    runs-on: ubuntu-22.04\n    permissions:\n      contents: read\n      id-token: write\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Terraform\n        uses: ./.github/actions/setup-terraform\n\n      - name: Configure AWS credentials\n        uses: ./.github/actions/configure-aws-credentials\n        with:\n          app_name: metabase\n          environment: ${{ inputs.environment }}\n\n      - name: Deploy metabase\n        run: make metabase-deploy APP_NAME=${{ inputs.app_name }} ENVIRONMENT=${{ inputs.environment }} IMAGE_TAG=${{ inputs.version }}"}
{"path":"analytics/src/analytics/integrations/db.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/db.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/deploy.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/deploy.yml\nSize: 1.67 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/src/analytics/integrations/etldb/__init__.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/etldb","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  workflow_call:\n    inputs:\n      app_name:\n        description: \"name of application folder under infra directory\"\n        required: true\n        type: string\n      environment:\n        description: \"the name of the application environment (e.g. dev, staging, prod)\"\n        required: true\n        type: string\n      version:\n        description: \"git reference to deploy (e.g., a branch, tag, or commit SHA)\"\n        required: true\n        type: string\n    outputs:\n      commit_hash:\n        description: The SHA that was deployed\n        value: ${{ jobs.database-migrations.outputs.commit_hash }}\n\nconcurrency: cd-${{inputs.app_name}}-${{ inputs.environment }}\n\njobs:\n  # Don't need to call the build-and-publish workflow since the database-migrations\n  # workflow already calls it\n  database-migrations:\n    name: Database migrations\n    uses: ./.github/workflows/database-migrations.yml\n    with:\n      app_name: ${{ inputs.app_name }}\n      environment: ${{ inputs.environment }}\n      version: ${{ inputs.version }}\n  deploy:\n    name: Deploy\n    runs-on: ubuntu-22.04\n    needs: [database-migrations]\n    permissions:\n      contents: read\n      id-token: write\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Terraform\n        uses: ./.github/actions/setup-terraform\n\n      - name: Configure AWS credentials\n        uses: ./.github/actions/configure-aws-credentials\n        with:\n          app_name: ${{ inputs.app_name }}\n          environment: ${{ inputs.environment }}\n\n      - name: Deploy release\n        run: make release-deploy APP_NAME=${{ inputs.app_name }} ENVIRONMENT=${{ inputs.environment }} IMAGE_TAG=${{ needs.database-migrations.outputs.commit_hash }}"}
{"path":"analytics/src/analytics/integrations/etldb/deliverable_model.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/etldb","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/deliverable_model.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/infra-service.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/infra-service.yml\nSize: 0.89 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/src/analytics/integrations/etldb/epic_model.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/etldb","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/epic_model.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  workflow_call:\n    inputs:\n      app_name:\n        description: \"name of application folder under infra directory\"\n        required: true\n        type: string\n\njobs:\n  infra-test-e2e:\n    name: Test service\n    runs-on: ubuntu-22.04\n\n    permissions:\n      contents: read\n      id-token: write\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: hashicorp/setup-terraform@v3\n        with:\n          terraform_version: 1.9.7\n          terraform_wrapper: false\n\n      - uses: actions/setup-go@v5\n        with:\n          go-version: \">=1.19.0\"\n\n      - name: Configure AWS credentials\n        uses: ./.github/actions/configure-aws-credentials\n        with:\n          app_name: ${{ inputs.app_name }}\n          # Run infra CI on dev environment\n          environment: dev\n\n      - name: Run Terratest\n        run: make APP_NAME=${{ inputs.app_name }} infra-test-service"}
{"path":"analytics/src/analytics/integrations/etldb/etldb.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/etldb","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/etldb.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/lint-set-points-and-sprint.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/lint-set-points-and-sprint.yml\nSize: 0.97 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/src/analytics/integrations/etldb/issue_model.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/etldb","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/issue_model.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"on:\n  # trigger on PRs that affect this file or a file used to run the linter\n  issues:\n    types: [closed]\n\ndefaults:\n  run:\n    working-directory: ./.github/linters # ensures that this job runs from the ./linters sub-directory\n\njobs:\n  run-project-linters:\n    name: Run set points and sprint values on close\n    runs-on: ubuntu-22.04\n    # Prevents duplicate runs of this linter for the same issue\n    concurrency:\n      group: issue-${{ github.event.issue.number }}\n      cancel-in-progress: true\n    env:\n      GH_TOKEN: ${{ secrets.GH_TOKEN_PROJECT_ACCESS }}\n      ISSUE_URL: ${{ github.event.issue.html_url }}\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set default values for sprint and points if unset\n        run: |\n          ./scripts/set-points-and-sprint.sh \\\n            --url \"$ISSUE_URL\" \\\n            --org HHS \\\n            --project 13 \\\n            --sprint-field \"Sprint\" \\\n            --points-field \"Story Points\""}
{"path":"analytics/src/analytics/integrations/etldb/main.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/etldb","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/main.py","size":1631931,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/send-slack-notification.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/send-slack-notification.yml\nSize: 1.12 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/src/analytics/integrations/etldb/migrations/versions/0001_alter_default.sql","language":"unknown","type":"code","directory":"analytics/src/analytics/integrations/etldb/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0001_alter_default.sql","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"on: workflow_call\n\njobs:\n  send-slack-notification:\n    name: Send Slack notification on failure\n    runs-on: ubuntu-22.04\n    steps:\n      - name: Send Slack notification\n        run: |\n          curl -X POST -H \"Authorization: Bearer ${{ secrets.ALERTS_SLACK_BOT_TOKEN }}\" \\\n          -H \"Content-Type: application/json; charset=utf-8\" \\\n          --data '{\n            \"channel\": \"${{ secrets.SLACK_ALERTS_CHANNEL_ID }}\",\n            \"text\": \":x: *GitHub Actions Failure Alert*\",\n            \"attachments\": [\n            {\n              \"color\": \"#ff0000\",\n              \"title\": \"Workflow *'\"${{ github.workflow }}\"'* failed\",\n              \"fields\": [\n              {\n                \"title\": \"Workflow URL\",\n                \"value\": \"'\"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"'\"\n              }\n              ],\n              \"footer\": \"GitHub Actions\",\n              \"footer_icon\": \"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\",\n              \"ts\": '$(date +%s)'\n            }\n            ]\n          }' https://slack.com/api/chat.postMessage"}
{"path":"analytics/src/analytics/integrations/etldb/migrations/versions/0002_create_tables_etldb.sql","language":"unknown","type":"code","directory":"analytics/src/analytics/integrations/etldb/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0002_create_tables_etldb.sql","size":0,"lastModified":"2025-02-14T17:08:31.121Z","content":"File: .github/workflows/vulnerability-scans.yml\nLanguage: yml\nType: code\nDirectory: .github/workflows\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/vulnerability-scans.yml\nSize: 8.33 KB\nLast Modified: 2025-02-14T17:08:26.404Z"}
{"path":"analytics/src/analytics/integrations/etldb/migrations/versions/0003_create_schema_versions.sql","language":"unknown","type":"code","directory":"analytics/src/analytics/integrations/etldb/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0003_create_schema_versions.sql","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"name: Vulnerability Scans\n\non:\n  workflow_call:\n    inputs:\n      app_name:\n        description: \"name of application folder under infra directory\"\n        required: true\n        type: string\n\njobs:\n  hadolint-scan:\n    name: Hadolint Scan\n    runs-on: ubuntu-22.04\n\n    steps:\n      - uses: actions/checkout@v4\n\n      # Scans Dockerfile for any bad practices or issues\n      - name: Scan Dockerfile by hadolint\n        uses: hadolint/hadolint-action@v3.1.0\n        with:\n          dockerfile: ${{ inputs.app_name }}/Dockerfile\n          format: tty\n          failure-threshold: warning\n          output-file: hadolint-results.txt\n\n      - name: Save output to workflow summary\n        if: always() # Runs even if there is a failure\n        run: |\n          cat hadolint-results.txt >> \"$GITHUB_STEP_SUMMARY\"\n\n  build-and-cache:\n    runs-on: ubuntu-22.04\n    outputs:\n      image: ${{ steps.shared-output.outputs.image }}\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@master\n\n      - name: Cache Docker layers\n        id: cache-buildx\n        uses: actions/cache@v4\n        with:\n          path: /tmp/.buildx-cache\n          key: ${{ inputs.app_name }}-buildx-${{ github.sha }}\n          restore-keys: |\n            ${{ inputs.app_name }}-buildx-\n\n      - name: Ensure Buildx cache exists\n        run: |\n          mkdir -p /tmp/.buildx-cache\n\n      - name: Set shared outputs\n        id: shared-output\n        run: |\n          IMAGE_NAME=$(make APP_NAME=${{ inputs.app_name }} release-image-name)\n          IMAGE_TAG=$(make release-image-tag)\n          echo \"image=$IMAGE_NAME:$IMAGE_TAG\" >> \"$GITHUB_OUTPUT\"\n\n      - name: Build and tag Docker image for scanning\n        # If there's an exact match in cache, skip build entirely\n        if: steps.cache-buildx.outputs.cache-hit != 'true'\n        run: |\n          make release-build \\\n          APP_NAME=${{ inputs.app_name }} \\\n          OPTIONAL_BUILD_FLAGS=\" \\\n          --cache-from=type=local,src=/tmp/.buildx-cache \\\n          --cache-to=type=local,dest=/tmp/.buildx-cache\"\n\n      - name: Save Docker image\n        if: steps.cache-buildx.outputs.cache-hit != 'true'\n        run: |\n          docker save ${{ steps.shared-output.outputs.image }} > /tmp/docker-image.tar\n\n      - name: Cache Docker image\n        if: steps.cache-buildx.outputs.cache-hit != 'true'\n        uses: actions/cache/save@v4\n        with:\n          path: /tmp/docker-image.tar\n          key: ${{ inputs.app_name }}-docker-image-${{ github.sha }}\n\n  trivy-scan:\n    name: Trivy Scan\n    runs-on: ubuntu-22.04\n    needs: build-and-cache\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Get current date\n        id: date\n        run: echo \"date=$(date +'%Y-%m-%d')\" >> \"$GITHUB_OUTPUT\"\n\n      - name: Restore cached trivy vulnerability and Java DBs\n        id: trivy-cache\n        uses: actions/cache/restore@v4\n        with:\n          path: ${{ github.workspace }}/.cache/trivy\n          key: trivy-cache-${{ steps.date.outputs.date }}\n\n      # Download and extract the vulnerability DB and Java DB\n      # This is based on the instructions here:\n      # https://github.com/aquasecurity/trivy-action/?tab=readme-ov-file#updating-caches-in-the-default-branch\n\n      - name: Setup oras\n        if: steps.trivy-cache.outputs.cache-hit != 'true'\n        uses: oras-project/setup-oras@v1\n\n      - name: Download and extract the vulnerability DB\n        if: steps.trivy-cache.outputs.cache-hit != 'true'\n        run: |\n          mkdir -p \"$GITHUB_WORKSPACE/.cache/trivy/db\"\n          oras pull ghcr.io/aquasecurity/trivy-db:2\n          tar -xzf db.tar.gz -C \"$GITHUB_WORKSPACE/.cache/trivy/db\"\n          rm db.tar.gz\n\n      - name: Download and extract the Java DB\n        if: steps.trivy-cache.outputs.cache-hit != 'true'\n        run: |\n          mkdir -p \"$GITHUB_WORKSPACE/.cache/trivy/java-db\"\n          oras pull ghcr.io/aquasecurity/trivy-java-db:1\n          tar -xzf javadb.tar.gz -C \"$GITHUB_WORKSPACE/.cache/trivy/java-db\"\n          rm javadb.tar.gz\n\n      - name: Cache DBs\n        if: steps.trivy-cache.outputs.cache-hit != 'true'\n        uses: actions/cache/save@v4\n        with:\n          path: ${{ github.workspace }}/.cache/trivy\n          key: trivy-cache-${{ steps.date.outputs.date }}\n\n      - name: Restore cached Docker image\n        uses: actions/cache/restore@v4\n        with:\n          path: /tmp/docker-image.tar\n          key: ${{ inputs.app_name }}-docker-image-${{ github.sha }}\n          restore-keys: |\n            ${{ inputs.app_name }}-docker-image-\n\n      - name: Load cached Docker image\n        run: |\n          docker load < /tmp/docker-image.tar\n\n      - name: Run Trivy vulnerability scan\n        uses: aquasecurity/trivy-action@master\n        with:\n          scan-type: image\n          image-ref: ${{ needs.build-and-cache.outputs.image }}\n          format: table\n          exit-code: 1\n          ignore-unfixed: true\n          vuln-type: os\n          scanners: vuln,secret\n        env:\n          TRIVY_SKIP_DB_UPDATE: true\n          TRIVY_SKIP_JAVA_DB_UPDATE: true\n          # PyJWT has an example with a fake JWT that Trivy flags.\n          # see: https://github.com/aquasecurity/trivy/discussions/5772\n          TRIVY_SKIP_FILES: \"/api/.venv/lib/python*/site-packages/PyJWT-*.dist-info/METADATA\"\n\n      - name: Save output to workflow summary\n        if: always() # Runs even if there is a failure\n        run: |\n          echo \"View results in GitHub Action logs\" >> \"$GITHUB_STEP_SUMMARY\"\n\n  anchore-scan:\n    name: Anchore Scan\n    runs-on: ubuntu-22.04\n    needs: build-and-cache\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Restore cached Docker image\n        uses: actions/cache/restore@v4\n        with:\n          path: /tmp/docker-image.tar\n          key: ${{ inputs.app_name }}-docker-image-${{ github.sha }}\n          restore-keys: |\n            ${{ inputs.app_name }}-docker-image-\n\n      - name: Load cached Docker image\n        run: |\n          docker load < /tmp/docker-image.tar\n\n      - name: Run Anchore vulnerability scan\n        if: always() # Runs even if there is a failure\n        uses: anchore/scan-action@v4\n        id: anchore-scan-json\n        with:\n          image: ${{ needs.build-and-cache.outputs.image }}\n          output-format: json\n          fail-build: true\n          severity-cutoff: medium\n\n      - name: Run Anchore vulnerability scan\n        if: always() # Runs even if there is a failure\n        uses: anchore/scan-action@v4\n        with:\n          image: ${{ needs.build-and-cache.outputs.image }}\n          output-format: table\n          fail-build: true\n          severity-cutoff: medium\n\n      - name: Print output to workflow summary\n        if: always() # Runs even if there is a failure\n        run: |\n          jq '.matches | map(.artifact | { name, version, location: .locations[0].path })' ${{ steps.anchore-scan-json.outputs.json }}\n\n  dockle-scan:\n    name: Dockle Scan\n    runs-on: ubuntu-22.04\n    needs: build-and-cache\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Restore cached Docker image\n        uses: actions/cache/restore@v4\n        with:\n          path: /tmp/docker-image.tar\n          key: ${{ inputs.app_name }}-docker-image-${{ github.sha }}\n          restore-keys: |\n            ${{ inputs.app_name }}-docker-image-\n\n      - name: Load cached Docker image\n        run: |\n          docker load < /tmp/docker-image.tar\n\n      # Dockle doesn't allow you to have an ignore file for the DOCKLE_ACCEPT_FILES\n      # variable, this will save the variable in this file to env for Dockle\n      - name: Set any acceptable Dockle files\n        run: |\n          if grep -q \"^DOCKLE_ACCEPT_FILES=.*\" .dockleconfig; then\n            grep -s '^DOCKLE_ACCEPT_FILES=' .dockleconfig >> \"$GITHUB_ENV\"\n          fi\n\n      - name: Run Dockle container linter\n        uses: erzz/dockle-action@v1.4.0\n        with:\n          image: ${{ needs.build-and-cache.outputs.image }}\n          exit-code: \"1\"\n          failure-threshold: WARN\n          accept-filenames: ${{ env.DOCKLE_ACCEPT_FILES }}\n\n      - name: Save output to workflow summary\n        if: failure() # Only runs if there is a failure\n        run: |\n          {\n            echo '```json'\n            cat dockle-report.json\n            echo '```'\n          } >> \"$GITHUB_STEP_SUMMARY\""}
{"path":"analytics/src/analytics/integrations/etldb/migrations/versions/0004_alter_tables_set_default_timestamp.sql","language":"unknown","type":"code","directory":"analytics/src/analytics/integrations/etldb/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0004_alter_tables_set_default_timestamp.sql","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: .grype.yml\nLanguage: yml\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.grype.yml\nSize: 1.58 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/src/analytics/integrations/etldb/migrations/versions/0005_create_tables_deliv_hist_and_project.sql","language":"unknown","type":"code","directory":"analytics/src/analytics/integrations/etldb/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0005_create_tables_deliv_hist_and_project.sql","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"# List of vulnerabilities to ignore for the anchore scan\n# https://github.com/anchore/grype#specifying-matches-to-ignore\n# More info can be found in the docs/infra/vulnerability-management.md file\n\n# Please add safelists in the following format to make it easier when checking\n# Package/module name: URL to vulnerability for checking updates\n#  Versions:     URL to the version history\n#  Dependencies: Name of any other packages or modules that are dependent on this version\n#                 Link to the dependencies for ease of checking for updates\n#  Issue:         Why there is a finding and why this is here or not been removed\n#  Last checked:  Date last checked in scans\n# - vulnerability: The-CVE-or-vuln-id # Remove comment at start of line\n\nignore:\n  # These settings ignore any findings that fall into these categories\n  - fix-state: not-fixed\n  - fix-state: wont-fix\n  - fix-state: unknown\n\n  # https://github.com/HHS/simpler-grants-gov/issues/2582\n  - vulnerability: CVE-2024-34158\n  - vulnerability: CVE-2024-34156\n  - vulnerability: CVE-2024-34155\n  # Issue due to crypto package pulled in by GitHub CLI\n  # Will be fixed in next GitHub CLI release\n  # Last Checked: Dec 19th, 2024\n  - vulnerability: GHSA-v778-237x-gjrc\n  - vulnerability: GHSA-w32m-9786-jp63\n  # Issue with asyncio library in Python, should be fixed\n  # in 3.13.2 (early February 2025)\n  - vulnerability: CVE-2024-12254\n\n  # https://github.com/HHS/simpler-grants-gov/issues/3855\n  # esbuild fixed in a new minor version, but we don't install/call esbuild directly\n  - vulnerability: GHSA-67mh-4wv8-2f99"}
{"path":"analytics/src/analytics/integrations/etldb/migrations/versions/0006_add_proj_col_to_issue_hist.sql","language":"unknown","type":"code","directory":"analytics/src/analytics/integrations/etldb/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0006_add_proj_col_to_issue_hist.sql","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: .hadolint.yaml\nLanguage: yaml\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.hadolint.yaml\nSize: 0.46 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/src/analytics/integrations/etldb/migrations/versions/0007_add_opportunity_tables.sql","language":"unknown","type":"code","directory":"analytics/src/analytics/integrations/etldb/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0007_add_opportunity_tables.sql","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"# For more information on any settings you can specify, see the actions' documentation here\n# https://github.com/hadolint/hadolint#configure\nfailure-threshold: warning\nignored: []\noverride:\n  info:\n    # Casts the apt-get install <package>=<version> finding as info\n    # We have this set since there is no way to specify version for\n    #  build-essentials in the Dockerfile\n    - DL3008"}
{"path":"analytics/src/analytics/integrations/etldb/project_model.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/etldb","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/project_model.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: .pre-commit-config.yaml\nLanguage: yaml\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.pre-commit-config.yaml\nSize: 0.16 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/src/analytics/integrations/etldb/quad_model.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/etldb","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/quad_model.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":""}
{"path":"analytics/src/analytics/integrations/etldb/sprint_model.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/etldb","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/sprint_model.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: .template-infra/app-analytics.yml\nLanguage: yml\nType: code\nDirectory: .template-infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.template-infra/app-analytics.yml\nSize: 0.17 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/src/analytics/integrations/extracts/__init__.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/extracts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/extracts/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":""}
{"path":"analytics/src/analytics/integrations/extracts/constants.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/extracts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/extracts/constants.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: .template-infra/app-api.yml\nLanguage: yml\nType: code\nDirectory: .template-infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.template-infra/app-api.yml\nSize: 0.16 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/src/analytics/integrations/extracts/load_opportunity_data.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/extracts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/extracts/load_opportunity_data.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":""}
{"path":"analytics/src/analytics/integrations/extracts/s3_config.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/extracts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/extracts/s3_config.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: .template-infra/app-frontend.yml\nLanguage: yml\nType: code\nDirectory: .template-infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.template-infra/app-frontend.yml\nSize: 0.17 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/src/analytics/integrations/github/__init__.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/github/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":""}
{"path":"analytics/src/analytics/integrations/github/client.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/github/client.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: .template-infra/base.yml\nLanguage: yml\nType: code\nDirectory: .template-infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.template-infra/base.yml\nSize: 0.15 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/src/analytics/integrations/github/getRoadmapData.graphql","language":"unknown","type":"code","directory":"analytics/src/analytics/integrations/github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/github/getRoadmapData.graphql","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":""}
{"path":"analytics/src/analytics/integrations/github/getSprintData.graphql","language":"unknown","type":"code","directory":"analytics/src/analytics/integrations/github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/github/getSprintData.graphql","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: .terraform-version\nLanguage: unknown\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.terraform-version\nSize: 0.01 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/src/analytics/integrations/github/main.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/github/main.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":""}
{"path":"analytics/src/analytics/integrations/github/validation.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations/github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/github/validation.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: .trivyignore\nLanguage: unknown\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.trivyignore\nSize: 0.58 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/src/analytics/integrations/slack.py","language":"python","type":"code","directory":"analytics/src/analytics/integrations","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/slack.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":""}
{"path":"analytics/src/analytics/logs/__init__.py","language":"python","type":"code","directory":"analytics/src/analytics/logs","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: .vscode/launch.json\nLanguage: json\nType: code\nDirectory: .vscode\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/.vscode/launch.json\nSize: 1.34 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/src/analytics/logs/app_logger.py","language":"python","type":"code","directory":"analytics/src/analytics/logs","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/app_logger.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"{\n        \"name\": \"API Remote Attach\",\n        \"type\": \"debugpy\",\n        \"request\": \"attach\",\n        \"connect\": {\n            \"host\": \"localhost\",\n            \"port\": 5678\n        },\n        \"pathMappings\": [\n            {\n                \"localRoot\": \"${workspaceFolder}/api\",\n                \"remoteRoot\": \".\"\n            }\n        ],\n        \"justMyCode\": false,\n      },\n      {\n        \"name\": \"Next.js: debug server-side\",\n        \"type\": \"node-terminal\",\n        \"request\": \"launch\",\n        \"command\": \"npm run dev\",\n        \"cwd\": \"${workspaceFolder}/frontend\"\n      },\n      {\n        \"name\": \"Next.js: debug server-side built\",\n        \"type\": \"node-terminal\",\n        \"request\": \"launch\",\n        \"command\": \"npm start\",\n        \"cwd\": \"${workspaceFolder}/frontend\"\n      },\n      {\n        \"name\": \"Next.js: debug client-side\",\n        \"type\": \"chrome\",\n        \"request\": \"launch\",\n        \"url\": \"http://localhost:3000\"\n      },\n      {\n        \"name\": \"Next.js: debug full stack\",\n        \"type\": \"node-terminal\",\n        \"request\": \"launch\",\n        \"command\": \"npm run dev\",\n        \"serverReadyAction\": {\n          \"pattern\": \"- Local:.+(https?://.+)\",\n          \"uriFormat\": \"%s\",\n          \"action\": \"debugWithChrome\"\n        },\n        \"cwd\": \"${workspaceFolder}/frontend\"\n      }\n    ]\n  }"}
{"path":"analytics/src/analytics/logs/config.py","language":"python","type":"code","directory":"analytics/src/analytics/logs","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/config.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: CODE_OF_CONDUCT.md\nLanguage: md\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/CODE_OF_CONDUCT.md\nSize: 1.21 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/src/analytics/logs/decodelog.py","language":"python","type":"code","directory":"analytics/src/analytics/logs","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/decodelog.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"As contributors and maintainers of this project, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities.\n\nWe are committed to making participation in this project a harassment-free experience for everyone, regardless of the level of experience, gender, gender identity, expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion.\n\nExamples of unacceptable behavior by participants include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned with this Code of Conduct.\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by opening an issue or contacting one or more of the project maintainers.\n\nThis Code of Conduct is adapted from the Contributor Covenant version 1.0.0, available at http://contributor-covenant.org/version/1/0/0 ."}
{"path":"analytics/src/analytics/logs/ecs_background_task.py","language":"python","type":"code","directory":"analytics/src/analytics/logs","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/ecs_background_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: COMMUNITY_GUIDELINES.md\nLanguage: md\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/COMMUNITY_GUIDELINES.md\nSize: 3.79 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/src/analytics/logs/formatters.py","language":"python","type":"code","directory":"analytics/src/analytics/logs","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/formatters.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"The grants.gov team is taking a community-first and open source approach to the product development of this tool. We believe government software should be made in the open and be built and licensed such that anyone can download the code, run it themselves without paying money to third parties or using proprietary software, and use it as they will.\n\nWe know that we can learn from a wide variety of communities, including those who will use or will be impacted by the tool, who are experts in technology, or who have experience with similar technologies deployed in other spaces. We are dedicated to creating forums for continuous conversation and feedback to help shape the design and development of the tool.\n\nWe also recognize capacity building as a key part of involving a diverse open source community. We are doing our best to use accessible language, provide technical and process documents in multiple languages, and offer support to community members with a wide variety of backgrounds and skillsets. If you have ideas for how we can improve or add to our capacity building efforts and methods for welcoming people into our community, please let us know by filing an issue on our GitHub repository.\n\n## Principles\n\nPrinciples and guidelines for participating in our open source community are linked belowhere. Please read them before joining or starting a conversation in this repo or one of the channels listed below. All community members and participants are expected to adhere to the community guidelines and code of conduct when participating in community spaces including: code repositories, communication channels and venues, and events.\n\nThese principles guide our data, product, and process decisions, architecture, and approach. These guidelines are inspired by the [Justice40 Community Guidelines](https://github.com/usds/justice40-tool/blob/main/COMMUNITY_GUIDELINES.md).\n\n- Open means transparent and participatory.\n- We take a modular and modern approach to software development.\n- We build open-source software and open-source processes.\n- We value ease of implementation.\n- Fostering community includes building capacity and making our software and processes accessible to participants with diverse backgrounds and skillsets.\n- Data (and data science) is as important as software and process. We build open data sets where possible.\n- We strive for transparency for algorithms and places we might be introducing bias.\n\nAll community members are expected to adhere to our [Code of Conduct](CODE_OF_CONDUCT.md).\n\n## Community Guidelines\n\n- When participating in the Simpler Grants open source community conversations and spaces, we ask individuals to follow the following guidelines:\n- When joining a conversation for the first time, please introduce yourself by providing a brief intro that includes:\n  - your related organization (if applicable)\n  - your pronouns, if you would like to share those\n  - disclosure of any current or potential financial interest in this work\n  - your superpower, and how you hope to use it for this project\n- Embrace a culture of learning, and educate each other. We are all entering this conversation from different starting points and with different backgrounds. There are no dumb questions.\n- Take space and give space. We strive to create an equitable environment in which all are welcome and able to participate. We hope individuals feel comfortable voicing their opinions and providing contributions and will do our best to recognize and make space for individuals who may be struggling to find space here. Likewise, we expect individuals to recognize when they are taking up significant space and take a step back to allow room for others.\n- Be present when joining synchronous conversations such as our community chat. Why be here if you're not going to be here?\n- Be respectful."}
{"path":"analytics/src/analytics/logs/pii.py","language":"python","type":"code","directory":"analytics/src/analytics/logs","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/pii.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: CONTRIBUTING.md\nLanguage: md\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/CONTRIBUTING.md\nSize: 7.65 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/tests/__init__.py","language":"python","type":"code","directory":"analytics/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"# How to Contribute as an External Contributor\n\nðŸŽ‰ First off, thanks for taking the time to contribute! ðŸŽ‰\n\nWe're so thankful you're considering contributing to an [open source project of the U.S. government](https://code.gov/)! If you're unsure about anything, just ask -- or submit the issue or pull request anyway. The worst that can happen is you'll be politely asked to change something. We appreciate all friendly contributions.\n\nWe encourage you to read this project's CONTRIBUTING-EXTERNAL policy (you are here), its [LICENSE](LICENSE.md), and its [README](README.md).\n\n> :information_source: This project initiated in third quarter of 2023, and is just ramping up efforts to include code contributors as well as contributors from many other disciplines in many different capacities.\n\n## How Can I Contribute?\n\nThere are a number of ways to contribute to this project.\n\n### Report a Bug\n\nIf you think you have found a bug in the code or static site, [search our issues list](https://github.com/HHS/simpler-grants-gov/issues) on GitHub for any similar bugs. If you find a similar bug, please update that issue with your details.\n\nIf you do not find your bug in our issues list, file a bug report. When reporting the bug, please follow these guidelines:\n\n- **Please use the [Bug Report](https://github.com/HHS/simpler-grants-gov/issues/new?assignees=octocat&labels=bug&projects=&template=bug_report.yml&title=%5BBug%5D%3A+) issue template** This is populated with information and questions that will help grants.gov developers resolve the issuethe right information\n- **Use a clear and descriptive issue title** for the issue to identify the problem.\n- **Describe the exact steps to reproduce the problem** in as much detail as possible. For example, start by explaining how you got to the page where you encountered the bug and what you were attempting to do when the bug occurred.\n- **Describe the behavior you observed after following the steps** and point out what exactly is the problem with that behavior.\n- **Explain which behavior you expected to see instead and why.**\n- **Include screenshots and animated GIFs** if possible, which show you following the described steps and clearly demonstrate the problem.\n- **If the problem wasn't triggered by a specific action**, describe what you were doing before the problem happened.\n\n### Suggest an Enhancement\n\nIf you don't have specific language or code to submit but would like to suggest a change, request a feature, or have something addressed, you can open an issue in this repository.\n\nPlease open an issue of type [Feature request](https://github.com/HHS/simpler-grants-gov/issues/new?assignees=octocat&labels=enhancement&projects=&template=feature_request.yml&title=%5BFeature+Request%5D%3A+):\n\nIn this issue, please describe the use case for the feature you would like to see -, what you need, why you need it, and how it should work. Team members will respond to the Feature request as soon as possible. Often, Feature request suggestions undergo a collaborative discussion with the community to help refine the need for the feature and how it can be implemented.\n\n### Non-Technical Contributions\n\n#### Documentation\n\nTo contribute to documentation you find in this repository, feel free to use the GitHub user interface to submit a pull request for your changes. Find more information about using the GitHub user interface for PRs here.\n\n### Contribute to community discussions\n\n> ðŸš§ Tools and expanding avenues for community engagement are coming soon.\n\n### Sharing your story\n\nSharing how you or your organization have used the Simpler Grants project is an important way for us to raise awareness about the project and its impact. Please tell us your story by [sending us an email at `simpler-grants-gov@hhs.gov`](mailto:simpler-grants-gov@hhs.gov).\n\n## Code Contributions\n\nThe following guidelines are for code contributions. Please see [DEVELOPMENT.md](./DEVELOPMENT.md) for more information about the software development lifecylce on the project.\n\n### Getting Started\n\nThis project is monorepo with several apps. Please see the [api](./api/README.md) and [frontend](./frontend/README.md) READMEs for information on spinning up those projects locally. Also see the project [documentation](./documentation) for more info.\n\n### Workflow and Branching\n\nThis project follows [trunk-based development](./DEVELOPMENT.md#branching-model), so all contributions are directed toward the `main` branch.\n\n1.  Fork the project\n1.  Check out the `main` branch\n1.  Create a feature branch\n1.  Write code and tests for your change\n1.  From your branch, make a pull request against `hhs/simpler-grants-gov/main`\n1.  Work with repo maintainers to get your change reviewed\n1.  Wait for your change to be pulled into `hhs/simpler-grants-gov/main`\n1.  Delete your feature branch\n\n### Testing, Coding Style and Linters\n\nEach application has its own testing and linters. Every commit is tested to adhere to tests and the linting guidelines. It is recommended to run tests and linters locally before committing.\n\n### Issues\n\nExternal contributors should use the _Bug Report_ or _Feature Request_ [issue templates](https://github.com/HHS/simpler-grants-gov/issues/new/choose).\n\n### Pull Requests\n\nPull requests should follow the conventions in [DEVELOPMENT.md](./DEVELOPMENT.md) with the following changes:\n\n1. Pull requests should be titled with `[Issue N] Description`. However if there is no issue, use `[External] Description` format.\n1. External contributors can't merge their own PRs, so an internal team member will pull in after changes are satisfactory.\n\n## Policies\n\n### Open Source Policy\n\nWe adhere to the [HHS Open Source Policy](https://github.com/CMSGov/cms-open-source-policy). If you have any questions, just [shoot us an email](mailto:simpler@grants.gov?subject=Question About Open Source Policy).\n\n### Security and Responsible Disclosure Policy\n\nThe Department of Health and Human Services is committed to ensuring the security of the American public by protecting their information from\nunwarranted disclosure. We want security researchers to feel comfortable reporting vulnerabilities they have discovered so we can fix them and keep our users safe. We developed our disclosure policy to reflect our values and uphold our sense of responsibility to security researchers who share their expertise with us in good faith.\n\n_Submit a vulnerability:_ Unfortunately, we cannot accept secure submissions via email or via GitHub Issues. Please use our website to submit vulnerabilities at [https://hhs.responsibledisclosure.com](https://hhs.responsibledisclosure.com). HHS maintains an acknowledgements page to recognize your efforts on behalf of the American public, but you are also welcome to submit anonymously.\n\nReview the HHS Disclosure Policy and websites in scope:\n[https://www.hhs.gov/vulnerability-disclosure-policy/index.html](https://www.hhs.gov/vulnerability-disclosure-policy/index.html).\n\nThis policy describes _what systems and types of research_ are covered under this policy, _how to send_ us vulnerability reports, and _how long_ we ask security researchers to wait before publicly disclosing vulnerabilities.\n\nIf you have other cybersecurity related questions, please contact us at [csirc@hhs.gov](mailto:csirc@hhs.gov).\n\n## Public domain\n\nThis project is in the public domain within the United States, and copyright and related rights in the work worldwide are waived through the [CC0 1.0 Universal public domain dedication](https://creativecommons.org/publicdomain/zero/1.0/).\n\nAll contributions to this project will be released under the CC0 dedication. By submitting a pull request or issue, you are agreeing to comply with this waiver of copyright interest."}
{"path":"analytics/tests/assertions.py","language":"python","type":"code","directory":"analytics/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/assertions.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: DEVELOPMENT.md\nLanguage: md\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/DEVELOPMENT.md\nSize: 9.10 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/tests/conftest.py","language":"python","type":"code","directory":"analytics/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/conftest.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"The following guide is for members of the project team who have access to the repository as well as code contributors. The main difference between internal and external contributions is that externabl contributors will need to fork the project and will not be able to merge their own pull requests. For more information on contribributing, see: [CONTRIBUTING.md](./CONTRIBUTING.md).\n\n## Local Development\n\nThis project is monorepo with several apps. Please see the [api](./api/README.md) and [frontend](./frontend/README.md) READMEs for information on spinning up those projects locally. Also see the project [documentation](./documentation) for more info.\n\n### Linting and Testing\n\nEach application has its own linting and testing guidelines. Lint and code tests are run on each commit, so linters and tests should be run locally before commiting.\n\n## Branching Model\n\nThis project follows [trunk-based development](https://trunkbaseddevelopment.com/), which means:\n\n- Make small changes in [short-lived feature branches](https://trunkbaseddevelopment.com/short-lived-feature-branches/) and merge to `main` frequently.\n- Be open to submitting multiple small pull requests for a single ticket (i.e. reference the same ticket across multiple pull requests).\n- Treat each change you merge to `main` as immediately deployable to production. Do not merge changes that depend on subsequent changes you plan to make, even if you plan to make those changes shortly.\n- Ticket any unfinished or partially finished work.\n- Tests should be written for changes introduced, and adhere to the text percentage threshold determined by the project.\n\nThis project uses **continuous deployment** using [Github Actions](https://github.com/features/actions) which is configured in the [./github/worfklows](.github/workflows) directory.\n\nPull-requests are merged to `main` and the changes are immediately deployed to the development environment. Releases are created to push changes to production.\n\n## Writing Pull Requests\n\nPrefix the branch name with your name, and include the ticket number in the branch name e.g. `cooldev/issue-1234-new-feature`.\n\nCommit messages should, but are not required, to follow [git best practice conventions](https://cbea.ms/git-commit/#seven-rules) for consistency and legibility. Commit messages will be squashed, so individual commit messages will only be visible in the commit history of the pull request.\n\n### Title\n\nPull request should have the following format: `[Issue N] Description`. The description should follow the imperative voice and lack of period from the [git best practice conventions](https://cbea.ms/git-commit/#seven-rules).\n\n### Recommendations\n\n**Use draft PRs to solicit early feedback.**\n\nIf your PR is a work-in-progress, or if you are looking for specific feedback on things, create a Draft Pull Request and state what you are looking for in the description.\n\n**Provide context for current and future team members.**\nWrite a full description that provides all the necessary context for your change. Consider your description as documentation. Include relevant context and business requirements, and add preemptive comments (in code or PR) for sections of code that may be confusing or worth debate.\n\n**Make things easy for your reviewers.**\n\nDo a self-review using the diff in github to make sure youâ€™re not sending through any obvious issues. Run any automations (testing, linters, etc.) before opening your PR.\n\nIf any manual testing was performed, document it in enough detail in the PR description that somebody else could recreate your test. Include reference to your test data, if applicable!\n\n### Reviewers\n\nAssign reviewers applicable to the domain of your pull request. See [CODEOWNERS](.github/CODEOWNERS) for more details.\n\n### Pull in Own Requests\n\nOnce a PR is accepted by a reviewer, the author should merge.\n\n### Squash Merge\n\nThis project uses the squash merge strategy. When squashing, retain the `[Issue N] Description` format. Any notes in the body of the commit should follow commit best practices.\n\nAll changes, including small ones, should have an issue. If they don't `[Hotfix]` should be used in lieu of the issue and number.\n\n## Reviewing Pull Requests\n\nThis project takes a very collaborative and [agile](https://agilemanifesto.org/) approach to code reviews. Working versions of code, self-organizing, and individuals are prioritized When reviewing pull requests:\n\n- **Be prompt**. Aim to respond to a review within 24 hours (although sooner is preferable), and if you cannot do so, be sure to communicate delays to the code author.\n- **Be kind and respectful** when leaving comments and maintain a collaborative tone. Donâ€™t use language that disparages or embarrasses the author (name calling, insults to intelligence, etc). Direct any negative feedback towards the code rather than towards the author.\n- **Present suggestions as requests rather than demands**; instead of â€œMove this function to file Bâ€ try â€œWould this function fit better in file B?â€ This allows the author to push back on the suggestion by answering a question rather than rejecting a demand, which helps keep things from getting combative.\n- **Praise and compliment** the good parts!\n- **Explain suggestions and recommendations**. These should be opportunities for learning/mentoring, not for criticism or giving orders.\n- **Offer to chat in person** for more complex discussions, or to ensure understanding of new logic.\n- **Review the testing** as well as the code. You may think of edge cases or other things that the authorâ€™s testing plan might have missed.\n- **Clearly designate between required and optional changes**. This can take many forms, but as examples: â€œ(optional) We might want to rename this variable to avoid confusionâ€ and â€œ(blocking) We donâ€™t properly handle deadlocks here, so weâ€™ll need to fix that.â€ It may also be helpful to clearly designate praises, questions, nits, etc to make a commentâ€™s intention very clear.\n- Consider using **[conventional comments](https://conventionalcomments.org/)** for messages.\n- Use the **\"Add a suggestion\"** feature to suggest small changes in PRs.\n\n![add a suggestion pop-up](https://github.com/HHS/simpler-grants-gov/assets/512243/e08efbd3-91de-43ce-a0d5-4529ccb1ac13)\n\n- **The \"Request Changes\"** feature _requires_ the reviewer approve changes. This takes autonomy from the engineer, and should only be used if there is an urgent need.\n\n## Releases\n\nReleases follow the [CalVer](https://calver.org/) versioning using a `YYYY.MM.DD-N` format.\n\nReleases are [created in Github](https://github.com/HHS/simpler-grants-gov/releases) and with a log of changes.\n\nStep by step instructions for creating a release:\n\n- In your terminal, `git switch main` and `git pull` to sync your local code with the latest main commit\n- In your terminal, `git tag YYYY.MM.DD-N` (FILLED IN!) and `git push --tags` to tag and push your CalVer tag\n  - Start with `N=1` for a release, incrementing if there is another release on the same day. So the first release on `2000.10.10` would be `2000.10.10-1`, ths second release on that day would be `2000.10.10-2`, etc.\n- On Github.com, open the Releases page: https://github.com/HHS/simpler-grants-gov/releases\n- Click \"Draft a new release\"\n- Choose the tag you just pushed as the current tag, and the last CalVer release as the previous tag. You can use the current tag as a release title.\n- Click \"Generate release notes\" to auto-generate release notes\n- Click \"Publish\" to finalize this step. Publishing the release will automatically trigger the necessary deployment jobs in Github Actions.\n- On Github.com, open the Actions page: https://github.com/HHS/simpler-grants-gov/actions\n- You should see Actions that correspond to your release. Follow their status to ensure that they succeed. Follow-up may be required if the Github Actions fail. Here's some places where you might see Github Actions status:\n  - https://github.com/HHS/simpler-grants-gov/actions/workflows/cd-api.yml\n  - https://github.com/HHS/simpler-grants-gov/actions/workflows/cd-frontend.yml\n  - https://github.com/HHS/simpler-grants-gov/actions/workflows/cd-storybook.yml\n\n## Documentation\n\nAny changes to features, tools, or workflows should include updates or additions to documentation.\n\n## Load Testing\n\n[Artillery.io](https://www.artillery.io/docs) is the open source tool used to load test the application. You can find the yml file for the frontend load test at [`/frontend/artillery-load-test.yml`](./frontend/artillery-load-test.yml). You can find the yml file for the backend API load test at [`/api/artillery-load-test.yml`](./api/artillery-load-test.yml).\n\nTo run the load test:\n\n1. Install artillery locally if you haven't done so with `npm install -g artillery@latest`\n2. For the frontend, download the required data from https://drive.google.com/file/d/1zknvVSRqL7xs8VGinztuKelfppYlgRoP and save \"params.json\" to `frontend/tests/artillery/params.json`\n3. `$ cd api` or or `$ cd frontend`\n4. `$ make load-test-<env>` where env is either `local`, `dev`, `staging`, or `production`\n\n- `make load-test-local`\n  - requires running a local container in another console\n- `make load-test-dev`\n- `make load-test-staging`\n- `make load-test-prod`"}
{"path":"analytics/tests/datasets/__init__.py","language":"python","type":"code","directory":"analytics/tests/datasets","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/datasets/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: LICENSE.md\nLanguage: md\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/LICENSE.md\nSize: 1.34 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/tests/datasets/test_base.py","language":"python","type":"code","directory":"analytics/tests/datasets","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/datasets/test_base.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"As a work of the [United States government](https://www.usa.gov/), this project\nis in the public domain within the United States of America.\n\nAdditionally, we waive copyright and related rights in the work worldwide\nthrough the CC0 1.0 Universal public domain dedication.\n\n## CC0 1.0 Universal Summary\n\nThis is a human-readable summary of the [Legal Code (read the full\ntext)](https://creativecommons.org/publicdomain/zero/1.0/legalcode).\n\n### No Copyright\n\nThe person who associated a work with this deed has dedicated the work to the\npublic domain by waiving all of their rights to the work worldwide under\ncopyright law, including all related and neighboring rights, to the extent\nallowed by law.\n\nYou can copy, modify, distribute, and perform the work, even for commercial\npurposes, all without asking permission.\n\n### Other Information\n\nIn no way are the patent or trademark rights of any person affected by CC0, nor\nare the rights that other persons may have in the work or in how the work is\nused, such as publicity or privacy rights.\n\nUnless expressly stated otherwise, the person who associated a work with this\ndeed makes no warranties about the work, and disclaims liability for all uses\nof the work, to the fullest extent permitted by applicable law. When using or\nciting the work, you should not imply endorsement by the author or the\naffirmer."}
{"path":"analytics/tests/datasets/test_etldb.py","language":"python","type":"code","directory":"analytics/tests/datasets","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/datasets/test_etldb.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: MAINTAINERS.md\nLanguage: md\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/MAINTAINERS.md\nSize: 0.62 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/tests/datasets/test_issues.py","language":"python","type":"code","directory":"analytics/tests/datasets","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/datasets/test_issues.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"This is a list of maintainers for this project. See [CODEOWNERS](/.github/CODEOWNERS) for list of reviewers for different parts of the codebase. Team members include:\n\n\n## Engineering\n\n* Aaron Couch\n* Brandon Tabaska\n* Bruk Abebe\n* Doug Schrashun\n* Kai Siren\n* Matt Dragon\n* Michael Chouinard\n* Mike Huneke\n\n## Content and Design\n\n* Andy Cochran\n* Crystabel Rangel\n* Emily Ianacone\n* Jenn Snyder\n* Michelle Min\n* Risha Lee\n* Senongo Akpem\n\n## Product and Delivery\n\n* Lucas Brown\n* Julius Chang\n* Sarah Knopp\n* Billy Daly\n* Eric Valenzuela\n* David Dudas\n\n## Product and Program Management\n\n* Margaret Spring\n* Max Kramer"}
{"path":"analytics/tests/etl/__init__.py","language":"python","type":"code","directory":"analytics/tests/etl","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/etl/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: Makefile\nLanguage: unknown\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/Makefile\nSize: 11.80 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/tests/etl/test_github.py","language":"python","type":"code","directory":"analytics/tests/etl","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/etl/test_github.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"# Use `=` instead of `:=` so that we only execute `./bin/current-account-alias` when needed\n# See https://www.gnu.org/software/make/manual/html_node/Flavors.html#Flavors\nCURRENT_ACCOUNT_ALIAS = `./bin/current-account-alias`\n\nCURRENT_ACCOUNT_ID = $(./bin/current-account-id)\n\n# Get the list of reusable terraform modules by getting out all the modules\n# in infra/modules and then stripping out the \"infra/modules/\" prefix\nMODULES := $(notdir $(wildcard infra/modules/*))\n\n# Check that given variables are set and all have non-empty values,\n# die with an error otherwise.\n#\n# Params:\n#   1. Variable name(s) to test.\n#   2. (optional) Error message to print.\n# Based off of https://stackoverflow.com/questions/10858261/how-to-abort-makefile-if-variable-not-set\ncheck_defined = \\\n\t$(strip $(foreach 1,$1, \\\n        $(call __check_defined,$1,$(strip $(value 2)))))\n__check_defined = \\\n\t$(if $(value $1),, \\\n\t\t$(error Undefined $1$(if $2, ($2))$(if $(value @), \\\n\t\t\trequired by target '$@')))\n\n\n.PHONY : \\\n\thelp \\\n\tinfra-check-app-database-roles \\\n\tinfra-check-compliance-checkov \\\n\tinfra-check-compliance-tfsec \\\n\tinfra-check-compliance \\\n\tinfra-check-github-actions-auth \\\n\tinfra-configure-app-build-repository \\\n\tinfra-configure-app-database \\\n\tinfra-configure-app-service \\\n\tinfra-configure-monitoring-secrets \\\n\tinfra-configure-network \\\n\tinfra-format \\\n\tinfra-lint \\\n\tinfra-lint-scripts \\\n\tinfra-lint-terraform \\\n\tinfra-lint-workflows \\\n\tinfra-set-up-account \\\n\tinfra-test-service \\\n\tinfra-update-app-build-repository \\\n\tinfra-update-app-database-roles \\\n\tinfra-update-app-database \\\n\tinfra-update-app-service \\\n\tinfra-update-current-account \\\n\tinfra-update-network \\\n\tinfra-validate-modules \\\n\trelease-build \\\n\trelease-deploy \\\n\trelease-image-name \\\n\trelease-image-tag \\\n\trelease-publish \\\n\trelease-run-database-migrations\n\n\n\ninfra-set-up-account: ## Configure and create resources for current AWS profile and save tfbackend file to infra/accounts/$ACCOUNT_NAME.ACCOUNT_ID.s3.tfbackend\n\t@:$(call check_defined, ACCOUNT_NAME, human readable name for account e.g. \"prod\" or the AWS account alias)\n\t./bin/set-up-current-account $(ACCOUNT_NAME)\n\ninfra-configure-network: ## Configure network $NETWORK_NAME\n\t@:$(call check_defined, NETWORK_NAME, the name of the network in /infra/networks)\n\t./bin/create-tfbackend infra/networks $(NETWORK_NAME)\n\ninfra-configure-app-build-repository: ## Configure infra/$APP_NAME/build-repository tfbackend and tfvars files\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t./bin/create-tfbackend \"infra/$(APP_NAME)/build-repository\" shared\n\ninfra-configure-app-database: ## Configure infra/$APP_NAME/database module's tfbackend and tfvars files for $ENVIRONMENT\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t@:$(call check_defined, ENVIRONMENT, the name of the application environment e.g. \"prod\" or \"staging\")\n\t./bin/create-tfbackend \"infra/$(APP_NAME)/database\" \"$(ENVIRONMENT)\"\n\ninfra-configure-monitoring-secrets: ## Set $APP_NAME's incident management service integration URL for $ENVIRONMENT\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t@:$(call check_defined, ENVIRONMENT, the name of the application environment e.g. \"prod\" or \"staging\")\n\t@:$(call check_defined, URL, incident management service (PagerDuty or VictorOps) integration URL)\n\t./bin/configure-monitoring-secret $(APP_NAME) $(ENVIRONMENT) $(URL)\n\ninfra-configure-app-service: ## Configure infra/$APP_NAME/service module's tfbackend and tfvars files for $ENVIRONMENT\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t@:$(call check_defined, ENVIRONMENT, the name of the application environment e.g. \"prod\" or \"staging\")\n\t./bin/create-tfbackend \"infra/$(APP_NAME)/service\" \"$(ENVIRONMENT)\"\n\ninfra-update-current-account: ## Update infra resources for current AWS profile\n\t./bin/terraform-init-and-apply infra/accounts `./bin/current-account-config-name`\n\ninfra-update-network: ## Update network\n\t@:$(call check_defined, NETWORK_NAME, the name of the network in /infra/networks)\n\tterraform -chdir=\"infra/networks\" init -input=false -reconfigure -backend-config=\"$(NETWORK_NAME).s3.tfbackend\"\n\tterraform -chdir=\"infra/networks\" apply -var=\"network_name=$(NETWORK_NAME)\"\n\ninfra-update-app-build-repository: ## Create or update $APP_NAME's build repository\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t./bin/terraform-init-and-apply infra/$(APP_NAME)/build-repository shared\n\ninfra-update-app-database: ## Create or update $APP_NAME's database module for $ENVIRONMENT\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t@:$(call check_defined, ENVIRONMENT, the name of the application environment e.g. \"prod\" or \"staging\")\n\tterraform -chdir=\"infra/$(APP_NAME)/database\" init -input=false -reconfigure -backend-config=\"$(ENVIRONMENT).s3.tfbackend\"\n\tterraform -chdir=\"infra/$(APP_NAME)/database\" apply -var=\"environment_name=$(ENVIRONMENT)\"\n\ninfra-update-app-database-roles: ## Create or update database roles and schemas for $APP_NAME's database in $ENVIRONMENT\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t@:$(call check_defined, ENVIRONMENT, the name of the application environment e.g. \"prod\" or \"staging\")\n\t./bin/create-or-update-database-roles $(APP_NAME) $(ENVIRONMENT)\n\ninfra-update-app-service: ## Create or update $APP_NAME's web service module\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t@:$(call check_defined, ENVIRONMENT, the name of the application environment e.g. \"prod\" or \"staging\")\n\tterraform -chdir=\"infra/$(APP_NAME)/service\" init -input=false -reconfigure -backend-config=\"$(ENVIRONMENT).s3.tfbackend\"\n\tterraform -chdir=\"infra/$(APP_NAME)/service\" apply -var=\"environment_name=$(ENVIRONMENT)\"\n\ninfra-update-metabase-service: ## Create or update $APP_NAME's web service module\n\t# APP_NAME has a default value defined above, but check anyways in case the default is ever removed\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t@:$(call check_defined, ENVIRONMENT, the name of the application environment e.g. \"prod\" or \"staging\")\n\tterraform -chdir=\"infra/analytics/metabase\" init -input=false -reconfigure -backend-config=\"$(ENVIRONMENT).s3.tfbackend\"\n\tterraform -chdir=\"infra/analytics/metabase\" apply -var=\"environment_name=$(ENVIRONMENT)\"\n\n# The prerequisite for this rule is obtained by\n# prefixing each module with the string \"infra-validate-module-\"\ninfra-validate-modules: ## Run terraform validate on reusable child modules\ninfra-validate-modules: $(patsubst %, infra-validate-module-%, $(MODULES))\n\ninfra-validate-module-%:\n\t@echo \"Validate library module: $*\"\n\tterraform -chdir=infra/modules/$* init -backend=false\n\tterraform -chdir=infra/modules/$* validate\n\ninfra-check-app-database-roles: ## Check that app database roles have been configured properly\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t@:$(call check_defined, ENVIRONMENT, the name of the application environment e.g. \"prod\" or \"staging\")\n\t./bin/check-database-roles $(APP_NAME) $(ENVIRONMENT)\n\ninfra-check-compliance: ## Run compliance checks\ninfra-check-compliance: infra-check-compliance-checkov infra-check-compliance-tfsec\n\ninfra-check-github-actions-auth: ## Check that GitHub actions can authenticate to the AWS account\n\t@:$(call check_defined, ACCOUNT_NAME, the name of account in infra/accounts)\n\t./bin/check-github-actions-auth $(ACCOUNT_NAME)\n\n\ninfra-check-compliance-checkov: ## Run checkov compliance checks\n\tcheckov --directory infra\n\ninfra-check-compliance-tfsec: ## Run tfsec compliance checks\n\ttfsec infra\n\ninfra-lint: lint-markdown infra-lint-scripts infra-lint-terraform infra-lint-workflows\n\ninfra-lint-scripts: ## Lint shell scripts\n\tshellcheck bin/**\n\ninfra-lint-terraform: ## Lint Terraform code\n\tterraform fmt -recursive -check infra\n\ninfra-lint-workflows: ## Lint GitHub actions\n\tactionlint\n\ninfra-format: ## Format infra code\n\tterraform fmt -recursive infra\n\ninfra-test-service: ## Run service layer infra test suite\n\tcd infra/test && go test -run TestService -v -timeout 30m\n\nlint-markdown: ## Lint Markdown docs for broken links\n\t./bin/lint-markdown\n\n########################\n## Release Management ##\n########################\n\n# Include project name in image name so that image name\n# does not conflict with other images during local development\nIMAGE_NAME := $(PROJECT_ROOT)-$(APP_NAME)\n\nGIT_REPO_AVAILABLE := $(shell git rev-parse --is-inside-work-tree 2>/dev/null)\n\n# Generate a unique tag based solely on the git hash.\n# This will be the identifier used for deployment via terraform.\nifdef GIT_REPO_AVAILABLE\nIMAGE_TAG := $(shell git rev-parse HEAD)\nelse\nIMAGE_TAG := \"unknown-dev.$(DATE)\"\nendif\n\n# Generate an informational tag so we can see where every image comes from.\nDATE := $(shell date -u '+%Y%m%d.%H%M%S')\nINFO_TAG := $(DATE).$(USER)\n\nrelease-build: ## Build release for $APP_NAME and tag it with current git hash\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\tcd $(APP_NAME) && $(MAKE) release-build \\\n\t\tOPTS=\"--tag $(IMAGE_NAME):latest --tag $(IMAGE_NAME):$(IMAGE_TAG) --load -t $(IMAGE_NAME):$(IMAGE_TAG) $(OPTIONAL_BUILD_FLAGS)\"\n\nrelease-publish: ## Publish release to $APP_NAME's build repository\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t./bin/publish-release $(APP_NAME) $(IMAGE_NAME) $(IMAGE_TAG)\n\nrelease-run-database-migrations: ## Run $APP_NAME's database migrations in $ENVIRONMENT\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t@:$(call check_defined, ENVIRONMENT, the name of the application environment e.g. \"prod\" or \"dev\")\n\t./bin/run-database-migrations $(APP_NAME) $(IMAGE_TAG) $(ENVIRONMENT)\n\nrelease-deploy: ## Deploy release to $APP_NAME's web service in $ENVIRONMENT\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t@:$(call check_defined, ENVIRONMENT, the name of the application environment e.g. \"prod\" or \"dev\")\n\t./bin/deploy-release $(APP_NAME) $(IMAGE_TAG) $(ENVIRONMENT)\n\nmetabase-deploy: ## Deploy metabase to $APP_NAME's web service in $ENVIRONMENT\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t@:$(call check_defined, ENVIRONMENT, the name of the application environment e.g. \"prod\" or \"dev\")\n\t./bin/deploy-metabase $(APP_NAME) $(IMAGE_TAG) $(ENVIRONMENT)\n\nrelease-image-name: ## Prints the image name of the release image\n\t@:$(call check_defined, APP_NAME, the name of subdirectory of /infra that holds the application's infrastructure code)\n\t@echo $(IMAGE_NAME)\n\nrelease-image-tag: ## Prints the image tag of the release image\n\t@echo $(IMAGE_TAG)\n\n########################\n## Scripts and Helper ##\n########################\n\nhelp: ## Prints the help documentation and info about each command\n\t@grep -Eh '^[[:print:]]+:.*?##' $(MAKEFILE_LIST) | \\\n\tsort -d | \\\n\tawk -F':.*?## ' '{printf \"\\033[36m%s\\033[0m\\t%s\\n\", $$1, $$2}' | \\\n\tcolumn -t -s \"$$(printf '\\t')\"\n\t@echo \"\"\n\t@echo \"APP_NAME=$(APP_NAME)\"\n\t@echo \"ENVIRONMENT=$(ENVIRONMENT)\"\n\t@echo \"IMAGE_NAME=$(IMAGE_NAME)\"\n\t@echo \"IMAGE_TAG=$(IMAGE_TAG)\"\n\t@echo \"INFO_TAG=$(INFO_TAG)\"\n\t@echo \"GIT_REPO_AVAILABLE=$(GIT_REPO_AVAILABLE)\"\n\t@echo \"SHELL=$(SHELL)\"\n\t@echo \"MAKE_VERSION=$(MAKE_VERSION)\"\n\t@echo \"MODULES=$(MODULES)\""}
{"path":"analytics/tests/etl/test_utils.py","language":"python","type":"code","directory":"analytics/tests/etl","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/etl/test_utils.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: OPERATIONS.md\nLanguage: md\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/OPERATIONS.md\nSize: 5.57 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/tests/etldb_test_01.json","language":"json","type":"code","directory":"analytics/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/etldb_test_01.json","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"## Deployment\n\n### Updating to our Terraform Version\n\n1. Install `tfenv`\n2. Get the terraform version to install from `terraform_version` this file: https://github.com/HHS/simpler-grants-gov/blob/main/.github/workflows/deploy.yml\n3. Follow `tfenv` instructions to instsall and utilize the given terraform version\n\n### Terraform State Locks\n\nTerraform state locks happen when multiple terraform deployments try to roll out simultaneously.\n\nYou can fix them on CLI by:\n\n1. Finding the job (via Github Action or otherwise) where the deployment failed. If you aren't sure, then it was probably in a Github Action. You can find a list of failing actions here: https://github.com/HHS/simpler-grants-gov/actions\n2. Wait for the deployment that caused the state lock to finish. If you can't find it, just wait 30 minutes.\n3. Identify the folder in which the state lock is happening. The `Path` attribute on the `Lock Info` block will identify this.\n4. Open up your terminal, setup AWS (eg. `export AWS_PROFILE=grants-bla-bla-bla` && `aws sso login`), and cd into the folder identified above\n5. Run `terraform init -backend-config=<ENVIRONMENT>.s3.tfbackend`, where `<ENVIRONMENT>` can be identified by the `Path` above.\n6. Run `terraform force-unlock -force <LOCK_ID>` where `<LOCK_ID>` is the value of `ID` in your state lock message.\n7. Re-run your deploy job\n\nSometimes CLI unlock won't work, that will look like (for example) the following error message:\n\n> terraform force-unlock -force <LOCK_ID>\n> Failed to unlock state: failed to retrieve lock info for lock ID <LOCK_ID>: unexpected end of JSON input\n\nWhen that happens, you need to unlock it via DynamoDB in the AWS console.\n\n1. Login to AWS\n2. [Open the DynamoDB console](https://us-east-1.console.aws.amazon.com/dynamodbv2/home?region=us-east-1)\n3. [Open the tables tab](https://us-east-1.console.aws.amazon.com/dynamodbv2/home?region=us-east-1#tables)\n4. Click on the state locks table. There should only be one.\n5. Click the `Explore Table Items` button\n6. Find the item that corresponds to the currently locked state, you can get that by again looking at the `Path` attribute in your locked job.\n7. Remove the `Digest` key, `Save and close`\n8. Re-run your deploy job\n\n## Scaling\n\nAll scaling options can be found in the following files:\n\nAPI:\n\n- [infra/api/app-config/dev.tf](infra/api/app-config/dev.tf)\n- [infra/api/app-config/staging.tf](infra/api/app-config/staging.tf)\n- [infra/api/app-config/prod.tf](infra/api/app-config/prod.tf)\n\nFrontend:\n\n- [infra/frontend/app-config/dev.tf](infra/frontend/app-config/dev.tf)\n- [infra/frontend/app-config/staging.tf](infra/frontend/app-config/staging.tf)\n- [infra/frontend/app-config/prod.tf](infra/frontend/app-config/prod.tf)\n\n### ECS\n\nScaling is handled by configuring the following values:\n\n- instance desired instance count\n- instance scaling minimum capacity\n- instance scaling maximum capacity\n- instance CPU\n- instance memory\n\nOur ECS instances auto scale based on both memory and CPU. You can view the autoscaling configuration\nhere: [infra/modules/service/autoscaling.tf](infra/modules/service/autoscaling.tf)\n\n### Database\n\nScaling is handled by configuring the following values:\n\n- Database minimum capacity\n- Database maximum capacity\n- Database instance count\n\nIn prod, the database maximum capacity is as high as it goes. Further scaling past the point will require scaling\nout the instance count. Effectively using the instance count scaling might require changes to our application layer.\n\n### OpenSearch\n\n- Search master instance type\n- Search data instance type\n- Search data volume size\n- Search data instance count\n- Search availability zone count\n\nWhen scaling openSearch, consider which attribute changes will trigger blue/green deploys, versus which attributes\ncan be edited in place. [You can find that information here](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/managedomains-configuration-changes.html). Requiring blue/green changes for the average configuration change is a\nnotable constraint of OpenSearch, relative to ECS and the Database.\n\n## Yearly Rotations\n\nWe manage several secret values that need to be rotated yearly.\n\n### Login.gov Certificates\n\n*These certificates were last updated in December 2024*\n\nWe need to manage a public certificate with login.gov for [private_jwt_auth](https://developers.login.gov/oidc/token/#client_assertion) in each of our environments.\n\nTo generate a certificate run:\n```shell\nopenssl req -nodes -x509 -days 365 -newkey rsa:2048 -keyout private.pem -out public.crt -subj \"/C=US/ST=Washington DC/L=Washington DC/O=Nava PBC/OU=Engineering/CN=Simpler Grants.gov/emailAddress=grantsteam@navapbc.com\"\n```\n\nNavigate to the [login.gov service provider page](https://dashboard.int.identitysandbox.gov/service_providers)\nand for each application edit it, and upload the public.crt file. Leave any prior cert files alone until we have\nswitched the API to using the new one.\n\nGo to SSM parameter store and change the value that maps to the `LOGIN_GOV_CLIENT_ASSERTION_PRIVATE_KEY` value\nfor the given environment to be the value from the `private.pem` key you generated.\n\nAfter the next deployment in an environment, we should be using the new keys, and can cleanup the old certificate.\n\n#### Prod Login.gov\n\nProd login.gov does not update immediately, and you must [request a deployment](https://developers.login.gov/production/#changes-to-production-applications) to get a certificate rotated.\n\nFor Prod, assume it will take at least two weeks from creating the certificate, before it is available for the API, and until it is, do not change the API's configured key."}
{"path":"analytics/tests/integrations/__init__.py","language":"python","type":"code","directory":"analytics/tests/integrations","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: README.md\nLanguage: md\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/README.md\nSize: 3.43 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/tests/integrations/extracts/__init__.py","language":"python","type":"code","directory":"analytics/tests/integrations/extracts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"A modernization effort for [Grants.gov](https://grants.gov/)\n\n## About the Project\n\nWe want Grants.gov to be an extremely simple, accessible, and easy-to-use tool for posting, finding, sharing, and applying for federal financial assistance. Our mission is to increase access to grants and improve the grants experience for everyone. Weâ€™re improving the way applicants search for and discover funding opportunities, making it easier to find and apply. For federal grantmaking agencies, weâ€™re making it easier for their communities to find the funding they need.\n\nGo to [Simpler.Grants.gov](https://simpler.grants.gov/) to learn about our transparent process and what weâ€™re doing now, or explore our existing user research and the findings that are guiding our work.\n\nSee [goals.md](./documentation/goals.md) for more information about the vision and goals for the project.\n\n## Core Team\n\nThe core team on the grants.gov project is a small group of content strategists, designers, developers, and product managers working for and with the Department of Health and Human Services, and other federal agencies, and community volunteers.\n\nAn up-to-date list of core team members can be found in [MAINTAINERS.md](./MAINTAINERS.md). At this time, the project is still building the core team and defining roles and responsibilities. We are eagerly seeking individuals who would like to join the community and help us define and fill these roles.\n\n## Repository Structure\n\n- [./.github](./.github) contains Github specific settings files and testing, linting, and CI/CD workflows\n- [./api](./api) contains an API built in Python using the Flask library\n- [./bin](./bin) contains scripts for managing infrastructure\n- [./documentation](./documentation) contains project guides, documentation, and decision records\n- [./frontend](./frontend) contains a web application built using Next.js\n- [./infra](./infra) contains Terraform modules and configuration for managing the AWS infrastructure\n\n## Development\n\n### API\n\nDocumentation for the API is linked to from the [API README.md](./api/README.md). For installation instructions, see the [development documentation](./documentation/api/development.md).\n\n### Front-end\n\nDocumentation and development instructions for the front-end are provided in the [Front-end README.md](./frontend/README.md).\n\n## Contributing\n\nThank you for considering contributing to an Open Source project of the US\nGovernment! For more information about our contribution guidelines, see\n[CONTRIBUTING.md](CONTRIBUTING.md) to learn more and join our community see our [wiki](https://wiki.simpler.hhs.gov).\n\n## Security\n\nFor more information about our Security, Vulnerability, and Responsible\nDisclosure Policies, see [SECURITY.md](SECURITY.md).\n\n## Authors and Maintainers\n\nFor more information about our Authors and maintainers, see [MAINTAINERS.md](MAINTAINERS.md).\n\nA full list of [contributors](https://github.com/HHS/simpler-grants-gov/graphs/contributors) can be found on GitHub.\n\n## Public domain\n\nThis project is licensed within in the public domain within the United States,\nand copyright and related rights in the work worldwide are waived through the\n[CC0 1.0 Universal public domain\ndedication](https://creativecommons.org/publicdomain/zero/1.0/).\n\nAll contributions to this project will be released under the CC0 dedication. By\nsubmitting a pull request or issue, you are agreeing to comply with this waiver\nof copyright interest."}
{"path":"analytics/tests/integrations/extracts/opportunity_tables_test_files/current_opportunity_summary.csv","language":"unknown","type":"code","directory":"analytics/tests/integrations/extracts/opportunity_tables_test_files","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/opportunity_tables_test_files/current_opportunity_summary.csv","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: SECURITY.md\nLanguage: md\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/SECURITY.md\nSize: 0.94 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/tests/integrations/extracts/opportunity_tables_test_files/lk_opportunity_category.csv","language":"unknown","type":"code","directory":"analytics/tests/integrations/extracts/opportunity_tables_test_files","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/opportunity_tables_test_files/lk_opportunity_category.csv","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"*Submit a vulnerability:* Unfortunately, we cannot accept secure submissions via\nemail or via GitHub Issues. Please use our website to submit vulnerabilities at\n[https://hhs.responsibledisclosure.com](https://hhs.responsibledisclosure.com).\nHHS maintains an acknowledgements page to recognize your efforts on behalf of\nthe American public, but you are also welcome to submit anonymously.\n\nReview the HHS Disclosure Policy and websites in scope:\n[https://www.hhs.gov/vulnerability-disclosure-policy/index.html](https://www.hhs.gov/vulnerability-disclosure-policy/index.html).\n\nThis policy describes *what systems and types of research* are covered under this\npolicy, *how to send* us vulnerability reports, and *how long* we ask security\nresearchers to wait before publicly disclosing vulnerabilities.\n\nIf you have other cybersecurity related questions, please contact us at\n[csirc@hhs.gov.](mailto:csirc@hhs.gov)."}
{"path":"analytics/tests/integrations/extracts/opportunity_tables_test_files/lk_opportunity_status.csv","language":"unknown","type":"code","directory":"analytics/tests/integrations/extracts/opportunity_tables_test_files","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/opportunity_tables_test_files/lk_opportunity_status.csv","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: analytics/.vscode/settings.json\nLanguage: json\nType: code\nDirectory: analytics/.vscode\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/.vscode/settings.json\nSize: 0.18 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/tests/integrations/extracts/opportunity_tables_test_files/opportunity.csv","language":"unknown","type":"code","directory":"analytics/tests/integrations/extracts/opportunity_tables_test_files","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/opportunity_tables_test_files/opportunity.csv","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":""}
{"path":"analytics/tests/integrations/extracts/opportunity_tables_test_files/opportunity_summary.csv","language":"unknown","type":"code","directory":"analytics/tests/integrations/extracts/opportunity_tables_test_files","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/opportunity_tables_test_files/opportunity_summary.csv","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: analytics/Dockerfile\nLanguage: unknown\nType: code\nDirectory: analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/Dockerfile\nSize: 3.70 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/tests/integrations/extracts/test_load_opportunity_data.py","language":"python","type":"code","directory":"analytics/tests/integrations/extracts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/test_load_opportunity_data.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"# The build stage that will be used to deploy to the various environments\n# needs to be called `release` in order to integrate with the repo's\n# top-level Makefile\nFROM python:3.13-slim AS base\n\n# Install poetry, the package manager.\n# https://python-poetry.org\nRUN pip install --no-cache-dir poetry==1.8.2 --upgrade\n\nRUN apt-get update \\\n  # Remove existing packages before installing their never versions\n  && apt-get remove --yes \\\n    build-essential \\\n    libc-dev \\\n    libpq-dev \\\n    postgresql \\\n    wget \\\n  # Install security updates\n  # https://pythonspeed.com/articles/security-updates-in-docker/\n  && apt-get upgrade --yes \\\n  && apt-get install --no-install-recommends --yes \\\n    build-essential \\\n    libc-dev \\\n    libpq-dev \\\n    postgresql \\\n    wget \\\n    libtasn1-6 \\\n  # Reduce the image size by clear apt cached lists\n  # Complies with https://github.com/codacy/codacy-hadolint/blob/master/codacy-hadolint/docs/description/DL3009.md\n  && rm -fr /var/lib/apt/lists/* \\\n  && rm /etc/ssl/private/ssl-cert-snakeoil.key\n\nARG RUN_UID\nARG RUN_USER\n\n# The following logic creates the RUN_USER home directory and the directory where\n# we will be storing the application in the image. This runs when the user is not root\nRUN : \"${RUN_USER:?RUN_USER and RUN_UID need to be set and non-empty.}\" && \\\n  [ \"${RUN_USER}\" = \"root\" ] || \\\n  (useradd --create-home --create --user-group --home \"/home/${RUN_USER}\" --uid ${RUN_UID} \"${RUN_USER}\" \\\n  && mkdir /analytics \\\n  && chown -R ${RUN_UID} \"/home/${RUN_USER}\" /analytics)\n\n# Set PYTHONPATH so that the tests can find the source code.\nENV PYTHONPATH=\"/analytics/src/:$PYTHONPATH\"\n\n#-----------\n# Dev image\n#-----------\n\nFROM base AS dev\nARG RUN_USER\n\n# In between ARG RUN_USER and USER ${RUN_USER}, the user is still root\n# If there is anything that needs to be ran as root, this is the spot\n\nUSER ${RUN_USER}\nWORKDIR /analytics\n\nCOPY pyproject.toml poetry.lock ./\n# Explicitly create a new virtualenv to avoid getting overridden by mounted .venv folders\nRUN poetry config virtualenvs.in-project false && poetry env use python\n# Install all dependencies including dev dependencies\nRUN poetry install --no-root --with dev\n\nCOPY . /analytics\n\n#---------\n# Release\n#---------\n\nFROM base AS release\nARG RUN_USER\n\n# Gunicorn requires this workaround to create writable temporary directory in\n# our readonly root file system. https://github.com/aws/containers-roadmap/issues/736\nRUN mkdir -p /tmp\nVOLUME [\"/tmp\"]\n\n# TODO(https://github.com/navapbc/template-application-flask/issues/23) Productionize the Docker image\n\nWORKDIR /analytics\n\nCOPY . /analytics\n\n# Remove any existing virtual environments that might exist. This\n# might happen if testing out building the release image from a local machine\n# that has a virtual environment within the project analytics folder.\nRUN rm -fr /analytics/.venv\n\n# Set virtualenv location to be in project to be easy to find\n# This will create a virtualenv in /analytics/.venv/\n# See https://python-poetry.org/docs/configuration/#virtualenvsin-project\n# See https://python-poetry.org/docs/configuration/#using-environment-variables\nENV POETRY_VIRTUALENVS_IN_PROJECT=true\n\n# Install production runtime dependencies only\nRUN poetry install --no-root --only main\n\n# Build the application binary (python wheel) defined in pyproject.toml\n# Note that this will only copy over python files, and files stated in the\n# include section in pyproject.toml.\nRUN poetry build --format wheel && \\\n  poetry run pip install --no-cache-dir dist/*.whl\n\n# Add project's virtual env to the PATH so we can directly run poetry scripts\n# defiend in pyproject.toml\nENV PATH=\"/analytics/.venv/bin:$PATH\"\n\n\nUSER ${RUN_USER}"}
{"path":"analytics/tests/integrations/github/__init__.py","language":"python","type":"code","directory":"analytics/tests/integrations/github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/github/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: analytics/Makefile\nLanguage: unknown\nType: code\nDirectory: analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/Makefile\nSize: 5.63 KB\nLast Modified: 2025-02-14T17:08:26.405Z"}
{"path":"analytics/tests/integrations/github/test_client.py","language":"python","type":"code","directory":"analytics/tests/integrations/github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/github/test_client.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"ORG ?= HHS\nREPO ?= simpler-grants-gov\nSPRINT_PROJECT ?= 13\nROADMAP_PROJECT ?= 12\nOUTPUT_DIR ?= $(shell mkdir -p data && echo data)\nCONFIG_DIR ?= config\nPROJECT_CONFIG_FILE ?= $(CONFIG_DIR)/github-projects.json\nISSUE_FILE ?= $(OUTPUT_DIR)/delivery-data.json\nSPRINT ?= @current\n# Names of the points and sprint fields in the GitHub project\nPOINTS_FIELD ?= Story Points\nSPRINT_FIELD ?= Sprint\nUNIT ?= points\nACTION ?= show-results\nMIN_TEST_COVERAGE ?= 80\nAPP_NAME ?= grants-analytics\nEFFECTIVE_DATE ?= $(shell date +\"%Y-%m-%d\")\n\n# Required for CI to work properly\nSHELL = /bin/bash -o pipefail\n\nifdef CI\n DOCKER_EXEC_ARGS := -T -e CI -e GH_TOKEN -e ANALYTICS_SLACK_BOT_TOKEN -e ANALYTICS_REPORTING_CHANNEL_ID\nelse\n DOCKER_EXEC_ARGS := -e GH_TOKEN\nendif\n\n# By default, all python/poetry commands will run inside of the docker container\n# if you wish to run this natively, add PY_RUN_APPROACH=local to your environment vars\n# You can set this by either running `export PY_RUN_APPROACH=local` in your shell or add\n# it to your ~/.zshrc file (and run `source ~/.zshrc`)\nifeq \"$(PY_RUN_APPROACH)\" \"local\"\nPOETRY := poetry run\nGITHUB := gh\nelse\nPOETRY := docker compose run $(DOCKER_EXEC_ARGS) --rm $(APP_NAME) poetry run\nGITHUB := docker compose run $(DOCKER_EXEC_ARGS) --rm $(APP_NAME) gh\nendif\n\n# Docker user configuration\n# This logic is to avoid issues with permissions and mounting local volumes,\n# which should be owned by the same UID for Linux distros. Mac OS can use root,\n# but it is best practice to run things as with least permission where possible\n\n# Can be set by adding user=<username> and/ or uid=<id> after the make command\n# If variables are not set explicitly: try looking up values from current\n# environment, otherwise fixed defaults.\n# uid= defaults to 0 if user= set (which makes sense if user=root, otherwise you\n# probably want to set uid as well).\nifeq ($(user),)\nRUN_USER ?= $(or $(strip $(USER)),nodummy)\nRUN_UID ?= $(or $(strip $(shell id -u)),4000)\nelse\nRUN_USER = $(user)\nRUN_UID = $(or $(strip $(uid)),0)\nendif\n\nexport RUN_USER\nexport RUN_UID\n\n##################\n# Build Commands #\n##################\n\ncheck-prereqs:\n\t@echo \"=> Checking for pre-requisites\"\n\t@if ! poetry --version; then echo \"=> Poetry isn't installed\"; fi\n\t@if ! gh --version; then echo \"=> GitHub CLI isn't installed\" && exit 1; fi\n\t@echo \"=> Poetry and GitHub CLI installed\"\n\ninstall: check-prereqs\n\t@echo \"=> Installing python dependencies\"\n\tpoetry install\n\nlogin:\n\t$(GITHUB) auth login\n\nsetup: install login\n\nbuild:\n\tdocker compose build\n\nrelease-build:\n\tdocker buildx build \\\n\t\t--target release \\\n\t\t--platform=linux/amd64 \\\n\t\t--build-arg RUN_USER=$(RUN_USER) \\\n\t\t--build-arg RUN_UID=$(RUN_UID) \\\n\t\t$(OPTS) \\\n\t\t.\n\n#########\n# Tests #\n#########\n\nunit-test:\n\t@echo \"=> Running unit tests\"\n\t@echo \"=============================\"\n\t$(POETRY) pytest --cov=src\n\ne2e-test:\n\t@echo \"=> Running end-to-end tests\"\n\t@echo \"=============================\"\n\t$(POETRY) pytest tests/integrations --cov=src --cov-append\n\ntest-audit: unit-test e2e-test\n\t@echo \"=> Running test coverage report\"\n\t@echo \"=============================\"\n\t$(POETRY) coverage report --show-missing --fail-under=$(MIN_TEST_COVERAGE)\n\n##########################\n# Formatting and Linting #\n##########################\n\nformat: ## runs code formatting\n\t@echo \"=> Running code formatting\"\n\t@echo \"=============================\"\n\t$(POETRY) black src tests\n\t$(POETRY) ruff check --fix src tests\n\t@echo \"=============================\"\n\t@echo \"=> Code formatting complete\"\n\nformat-check: ## runs code formatting checks\n\t@echo \"=> Running code formatting checks\"\n\t@echo \"=============================\"\n\t$(POETRY) black --check src tests\n\t$(POETRY) ruff check src tests\n\t@echo \"=============================\"\n\t@echo \"=> All checks succeeded\"\n\nlint: ## runs code quality checks\n\t@echo \"=> Running code quality checks\"\n\t@echo \"=============================\"\n\t$(POETRY) pylint src tests\n\t$(POETRY) mypy src\n\t@echo \"=============================\"\n\t@echo \"=> All checks succeeded\"\n\n#################\n# Data Commands #\n#################\n\ndb-migrate:\n\t@echo \"=> Migrating the database schema\"\n\t@echo \"=====================================================\"\n\t$(POETRY) analytics etl db_migrate\n\t@echo \"=====================================================\"\n\nopportunity-load:\n\t@echo \"=> Ingesting opportunity data into the database\"\n\t@echo \"=====================================================\"\n\t$(POETRY) analytics etl opportunity-load\n\t@echo \"=====================================================\"\n\ngh-transform-and-load:\n\t@echo \"=> Transforming and loading GitHub data into the database\"\n\t@echo \"=====================================================\"\n\t$(POETRY) analytics etl transform_and_load \\\n\t--issue-file $(ISSUE_FILE) \\\n\t--effective-date $(EFFECTIVE_DATE)\n\t@echo \"=====================================================\"\n\ngh-extract-transform-and-load:\n\t@echo \"=> Extract, transform and load GitHub data into analytics warebase\"\n\t@echo \"=====================================================\"\n\t$(POETRY) analytics etl extract_transform_and_load \\\n\t--config-file $(PROJECT_CONFIG_FILE) \\\n\t--effective-date $(EFFECTIVE_DATE)\n\t@echo \"=====================================================\"\n\ngh-db-data-import:\n\t@echo \"=> Importing sprint data to the database\"\n\t@echo \"=====================================================\"\n\t$(POETRY) analytics import db_import --delivery-file $(ISSUE_FILE)\n\ngh-data-export:\n\t@echo \"=> Exporting GitHub issue and sprint data for delivery metrics\"\n\t@echo \"=====================================================\"\n\t$(POETRY)  analytics export gh_delivery_data \\\n\t--config-file $(PROJECT_CONFIG_FILE) \\\n\t--output-file $(ISSUE_FILE) \\\n\t--temp-dir $(OUTPUT_DIR)"}
{"path":"analytics/tests/integrations/github/test_validation.py","language":"python","type":"code","directory":"analytics/tests/integrations/github","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/github/test_validation.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: analytics/README.md\nLanguage: md\nType: code\nDirectory: analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/README.md\nSize: 2.88 KB\nLast Modified: 2025-02-14T17:08:26.406Z"}
{"path":"analytics/tests/integrations/test_etldb.py","language":"python","type":"code","directory":"analytics/tests/integrations","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/test_etldb.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"## Introduction\n\nThis package encapsulates a data pipeline service. The service is responsible for extracting project data from GitHub and transforming the extracted data into rows in a data warehouse. We're using Metabase to provide data visualization and Business Intelligence for the data warehouse. As an example, our [dashboard that demonstrates the flow of Simpler Grants.gov Opportunity Data from the Operational DB to the Data Warehouse](http://metabase-prod-899895252.us-east-1.elb.amazonaws.com/dashboard/100-operational-data).\n\n## Project Directory Structure\n\nThe structure of the analytics codebase is outlined below, relative to the root of the `simpler-grants-gov` repo.\n\n```text\nroot\nâ”œâ”€â”€ analytics\nâ”‚   â””â”€â”€ src\nâ”‚       â””â”€â”€ analytics\nâ”‚           â””â”€â”€ datasets      Create re-usable data interfaces for calculating metrics\nâ”‚           â””â”€â”€ integrations  Integrate with external systems used to export data or metrics\nâ”‚   â””â”€â”€ tests\nâ”‚       â””â”€â”€ integrations      Integration tests, mostly for src/analytics/integrations\nâ”‚       â””â”€â”€ datasets          Unit tests for src/analytics/datasets\n|\nâ”‚   â””â”€â”€ config.py             Load configurations from environment vars or local .toml files\nâ”‚   â””â”€â”€ settings.toml         Default configuration settings, tracked by git\nâ”‚   â””â”€â”€ .secrets.toml         Gitignored file for secrets and configuration management\nâ”‚   â””â”€â”€ Makefile              Frequently used commands for setup, development, and CLI usage\nâ”‚   â””â”€â”€ pyproject.toml        Python project configuration file\n```\n\n## Data Pipeline\n\nThe service in this package provides capabilities to satisfy the middle step (denoted as \"ETL\") in the following data flow diagram:\n\n  `SGG Project Data â†’ GitHub â†’ ETL â†’ Postgres DW â†’ Metabase â†’ End User`\n\nThe service does not listen on a port or run as a daemon. Instead, it must be triggered manually, via `Make` commands on the command-line, or via a text-based interactive tool written in Python and referred to as CLI.\n\nIn current practice, the service is triggered daily via an AWS Step Function (akin to a cron job) orchestrated with Terraform. This results in a daily update to the analytics data warehouse in Postgres, and a visible data refresh for viewers of SGG program-level metrics dashboards in Metabase. \n\n##  Developer Information\n\nThe service is open-source and can be installed and run in a local development environment, which is useful for project maintainers and/or open source contributors. Follow the links below for more information:\n\n1. [Technical Overview](../documentation/analytics/technical-overview.md)\n2. [Getting Started Guide for Developers](../documentation/analytics/development.md)\n3. [Writing and Running Tests](../documentation/analytics/testing.md)\n4. [Usage Guide: Data Pipeline Service & CLI](../documentation/analytics/usage.md)"}
{"path":"analytics/tests/integrations/test_slack.py","language":"python","type":"code","directory":"analytics/tests/integrations","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/test_slack.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: analytics/config/github-projects.json\nLanguage: json\nType: code\nDirectory: analytics/config\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/config/github-projects.json\nSize: 0.41 KB\nLast Modified: 2025-02-14T17:08:26.406Z"}
{"path":"analytics/tests/logs/__init__.py","language":"python","type":"code","directory":"analytics/tests/logs","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/logs/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":""}
{"path":"analytics/tests/logs/test_ecs_background_task.py","language":"python","type":"code","directory":"analytics/tests/logs","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/logs/test_ecs_background_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: analytics/config.py\nLanguage: py\nType: code\nDirectory: analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/config.py\nSize: 1.18 KB\nLast Modified: 2025-02-14T17:08:26.406Z"}
{"path":"analytics/tests/logs/test_formatters.py","language":"python","type":"code","directory":"analytics/tests/logs","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/logs/test_formatters.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"\"\"\"\nimport os\nfrom typing import Optional\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom pydantic import Field\n\n# reads environment variables from .env files defaulting to \"local.env\"\nclass PydanticBaseEnvConfig(BaseSettings):\n    model_config = SettingsConfigDict(env_file=\"%s.env\" % os.getenv(\"ENVIRONMENT\", \"local\"), extra=\"allow\")\n\nclass DBSettings(PydanticBaseEnvConfig):\n     db_host: str = Field(alias=\"DB_HOST\")\n     name: str = Field(alias=\"DB_NAME\")\n     port: int = Field(5432,alias=\"DB_PORT\")\n     user: str = Field (alias=\"DB_USER\")\n     password: Optional[str] = Field(None, alias=\"DB_PASSWORD\")\n     ssl_mode: str = Field(\"require\", alias=\"DB_SSL_MODE\")\n     db_schema: str = Field (\"app\", alias=\"DB_SCHEMA\")\n     slack_bot_token: str = Field(alias=\"ANALYTICS_SLACK_BOT_TOKEN\")\n     github_token: str = Field(alias=\"GH_TOKEN\")\n     reporting_channel_id: str = Field(alias=\"ANALYTICS_REPORTING_CHANNEL_ID\")\n     aws_region: Optional[str] = Field(None, alias=\"AWS_REGION\")\n     local_env: bool = True if os.getenv(\"ENVIRONMENT\", \"local\") == \"local\" else False\n\ndef get_db_settings() -> DBSettings:\n     return DBSettings()"}
{"path":"analytics/tests/logs/test_logging.py","language":"python","type":"code","directory":"analytics/tests/logs","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/logs/test_logging.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: analytics/docker-compose.yml\nLanguage: yml\nType: code\nDirectory: analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/docker-compose.yml\nSize: 0.85 KB\nLast Modified: 2025-02-14T17:08:26.406Z"}
{"path":"analytics/tests/logs/test_pii.py","language":"python","type":"code","directory":"analytics/tests/logs","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/logs/test_pii.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"grants-analytics-db:\n    image: postgres:15-alpine\n    container_name: grants-analytics-db\n    command: postgres -c \"log_lock_waits=on\" -N 1000 -c \"fsync=off\"\n    env_file: ./local.env\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - grantsanalyticsdbdata:/var/lib/postgresql/data\n\n  grants-analytics:\n    build:\n      context: .\n      target: dev\n      args:\n        - RUN_UID=${RUN_UID:-4000}\n        - RUN_USER=${RUN_USER:-analytics}\n    container_name: grants-analytics\n    env_file: ./local.env\n    volumes:\n      - .:/analytics\n      - ~/.ssh:/home/${RUN_USER:-analytics}/.ssh\n    depends_on:\n      - grants-analytics-db\n\n  grants-metabase:\n    image: metabase/metabase:latest\n    container_name: grants-metabase\n    volumes:\n    - /dev/urandom:/dev/random:ro\n    ports:\n      - 3100:3000\n    env_file: ./local.env\n\nvolumes:\n  grantsanalyticsdbdata:"}
{"path":"analytics/tests/ruff.toml","language":"unknown","type":"code","directory":"analytics/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/ruff.toml","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: analytics/local.env\nLanguage: env\nType: code\nDirectory: analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/local.env\nSize: 1.51 KB\nLast Modified: 2025-02-14T17:08:26.406Z"}
{"path":"analytics/tests/test_cli.py","language":"python","type":"code","directory":"analytics/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/test_cli.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"# These are used by the Postgres image to create the admin user\nPOSTGRES_USER=app\nPOSTGRES_PASSWORD=secret123\n\n# Set DB_HOST to localhost if accessing a non-dockerized database\nDB_HOST=grants-analytics-db\nDB_NAME=app\nDB_USER=app\nDB_PASSWORD=secret123\nDB_SSL_MODE=allow\nDB_SCHEMA=public\n\n# When an error occurs with a SQL query,\n# whether or not to hide the parameters which\n# could contain sensitive information.\nHIDE_SQL_PARAMETER_LOGS=TRUE\n\n##################################\n# Metabase Environment Variables #\n##################################\n\nMB_DB_TYPE=postgres\nMB_DB_DBNAME=app\nMB_DB_PORT=5432\nMB_DB_USER=app\nMB_DB_PASS=secret123\nMB_DB_HOST=grants-analytics-db\n\n###########################\n# Secret Configuration   #\n###########################\n# Do not add these values to this file\n# to avoid mistakenly committing them.\n# Set these in your shell\n# by doing `export ANALYTICS_REPORTING_CHANNEL_ID=whatever`\nANALYTICS_REPORTING_CHANNEL_ID=DO_NOT_SET_HERE\nANALYTICS_SLACK_BOT_TOKEN=DO_NOT_SET_HERE\nGH_TOKEN=DO_NOT_SET_HERE\n\n############################\n# Logging\n############################\n\n# Can be \"human-readable\" OR \"json\"\nLOG_FORMAT=human-readable\n\n# Set log level. Valid values are DEBUG, INFO, WARNING, CRITICAL\n# LOG_LEVEL=INFO\n\n# Change the message length for the human readable formatter\n# LOG_HUMAN_READABLE_FORMATTER__MESSAGE_WIDTH=50\n\n############################\n# S3\n############################\n\nAPI_ANALYTICS_DB_EXTRACTS_PATH=/tmp"}
{"path":"analytics/tests/test_version.py","language":"python","type":"code","directory":"analytics/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/test_version.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: analytics/pyproject.toml\nLanguage: toml\nType: code\nDirectory: analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/pyproject.toml\nSize: 2.57 KB\nLast Modified: 2025-02-14T17:08:26.407Z"}
{"path":"api/Dockerfile","language":"unknown","type":"code","directory":"api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/Dockerfile","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"[tool.poetry.scripts]\nanalytics = \"analytics.cli:app\"\ndb-migrate = \"analytics.cli:migrate_database\"\n\n[tool.poetry.dependencies]\ndynaconf = \"^3.2.4\"\njinja2 = \">=3.1.5\"\nkaleido = \"0.2.1\"\nnotebook = \"^7.0.0\"                               # Goal is to replace this with another method of presenting charts\npandas = \"^2.0.3\"\npandas-stubs = \"^2.0.2.230605\"\nplotly = \"^5.15.0\"\npydantic = \"^2.0.3\"\npython = \"~3.13\"\nslack-sdk = \"^3.23.0\"\ntyper = { extras = [\"all\"], version = \"^0.15.0\" }\nsqlalchemy = \"^2.0.30\"\npydantic-settings = \"^2.3.4\"\nboto3 = \"^1.35.56\"\nboto3-stubs = \"^1.35.56\"\npsycopg = \"^3.2.3\"\nsmart-open = \"^7.0.5\"\n\n[tool.poetry.group.dev.dependencies]\nblack = \"^24.0.0\"\nmypy = \"^1.4.1\"\npylint = \"^3.0.2\"\npytest = \"^8.0.0\"\npytest-cov = \"^5.0.0\"\nruff = \"^0.8.0\"\nsafety = \"^3.0.0\"\ntypes-requests = \"^2.32.0.20241016\"\nmoto = \"^5.0.22\"\n\n[build-system]\nbuild-backend = \"poetry.core.masonry.api\"\nrequires = [\"poetry-core\"]\n\n[tool.mypy]\npython_version = \"3.12\"\n\n[[tool.mypy.overrides]]\nignore_missing_imports = true\nmodule = [\"plotly.*\", \"dynaconf.*\"]\n\n[tool.pylint.\"MESSAGE CONTROL\"]\ndisable = [\n  \"R0801\", # duplicate-code\n  \"W0511\", # fix-me\n  \"R0913\", # too-many-arguments\n  \"R0917\", # too-many-positional-arguments\n  \"R0902\", # too-many-instance-attributes\n  \"R0903\", # too-few-public-methods\n  \"W1514\", # unspecified-encoding\n]\n\n[tool.ruff]\nline-length = 100\n\n[tool.ruff.lint]\nselect = [\"ALL\"]\nignore = [\n  \"D203\",    # no blank line before class\n  \"D212\",    # multi-line summary first line\n  \"FIX002\",  # line contains TODO\n  \"PD901\",   # pandas df variable name\n  \"PLR0913\", # Too many arguments to function call\n  \"PTH123\",  # `open()` should be replaced by `Path.open()`\n  \"RUF012\",  # Mutable class attributes should be annotated with `typing.ClassVar`\n  \"TD003\",   # missing an issue link on TODO\n  \"FA102\",   # Adding \"from __future__ import annotations\" to any new-style type annotation\n]\n\n[tool.ruff.lint.flake8-builtins]\n# Don't complain about shadowing the builtin format function\n# Many builtin classes we extend use format as a function name\nbuiltins-ignorelist = [\"format\"]\n\n[tool.pytest.ini_options]\nfilterwarnings = [\n  # kaleido is throwing a Deprecation warning in one of its dependencies\n  # TODO(widal001): 2022-12-12 - Try removing after Kaleido issues a new release\n  'ignore:.*setDaemon\\(\\) is deprecated.*:DeprecationWarning',\n]"}
{"path":"api/Makefile","language":"unknown","type":"code","directory":"api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/Makefile","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: analytics/reporting.ipynb\nLanguage: ipynb\nType: code\nDirectory: analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/reporting.ipynb\nSize: 73.47 KB\nLast Modified: 2025-02-14T17:08:26.407Z"}
{"path":"api/README.md","language":"markdown","type":"code","directory":"api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.122Z","content":""}
{"path":"api/artillery-load-test.yml","language":"yaml","type":"code","directory":"api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/artillery-load-test.yml","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: analytics/src/analytics/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/__init__.py\nSize: 0.05 KB\nLast Modified: 2025-02-14T17:08:26.407Z"}
{"path":"api/bin/__init__.py","language":"python","type":"code","directory":"api/bin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":""}
{"path":"api/bin/create_erds.py","language":"python","type":"code","directory":"api/bin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/create_erds.py","size":1631931,"lastModified":"2025-02-14T17:08:31.122Z","content":"File: analytics/src/analytics/cli.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/cli.py\nSize: 7.65 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/bin/execute_sql_rds.sh","language":"unknown","type":"code","directory":"api/bin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/execute_sql_rds.sh","size":0,"lastModified":"2025-02-14T17:08:31.122Z","content":"import logging\nimport logging.config\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Annotated\n\nimport typer\nfrom sqlalchemy import text\n\nfrom analytics.datasets.etl_dataset import EtlDataset\nfrom analytics.datasets.issues import GitHubIssues\nfrom analytics.etl.github import GitHubProjectConfig, GitHubProjectETL\nfrom analytics.etl.utils import load_config\nfrom analytics.integrations import etldb\nfrom analytics.integrations.db import PostgresDbClient\nfrom analytics.integrations.extracts.load_opportunity_data import (\n    extract_copy_opportunity_data,\n)\nfrom analytics.logs import init as init_logging\nfrom analytics.logs.app_logger import init_app\nfrom analytics.logs.ecs_background_task import ecs_background_task\n\nlogger = logging.getLogger(__name__)\n\n# fmt: off\n# Instantiate typer options with help text for the commands below\nCONFIG_FILE_ARG = typer.Option(help=\"Path to JSON file with configurations for this entrypoint\")\nISSUE_FILE_ARG = typer.Option(help=\"Path to file with exported issue data\")\nOUTPUT_FILE_ARG = typer.Option(help=\"Path to file where exported data will be saved\")\nOUTPUT_DIR_ARG = typer.Option(help=\"Path to directory where output files will be saved\")\nTMP_DIR_ARG = typer.Option(help=\"Path to directory where intermediate files will be saved\")\nOWNER_ARG = typer.Option(help=\"Name of the GitHub project owner, e.g. HHS\")\nPROJECT_ARG = typer.Option(help=\"Number of the GitHub project, e.g. 13\")\nEFFECTIVE_DATE_ARG = typer.Option(help=\"YYYY-MM-DD effective date to apply to each imported row\")\n# fmt: on\n\n# instantiate the main CLI entrypoint\napp = typer.Typer()\n# instantiate sub-commands for exporting data and calculating metrics\nexport_app = typer.Typer()\nimport_app = typer.Typer()\netl_app = typer.Typer()\n# add sub-commands to main entrypoint\napp.add_typer(export_app, name=\"export\", help=\"Export data needed to calculate metrics\")\napp.add_typer(import_app, name=\"import\", help=\"Import data into the database\")\napp.add_typer(etl_app, name=\"etl\", help=\"Transform and load local file\")\n\n\ndef init() -> None:\n    \"\"\"Shared init function for all scripts.\"\"\"\n    # Setup logging\n    init_logging(__package__)\n    init_app(logging.root)\n\n\n@app.callback()\ndef callback() -> None:\n    \"\"\"Analyze data about the Simpler.Grants.gov project.\"\"\"\n    # If you override this callback, remember to call init()\n    init()\n\n\n# ===========================================================\n# Export commands\n# ===========================================================\n\n\n@export_app.command(name=\"gh_delivery_data\")\ndef export_github_data(\n    config_file: Annotated[str, CONFIG_FILE_ARG],\n    output_file: Annotated[str, OUTPUT_FILE_ARG],\n    temp_dir: Annotated[str, TMP_DIR_ARG],\n) -> None:\n    \"\"\"Export and flatten metadata about GitHub issues used for delivery metrics.\"\"\"\n    # Configure ETL pipeline\n    config_path = Path(config_file)\n    if not config_path.exists():\n        typer.echo(f\"Not a path to a valid config file: {config_path}\")\n    config = load_config(config_path, GitHubProjectConfig)\n    config.temp_dir = temp_dir\n    config.output_file = output_file\n    # Run ETL pipeline\n    GitHubProjectETL(config).run()\n\n\n# ===========================================================\n# Import commands\n# ===========================================================\n\n\n@import_app.command(name=\"test_connection\")\ndef test_connection() -> None:\n    \"\"\"Test function that ensures the DB connection works.\"\"\"\n    client = PostgresDbClient()\n    connection = client.connect()\n\n    # Test INSERT INTO action\n    result = connection.execute(\n        text(\n            \"INSERT INTO audit_log (topic,timestamp, end_timestamp, user_id, details)\"\n            \"VALUES('test','2024-06-11 10:41:15','2024-06-11 10:54:15',87654,'test from command');\",\n        ),\n    )\n    # Test SELECT action\n    result = connection.execute(text(\"SELECT * FROM audit_log WHERE user_id=87654;\"))\n    for row in result:\n        print(row)\n    # commits the transaction to the db\n    connection.commit()\n    result.close()\n\n\n@import_app.command(name=\"db_import\")\ndef export_json_to_database(delivery_file: Annotated[str, ISSUE_FILE_ARG]) -> None:\n    \"\"\"Import JSON data to the database.\"\"\"\n    logger.info(\"Beginning import\")\n\n    # Get the database engine and establish a connection\n    client = PostgresDbClient()\n\n    # Load data from the sprint board\n    issues = GitHubIssues.from_json(delivery_file)\n\n    issues.to_sql(\n        output_table=\"github_project_data\",\n        engine=client.engine(),\n        replace_table=True,\n    )\n    rows = len(issues.to_dict())\n    logger.info(\"Number of rows in table: %s\", rows)\n\n\n# ===========================================================\n# Etl commands\n# ===========================================================\n\n\n@etl_app.command(name=\"db_migrate\")\n@ecs_background_task(\"db_migrate\")\ndef migrate_database() -> None:\n    \"\"\"Initialize etl database.\"\"\"\n    logger.info(\"initializing database\")\n    etldb.migrate_database()\n    logger.info(\"done\")\n\n\n@etl_app.command(name=\"transform_and_load\")\ndef transform_and_load(\n    issue_file: Annotated[str, ISSUE_FILE_ARG],\n    effective_date: Annotated[str, EFFECTIVE_DATE_ARG],\n) -> None:\n    \"\"\"Transform and load etl data.\"\"\"\n    # validate effective date arg\n    datestamp = validate_effective_date(effective_date)\n    if datestamp is None:\n        logger.error(\n            \"FATAL ERROR: malformed effective date, expected YYYY-MM-DD format\",\n        )\n        return\n    logger.info(\"running transform and load with effective date %s\", datestamp)\n\n    # hydrate a dataset instance from the input data\n    dataset = EtlDataset.load_from_json_file(file_path=issue_file)\n\n    # sync data to db\n    etldb.sync_data(dataset, datestamp)\n\n    # finish\n    logger.info(\"transform and load is done\")\n\n\n@etl_app.command(name=\"extract_transform_and_load\")\ndef extract_transform_and_load(\n    config_file: Annotated[str, CONFIG_FILE_ARG],\n    effective_date: Annotated[str, EFFECTIVE_DATE_ARG],\n) -> None:\n    \"\"\"Export data from GitHub, transform it, and load into analytics warehouse.\"\"\"\n    # get configuration\n    config_path = Path(config_file)\n    if not config_path.exists():\n        typer.echo(f\"Not a path to a valid config file: {config_path}\")\n    config = load_config(config_path, GitHubProjectConfig)\n\n    # validate effective date arg\n    datestamp = validate_effective_date(effective_date)\n    if datestamp is None:\n        logger.error(\n            \"FATAL ERROR: malformed effective date, expected YYYY-MM-DD format\",\n        )\n        return\n    logger.info(\"running transform and load with effective date %s\", datestamp)\n\n    # extract data from GitHub\n    logger.info(\"extracting data from GitHub\")\n    extracted_json = GitHubProjectETL(config).extract_and_transform_in_memory()\n\n    # hydrate a dataset instance from the input data\n    logger.info(\"transforming data\")\n    dataset = EtlDataset.load_from_json_object(json_data=extracted_json)\n\n    # sync dataset to db\n    logger.info(\"writing to database\")\n    etldb.sync_data(dataset, datestamp)\n\n    logger.info(\"workflow is done!\")\n\n\ndef validate_effective_date(effective_date: str) -> str | None:\n    \"\"\"Validate that string value conforms to effective date expected format.\"\"\"\n    stamp = None\n\n    try:\n        dateformat = \"%Y-%m-%d\"\n        stamp = (\n            datetime.strptime(effective_date, dateformat)\n            .astimezone()\n            .strftime(dateformat)\n        )\n    except ValueError:\n        stamp = None\n\n    return stamp\n\n\n@etl_app.command(name=\"opportunity-load\")\ndef load_opportunity_data() -> None:\n    \"\"\"Grabs data from s3 bucket and loads it into opportunity tables.\"\"\"\n    extract_copy_opportunity_data()"}
{"path":"api/bin/setup-env-override-file.sh","language":"unknown","type":"code","directory":"api/bin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/setup-env-override-file.sh","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/datasets/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/datasets\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/datasets/__init__.py\nSize: 0.07 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/bin/setup_localstack.py","language":"python","type":"code","directory":"api/bin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/setup_localstack.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":""}
{"path":"api/bin/sql/select_from_foreign_table.sql","language":"unknown","type":"code","directory":"api/bin/sql","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/sql/select_from_foreign_table.sql","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/datasets/base.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/datasets\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/datasets/base.py\nSize: 4.80 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/bin/sql/table_list.sql","language":"unknown","type":"code","directory":"api/bin/sql","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/sql/table_list.sql","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"import numpy as np\nimport pandas as pd\nfrom sqlalchemy import Engine\n\nfrom analytics.datasets.utils import dump_to_json, load_json_file\n\n\nclass BaseDataset:\n    \"\"\"Base class for all datasets.\"\"\"\n\n    def __init__(self, df: pd.DataFrame) -> None:\n        \"\"\"Instantiate the dataset.\"\"\"\n        self.df = df\n\n    @classmethod\n    def from_csv(cls, file_path: str | Path) -> Self:\n        \"\"\"Load and instantiate the dataset from a csv file.\"\"\"\n        return cls(df=pd.read_csv(file_path))\n\n    @classmethod\n    def from_dict(cls, data: list[dict]) -> Self:\n        \"\"\"Load the dataset from a list of python dictionaries representing records.\"\"\"\n        return cls(df=pd.DataFrame(data))\n\n    @classmethod\n    def from_json(cls, file_path: str | Path) -> Self:\n        \"\"\"Load the dataset from a JSON file.\"\"\"\n        data = load_json_file(str(file_path))\n        return cls(df=pd.DataFrame(data))\n\n    def to_sql(\n        self,\n        output_table: str,\n        engine: Engine,\n        *,\n        replace_table: bool = True,\n    ) -> None:\n        \"\"\"\n        Write the contents of a pandas DataFrame to a SQL table.\n\n        This function takes a pandas DataFrame (`self.df`), an output table name (`output_table`),\n        and a SQLAlchemy Engine object (`engine`) as required arguments. It optionally accepts\n        a `replace_table` argument (default: True) that determines how existing data in the\n        target table is handled.\n\n        **Parameters:**\n\n        * self (required): The instance of the class containing the DataFrame (`self.df`)\n            to be written to the database.\n        * output_table (str, required): The name of the table in the database where the\n            data will be inserted.\n        * engine (sqlalchemy.engine.Engine, required): A SQLAlchemy Engine object representing\n            the connection to the database.\n        * replace_table (bool, default=True):\n            * If True (default), the function will completely replace the contents of the\n            existing table with the data from the DataFrame. (if_exists=\"replace\")\n            * If False, the data from the DataFrame will be appended to the existing table.\n            (if_exists=\"append\")\n\n        **Returns:**\n\n        * None\n\n        **Raises:**\n\n        * Potential exceptions raised by the underlying pandas.to_sql function, such as\n            database connection errors or errors related to data type mismatches.\n        \"\"\"\n        if replace_table:\n            self.df.to_sql(output_table, engine, if_exists=\"replace\", index=False)\n        else:\n            self.df.to_sql(output_table, engine, if_exists=\"append\", index=False)\n\n    @classmethod\n    def from_sql(\n        cls,\n        source_table: str,\n        engine: Engine,\n    ) -> Self:\n        \"\"\"\n        Read data from a SQL table into a pandas DataFrame and creates an instance of the current class.\n\n        This function takes a source table name (`source_table`) and a SQLAlchemy Engine object (`engine`) as required arguments.\n        It utilizes pandas.read_sql to retrieve the data from the database and then creates a new instance of the current class (`cls`) initialized with the resulting DataFrame (`df`).\n\n        **Parameters:**\n\n        * cls (class, required): The class that will be instantiated with the data from the\n        SQL table. This allows for creating objects of the same type as the function is called on.\n        * source_table (str, required): The name of the table in the database from which the\n        data will be read.\n        * engine (sqlalchemy.engine.Engine, required): A SQLAlchemy Engine object representing\n        the connection to the database.\n\n        **Returns:**\n\n        * Self: A new instance of the current class (`cls`) initialized with the DataFrame\n        containing the data from the SQL table.\n\n        **Raises:**\n\n        * Potential exceptions raised by the underlying pandas.read_sql function, such as\n        database connection errors or errors related to data type mismatches.\n        \"\"\"\n        return cls(df=pd.read_sql(source_table, engine))\n\n    def to_csv(\n        self,\n        output_file: Path,\n        *,  # force include_index to be passed as keyword instead of positional arg\n        include_index: bool = False,\n    ) -> None:\n        \"\"\"Export the dataset to a csv.\"\"\"\n        return self.df.to_csv(output_file, index=include_index)\n\n    def to_dict(self) -> list[dict]:\n        \"\"\"Export the dataset to a list of python dictionaries representing records.\"\"\"\n        return self.df.replace([np.nan], [None], regex=False).to_dict(orient=\"records\")\n\n    def to_json(self, output_file: str) -> None:\n        \"\"\"Dump dataset to JSON.\"\"\"\n        return dump_to_json(output_file, self.to_dict())"}
{"path":"api/bin/wait-for-api.sh","language":"unknown","type":"code","directory":"api/bin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/wait-for-api.sh","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/datasets/etl_dataset.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/datasets\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/datasets/etl_dataset.py\nSize: 6.23 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/bin/wait-for-local-db.sh","language":"unknown","type":"code","directory":"api/bin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/wait-for-local-db.sh","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"This is a sub-class of BaseDataset that models\nquad, deliverable, epic, issue, and sprint data.\n\"\"\"\n\nfrom enum import Enum\nfrom typing import Any, Self\n\nimport pandas as pd\nfrom numpy.typing import NDArray\n\nfrom analytics.datasets.base import BaseDataset\nfrom analytics.datasets.utils import (\n    load_json_data_as_df,\n    load_json_data_as_df_from_object,\n)\n\n\nclass EtlEntityType(Enum):\n    \"\"\"Define entity types in the db schema.\"\"\"\n\n    DELIVERABLE = \"deliverable\"\n    EPIC = \"epic\"\n    ISSUE = \"issue\"\n    PROJECT = \"project\"\n    QUAD = \"quad\"\n    SPRINT = \"sprint\"\n\n\nclass EtlDataset(BaseDataset):\n    \"\"\"Encapsulate data exported from github.\"\"\"\n\n    COLUMN_MAP = {\n        \"deliverable_url\": \"deliverable_ghid\",\n        \"deliverable_title\": \"deliverable_title\",\n        \"deliverable_pillar\": \"deliverable_pillar\",\n        \"deliverable_status\": \"deliverable_status\",\n        \"epic_url\": \"epic_ghid\",\n        \"epic_title\": \"epic_title\",\n        \"issue_url\": \"issue_ghid\",\n        \"issue_title\": \"issue_title\",\n        \"issue_parent\": \"issue_parent\",\n        \"issue_type\": \"issue_type\",\n        \"issue_is_closed\": \"issue_is_closed\",\n        \"issue_opened_at\": \"issue_opened_at\",\n        \"issue_closed_at\": \"issue_closed_at\",\n        \"issue_points\": \"issue_points\",\n        \"issue_status\": \"issue_status\",\n        \"project_owner\": \"project_name\",\n        \"project_number\": \"project_ghid\",\n        \"sprint_id\": \"sprint_ghid\",\n        \"sprint_name\": \"sprint_name\",\n        \"sprint_start\": \"sprint_start\",\n        \"sprint_length\": \"sprint_length\",\n        \"sprint_end\": \"sprint_end\",\n        \"quad_id\": \"quad_ghid\",\n        \"quad_name\": \"quad_name\",\n        \"quad_start\": \"quad_start\",\n        \"quad_length\": \"quad_length\",\n        \"quad_end\": \"quad_end\",\n    }\n\n    @classmethod\n    def load_from_json_file(cls, file_path: str) -> Self:\n        \"\"\"\n        Load the input json file and instantiates an instance of EtlDataset.\n\n        Parameters\n        ----------\n        file_path: str\n            Path to the local json file containing data exported from GitHub\n\n        Returns\n        -------\n        Self:\n            An instance of the EtlDataset dataset class\n\n        \"\"\"\n        # load input datasets\n        df = load_json_data_as_df(\n            file_path=file_path,\n            column_map=cls.COLUMN_MAP,\n            date_cols=None,\n        )\n\n        # transform entity id columns\n        df = EtlDataset.transform_entity_id_columns(df)\n\n        return cls(df)\n\n    @classmethod\n    def load_from_json_object(cls, json_data: list) -> Self:\n        \"\"\"\n        Instantiate an instance of EtlDataset from a json object.\n\n        Parameters\n        ----------\n        json_data: list\n            In-memory json object containing data exported from GitHub\n\n        Returns\n        -------\n        Self:\n            An instance of the EtlDataset dataset class\n\n        \"\"\"\n        # load input datasets\n        df = load_json_data_as_df_from_object(\n            json_data=json_data,\n            column_map=cls.COLUMN_MAP,\n            date_cols=None,\n        )\n\n        # transform entity id columns\n        df = EtlDataset.transform_entity_id_columns(df)\n\n        return cls(df)\n\n    @classmethod\n    def transform_entity_id_columns(cls, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Remove fqdn from urls.\"\"\"\n        prefix = \"https://github.com/\"\n        for col in (\"deliverable_ghid\", \"epic_ghid\", \"issue_ghid\", \"issue_parent\"):\n            df[col] = df[col].str.replace(prefix, \"\")\n\n        return df\n\n    # QUAD getters\n\n    def get_quad(self, quad_ghid: str) -> pd.Series:\n        \"\"\"Fetch data about a given quad.\"\"\"\n        query_string = f\"quad_ghid == '{quad_ghid}'\"\n        return self.df.query(query_string).iloc[0]\n\n    def get_quad_ghids(self) -> NDArray[Any]:\n        \"\"\"Fetch an array of unique non-null quad ghids.\"\"\"\n        df = self.df[self.df.quad_ghid.notna()]\n        return df.quad_ghid.unique()\n\n    # DELIVERABLE getters\n\n    def get_deliverable(self, deliverable_ghid: str) -> pd.Series:\n        \"\"\"Fetch data about a given deliverable.\"\"\"\n        query_string = f\"deliverable_ghid == '{deliverable_ghid}'\"\n        return self.df.query(query_string).iloc[0]\n\n    def get_deliverable_ghids(self) -> NDArray[Any]:\n        \"\"\"Fetch an array of unique non-null deliverable ghids.\"\"\"\n        df = self.df[self.df.deliverable_ghid.notna()]\n        return df.deliverable_ghid.unique()\n\n    # SPRINT getters\n\n    def get_sprint(self, sprint_ghid: str) -> pd.Series:\n        \"\"\"Fetch data about a given sprint.\"\"\"\n        query_string = f\"sprint_ghid == '{sprint_ghid}'\"\n        return self.df.query(query_string).iloc[0]\n\n    def get_sprint_ghids(self) -> NDArray[Any]:\n        \"\"\"Fetch an array of unique non-null sprint ghids.\"\"\"\n        df = self.df[self.df.sprint_ghid.notna()]\n        return df.sprint_ghid.unique()\n\n    # EPIC getters\n\n    def get_epic(self, epic_ghid: str) -> pd.Series:\n        \"\"\"Fetch data about a given epic.\"\"\"\n        query_string = f\"epic_ghid == '{epic_ghid}'\"\n        return self.df.query(query_string).iloc[0]\n\n    def get_epic_ghids(self) -> NDArray[Any]:\n        \"\"\"Fetch an array of unique non-null epic ghids.\"\"\"\n        df = self.df[self.df.epic_ghid.notna()]\n        return df.epic_ghid.unique()\n\n    # ISSUE getters\n\n    def get_issue(self, issue_ghid: str) -> pd.Series:\n        \"\"\"Fetch data about a given issue.\"\"\"\n        query_string = f\"issue_ghid == '{issue_ghid}'\"\n        return self.df.query(query_string).iloc[0]\n\n    def get_issues(self, issue_ghid: str) -> pd.DataFrame:\n        \"\"\"Fetch data about a given issue.\"\"\"\n        query_string = f\"issue_ghid == '{issue_ghid}'\"\n        return self.df.query(query_string)\n\n    def get_issue_ghids(self) -> NDArray[Any]:\n        \"\"\"Fetch an array of unique non-null issue ghids.\"\"\"\n        df = self.df[self.df.issue_ghid.notna()]\n        return df.issue_ghid.unique()\n\n    # PROJECT getters\n\n    def get_project(self, project_ghid: int) -> pd.Series:\n        \"\"\"Fetch data about a given project.\"\"\"\n        query_string = f\"project_ghid == {project_ghid}\"\n        return self.df.query(query_string).iloc[0]\n\n    def get_project_ghids(self) -> NDArray[Any]:\n        \"\"\"Fetch an array of unique non-null project ghids.\"\"\"\n        df = self.df[self.df.project_ghid.notna()]\n        return df.project_ghid.unique()"}
{"path":"api/bin/wait-for-local-opensearch.sh","language":"unknown","type":"code","directory":"api/bin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/wait-for-local-opensearch.sh","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/datasets/issues.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/datasets\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/datasets/issues.py\nSize: 5.79 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/docker-compose.debug.yml","language":"yaml","type":"code","directory":"api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/docker-compose.debug.yml","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"import logging\nfrom enum import Enum\n\nimport pandas as pd\nfrom pydantic import BaseModel, Field, computed_field\n\nfrom analytics.datasets.base import BaseDataset\n\nlogger = logging.getLogger(__name__)\n\n# ===============================================================\n# Dataset schema and enums\n# ===============================================================\n\n\nclass IssueType(Enum):\n    \"\"\"Supported issue types.\"\"\"\n\n    BUG = \"Bug\"\n    TASK = \"Task\"\n    EPIC = \"Epic\"\n    ENHANCEMENT = \"Enhancement\"\n    DELIVERABLE = \"Deliverable\"\n    NONE = None\n\n\nclass IssueState(Enum):\n    \"\"\"Whether the issue is open or closed.\"\"\"\n\n    OPEN = \"open\"\n    CLOSED = \"closed\"\n\n\nclass IssueMetadata(BaseModel):\n    \"\"\"Stores information about issue type and parent (if applicable).\"\"\"\n\n    # Project metadata -- attributes about the sprint project board\n    project_owner: str\n    project_number: int\n    # Issue metadata -- attributes about the issue common to both projects\n    issue_title: str\n    issue_url: str\n    issue_parent: str | None\n    issue_type: str | None\n    issue_is_closed: bool\n    issue_opened_at: str\n    issue_closed_at: str | None\n    # Sprint metadata -- custom fields specific to the sprint board project\n    issue_points: int | float | None = Field(default=None)\n    issue_status: str | None = Field(default=None)\n    sprint_id: str | None = Field(default=None)\n    sprint_name: str | None = Field(default=None)\n    sprint_start: str | None = Field(default=None)\n    sprint_length: int | None = Field(default=None)\n    sprint_end: str | None = Field(default=None)\n    # Roadmap metadata -- custom fields specific to the roadmap project\n    quad_id: str | None = Field(default=None)\n    quad_name: str | None = Field(default=None)\n    quad_start: str | None = Field(default=None)\n    quad_length: int | None = Field(default=None)\n    quad_end: str | None = Field(default=None)\n    deliverable_pillar: str | None = Field(default=None)\n    # Parent metadata -- attributes about parent issues populated via lookup\n    deliverable_url: str | None = Field(default=None)\n    deliverable_title: str | None = Field(default=None)\n    deliverable_status: str | None = Field(default=None)\n    epic_url: str | None = Field(default=None)\n    epic_title: str | None = Field(default=None)\n\n    # See https://docs.pydantic.dev/2.0/usage/computed_fields/\n    @computed_field  # type: ignore[misc]\n    @property\n    def issue_state(self) -> str:\n        \"\"\"Whether the issue is open or closed.\"\"\"\n        if self.issue_is_closed:\n            return IssueState.CLOSED.value\n        return IssueState.OPEN.value\n\n\n# ===============================================================\n# Dataset class\n# ===============================================================\n\n\nclass GitHubIssues(BaseDataset):\n    \"\"\"GitHub issues with metadata about their parents (Epics and Deliverables) and sprints.\"\"\"\n\n    def __init__(self, df: pd.DataFrame) -> None:\n        \"\"\"Initialize the GitHub Issues dataset.\"\"\"\n        self.opened_col = \"issue_opened_at\"\n        self.closed_col = \"issue_closed_at\"\n        self.points_col = \"issue_points\"\n        self.sprint_col = \"sprint_name\"\n        self.sprint_start_col = \"sprint_start\"\n        self.sprint_end_col = \"sprint_end\"\n        self.project_col = \"project_number\"\n        self.date_cols = [\n            self.sprint_start_col,\n            self.sprint_end_col,\n            self.opened_col,\n            self.closed_col,\n        ]\n        # Convert date cols into dates\n        for col in self.date_cols:\n            # strip off the timestamp portion of the date\n            df[col] = pd.to_datetime(df[col]).dt.floor(\"d\")\n        super().__init__(df)\n\n    def sprint_start(self, sprint: str) -> pd.Timestamp:\n        \"\"\"Return the date on which a given sprint started.\"\"\"\n        sprint_mask = self.df[self.sprint_col] == sprint\n        return self.df.loc[sprint_mask, self.sprint_start_col].min()\n\n    def sprint_end(self, sprint: str) -> pd.Timestamp:\n        \"\"\"Return the date on which a given sprint ended.\"\"\"\n        sprint_mask = self.df[self.sprint_col] == sprint\n        return self.df.loc[sprint_mask, self.sprint_end_col].max()\n\n    @property\n    def sprints(self) -> pd.DataFrame:\n        \"\"\"Return the unique list of sprints with their start and end dates.\"\"\"\n        sprint_cols = [self.sprint_col, self.sprint_start_col, self.sprint_end_col]\n        return self.df[sprint_cols].drop_duplicates()\n\n    @property\n    def current_sprint(self) -> str | None:\n        \"\"\"Return the name of the current sprint, if a sprint is currently active.\"\"\"\n        return self.get_sprint_name_from_date(pd.Timestamp.today().floor(\"d\"))\n\n    def get_sprint_name_from_date(self, date: pd.Timestamp) -> str | None:\n        \"\"\"Get the name of a sprint from a given date, if that date falls in a sprint.\"\"\"\n        # fmt: off\n        date_filter = (\n            (self.sprints[self.sprint_start_col] < date)  # after sprint start\n            & (self.sprints[self.sprint_end_col] >= date)  # before sprint end\n        )\n        # fmt: on\n        matching_sprints = self.sprints.loc[date_filter, self.sprint_col]\n        # if there aren't any sprints return None\n        if len(matching_sprints) == 0:\n            return None\n        # if there are, return the first value as a string\n        return str(matching_sprints.squeeze())\n\n    def to_dict(self) -> list[dict]:\n        \"\"\"Convert this dataset to a python dictionary.\"\"\"\n        # Temporarily convert date cols into strings before exporting\n        for col in self.date_cols:\n            self.df[col] = self.df[col].dt.strftime(\"%Y-%m-%d\")\n        # Return the dictionary\n        export_dict = super().to_dict()\n        # Convert date columns back into dates\n        for col in self.date_cols:\n            self.df[col] = pd.to_datetime(self.df[col]).dt.floor(\"d\")\n        return export_dict"}
{"path":"api/docker-compose.yml","language":"yaml","type":"code","directory":"api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/docker-compose.yml","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/datasets/utils.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/datasets\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/datasets/utils.py\nSize: 2.62 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/gunicorn.conf.py","language":"python","type":"code","directory":"api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/gunicorn.conf.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"import json\n\nimport pandas as pd\n\n\ndef load_json_data_as_df(\n    file_path: str,\n    column_map: dict,\n    date_cols: list[str] | None = None,\n) -> pd.DataFrame:\n    \"\"\"\n    Load a file that contains JSON data and format is as a DataFrame.\n\n    Parameters\n    ----------\n    file_path: str\n        Path to the JSON file with the exported issue data\n    column_map: dict\n        Dictionary mapping of existing JSON keys to their new column names\n    date_cols: list[str]\n        List of columns that need to be converted to date types\n\n    Returns\n    -------\n    pd.DataFrame\n        Pandas dataframe with columns renamed to match the values of the column map\n\n    \"\"\"\n    # load json data from the local file\n    with open(file_path, encoding=\"utf-8\") as f:\n        json_data = json.loads(f.read())\n\n    return load_json_data_as_df_from_object(\n        json_data,\n        column_map,\n        date_cols,\n    )\n\n\ndef load_json_data_as_df_from_object(\n    json_data: list,\n    column_map: dict,\n    date_cols: list[str] | None = None,\n) -> pd.DataFrame:\n    \"\"\"\n    Load JSON data and format it as a DataFrame.\n\n    Parameters\n    ----------\n    json_data: list\n        JSON object with the exported issue data\n    column_map: dict\n        Dictionary mapping of existing JSON keys to their new column names\n    date_cols: list[str]\n        List of columns that need to be converted to date types\n    key_for_nested_items: Optional[str]\n        Name of the key containing a list of objects to load as a dataframe.\n        Only needed if the JSON loaded is an object instead of a list\n\n    Returns\n    -------\n    pd.DataFrame\n        Pandas dataframe with columns renamed to match the values of the column map\n\n    \"\"\"\n    # flatten the nested json into a dataframe\n    df = pd.json_normalize(json_data)\n    # reorder and rename the columns\n    df = df[column_map.keys()]\n    df = df.rename(columns=column_map)\n    # convert datetime columns to date\n    if date_cols:\n        for col in date_cols:\n            # strip off the timestamp portion of the date\n            df[col] = pd.to_datetime(df[col]).dt.floor(\"d\")\n    return df\n\n\ndef load_json_file(path: str) -> list[dict]:\n    \"\"\"Load contents of a JSON file into a dictionary.\"\"\"\n    with open(path) as f:\n        return json.load(f)\n\n\ndef dump_to_json(path: str, data: dict | list[dict]) -> None:\n    \"\"\"Write a dictionary or list of dicts to a json file.\"\"\"\n    with open(path, \"w\") as f:\n        # Uses ensure_ascii=False to preserve emoji characters in output\n        # https://stackoverflow.com/a/52206290/7338319\n        json.dump(data, f, indent=2, ensure_ascii=False)"}
{"path":"api/local.env","language":"unknown","type":"code","directory":"api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/local.env","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/etl/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/etl\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/etl/__init__.py\nSize: 0.08 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/mock-oauth/config.json","language":"json","type":"code","directory":"api/mock-oauth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/mock-oauth/config.json","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":""}
{"path":"api/mock-oauth/mock-server-key.json","language":"json","type":"code","directory":"api/mock-oauth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/mock-oauth/mock-server-key.json","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/etl/github.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/etl\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/etl/github.py\nSize: 11.79 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/newrelic.ini","language":"unknown","type":"code","directory":"api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/newrelic.ini","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"from __future__ import annotations\n\nimport logging\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nimport pandas as pd\nfrom pydantic import BaseModel, ValidationError\n\nfrom analytics.datasets.issues import (\n    GitHubIssues,\n    IssueMetadata,\n    IssueType,\n)\nfrom analytics.datasets.utils import load_json_file\nfrom analytics.integrations import github\n\nlogger = logging.getLogger(__name__)\n\n# ========================================================\n# Config interfaces\n# ========================================================\n\n\n@dataclass\nclass InputFiles:\n    \"\"\"Expected input files for loading this dataset.\"\"\"\n\n    roadmap: str\n    sprint: str\n\n\nclass GitHubProjectConfig(BaseModel):\n    \"\"\"Configurations for GitHub projects ETL.\"\"\"\n\n    roadmap_project: RoadmapConfig\n    sprint_projects: list[SprintBoardConfig]\n    temp_dir: str = \"data\"\n    output_file: str = \"data/delivery-data.json\"\n\n\nclass RoadmapConfig(BaseModel):\n    \"\"\"\n    Configuration details for a roadmap project.\n\n    Attributes\n    ----------\n    owner\n        The GitHub login for the project owner.\n    project_number\n        The number of the project.\n    quad_field\n        The name of the project field that stores the Quad value for a deliverable.\n    pillar_field\n        The name of the project field that stores the Pillar value for a deliverable.\n\n    \"\"\"\n\n    owner: str\n    project_number: int\n    quad_field: str = \"Quad\"\n    pillar_field: str = \"Pillar\"\n\n\nclass SprintBoardConfig(BaseModel):\n    \"\"\"\n    Configuration details for a sprint board project.\n\n    Attributes\n    ----------\n    owner\n        The GitHub login for the project owner.\n    number\n        The number of the project.\n    points_field\n        The name of the project field that stores story points or estimates.\n    sprint_field\n        The name of the project field that stores the sprint value.\n\n    \"\"\"\n\n    owner: str\n    project_number: int\n    points_field: str = \"Points\"\n    sprint_field: str = \"Sprint\"\n\n\n# ========================================================\n# ETL pipeline\n# ========================================================\n\n\nclass GitHubProjectETL:\n    \"\"\"Manage the ETL pipeline for GitHub project data.\"\"\"\n\n    def __init__(self, config: GitHubProjectConfig) -> None:\n        \"\"\"Initialize and run the ETL pipeline for Github project data.\"\"\"\n        # Store the config\n        self.config = config\n        # Declare private attributes shared across ETL steps\n        self._transient_files: list[InputFiles]\n        self.client = github.GitHubGraphqlClient()\n        self.dataset: GitHubIssues\n\n    def run(self) -> None:\n        \"\"\"Run the ETL pipeline.\"\"\"\n        self.extract()\n        self.transform()\n        self.write_to_file()\n\n    def extract(self) -> None:\n        \"\"\"Run the extract step of the ETL pipeline.\"\"\"\n        temp_dir = Path(self.config.temp_dir)\n\n        # Export the roadmap data\n        roadmap_file_path = str(temp_dir / \"roadmap-data.json\")\n        roadmap = self.config.roadmap_project\n        self._export_roadmap_data_to_file(\n            roadmap=roadmap,\n            output_file_path=roadmap_file_path,\n        )\n\n        # Export sprint data for each GitHub project that the scrum teams use\n        # to manage their sprints, e.g. HHS/17 and HHS/13\n        input_files: list[InputFiles] = []\n        for sprint_board in self.config.sprint_projects:\n            n = sprint_board.project_number\n            sprint_file_path = str(temp_dir / f\"sprint-data-{n}.json\")\n            self._export_sprint_data_to_file(\n                sprint_board=sprint_board,\n                output_file_path=sprint_file_path,\n            )\n            # Add to file list\n            input_files.append(\n                InputFiles(\n                    roadmap=roadmap_file_path,\n                    sprint=sprint_file_path,\n                ),\n            )\n        # store transient files for re-use during the transform step\n        self._transient_files = input_files\n\n    def transform(self) -> None:\n        \"\"\"Transform exported data and write to file.\"\"\"\n        # Load sprint and roadmap data\n        issues = []\n        for f in self._transient_files:\n            issues.extend(run_transformation_pipeline(files=f))\n        self.dataset = GitHubIssues(pd.DataFrame(data=issues))\n\n    def write_to_file(self) -> None:\n        \"\"\"Dump dataset to file.\"\"\"\n        self.dataset.to_json(self.config.output_file)\n\n    def _export_roadmap_data_to_file(\n        self,\n        roadmap: RoadmapConfig,\n        output_file_path: str,\n    ) -> None:\n\n        logger.info(\n            \"Exporting roadmap data from %s/%d\",\n            roadmap.owner,\n            roadmap.project_number,\n        )\n        github.export_roadmap_data_to_file(\n            client=self.client,\n            owner=roadmap.owner,\n            project=roadmap.project_number,\n            quad_field=roadmap.quad_field,\n            pillar_field=roadmap.pillar_field,\n            output_file=output_file_path,\n        )\n\n    def _export_sprint_data_to_file(\n        self,\n        sprint_board: SprintBoardConfig,\n        output_file_path: str,\n    ) -> None:\n\n        logger.info(\n            \"Exporting sprint data from %s/%d\",\n            sprint_board.owner,\n            sprint_board.project_number,\n        )\n        github.export_sprint_data_to_file(\n            client=self.client,\n            owner=sprint_board.owner,\n            project=sprint_board.project_number,\n            sprint_field=sprint_board.sprint_field,\n            points_field=sprint_board.points_field,\n            output_file=output_file_path,\n        )\n\n    def extract_and_transform_in_memory(self) -> list[dict]:\n        \"\"\"Export from GitHub and transform to JSON.\"\"\"\n        # export roadmap data\n        roadmap = self.config.roadmap_project\n        roadmap_json = github.export_roadmap_data_to_object(\n            client=self.client,\n            owner=roadmap.owner,\n            project=roadmap.project_number,\n            quad_field=roadmap.quad_field,\n            pillar_field=roadmap.pillar_field,\n        )\n\n        # export sprint data\n        issues = []\n        for sprint_board in self.config.sprint_projects:\n            sprint_json = github.export_sprint_data_to_object(\n                client=self.client,\n                owner=sprint_board.owner,\n                project=sprint_board.project_number,\n                sprint_field=sprint_board.sprint_field,\n                points_field=sprint_board.points_field,\n            )\n\n            # flatten sprint and roadmap data into issue data\n            issues.extend(\n                run_transformation_pipeline_on_json(\n                    roadmap=roadmap_json,\n                    sprint=sprint_json,\n                ),\n            )\n\n        # hydrate issue dataset\n        dataset = GitHubIssues(pd.DataFrame(data=issues))\n\n        # dump issue dataset to JSON\n        return dataset.to_dict()\n\n\n# ===============================================================\n# Transformation helper functions\n# ===============================================================\n\n\ndef run_transformation_pipeline(files: InputFiles) -> list[dict]:\n    \"\"\"Load data from input files and apply transformations.\"\"\"\n    # Log the current sprint for which we're running the transformations\n    logger.info(\"Running transformations for sprint: %s\", files.sprint)\n    # Load sprint and roadmap data\n    sprint_data_in = load_json_file(files.sprint)\n    roadmap_data_in = load_json_file(files.roadmap)\n    # Populate a lookup table with this data\n    lookup: dict = {}\n    lookup = populate_issue_lookup_table(lookup, roadmap_data_in)\n    lookup = populate_issue_lookup_table(lookup, sprint_data_in)\n    # Flatten and write issue level data to output file\n    return flatten_issue_data(lookup)\n\n\ndef run_transformation_pipeline_on_json(\n    roadmap: list[dict],\n    sprint: list[dict],\n) -> list[dict]:\n    \"\"\"Apply transformations.\"\"\"\n    # Populate a lookup table with this data\n    lookup: dict = {}\n    lookup = populate_issue_lookup_table(lookup, roadmap)\n    lookup = populate_issue_lookup_table(lookup, sprint)\n    # Flatten and write issue level data to output file\n    return flatten_issue_data(lookup)\n\n\ndef populate_issue_lookup_table(\n    lookup: dict[str, IssueMetadata],\n    issues: list[dict],\n) -> dict[str, IssueMetadata]:\n    \"\"\"Populate a lookup table that maps issue URLs to their issue type and parent.\"\"\"\n    for i, issue in enumerate(issues):\n        try:\n            entry = IssueMetadata.model_validate(issue)\n        except ValidationError as err:\n            logger.error(\"Error parsing row %d, skipped.\", i)  # noqa: TRY400\n            logger.debug(\"Error: %s\", err)\n            continue\n        lookup[entry.issue_url] = entry\n    return lookup\n\n\ndef get_parent_with_type(\n    child_url: str,\n    lookup: dict[str, IssueMetadata],\n    type_wanted: IssueType,\n) -> IssueMetadata | None:\n    \"\"\"\n    Traverse the lookup table to find an issue's parent with a specific type.\n\n    This is useful if we have multiple nested issues, and we want to find the\n    top level deliverable or epic that a given task or bug is related to.\n    \"\"\"\n    # Get the initial child issue and its parent (if applicable) from the URL\n    child = lookup.get(child_url)\n    if not child:\n        err = f\"Lookup doesn't contain issue with url: {child_url}\"\n        raise ValueError(err)\n    if not child.issue_parent:\n        return None\n\n    # Travel up the issue hierarchy until we:\n    #  - Find a parent issue with the desired type\n    #  - Get to an issue without a parent\n    #  - Have traversed 5 issues (breaks out of issue cycles)\n    max_traversal = 5\n    parent_url = child.issue_parent\n    for _ in range(max_traversal):\n        parent = lookup.get(parent_url)\n        # If no parent is found, return None\n        if not parent:\n            return None\n        # If the parent matches the desired type, return it\n        if IssueType(parent.issue_type) == type_wanted:\n            return parent\n        # If the parent doesn't have a its own parent, return None\n        if not parent.issue_parent:\n            return None\n        # Otherwise update the parent_url to \"grandparent\" and continue\n        parent_url = parent.issue_parent\n\n    # Return the URL of the parent deliverable (or None)\n    return None\n\n\ndef flatten_issue_data(lookup: dict[str, IssueMetadata]) -> list[dict]:\n    \"\"\"Flatten issue data and inherit data from parent epic an deliverable.\"\"\"\n    result: list[dict] = []\n    for issue in lookup.values():\n        # If the issue is a deliverable or epic, move to the next one\n        if IssueType(issue.issue_type) in [IssueType.DELIVERABLE, IssueType.EPIC]:\n            continue\n\n        # Get the parent deliverable, if the issue has one\n        deliverable = get_parent_with_type(\n            child_url=issue.issue_url,\n            lookup=lookup,\n            type_wanted=IssueType.DELIVERABLE,\n        )\n        if deliverable:\n            # Set deliverable metadata\n            issue.deliverable_title = deliverable.issue_title\n            issue.deliverable_url = deliverable.issue_url\n            issue.deliverable_pillar = deliverable.deliverable_pillar\n            issue.deliverable_status = deliverable.issue_status\n            # Set quad metadata\n            issue.quad_id = deliverable.quad_id\n            issue.quad_name = deliverable.quad_name\n            issue.quad_start = deliverable.quad_start\n            issue.quad_end = deliverable.quad_end\n            issue.quad_length = deliverable.quad_length\n\n        # Get the parent epic, if the issue has one\n        epic = get_parent_with_type(\n            child_url=issue.issue_url,\n            lookup=lookup,\n            type_wanted=IssueType.EPIC,\n        )\n        if epic:\n            issue.epic_title = epic.issue_title\n            issue.epic_url = epic.issue_url\n\n        # Add the issue to the results\n        result.append(issue.model_dump())\n\n    # Return the results\n    return result"}
{"path":"api/openapi.generated.yml","language":"yaml","type":"code","directory":"api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/openapi.generated.yml","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/etl/utils.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/etl\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/etl/utils.py\nSize: 0.72 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/opensearch/Dockerfile","language":"unknown","type":"code","directory":"api/opensearch","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/opensearch/Dockerfile","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"import json\nfrom pathlib import Path\nfrom typing import TypeVar\n\nfrom pydantic import BaseModel\n\n# Define a generic type variable for the schema\nT = TypeVar(\"T\", bound=BaseModel)\n\n\ndef load_config(config_path: Path, schema: type[T]) -> T:\n    \"\"\"\n    Load a JSON config file and validate it against a Pydantic schema.\n\n    Parameters\n    ----------\n    config_path:\n        Path to the JSON config file.\n    schema:\n        The Pydantic schema class to validate the JSON data.\n\n    Returns\n    -------\n    An instance of the schema containing validated config data.\n\n    \"\"\"\n    with open(config_path) as f:\n        config_data = json.load(f)\n\n    return schema(**config_data)"}
{"path":"api/pyproject.toml","language":"unknown","type":"code","directory":"api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/pyproject.toml","size":0,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/__init__.py\nSize: 0.08 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/src/__init__.py","language":"python","type":"code","directory":"api/src","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":""}
{"path":"api/src/adapters/__init__.py","language":"python","type":"code","directory":"api/src/adapters","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/db.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/db.py\nSize: 2.47 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/src/adapters/aws/__init__.py","language":"python","type":"code","directory":"api/src/adapters/aws","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/aws/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"from typing import Any, cast\n\nimport boto3\nimport psycopg\nfrom sqlalchemy import Connection, Engine, create_engine, pool\n\nfrom config import DBSettings, get_db_settings\n\n# The variables used in the connection url are pulled from local.env\n# and configured in the DBSettings class found in config.py\n\n\nclass PostgresDbClient:\n    \"\"\"An implementation of of a Postgres db client.\"\"\"\n\n    def __init__(self, config: DBSettings | None = None) -> None:\n        \"\"\"Construct a class instance.\"\"\"\n        if not config:\n            config = get_db_settings()\n        self._engine = self._configure_engine(config)\n\n    def _configure_engine(self, config: DBSettings) -> Engine:\n        \"\"\"Configure db engine to use short-lived IAM tokens for access.\"\"\"\n\n        # inspired by /api/src/adapters/db/clients/postgres_client.py\n        def get_conn() -> psycopg.Connection:\n            \"\"\"Get a psycopg connection.\"\"\"\n            return psycopg.connect(**get_connection_parameters(config))\n\n        conn_pool = pool.QueuePool(cast(Any, get_conn), max_overflow=5, pool_size=10)\n\n        return create_engine(\n            \"postgresql+psycopg://\",\n            pool=conn_pool,\n            hide_parameters=True,\n        )\n\n    def connect(self) -> Connection:\n        \"\"\"Get a new database connection object.\"\"\"\n        return self._engine.connect()\n\n    def engine(self) -> Engine:\n        \"\"\"Get reference to db engine.\"\"\"\n        return self._engine\n\n\ndef get_connection_parameters(config: DBSettings) -> dict[str, Any]:\n    \"\"\"Get parameters for db connection.\"\"\"\n    token = (\n        config.password if config.local_env is True else generate_iam_auth_token(config)\n    )\n    return {\n        \"host\": config.db_host,\n        \"dbname\": config.name,\n        \"user\": config.user,\n        \"password\": token,\n        \"port\": config.port,\n        \"connect_timeout\": 20,\n        \"sslmode\": config.ssl_mode,\n        \"options\": f\"-c search_path={config.db_schema}\",\n    }\n\n\ndef generate_iam_auth_token(config: DBSettings) -> str:\n    \"\"\"Generate IAM auth token.\"\"\"\n    if config.aws_region is None:\n        msg = \"AWS region needs to be configured for DB IAM auth\"\n        raise ValueError(msg)\n    client = boto3.client(\"rds\", region_name=config.aws_region)\n    return client.generate_db_auth_token(\n        DBHostname=config.db_host,\n        Port=config.port,\n        DBUsername=config.user,\n        Region=config.aws_region,\n    )"}
{"path":"api/src/adapters/aws/aws_session.py","language":"python","type":"code","directory":"api/src/adapters/aws","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/aws/aws_session.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations/etldb\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/__init__.py\nSize: 0.20 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/src/adapters/aws/pinpoint_adapter.py","language":"python","type":"code","directory":"api/src/adapters/aws","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/aws/pinpoint_adapter.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"__all__ = [\n    \"migrate_database\",\n    \"sync_data\",\n]\n\nfrom analytics.integrations.etldb.main import (\n    migrate_database,\n    sync_data,\n)"}
{"path":"api/src/adapters/aws/s3_adapter.py","language":"python","type":"code","directory":"api/src/adapters/aws","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/aws/s3_adapter.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/deliverable_model.py\nLanguage: py\nType: model\nDirectory: analytics/src/analytics/integrations/etldb\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/deliverable_model.py\nSize: 6.14 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/src/adapters/db/__init__.py","language":"python","type":"code","directory":"api/src/adapters/db","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"from pandas import Series\nfrom psycopg.errors import InsufficientPrivilege\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import OperationalError, ProgrammingError\n\nfrom analytics.datasets.etl_dataset import EtlEntityType\nfrom analytics.integrations.etldb.etldb import EtlChangeType, EtlDb\n\n\nclass EtlDeliverableModel:\n    \"\"\"Encapsulate CRUD operations for deliverable entity.\"\"\"\n\n    def __init__(self, dbh: EtlDb) -> None:\n        \"\"\"Instantiate a class instance.\"\"\"\n        self.dbh = dbh\n\n    def sync_deliverable(\n        self,\n        deliverable_df: Series,\n        ghid_map: dict,\n    ) -> tuple[int | None, EtlChangeType]:\n        \"\"\"Write deliverable data to etl database.\"\"\"\n        # initialize return value\n        deliverable_id = None\n        change_type = EtlChangeType.NONE\n\n        try:\n            # insert dimensions\n            deliverable_id = self._insert_dimensions(deliverable_df)\n            if deliverable_id is not None:\n                change_type = EtlChangeType.INSERT\n\n            # if insert failed, select and update\n            if deliverable_id is None:\n                deliverable_id, change_type = self._update_dimensions(deliverable_df)\n\n            # insert facts\n            if deliverable_id is not None:\n                _ = self._insert_facts(deliverable_id, deliverable_df, ghid_map)\n        except (\n            InsufficientPrivilege,\n            OperationalError,\n            ProgrammingError,\n            RuntimeError,\n        ) as e:\n            message = f\"FATAL: Failed to sync deliverable data: {e}\"\n            raise RuntimeError(message) from e\n\n        return deliverable_id, change_type\n\n    def _insert_dimensions(self, deliverable_df: Series) -> int | None:\n        \"\"\"Write deliverable dimension data to etl database.\"\"\"\n        # insert into dimension table: deliverable\n        new_row_id = None\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\n                \"insert into gh_deliverable(ghid, title, pillar) \"\n                \"values (:ghid, :title, :pillar) \"\n                \"on conflict(ghid) do nothing returning id\",\n            ),\n            {\n                \"ghid\": deliverable_df[\"deliverable_ghid\"],\n                \"title\": deliverable_df[\"deliverable_title\"],\n                \"pillar\": deliverable_df[\"deliverable_pillar\"],\n            },\n        )\n        row = result.fetchone()\n        if row:\n            new_row_id = row[0]\n\n        # commit\n        self.dbh.commit(cursor)\n\n        return new_row_id\n\n    def _insert_facts(\n        self,\n        deliverable_id: int,\n        deliverable_df: Series,\n        ghid_map: dict,\n    ) -> tuple[int | None, int | None]:\n        \"\"\"Write deliverable fact data to etl database.\"\"\"\n        # insert into fact table: deliverable_quad_map\n        map_id = None\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\n                \"insert into gh_deliverable_quad_map(deliverable_id, quad_id, d_effective) \"\n                \"values (:deliverable_id, :quad_id, :effective) \"\n                \"on conflict(deliverable_id, d_effective) do update \"\n                \"set (quad_id, t_modified) = (:quad_id, current_timestamp) returning id\",\n            ),\n            {\n                \"deliverable_id\": deliverable_id,\n                \"quad_id\": ghid_map[EtlEntityType.QUAD].get(\n                    deliverable_df[\"quad_ghid\"],\n                ),\n                \"effective\": self.dbh.effective_date,\n            },\n        )\n        row = result.fetchone()\n        if row:\n            map_id = row[0]\n\n        # insert into fact table: deliverable_history\n        history_id = None\n        result = cursor.execute(\n            text(\n                \"insert into gh_deliverable_history(deliverable_id, status, d_effective) \"\n                \"values (:deliverable_id, :status, :effective) \"\n                \"on conflict(deliverable_id, d_effective) do update \"\n                \"set (status, t_modified) = (:status, current_timestamp) returning id\",\n            ),\n            {\n                \"deliverable_id\": deliverable_id,\n                \"status\": deliverable_df[\"deliverable_status\"],\n                \"effective\": self.dbh.effective_date,\n            },\n        )\n        row = result.fetchone()\n        if row:\n            history_id = row[0]\n\n        # commit\n        self.dbh.commit(cursor)\n\n        return history_id, map_id\n\n    def _update_dimensions(\n        self,\n        deliverable_df: Series,\n    ) -> tuple[int | None, EtlChangeType]:\n        \"\"\"Update deliverable fact data in etl database.\"\"\"\n        # initialize return value\n        change_type = EtlChangeType.NONE\n\n        # get new values\n        new_title = deliverable_df[\"deliverable_title\"]\n        new_pillar = deliverable_df[\"deliverable_pillar\"]\n        new_values = (new_title, new_pillar)\n\n        # select old values\n        deliverable_id, old_title, old_pillar = self._select(\n            deliverable_df[\"deliverable_ghid\"],\n        )\n        old_values = (old_title, old_pillar)\n\n        # compare\n        if deliverable_id is not None and new_values != old_values:\n            change_type = EtlChangeType.UPDATE\n            cursor = self.dbh.connection()\n            update_sql = text(\n                \"update gh_deliverable set title = :new_title, pillar = :new_pillar, \"\n                \"t_modified = current_timestamp where id = :deliverable_id\",\n            )\n            update_values = {\n                \"new_title\": new_title,\n                \"new_pillar\": new_pillar,\n                \"deliverable_id\": deliverable_id,\n            }\n            cursor.execute(update_sql, update_values)\n            self.dbh.commit(cursor)\n\n        return deliverable_id, change_type\n\n    def _select(self, ghid: str) -> tuple[int | None, str | None, str | None]:\n        \"\"\"Select deliverable data from etl database.\"\"\"\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\"select id, title, pillar from gh_deliverable where ghid = :ghid\"),\n            {\"ghid\": ghid},\n        )\n        row = result.fetchone()\n        if row:\n            return row[0], row[1], row[2]\n\n        return None, None, None"}
{"path":"api/src/adapters/db/client.py","language":"python","type":"code","directory":"api/src/adapters/db","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/client.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/epic_model.py\nLanguage: py\nType: model\nDirectory: analytics/src/analytics/integrations/etldb\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/epic_model.py\nSize: 4.66 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/src/adapters/db/clients/__init__.py","language":"python","type":"code","directory":"api/src/adapters/db/clients","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/clients/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"from pandas import Series\nfrom psycopg.errors import InsufficientPrivilege\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import OperationalError, ProgrammingError\n\nfrom analytics.datasets.etl_dataset import EtlEntityType\nfrom analytics.integrations.etldb.etldb import EtlChangeType, EtlDb\n\n\nclass EtlEpicModel:\n    \"\"\"Encapsulate CRUD operations for epic entity.\"\"\"\n\n    def __init__(self, dbh: EtlDb) -> None:\n        \"\"\"Instantiate a class instance.\"\"\"\n        self.dbh = dbh\n\n    def sync_epic(\n        self,\n        epic_df: Series,\n        ghid_map: dict,\n    ) -> tuple[int | None, EtlChangeType]:\n        \"\"\"Write epic data to etl database.\"\"\"\n        # initialize return value\n        epic_id = None\n        change_type = EtlChangeType.NONE\n\n        try:\n            # insert dimensions\n            epic_id = self._insert_dimensions(epic_df)\n            if epic_id is not None:\n                change_type = EtlChangeType.INSERT\n\n            # if insert failed, select and update\n            if epic_id is None:\n                epic_id, change_type = self._update_dimensions(epic_df)\n\n            # insert facts\n            if epic_id is not None:\n                self._insert_facts(epic_id, epic_df, ghid_map)\n        except (\n            InsufficientPrivilege,\n            OperationalError,\n            ProgrammingError,\n            RuntimeError,\n        ) as e:\n            message = f\"FATAL: Failed to sync epic data: {e}\"\n            raise RuntimeError(message) from e\n\n        return epic_id, change_type\n\n    def _insert_dimensions(self, epic_df: Series) -> int | None:\n        \"\"\"Write epic dimension data to etl database.\"\"\"\n        # insert into dimension table: epic\n        new_row_id = None\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\n                \"insert into gh_epic(ghid, title) values (:ghid, :title) \"\n                \"on conflict(ghid) do nothing returning id\",\n            ),\n            {\n                \"ghid\": epic_df[\"epic_ghid\"],\n                \"title\": epic_df[\"epic_title\"],\n            },\n        )\n        row = result.fetchone()\n        if row:\n            new_row_id = row[0]\n\n        # commit\n        self.dbh.commit(cursor)\n\n        return new_row_id\n\n    def _insert_facts(\n        self,\n        epic_id: int,\n        epic_df: Series,\n        ghid_map: dict,\n    ) -> int | None:\n        \"\"\"Write epic fact data to etl database.\"\"\"\n        # insert into fact table: epic_deliverable_map\n        new_row_id = None\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\n                \"insert into gh_epic_deliverable_map(epic_id, deliverable_id, d_effective) \"\n                \"values (:epic_id, :deliverable_id, :effective) \"\n                \"on conflict(epic_id, d_effective) do update \"\n                \"set (deliverable_id, t_modified) = (:deliverable_id, current_timestamp) \"\n                \"returning id\",\n            ),\n            {\n                \"deliverable_id\": ghid_map[EtlEntityType.DELIVERABLE].get(\n                    epic_df[\"deliverable_ghid\"],\n                ),\n                \"epic_id\": epic_id,\n                \"effective\": self.dbh.effective_date,\n            },\n        )\n        row = result.fetchone()\n        if row:\n            new_row_id = row[0]\n\n        # commit\n        self.dbh.commit(cursor)\n\n        return new_row_id\n\n    def _update_dimensions(self, epic_df: Series) -> tuple[int | None, EtlChangeType]:\n        \"\"\"Update epic dimension data in etl database.\"\"\"\n        # initialize return value\n        change_type = EtlChangeType.NONE\n\n        # get new values\n        new_title = epic_df[\"epic_title\"]\n\n        # select old values\n        epic_id, old_title = self._select(epic_df[\"epic_ghid\"])\n\n        # compare\n        if epic_id is not None and (new_title,) != (old_title,):\n            change_type = EtlChangeType.UPDATE\n            cursor = self.dbh.connection()\n            cursor.execute(\n                text(\n                    \"update gh_epic set title = :new_title, t_modified = current_timestamp \"\n                    \"where id = :epic_id\",\n                ),\n                {\"new_title\": new_title, \"epic_id\": epic_id},\n            )\n            self.dbh.commit(cursor)\n\n        return epic_id, change_type\n\n    def _select(self, ghid: str) -> tuple[int | None, str | None]:\n        \"\"\"Select epic data from etl database.\"\"\"\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\"select id, title from gh_epic where ghid = :ghid\"),\n            {\"ghid\": ghid},\n        )\n        row = result.fetchone()\n        if row:\n            return row[0], row[1]\n\n        return None, None"}
{"path":"api/src/adapters/db/clients/postgres_client.py","language":"python","type":"code","directory":"api/src/adapters/db/clients","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/clients/postgres_client.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/etldb.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations/etldb\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/etldb.py\nSize: 4.01 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/src/adapters/db/clients/postgres_config.py","language":"python","type":"code","directory":"api/src/adapters/db/clients","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/clients/postgres_config.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"import logging\nfrom enum import Enum\n\nfrom sqlalchemy import Connection, text\n\nfrom analytics.integrations.db import PostgresDbClient\n\nlogger = logging.getLogger(__name__)\n\n\nclass EtlDb:\n    \"\"\"Encapsulate etl database connections.\"\"\"\n\n    def __init__(self, effective: str | None = None) -> None:\n        \"\"\"Construct instance.\"\"\"\n        try:\n            self._db_client = PostgresDbClient()\n        except RuntimeError as e:\n            message = f\"Failed to instantiate database engine: {e}\"\n            raise RuntimeError(message) from e\n\n        self._connection: Connection | None = None\n        self.effective_date = effective\n        self.dateformat = \"%Y-%m-%d\"\n\n    def connection(self) -> Connection:\n        \"\"\"Get a connection object from the database engine.\"\"\"\n        if self._connection is None:\n            try:\n                self._connection = self._db_client.connect()\n            except RuntimeError as e:\n                message = f\"Failed to connect to database: {e}\"\n                raise RuntimeError(message) from e\n        return self._connection\n\n    def commit(self, connection: Connection) -> None:\n        \"\"\"Commit an open transaction.\"\"\"\n        connection.commit()\n\n    def get_schema_version(self) -> int:\n        \"\"\"Select schema version from etl database.\"\"\"\n        version = 0\n\n        if self.schema_versioning_exists():\n            result = self.connection().execute(\n                text(\"select version from schema_version\"),\n            )\n            row = result.fetchone()\n            if row:\n                version = row[0]\n\n        return version\n\n    def set_schema_version(self, new_value: int) -> bool:\n        \"\"\"Set schema version number.\"\"\"\n        if not self.schema_versioning_exists():\n            return False\n\n        # sanity check new version number\n        current_version = self.get_schema_version()\n        if new_value < current_version:\n            message = (\n                \"WARNING: cannot bump schema version \"\n                f\"from {current_version} to {new_value}\"\n            )\n            logger.info(message)\n            return False\n\n        if new_value > current_version:\n            cursor = self.connection()\n            cursor.execute(\n                text(\n                    \"insert into schema_version (version) values (:new_value) \"\n                    \"on conflict(one_row) do update \"\n                    \"set version = :new_value\",\n                ),\n                {\"new_value\": new_value},\n            )\n            self.commit(cursor)\n            return True\n\n        return False\n\n    def revert_to_schema_version(self, new_value: int) -> bool:\n        \"\"\"Revert schema version number to the previous version.\"\"\"\n        if not self.schema_versioning_exists():\n            return False\n\n        # sanity check new version number\n        current_version = self.get_schema_version()\n        if new_value != current_version - 1 or new_value < 0:\n            message = (\n                \"WARNING: cannot bump schema version \"\n                f\"from {current_version} to {new_value}\"\n            )\n            logger.info(message)\n            return False\n\n        cursor = self.connection()\n        cursor.execute(\n            text(\n                \"insert into schema_version (version) values (:new_value) \"\n                \"on conflict(one_row) do update \"\n                \"set version = :new_value\",\n            ),\n            {\"new_value\": new_value},\n        )\n        self.commit(cursor)\n        return True\n\n    def schema_versioning_exists(self) -> bool:\n        \"\"\"Determine whether schema version table exists.\"\"\"\n        result = self.connection().execute(\n            text(\n                \"select table_name from information_schema.tables \"\n                \"where table_name = 'schema_version'\",\n            ),\n        )\n        row = result.fetchone()\n        return bool(row and row[0] == \"schema_version\")\n\n\nclass EtlChangeType(Enum):\n    \"\"\"An enum to describe ETL change types.\"\"\"\n\n    NONE = 0\n    INSERT = 1\n    UPDATE = 2"}
{"path":"api/src/adapters/db/flask_db.py","language":"python","type":"code","directory":"api/src/adapters/db","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/flask_db.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/issue_model.py\nLanguage: py\nType: model\nDirectory: analytics/src/analytics/integrations/etldb\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/issue_model.py\nSize: 7.73 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/src/adapters/db/type_decorators/__init__.py","language":"python","type":"code","directory":"api/src/adapters/db/type_decorators","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/type_decorators/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"from datetime import datetime\n\nfrom pandas import Series\nfrom psycopg.errors import InsufficientPrivilege\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import OperationalError, ProgrammingError\n\nfrom analytics.datasets.etl_dataset import EtlEntityType\nfrom analytics.integrations.etldb.etldb import EtlChangeType, EtlDb\n\n\nclass EtlIssueModel:\n    \"\"\"Encapsulate CRUD operations for issue entity.\"\"\"\n\n    def __init__(self, dbh: EtlDb) -> None:\n        \"\"\"Instantiate a class instance.\"\"\"\n        self.dbh = dbh\n\n    def sync_issue(\n        self,\n        issue_df: Series,\n        ghid_map: dict,\n    ) -> tuple[int | None, EtlChangeType]:\n        \"\"\"Write issue data to etl database.\"\"\"\n        # initialize return value\n        issue_id = None\n        change_type = EtlChangeType.NONE\n\n        try:\n            # insert dimensions\n            issue_id = self._insert_dimensions(issue_df, ghid_map)\n            if issue_id is not None:\n                change_type = EtlChangeType.INSERT\n\n            # if insert failed, select and update\n            if issue_id is None:\n                issue_id, change_type = self._update_dimensions(issue_df, ghid_map)\n\n            # insert facts\n            if issue_id is not None:\n                self._insert_facts(issue_id, issue_df, ghid_map)\n        except (\n            InsufficientPrivilege,\n            OperationalError,\n            ProgrammingError,\n            RuntimeError,\n        ) as e:\n            message = f\"FATAL: Failed to sync issue data: {e}\"\n            raise RuntimeError(message) from e\n\n        return issue_id, change_type\n\n    def _insert_dimensions(self, issue_df: Series, ghid_map: dict) -> int | None:\n        \"\"\"Write issue dimension data to etl database.\"\"\"\n        # insert into dimension table: issue\n        new_row_id = None\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\n                \"insert into gh_issue \"\n                \"(ghid, title, type, opened_date, closed_date, parent_issue_ghid, epic_id) \"\n                \"values (:ghid, :title, :type, :opened_date, :closed_date, :parent_ghid, :epic_id) \"\n                \"on conflict(ghid) do nothing returning id\",\n            ),\n            {\n                \"ghid\": issue_df[\"issue_ghid\"],\n                \"title\": issue_df[\"issue_title\"],\n                \"type\": issue_df[\"issue_type\"] or \"None\",\n                \"opened_date\": issue_df[\"issue_opened_at\"],\n                \"closed_date\": issue_df[\"issue_closed_at\"],\n                \"parent_ghid\": issue_df[\"issue_parent\"],\n                \"epic_id\": ghid_map[EtlEntityType.EPIC].get(issue_df[\"epic_ghid\"]),\n            },\n        )\n        row = result.fetchone()\n        if row:\n            new_row_id = row[0]\n\n        # commit\n        self.dbh.commit(cursor)\n\n        return new_row_id\n\n    def _insert_facts(\n        self,\n        issue_id: int,\n        issue_df: Series,\n        ghid_map: dict,\n    ) -> tuple[int | None, int | None]:\n        \"\"\"Write issue fact data to etl database.\"\"\"\n        # get values needed for sql statement\n        issue_df = issue_df.fillna(0)\n        insert_values = {\n            \"issue_id\": issue_id,\n            \"status\": issue_df[\"issue_status\"],\n            \"is_closed\": int(issue_df[\"issue_is_closed\"]),\n            \"points\": issue_df[\"issue_points\"],\n            \"sprint_id\": ghid_map[EtlEntityType.SPRINT].get(issue_df[\"sprint_ghid\"]),\n            \"effective\": self.dbh.effective_date,\n            \"project_id\": ghid_map[EtlEntityType.PROJECT].get(issue_df[\"project_ghid\"]),\n        }\n        history_id = None\n        map_id = None\n\n        # insert into fact table: issue_history\n        cursor = self.dbh.connection()\n        insert_sql1 = text(\n            \"insert into gh_issue_history \"\n            \"(issue_id, status, is_closed, points, d_effective, project_id, sprint_id) \"\n            \"values \"\n            \"(:issue_id, :status, :is_closed, :points, :effective, :project_id, :sprint_id) \"\n            \"on conflict (issue_id, project_id, d_effective) \"\n            \"do update set (status, is_closed, points, t_modified, sprint_id) = \"\n            \"(:status, :is_closed, :points, current_timestamp, :sprint_id) \"\n            \"returning id\",\n        )\n        result1 = cursor.execute(insert_sql1, insert_values)\n        row1 = result1.fetchone()\n        if row1:\n            history_id = row1[0]\n\n        # insert into fact table: issue_sprint_map\n        # note: issue_sprint_map will be removed after validating changes to issue_history\n        insert_sql2 = text(\n            \"insert into gh_issue_sprint_map (issue_id, sprint_id, d_effective) \"\n            \"values (:issue_id, :sprint_id, :effective) \"\n            \"on conflict (issue_id, d_effective) \"\n            \"do update set (sprint_id, t_modified) = \"\n            \"(:sprint_id, current_timestamp) returning id\",\n        )\n        result2 = cursor.execute(insert_sql2, insert_values)\n        row2 = result2.fetchone()\n        if row2:\n            map_id = row2[0]\n\n        # commit\n        self.dbh.commit(cursor)\n\n        return history_id, map_id\n\n    def _update_dimensions(\n        self,\n        issue_df: Series,\n        ghid_map: dict,\n    ) -> tuple[int | None, EtlChangeType]:\n        \"\"\"Update issue dimension data in etl database.\"\"\"\n        # initialize return value\n        change_type = EtlChangeType.NONE\n\n        # get new values\n        new_values = (\n            issue_df[\"issue_title\"],\n            issue_df[\"issue_type\"] or \"None\",\n            issue_df[\"issue_opened_at\"],\n            issue_df[\"issue_closed_at\"],\n            issue_df[\"issue_parent\"],\n            ghid_map[EtlEntityType.EPIC].get(issue_df[\"epic_ghid\"]),\n        )\n\n        # select old values\n        issue_id, o_title, o_type, o_opened, o_closed, o_parent, o_epic_id = (\n            self._select(issue_df[\"issue_ghid\"])\n        )\n        old_values = (o_title, o_type, o_opened, o_closed, o_parent, o_epic_id)\n\n        # compare\n        if issue_id is not None and new_values != old_values:\n            change_type = EtlChangeType.UPDATE\n            cursor = self.dbh.connection()\n            cursor.execute(\n                text(\n                    \"update gh_issue set \"\n                    \"title = :new_title, type = :new_type, opened_date = :new_opened, \"\n                    \"closed_date = :new_closed, parent_issue_ghid = :new_parent, \"\n                    \"epic_id = :new_epic_id, t_modified = current_timestamp \"\n                    \"where id = :issue_id\",\n                ),\n                {\n                    \"new_title\": issue_df[\"issue_title\"],\n                    \"new_type\": issue_df[\"issue_type\"] or \"None\",\n                    \"new_opened\": issue_df[\"issue_opened_at\"],\n                    \"new_closed\": issue_df[\"issue_closed_at\"],\n                    \"new_parent\": issue_df[\"issue_parent\"],\n                    \"new_epic_id\": ghid_map[EtlEntityType.EPIC].get(\n                        issue_df[\"epic_ghid\"],\n                    ),\n                    \"issue_id\": issue_id,\n                },\n            )\n            self.dbh.commit(cursor)\n\n        return issue_id, change_type\n\n    def _select(self, ghid: str) -> tuple[\n        int | None,\n        str | None,\n        str | None,\n        datetime | None,\n        datetime | None,\n        str | None,\n        int | None,\n    ]:\n        \"\"\"Select issue data from etl database.\"\"\"\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\n                \"select id, title, type, opened_date, closed_date, parent_issue_ghid, epic_id \"\n                \"from gh_issue where ghid = :ghid\",\n            ),\n            {\"ghid\": ghid},\n        )\n        row = result.fetchone()\n        if row:\n            return row[0], row[1], row[2], row[3], row[4], row[5], row[6]\n\n        return None, None, None, None, None, None, None"}
{"path":"api/src/adapters/db/type_decorators/postgres_type_decorators.py","language":"python","type":"code","directory":"api/src/adapters/db/type_decorators","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/type_decorators/postgres_type_decorators.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/main.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations/etldb\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/main.py\nSize: 8.69 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/src/adapters/newrelic/__init__.py","language":"python","type":"code","directory":"api/src/adapters/newrelic","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/newrelic/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"import logging\nimport os\nimport re\nfrom pathlib import Path\n\nfrom sqlalchemy import text\n\nfrom analytics.datasets.etl_dataset import EtlDataset, EtlEntityType\nfrom analytics.integrations.etldb.deliverable_model import EtlDeliverableModel\nfrom analytics.integrations.etldb.epic_model import EtlEpicModel\nfrom analytics.integrations.etldb.etldb import EtlDb\nfrom analytics.integrations.etldb.issue_model import EtlIssueModel\nfrom analytics.integrations.etldb.project_model import EtlProjectModel\nfrom analytics.integrations.etldb.quad_model import EtlQuadModel\nfrom analytics.integrations.etldb.sprint_model import EtlSprintModel\n\nVERBOSE = False\n\nlogger = logging.getLogger(__name__)\n\n\ndef migrate_database() -> None:\n    \"\"\"\n    Create and/or update an etl database by applying a sequential set of migration scripts.\n\n    It applies the migrations using the following steps:\n    - Check the current schema version listed in the database\n    - Retrieve the list of migration scripts ordered by version\n    - If the current schema version is less than the version of the latest migration script\n      run the remaining migrations in order\n    - Bump the schema version in the database to the latest version\n    - If the current schema version matches the latest script, do nothing\n    \"\"\"\n    # get connection to database\n    etldb = EtlDb()\n    current_version = etldb.get_schema_version()\n    m = f\"current schema version: {current_version}\"\n    logger.info(m)\n\n    # get all sql file paths and respective version numbers\n    sql_file_path_map = get_sql_file_paths()\n    all_versions = sorted(sql_file_path_map.keys())\n\n    # iterate sql files\n    migration_count = 0\n    for next_version in all_versions:\n        if next_version <= current_version:\n            continue\n        # read sql file\n        with open(sql_file_path_map[next_version]) as f:\n            sql = f.read()\n            # execute sql\n            m = f\"applying migration for schema version: {next_version}\"\n            logger.info(m)\n            m = f\"migration source file: {sql_file_path_map[next_version]}\"\n            logger.info(m)\n            cursor = etldb.connection()\n            cursor.execute(\n                text(sql),\n            )\n            # commit changes\n            etldb.commit(cursor)\n            # bump schema version number\n            _ = etldb.set_schema_version(next_version)\n            current_version = next_version\n            migration_count += 1\n\n    # summarize results in output\n    m = f\"total migrations applied: {migration_count}\"\n    logger.info(m)\n    m = f\"new schema version: {current_version}\"\n    logger.info(m)\n\n\ndef sync_data(dataset: EtlDataset, effective: str) -> None:\n    \"\"\"Write github data to etl database.\"\"\"\n    # initialize a map of github id to db row id\n    ghid_map: dict[EtlEntityType, dict[str, int]] = {\n        EtlEntityType.DELIVERABLE: {},\n        EtlEntityType.EPIC: {},\n        EtlEntityType.PROJECT: {},\n        EtlEntityType.QUAD: {},\n        EtlEntityType.SPRINT: {},\n    }\n\n    # initialize db connection\n    db = EtlDb(effective)\n\n    # note: the following code assumes SCHEMA VERSION >= 4\n    # sync project data to db resulting in row id for each project\n    ghid_map[EtlEntityType.PROJECT] = sync_projects(db, dataset)\n    m = f\"project row(s) processed: {len(ghid_map[EtlEntityType.PROJECT])}\"\n    logger.info(m)\n\n    # sync quad data to db resulting in row id for each quad\n    ghid_map[EtlEntityType.QUAD] = sync_quads(db, dataset)\n    m = f\"quad row(s) processed: {len(ghid_map[EtlEntityType.QUAD])}\"\n    logger.info(m)\n\n    # sync deliverable data to db resulting in row id for each deliverable\n    ghid_map[EtlEntityType.DELIVERABLE] = sync_deliverables(\n        db,\n        dataset,\n        ghid_map,\n    )\n    m = f\"deliverable row(s) processed: {len(ghid_map[EtlEntityType.DELIVERABLE])}\"\n    logger.info(m)\n\n    # sync sprint data to db resulting in row id for each sprint\n    ghid_map[EtlEntityType.SPRINT] = sync_sprints(db, dataset, ghid_map)\n    m = f\"sprint row(s) processed: {len(ghid_map[EtlEntityType.SPRINT])}\"\n    logger.info(m)\n\n    # sync epic data to db resulting in row id for each epic\n    ghid_map[EtlEntityType.EPIC] = sync_epics(db, dataset, ghid_map)\n    m = f\"epic row(s) processed: {len(ghid_map[EtlEntityType.EPIC])}\"\n    logger.info(m)\n\n    # sync issue data to db resulting in row id for each issue\n    issue_map = sync_issues(db, dataset, ghid_map)\n    m = f\"issue row(s) processed: {len(issue_map)}\"\n    logger.info(m)\n\n\ndef sync_deliverables(db: EtlDb, dataset: EtlDataset, ghid_map: dict) -> dict:\n    \"\"\"Insert or update (if necessary) a row for each deliverable and return a map of row ids.\"\"\"\n    result = {}\n    model = EtlDeliverableModel(db)\n    for ghid in dataset.get_deliverable_ghids():\n        deliverable_df = dataset.get_deliverable(ghid)\n        result[ghid], _ = model.sync_deliverable(deliverable_df, ghid_map)\n        if VERBOSE:\n            m = f\"DELIVERABLE '{ghid}' row_id = {result[ghid]}\"\n            logger.info(m)\n    return result\n\n\ndef sync_epics(db: EtlDb, dataset: EtlDataset, ghid_map: dict) -> dict:\n    \"\"\"Insert or update (if necessary) a row for each epic and return a map of row ids.\"\"\"\n    result = {}\n    model = EtlEpicModel(db)\n    for ghid in dataset.get_epic_ghids():\n        epic_df = dataset.get_epic(ghid)\n        result[ghid], _ = model.sync_epic(epic_df, ghid_map)\n        if VERBOSE:\n            m = f\"EPIC '{ghid}' row_id = {result[ghid]}\"\n            logger.info(m)\n    return result\n\n\ndef sync_issues(db: EtlDb, dataset: EtlDataset, ghid_map: dict) -> dict:\n    \"\"\"Insert or update (if necessary) a row for each issue and return a map of row ids.\"\"\"\n    result = {}\n    model = EtlIssueModel(db)\n    for ghid in dataset.get_issue_ghids():\n        all_rows = dataset.get_issues(ghid)\n        for _, issue_df in all_rows.iterrows():\n            result[ghid], _ = model.sync_issue(issue_df, ghid_map)\n            if VERBOSE:\n                m = f\"ISSUE '{ghid}' issue_id = {result[ghid]}\"\n                logger.info(m)\n    return result\n\n\ndef sync_projects(db: EtlDb, dataset: EtlDataset) -> dict:\n    \"\"\"Insert or update (if necessary) a row for each project and return a map of row ids.\"\"\"\n    result = {}\n    model = EtlProjectModel(db)\n    for ghid in dataset.get_project_ghids():\n        project_df = dataset.get_project(ghid)\n        result[ghid], _ = model.sync_project(project_df)\n        if VERBOSE:\n            m = (\n                f\"PROJECT '{ghid}' title = '{project_df['project_name']}', row_id = {result[ghid]}\",\n            )\n            logger.info(m)\n    return result\n\n\ndef sync_sprints(db: EtlDb, dataset: EtlDataset, ghid_map: dict) -> dict:\n    \"\"\"Insert or update (if necessary) a row for each sprint and return a map of row ids.\"\"\"\n    result = {}\n    model = EtlSprintModel(db)\n    for ghid in dataset.get_sprint_ghids():\n        sprint_df = dataset.get_sprint(ghid)\n        result[ghid], _ = model.sync_sprint(sprint_df, ghid_map)\n        if VERBOSE:\n            m = f\"SPRINT '{ghid}' row_id = {result[ghid]}\"\n            logger.info(m)\n    return result\n\n\ndef sync_quads(db: EtlDb, dataset: EtlDataset) -> dict:\n    \"\"\"Insert or update (if necessary) a row for each quad and return a map of row ids.\"\"\"\n    result = {}\n    model = EtlQuadModel(db)\n    for ghid in dataset.get_quad_ghids():\n        quad_df = dataset.get_quad(ghid)\n        result[ghid], _ = model.sync_quad(quad_df)\n        if VERBOSE:\n            m = (\n                f\"QUAD '{ghid}' title = '{quad_df['quad_name']}', row_id = {result[ghid]}\",\n            )\n            logger.info(m)\n    return result\n\n\ndef get_sql_file_paths() -> dict[int, str]:\n    \"\"\"Get all sql files needed for database initialization.\"\"\"\n    result = {}\n\n    # define the path to the sql files\n    sql_file_directory = f\"{Path(__file__).resolve().parent}/migrations/versions\"\n\n    # get list of sorted filenames\n    filename_list = sorted(os.listdir(sql_file_directory))\n\n    # expected filename format: {4_digit_version_number}_{short_description}.sql\n    # example: 0003_alter_tables_set_default_timestamp.sql\n    pattern = re.compile(r\"^\\d\\d\\d\\d_.+\\.sql$\")\n\n    # compile dict of results\n    for filename in filename_list:\n        # validate filename format\n        if not pattern.match(filename):\n            message = f\"FATAL: malformed db migration filename: {filename}\"\n            raise RuntimeError(message)\n\n        # extrace version number from filename\n        version = int(filename[:4])\n\n        # do not allow duplicate version number\n        if version in result:\n            message = f\"FATAL: Duplicate db migration version number: {version} \"\n            raise RuntimeError(message)\n\n        # map the version number to the file path\n        result[version] = f\"{sql_file_directory}/{filename}\"\n\n    return result"}
{"path":"api/src/adapters/newrelic/events.py","language":"python","type":"code","directory":"api/src/adapters/newrelic","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/newrelic/events.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/migrations/versions/0001_alter_default.sql\nLanguage: sql\nType: code\nDirectory: analytics/src/analytics/integrations/etldb/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0001_alter_default.sql\nSize: 0.16 KB\nLast Modified: 2025-02-14T17:08:26.408Z"}
{"path":"api/src/adapters/oauth/__init__.py","language":"python","type":"code","directory":"api/src/adapters/oauth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/oauth/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":""}
{"path":"api/src/adapters/oauth/login_gov/__init__.py","language":"python","type":"code","directory":"api/src/adapters/oauth/login_gov","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/oauth/login_gov/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/migrations/versions/0002_create_tables_etldb.sql\nLanguage: sql\nType: code\nDirectory: analytics/src/analytics/integrations/etldb/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0002_create_tables_etldb.sql\nSize: 2.60 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/adapters/oauth/login_gov/login_gov_oauth_client.py","language":"python","type":"code","directory":"api/src/adapters/oauth/login_gov","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/oauth/login_gov/login_gov_oauth_client.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"CREATE TABLE IF NOT EXISTS gh_deliverable_quad_map (\n\tid SERIAL PRIMARY KEY,\n\tdeliverable_id INTEGER NOT NULL,\n\tquad_id INTEGER,\n\td_effective DATE NOT NULL,\n\tt_created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n\tt_modified TIMESTAMP,\n\tUNIQUE(deliverable_id, d_effective)\n);\nCREATE INDEX IF NOT EXISTS gh_dqm_i1 on gh_deliverable_quad_map(quad_id, d_effective);\n\nCREATE TABLE IF NOT EXISTS gh_epic (\n\tid SERIAL PRIMARY KEY,\n\tghid TEXT UNIQUE NOT NULL,\n\ttitle TEXT NOT NULL,\n\tt_created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n\tt_modified TIMESTAMP \n);\n\nCREATE TABLE IF NOT EXISTS gh_epic_deliverable_map (\n\tid SERIAL PRIMARY KEY,\n\tepic_id INTEGER NOT NULL,\n\tdeliverable_id INTEGER,\n\td_effective DATE NOT NULL,\n\tt_created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n\tt_modified TIMESTAMP,\n\tUNIQUE(epic_id, d_effective)\n);\nCREATE INDEX IF NOT EXISTS gh_edm_i1 on gh_epic_deliverable_map(deliverable_id, d_effective);\n\nCREATE TABLE IF NOT EXISTS gh_issue (\n\tid SERIAL PRIMARY KEY,\n\tghid TEXT UNIQUE NOT NULL,\n\ttitle TEXT NOT NULL,\n\ttype TEXT NOT NULL,\n\topened_date DATE,\n\tclosed_date DATE,\n\tparent_issue_ghid TEXT,\n\tepic_id INTEGER,\n\tt_created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n\tt_modified TIMESTAMP \n);\nCREATE INDEX IF NOT EXISTS gh_issue_i1 on gh_issue(epic_id);\n\nCREATE TABLE IF NOT EXISTS gh_issue_history (\n\tid SERIAL PRIMARY KEY,\n\tissue_id INTEGER NOT NULL,\n\tstatus TEXT,\n\tis_closed INTEGER NOT NULL,\n\tpoints INTEGER NOT NULL DEFAULT 0,\n\td_effective DATE NOT NULL,\n\tt_created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n\tt_modified TIMESTAMP,\n\tUNIQUE(issue_id, d_effective)\n);\nCREATE INDEX IF NOT EXISTS gh_ih_i1 on gh_issue_history(issue_id, d_effective);\n\nCREATE TABLE IF NOT EXISTS gh_issue_sprint_map (\n\tid SERIAL PRIMARY KEY,\n\tissue_id INTEGER NOT NULL,\n\tsprint_id INTEGER,\n\td_effective DATE NOT NULL,\n\tt_created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n\tt_modified TIMESTAMP,\n\tUNIQUE(issue_id, d_effective)\n);\n\nCREATE TABLE IF NOT EXISTS gh_sprint (\n\tid SERIAL PRIMARY KEY,\n\tghid TEXT UNIQUE NOT NULL,\n\tname TEXT NOT NULL,\n\tstart_date DATE,\n\tend_date DATE,\n\tduration INTEGER,\n\tquad_id INTEGER,\n\tt_created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n\tt_modified TIMESTAMP \n);\n\nCREATE TABLE IF NOT EXISTS gh_quad (\n\tid SERIAL PRIMARY KEY,\n\tghid TEXT UNIQUE NOT NULL,\n\tname TEXT NOT NULL,\n\tstart_date DATE,\n\tend_date DATE,\n\tduration INTEGER,\n\tt_created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n\tt_modified TIMESTAMP \n);\nCREATE INDEX IF NOT EXISTS gh_quad_i1 on gh_quad(start_date);"}
{"path":"api/src/adapters/oauth/login_gov/mock_login_gov_oauth_client.py","language":"python","type":"code","directory":"api/src/adapters/oauth/login_gov","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/oauth/login_gov/mock_login_gov_oauth_client.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/migrations/versions/0003_create_schema_versions.sql\nLanguage: sql\nType: code\nDirectory: analytics/src/analytics/integrations/etldb/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0003_create_schema_versions.sql\nSize: 0.28 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/adapters/oauth/oauth_client.py","language":"python","type":"code","directory":"api/src/adapters/oauth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/oauth/oauth_client.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":""}
{"path":"api/src/adapters/oauth/oauth_client_models.py","language":"python","type":"code","directory":"api/src/adapters/oauth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/oauth/oauth_client_models.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/migrations/versions/0004_alter_tables_set_default_timestamp.sql\nLanguage: sql\nType: code\nDirectory: analytics/src/analytics/integrations/etldb/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0004_alter_tables_set_default_timestamp.sql\nSize: 0.72 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/adapters/search/__init__.py","language":"python","type":"code","directory":"api/src/adapters/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/search/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":""}
{"path":"api/src/adapters/search/flask_opensearch.py","language":"python","type":"code","directory":"api/src/adapters/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/search/flask_opensearch.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/migrations/versions/0005_create_tables_deliv_hist_and_project.sql\nLanguage: sql\nType: code\nDirectory: analytics/src/analytics/integrations/etldb/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0005_create_tables_deliv_hist_and_project.sql\nSize: 0.66 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/adapters/search/opensearch_client.py","language":"python","type":"code","directory":"api/src/adapters/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/search/opensearch_client.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"ALTER TABLE IF EXISTS gh_sprint ADD COLUMN IF NOT EXISTS project_id INTEGER;\n\nCREATE TABLE IF NOT EXISTS gh_deliverable_history (\n\tid SERIAL PRIMARY KEY,\n\tdeliverable_id INTEGER NOT NULL,\n\tstatus TEXT,\n\td_effective DATE NOT NULL,\n\tt_created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n\tt_modified TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n\tUNIQUE(deliverable_id, d_effective)\n);\nCREATE INDEX IF NOT EXISTS gh_dh_i1 on gh_deliverable_history(deliverable_id, d_effective);"}
{"path":"api/src/adapters/search/opensearch_config.py","language":"python","type":"code","directory":"api/src/adapters/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/search/opensearch_config.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/migrations/versions/0006_add_proj_col_to_issue_hist.sql\nLanguage: sql\nType: code\nDirectory: analytics/src/analytics/integrations/etldb/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0006_add_proj_col_to_issue_hist.sql\nSize: 0.45 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/adapters/search/opensearch_query_builder.py","language":"python","type":"code","directory":"api/src/adapters/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/search/opensearch_query_builder.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":""}
{"path":"api/src/adapters/search/opensearch_response.py","language":"python","type":"code","directory":"api/src/adapters/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/search/opensearch_response.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/migrations/versions/0007_add_opportunity_tables.sql\nLanguage: sql\nType: code\nDirectory: analytics/src/analytics/integrations/etldb/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/migrations/versions/0007_add_opportunity_tables.sql\nSize: 5.33 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/__init__.py","language":"python","type":"code","directory":"api/src/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"CREATE TABLE IF NOT EXISTS lk_opportunity_category\n(\n    opportunity_category_id SERIAL\n        PRIMARY KEY,\n    description             TEXT                                   NOT NULL,\n    created_at              TIMESTAMP WITH TIME ZONE               NOT NULL,\n    updated_at              TIMESTAMP WITH TIME ZONE               NOT NULL\n);\n\n\nCREATE TABLE IF NOT EXISTS opportunity\n(\n    opportunity_id          BIGSERIAL\n        primary key,\n    opportunity_number      TEXT,\n    opportunity_title       TEXT,\n    agency_code             TEXT,\n    opportunity_category_id INTEGER\n        CONSTRAINT opportunity_opportunity_category_id_lk_opportunity_cate_c6e9\n            REFERENCES lk_opportunity_category,\n    category_explanation    TEXT,\n    is_draft                BOOLEAN                                NOT NULL,\n    revision_number         INTEGER,\n    modified_comments       TEXT,\n    publisher_user_id       TEXT,\n    publisher_profile_id    BIGINT,\n    created_at              TIMESTAMP WITH TIME ZONE               NOT NULL,\n    updated_at              TIMESTAMP WITH TIME ZONE               NOT NULL\n);\n\nCREATE INDEX IF NOT EXISTS opportunity_is_draft_idx\n    ON opportunity (is_draft);\n\nCREATE INDEX IF NOT EXISTS opportunity_opportunity_category_id_idx\n    ON opportunity (opportunity_category_id);\n\nCREATE INDEX IF NOT EXISTS opportunity_opportunity_title_idx\n    ON opportunity (opportunity_title);\n\nCREATE INDEX IF NOT EXISTS opportunity_agency_code_idx\n    ON opportunity (agency_code);\n\n\nCREATE TABLE IF NOT EXISTS opportunity_summary\n(\n    opportunity_summary_id            BIGSERIAL\n        PRIMARY KEY,\n    opportunity_id                    BIGINT                                 NOT NULL\n        CONSTRAINT opportunity_summary_opportunity_id_opportunity_fkey\n            REFERENCES opportunity,\n    summary_description               TEXT,\n    is_cost_sharing                   BOOLEAN,\n    is_forecast                       BOOLEAN                                NOT NULL,\n    post_date                         DATE,\n    close_date                        DATE,\n    close_date_description            TEXT,\n    archive_date                      DATE,\n    unarchive_date                    DATE,\n    expected_number_of_awards         BIGINT,\n    estimated_total_program_funding   BIGINT,\n    award_floor                       BIGINT,\n    award_ceiling                     BIGINT,\n    additional_info_url               TEXT,\n    additional_info_url_description   TEXT,\n    forecasted_post_date              DATE,\n    forecasted_close_date             DATE,\n    forecasted_close_date_description TEXT,\n    forecasted_award_date             DATE,\n    forecasted_project_start_date     DATE,\n    fiscal_year                       INTEGER,\n    revision_number                   INTEGER,\n    modification_comments             TEXT,\n    funding_category_description      TEXT,\n    applicant_eligibility_description TEXT,\n    agency_code                       TEXT,\n    agency_name                       TEXT,\n    agency_phone_number               TEXT,\n    agency_contact_description        TEXT,\n    agency_email_address              TEXT,\n    agency_email_address_description  TEXT,\n    is_deleted                        BOOLEAN,\n    can_send_mail                     BOOLEAN,\n    publisher_profile_id              BIGINT,\n    publisher_user_id                 TEXT,\n    updated_by                        TEXT,\n    created_by                        TEXT,\n    created_at                        TIMESTAMP WITH TIME ZONE   NOT NULL,\n    updated_at                        TIMESTAMP WITH TIME ZONE   NOT NULL,\n    version_number                    INTEGER,\n    CONSTRAINT opportunity_summary_is_forecast_uniq\n        UNIQUE (is_forecast, revision_number, opportunity_id)\n);\n\nCREATE INDEX IF NOT EXISTS opportunity_summary_opportunity_id_idx\n    ON opportunity_summary (opportunity_id);\n\nCREATE TABLE IF NOT EXISTS  current_opportunity_summary\n(\n    opportunity_id         BIGINT                                 NOT NULL\n        CONSTRAINT current_opportunity_summary_opportunity_id_opportunity_fkey\n            REFERENCES  opportunity,\n    opportunity_summary_id BIGINT                                 NOT NULL\n        CONSTRAINT current_opportunity_summary_opportunity_summary_id_oppo_8251\n            REFERENCES  opportunity_summary,\n    opportunity_status_id  INTEGER                                NOT NULL\n        CONSTRAINT current_opportunity_summary_opportunity_status_id_lk_op_3147\n            REFERENCES  lk_opportunity_status,\n    created_at             TIMESTAMP WITH TIME ZONE   NOT NULL,\n    updated_at             TIMESTAMP WITH TIME ZONE   NOT NULL,\n    PRIMARY KEY (opportunity_id, opportunity_summary_id)\n);\n\nCREATE INDEX IF NOT EXISTS current_opportunity_summary_opportunity_id_idx\n    ON  current_opportunity_summary (opportunity_id);\n\nCREATE INDEX IF NOT EXISTS current_opportunity_summary_opportunity_status_id_idx\n    ON  current_opportunity_summary (opportunity_status_id);\n\nCREATE INDEX IF NOT EXISTS current_opportunity_summary_opportunity_summary_id_idx\n    ON  current_opportunity_summary (opportunity_summary_id);"}
{"path":"api/src/api/agencies_v1/__init__.py","language":"python","type":"code","directory":"api/src/api/agencies_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/agencies_v1/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/project_model.py\nLanguage: py\nType: model\nDirectory: analytics/src/analytics/integrations/etldb\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/project_model.py\nSize: 3.52 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/agencies_v1/agency_blueprint.py","language":"python","type":"code","directory":"api/src/api/agencies_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/agencies_v1/agency_blueprint.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"from pandas import Series\nfrom psycopg.errors import InsufficientPrivilege\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import OperationalError, ProgrammingError\n\nfrom analytics.integrations.etldb.etldb import EtlChangeType, EtlDb\n\n\nclass EtlProjectModel:\n    \"\"\"Encapsulates CRUD operations for project entity.\"\"\"\n\n    def __init__(self, dbh: EtlDb) -> None:\n        \"\"\"Instantiate a class instance.\"\"\"\n        self.dbh = dbh\n\n    def sync_project(self, project_df: Series) -> tuple[int | None, EtlChangeType]:\n        \"\"\"Write project data to etl database.\"\"\"\n        # initialize return value\n        project_id = None\n        change_type = EtlChangeType.NONE\n\n        try:\n            # insert dimensions\n            project_id = self._insert_dimensions(project_df)\n            if project_id is not None:\n                change_type = EtlChangeType.INSERT\n\n            # if insert failed, then select and update\n            if project_id is None:\n                project_id, change_type = self._update_dimensions(project_df)\n        except (\n            InsufficientPrivilege,\n            OperationalError,\n            ProgrammingError,\n            RuntimeError,\n        ) as e:\n            message = f\"FATAL: Failed to sync project data: {e}\"\n            raise RuntimeError(message) from e\n\n        return project_id, change_type\n\n    def _insert_dimensions(self, project_df: Series) -> int | None:\n        \"\"\"Write project dimension data to etl database.\"\"\"\n        # insert into dimension table: project\n        new_row_id = None\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\n                \"insert into gh_project(ghid, name) values (:ghid, :name) \"\n                \"on conflict(ghid) do nothing returning id\",\n            ),\n            {\"ghid\": project_df[\"project_ghid\"], \"name\": project_df[\"project_name\"]},\n        )\n        row = result.fetchone()\n        if row:\n            new_row_id = row[0]\n\n        # commit\n        self.dbh.commit(cursor)\n\n        return new_row_id\n\n    def _update_dimensions(\n        self,\n        project_df: Series,\n    ) -> tuple[int | None, EtlChangeType]:\n        \"\"\"Update project dimension data in etl database.\"\"\"\n        # initialize return value\n        change_type = EtlChangeType.NONE\n\n        # get new values\n        new_name = project_df[\"project_name\"]\n\n        # select old values\n        project_id, old_name = self._select(\n            project_df[\"project_ghid\"],\n        )\n\n        # compare\n        if project_id is not None and new_name != old_name:\n            change_type = EtlChangeType.UPDATE\n            cursor = self.dbh.connection()\n            cursor.execute(\n                text(\n                    \"update gh_project set name = :new_name, \"\n                    \"t_modified = current_timestamp where id = :project_id\",\n                ),\n                {\n                    \"new_name\": new_name,\n                    \"project_id\": project_id,\n                },\n            )\n            self.dbh.commit(cursor)\n\n        return project_id, change_type\n\n    def _select(self, ghid: str) -> tuple[\n        int | None,\n        str | None,\n    ]:\n        \"\"\"Select epic data from etl database.\"\"\"\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\n                \"select id, name from gh_project where ghid = :ghid\",\n            ),\n            {\"ghid\": ghid},\n        )\n        row = result.fetchone()\n        if row:\n            return row[0], row[1]\n\n        return None, None"}
{"path":"api/src/api/agencies_v1/agency_routes.py","language":"python","type":"code","directory":"api/src/api/agencies_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/agencies_v1/agency_routes.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/quad_model.py\nLanguage: py\nType: model\nDirectory: analytics/src/analytics/integrations/etldb\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/quad_model.py\nSize: 4.54 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/agencies_v1/agency_schema.py","language":"python","type":"code","directory":"api/src/api/agencies_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/agencies_v1/agency_schema.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"from datetime import datetime\n\nfrom pandas import Series\nfrom psycopg.errors import InsufficientPrivilege\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import OperationalError, ProgrammingError\n\nfrom analytics.integrations.etldb.etldb import EtlChangeType, EtlDb\n\n\nclass EtlQuadModel:\n    \"\"\"Encapsulates CRUD operations for quad entity.\"\"\"\n\n    def __init__(self, dbh: EtlDb) -> None:\n        \"\"\"Instantiate a class instance.\"\"\"\n        self.dbh = dbh\n\n    def sync_quad(self, quad_df: Series) -> tuple[int | None, EtlChangeType]:\n        \"\"\"Write quad data to etl database.\"\"\"\n        # initialize return value\n        quad_id = None\n        change_type = EtlChangeType.NONE\n\n        try:\n            # insert dimensions\n            quad_id = self._insert_dimensions(quad_df)\n            if quad_id is not None:\n                change_type = EtlChangeType.INSERT\n\n            # if insert failed, then select and update\n            if quad_id is None:\n                quad_id, change_type = self._update_dimensions(quad_df)\n        except (\n            InsufficientPrivilege,\n            OperationalError,\n            ProgrammingError,\n            RuntimeError,\n        ) as e:\n            message = f\"FATAL: Failed to sync quad data: {e}\"\n            raise RuntimeError(message) from e\n\n        return quad_id, change_type\n\n    def _insert_dimensions(self, quad_df: Series) -> int | None:\n        \"\"\"Write quad dimension data to etl database.\"\"\"\n        # insert into dimension table: quad\n        new_row_id = None\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\n                \"insert into gh_quad(ghid, name, start_date, end_date, duration) \"\n                \"values (:ghid, :name, :start_date, :end_date, :duration) \"\n                \"on conflict(ghid) do nothing returning id\",\n            ),\n            {\n                \"ghid\": quad_df[\"quad_ghid\"],\n                \"name\": quad_df[\"quad_name\"],\n                \"start_date\": quad_df[\"quad_start\"],\n                \"end_date\": quad_df[\"quad_end\"],\n                \"duration\": quad_df[\"quad_length\"],\n            },\n        )\n        row = result.fetchone()\n        if row:\n            new_row_id = row[0]\n\n        # commit\n        self.dbh.commit(cursor)\n\n        return new_row_id\n\n    def _update_dimensions(self, quad_df: Series) -> tuple[int | None, EtlChangeType]:\n        \"\"\"Update quad dimension data in etl database.\"\"\"\n        # initialize return value\n        change_type = EtlChangeType.NONE\n\n        # get new values\n        new_values = (\n            quad_df[\"quad_name\"],\n            quad_df[\"quad_start\"],\n            quad_df[\"quad_end\"],\n            int(quad_df[\"quad_length\"]),\n        )\n\n        # select old values\n        quad_id, old_name, old_start, old_end, old_duration = self._select(\n            quad_df[\"quad_ghid\"],\n        )\n        old_values = (\n            old_name,\n            old_start.strftime(self.dbh.dateformat) if old_start is not None else None,\n            old_end.strftime(self.dbh.dateformat) if old_end is not None else None,\n            old_duration,\n        )\n\n        # compare\n        if quad_id is not None and new_values != old_values:\n            change_type = EtlChangeType.UPDATE\n            cursor = self.dbh.connection()\n            cursor.execute(\n                text(\n                    \"update gh_quad set name = :new_name, \"\n                    \"start_date = :new_start, end_date = :new_end, \"\n                    \"duration = :new_duration, t_modified = current_timestamp \"\n                    \"where id = :quad_id\",\n                ),\n                {\n                    \"new_name\": new_values[0],\n                    \"new_start\": new_values[1],\n                    \"new_end\": new_values[2],\n                    \"new_duration\": new_values[3],\n                    \"quad_id\": quad_id,\n                },\n            )\n            self.dbh.commit(cursor)\n\n        return quad_id, change_type\n\n    def _select(self, ghid: str) -> tuple[\n        int | None,\n        str | None,\n        datetime | None,\n        datetime | None,\n        int | None,\n    ]:\n        \"\"\"Select epic data from etl database.\"\"\"\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\n                \"select id, name, start_date, end_date, duration \"\n                \"from gh_quad where ghid = :ghid\",\n            ),\n            {\"ghid\": ghid},\n        )\n        row = result.fetchone()\n        if row:\n            return row[0], row[1], row[2], row[3], row[4]\n\n        return None, None, None, None, None"}
{"path":"api/src/api/extracts_v1/__init__.py","language":"python","type":"code","directory":"api/src/api/extracts_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/extracts_v1/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/etldb/sprint_model.py\nLanguage: py\nType: model\nDirectory: analytics/src/analytics/integrations/etldb\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/etldb/sprint_model.py\nSize: 5.23 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/extracts_v1/extract_blueprint.py","language":"python","type":"code","directory":"api/src/api/extracts_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/extracts_v1/extract_blueprint.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"from pandas import Series\nfrom psycopg.errors import InsufficientPrivilege\nfrom sqlalchemy import text\nfrom sqlalchemy.exc import OperationalError, ProgrammingError\n\nfrom analytics.datasets.etl_dataset import EtlEntityType\nfrom analytics.integrations.etldb.etldb import EtlChangeType, EtlDb\n\n\nclass EtlSprintModel:\n    \"\"\"Encapsulate CRUD operations for sprint entity.\"\"\"\n\n    def __init__(self, dbh: EtlDb) -> None:\n        \"\"\"Instantiate a class instance.\"\"\"\n        self.dbh = dbh\n\n    def sync_sprint(self, sprint_df: Series, ghid_map: dict) -> tuple[\n        int | None,\n        EtlChangeType,\n    ]:\n        \"\"\"Write sprint data to etl database.\"\"\"\n        # initialize return value\n        sprint_id = None\n        change_type = EtlChangeType.NONE\n\n        try:\n            # insert dimensions\n            sprint_id = self._insert_dimensions(sprint_df, ghid_map)\n            if sprint_id is not None:\n                change_type = EtlChangeType.INSERT\n\n            # if insert failed, select and update\n            if sprint_id is None:\n                sprint_id, change_type = self._update_dimensions(sprint_df, ghid_map)\n        except (\n            InsufficientPrivilege,\n            OperationalError,\n            ProgrammingError,\n            RuntimeError,\n        ) as e:\n            message = f\"FATAL: Failed to sync sprint data: {e}\"\n            raise RuntimeError(message) from e\n\n        return sprint_id, change_type\n\n    def _insert_dimensions(self, sprint_df: Series, ghid_map: dict) -> int | None:\n        \"\"\"Write sprint dimension data in etl database.\"\"\"\n        # insert into dimension table: sprint\n        new_row_id = None\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\n                \"insert into gh_sprint \"\n                \"(ghid, name, start_date, end_date, duration, quad_id, project_id) \"\n                \"values (:ghid, :name, :start, :end, :duration, :quad_id, :project_id) \"\n                \"on conflict(ghid) do nothing returning id\",\n            ),\n            {\n                \"ghid\": sprint_df[\"sprint_ghid\"],\n                \"name\": sprint_df[\"sprint_name\"],\n                \"start\": sprint_df[\"sprint_start\"],\n                \"end\": sprint_df[\"sprint_end\"],\n                \"duration\": sprint_df[\"sprint_length\"],\n                \"quad_id\": ghid_map[EtlEntityType.QUAD].get(sprint_df[\"quad_ghid\"]),\n                \"project_id\": ghid_map[EtlEntityType.PROJECT].get(\n                    sprint_df[\"project_ghid\"],\n                ),\n            },\n        )\n        row = result.fetchone()\n        if row:\n            new_row_id = row[0]\n\n        # commit\n        self.dbh.commit(cursor)\n\n        return new_row_id\n\n    def _update_dimensions(self, sprint_df: Series, ghid_map: dict) -> tuple[\n        int | None,\n        EtlChangeType,\n    ]:\n        \"\"\"Update sprint dimension data in etl database.\"\"\"\n        # initialize return value\n        change_type = EtlChangeType.NONE\n\n        # get new values\n        new_values = (\n            sprint_df[\"sprint_name\"],\n            sprint_df[\"sprint_start\"],\n            sprint_df[\"sprint_end\"],\n            sprint_df[\"sprint_length\"],\n            ghid_map[EtlEntityType.QUAD].get(sprint_df[\"quad_ghid\"]),\n            ghid_map[EtlEntityType.PROJECT].get(sprint_df[\"project_ghid\"]),\n        )\n\n        # select old values\n        sprint_id, o_name, o_start, o_end, o_duration, o_quad_id, o_project_id = (\n            self._select(sprint_df[\"sprint_ghid\"])\n        )\n        old_values = (o_name, o_start, o_end, o_duration, o_quad_id, o_project_id)\n\n        # compare\n        if sprint_id is not None and new_values != old_values:\n            change_type = EtlChangeType.UPDATE\n            cursor = self.dbh.connection()\n            cursor.execute(\n                text(\n                    \"update gh_sprint set name = :new_name, start_date = :new_start, \"\n                    \"end_date = :new_end, duration = :new_duration, quad_id = :quad_id, \"\n                    \"project_id = :project_id, t_modified = current_timestamp \"\n                    \"where id = :sprint_id\",\n                ),\n                {\n                    \"new_name\": new_values[0],\n                    \"new_start\": new_values[1],\n                    \"new_end\": new_values[2],\n                    \"new_duration\": new_values[3],\n                    \"quad_id\": new_values[4],\n                    \"project_id\": new_values[5],\n                    \"sprint_id\": sprint_id,\n                },\n            )\n            self.dbh.commit(cursor)\n\n        return sprint_id, change_type\n\n    def _select(self, ghid: str) -> tuple[\n        int | None,\n        str | None,\n        str | None,\n        str | None,\n        int | None,\n        int | None,\n        int | None,\n    ]:\n        \"\"\"Select epic data from etl database.\"\"\"\n        cursor = self.dbh.connection()\n        result = cursor.execute(\n            text(\n                \"select id, name, start_date, end_date, duration, quad_id, project_id \"\n                \"from gh_sprint where ghid = :ghid\",\n            ),\n            {\"ghid\": ghid},\n        )\n        row = result.fetchone()\n        if row:\n            return row[0], row[1], row[2], row[3], row[4], row[5], row[6]\n\n        return None, None, None, None, None, None, None"}
{"path":"api/src/api/extracts_v1/extract_routes.py","language":"python","type":"code","directory":"api/src/api/extracts_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/extracts_v1/extract_routes.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/extracts/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations/extracts\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/extracts/__init__.py\nSize: 0.15 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/extracts_v1/extract_schema.py","language":"python","type":"code","directory":"api/src/api/extracts_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/extracts_v1/extract_schema.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"It extracts CSV files from S3 bucket and loads the records into respective\nopportunity tables.\n\"\"\""}
{"path":"api/src/api/feature_flags/__init__.py","language":"python","type":"code","directory":"api/src/api/feature_flags","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/feature_flags/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"File: analytics/src/analytics/integrations/extracts/constants.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations/extracts\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/extracts/constants.py\nSize: 2.61 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/feature_flags/feature_flag.py","language":"python","type":"code","directory":"api/src/api/feature_flags","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/feature_flags/feature_flag.py","size":1631931,"lastModified":"2025-02-14T17:08:31.123Z","content":"from enum import StrEnum\n\n\nclass OpportunityTables(StrEnum):\n    \"\"\"Opportunity tables that are copied over to analytics database.\"\"\"\n\n    LK_OPPORTUNITY_STATUS = \"lk_opportunity_status\"\n    LK_OPPORTUNITY_CATEGORY = \"lk_opportunity_category\"\n    OPPORTUNITY = \"opportunity\"\n    OPPORTUNITY_SUMMARY = \"opportunity_summary\"\n    CURRENT_OPPORTUNITY_SUMMARY = \"current_opportunity_summary\"\n\n\nLK_OPPORTUNITY_STATUS_COLS = (\n    \"OPPORTUNITY_STATUS_ID\",\n    \"DESCRIPTION\",\n    \"CREATED_AT\",\n    \"UPDATED_AT\",\n)\n\nLK_OPPORTUNITY_CATEGORY_COLS = (\n    \"OPPORTUNITY_CATEGORY_ID\",\n    \"DESCRIPTION\",\n    \"CREATED_AT\",\n    \"UPDATED_AT\",\n)\nOPPORTUNITY_COLS = (\n    \"OPPORTUNITY_ID\",\n    \"OPPORTUNITY_NUMBER\",\n    \"OPPORTUNITY_TITLE\",\n    \"AGENCY_CODE\",\n    \"OPPORTUNITY_CATEGORY_ID\",\n    \"CATEGORY_EXPLANATION\",\n    \"IS_DRAFT\",\n    \"REVISION_NUMBER\",\n    \"MODIFIED_COMMENTS\",\n    \"PUBLISHER_USER_ID\",\n    \"PUBLISHER_PROFILE_ID\",\n    \"CREATED_AT\",\n    \"UPDATED_AT\",\n)\nOPOORTUNITY_SUMMARY_COLS = (\n    \"OPPORTUNITY_SUMMARY_ID\",\n    \"OPPORTUNITY_ID\",\n    \"SUMMARY_DESCRIPTION\",\n    \"IS_COST_SHARING\",\n    \"IS_FORECAST\",\n    \"POST_DATE\",\n    \"CLOSE_DATE\",\n    \"CLOSE_DATE_DESCRIPTION\",\n    \"ARCHIVE_DATE\",\n    \"UNARCHIVE_DATE\",\n    \"EXPECTED_NUMBER_OF_AWARDS\",\n    \"ESTIMATED_TOTAL_PROGRAM_FUNDING\",\n    \"AWARD_FLOOR\",\n    \"AWARD_CEILING\",\n    \"ADDITIONAL_INFO_URL\",\n    \"ADDITIONAL_INFO_URL_DESCRIPTION\",\n    \"FORECASTED_POST_DATE\",\n    \"FORECASTED_CLOSE_DATE\",\n    \"FORECASTED_CLOSE_DATE_DESCRIPTION\",\n    \"FORECASTED_AWARD_DATE\",\n    \"FORECASTED_PROJECT_START_DATE\",\n    \"FISCAL_YEAR\",\n    \"REVISION_NUMBER\",\n    \"MODIFICATION_COMMENTS\",\n    \"FUNDING_CATEGORY_DESCRIPTION\",\n    \"APPLICANT_ELIGIBILITY_DESCRIPTION\",\n    \"AGENCY_CODE\",\n    \"AGENCY_NAME\",\n    \"AGENCY_PHONE_NUMBER\",\n    \"AGENCY_CONTACT_DESCRIPTION\",\n    \"AGENCY_EMAIL_ADDRESS\",\n    \"AGENCY_EMAIL_ADDRESS_DESCRIPTION\",\n    \"IS_DELETED\",\n    \"CAN_SEND_MAIL\",\n    \"PUBLISHER_PROFILE_ID\",\n    \"PUBLISHER_USER_ID\",\n    \"UPDATED_BY\",\n    \"CREATED_BY\",\n    \"CREATED_AT\",\n    \"UPDATED_AT\",\n    \"VERSION_NUMBER\",\n)\nCURRENT_OPPORTUNITY_SUMMARY_COLS = (\n    \"OPPORTUNITY_ID\",\n    \"OPPORTUNITY_SUMMARY_ID\",\n    \"OPPORTUNITY_STATUS_ID\",\n    \"CREATED_AT\",\n    \"UPDATED_AT\",\n)\n\nMAP_TABLES_TO_COLS: dict[OpportunityTables, tuple[str, ...]] = {\n    OpportunityTables.LK_OPPORTUNITY_STATUS: LK_OPPORTUNITY_STATUS_COLS,\n    OpportunityTables.LK_OPPORTUNITY_CATEGORY: LK_OPPORTUNITY_CATEGORY_COLS,\n    OpportunityTables.OPPORTUNITY: OPPORTUNITY_COLS,\n    OpportunityTables.OPPORTUNITY_SUMMARY: OPOORTUNITY_SUMMARY_COLS,\n    OpportunityTables.CURRENT_OPPORTUNITY_SUMMARY: CURRENT_OPPORTUNITY_SUMMARY_COLS,\n}"}
{"path":"api/src/api/feature_flags/feature_flag_config.py","language":"python","type":"code","directory":"api/src/api/feature_flags","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/feature_flags/feature_flag_config.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/integrations/extracts/load_opportunity_data.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations/extracts\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/extracts/load_opportunity_data.py\nSize: 2.51 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/healthcheck.py","language":"python","type":"code","directory":"api/src/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/healthcheck.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"import logging\nimport os\nfrom contextlib import ExitStack\n\nimport smart_open  # type: ignore[import]\nfrom pydantic import Field\nfrom pydantic_settings import BaseSettings\nfrom sqlalchemy import Connection\n\nfrom analytics.integrations.etldb.etldb import EtlDb\nfrom analytics.integrations.extracts.constants import (\n    MAP_TABLES_TO_COLS,\n    OpportunityTables,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass LoadOpportunityDataFileConfig(BaseSettings):\n    \"\"\"Configure S3 properties for opportunity data.\"\"\"\n\n    load_opportunity_data_file_path: str | None = Field(\n        default=None,\n        alias=\"API_ANALYTICS_DB_EXTRACTS_PATH\",\n    )\n\n\ndef extract_copy_opportunity_data() -> None:\n    \"\"\"Instantiate Etldb class and call helper funcs to truncate and insert data in one txn.\"\"\"\n    etldb_conn = EtlDb()\n\n    with etldb_conn.connection() as conn, conn.begin():\n        _trancate_opportunity_table_records(conn)\n\n        _fetch_insert_opportunity_data(conn)\n\n    logger.info(\"Extract opportunity data completed successfully\")\n\n\ndef _trancate_opportunity_table_records(conn: Connection) -> None:\n    \"\"\"Truncate existing records from all tables.\"\"\"\n    cursor = conn.connection.cursor()\n    schema = os.environ[\"DB_SCHEMA\"]\n    for table in OpportunityTables:\n        stmt_trct = f\"TRUNCATE TABLE {schema}.{table} CASCADE\"\n        cursor.execute(stmt_trct)\n    logger.info(\"Truncated all records from all tables\")\n\n\ndef _fetch_insert_opportunity_data(conn: Connection) -> None:\n    \"\"\"Streamlines opportunity tables from S3 and insert into the database.\"\"\"\n    s3_config = LoadOpportunityDataFileConfig()\n\n    cursor = conn.connection.cursor()\n    for table in OpportunityTables:\n        logger.info(\"Copying data for table: %s\", table)\n\n        columns = MAP_TABLES_TO_COLS.get(table, ())\n        query = f\"\"\"\n                       COPY {f\"{os.getenv(\"DB_SCHEMA\")}.{table} ({', '.join(columns)})\"}\n                       FROM STDIN WITH (FORMAT CSV, DELIMITER ',', QUOTE '\"', HEADER)\n                    \"\"\"\n\n        with ExitStack() as stack:\n            file = stack.enter_context(\n                smart_open.open(\n                    f\"{s3_config.load_opportunity_data_file_path}/{table}.csv\",\n                    \"r\",\n                ),\n            )\n            copy = stack.enter_context(cursor.copy(query))\n\n            while data := file.read():\n                copy.write(data)\n\n        logger.info(\"Successfully loaded data for table: %s\", table)"}
{"path":"api/src/api/opportunities_v1/__init__.py","language":"python","type":"code","directory":"api/src/api/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/opportunities_v1/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/integrations/extracts/s3_config.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations/extracts\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/extracts/s3_config.py\nSize: 0.46 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/opportunities_v1/opportunity_blueprint.py","language":"python","type":"code","directory":"api/src/api/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/opportunities_v1/opportunity_blueprint.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"import boto3\nimport botocore\n\n\ndef get_s3_client(\n    session: boto3.Session | None = None,\n    boto_config: botocore.config.Config | None = None,\n) -> botocore.client.BaseClient:\n    \"\"\"Return an S3 client.\"\"\"\n    if boto_config is None:\n        boto_config = botocore.config.Config(signature_version=\"s3v4\")\n\n    if session is not None:\n        return session.client(\"s3\", config=boto_config)\n\n    return boto3.client(\"s3\", config=boto_config)"}
{"path":"api/src/api/opportunities_v1/opportunity_routes.py","language":"python","type":"code","directory":"api/src/api/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/opportunities_v1/opportunity_routes.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/integrations/github/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations/github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/github/__init__.py\nSize: 0.46 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/opportunities_v1/opportunity_schemas.py","language":"python","type":"code","directory":"api/src/api/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/opportunities_v1/opportunity_schemas.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"__all__ = [\n    \"GitHubGraphqlClient\",\n    \"export_roadmap_data_to_file\",\n    \"export_roadmap_data_to_object\",\n    \"export_sprint_data_to_file\",\n    \"export_sprint_data_to_object\",\n]\n\nfrom analytics.integrations.github.client import GitHubGraphqlClient\nfrom analytics.integrations.github.main import (\n    export_roadmap_data_to_file,\n    export_roadmap_data_to_object,\n    export_sprint_data_to_file,\n    export_sprint_data_to_object,\n)"}
{"path":"api/src/api/response.py","language":"python","type":"code","directory":"api/src/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/response.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/integrations/github/client.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations/github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/github/client.py\nSize: 3.67 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/route_utils.py","language":"python","type":"code","directory":"api/src/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/route_utils.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"import logging\nfrom typing import Any\n\nimport requests\n\nfrom config import get_db_settings\n\nlogger = logging.getLogger(__name__)\n\n\nclass GraphqlError(Exception):\n    \"\"\"\n    Exception raised for errors returned by the GraphQL API.\n\n    Attributes\n    ----------\n    errors : list\n        List of error details returned by the API.\n    message : str\n        Human-readable explanation of the error.\n\n    \"\"\"\n\n    def __init__(self, errors: list[dict]) -> None:\n        \"\"\"Initialize the GraphqlError.\"\"\"\n        self.errors = errors\n        self.message = f\"GraphQL API returned errors: {errors}\"\n        super().__init__(self.message)\n\n\nclass GitHubGraphqlClient:\n    \"\"\"\n    A client to interact with GitHub's GraphQL API.\n\n    Methods\n    -------\n    execute_paginated_query(query, variables, data_path, batch_size=100)\n        Executes a paginated GraphQL query and returns all results.\n\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initialize the GitHubClient.\n\n        Parameters\n        ----------\n        token : str\n            GitHub personal access token for authentication.\n\n        \"\"\"\n        settings = get_db_settings()\n        self.endpoint = \"https://api.github.com/graphql\"\n        self.headers = {\n            \"Authorization\": f\"Bearer {settings.github_token}\",\n            \"Content-Type\": \"application/json\",\n            \"GraphQL-Features\": \"sub_issues,issue_types\",\n        }\n\n    def execute_query(self, query: str, variables: dict[str, str | int]) -> dict:\n        \"\"\"\n        Make a POST request to the GitHub GraphQL API.\n\n        Parameters\n        ----------\n        query : str\n            The GraphQL query string.\n        variables : dict\n            A dictionary of variables to pass to the query.\n\n        Returns\n        -------\n        dict\n            The JSON response from the API.\n\n        \"\"\"\n        response = requests.post(\n            self.endpoint,\n            headers=self.headers,\n            json={\"query\": query, \"variables\": variables},\n            timeout=60,\n        )\n        response.raise_for_status()\n        result = response.json()\n        if \"errors\" in result:\n            raise GraphqlError(result[\"errors\"])\n        return result\n\n    def execute_paginated_query(\n        self,\n        query: str,\n        variables: dict[str, Any],\n        path_to_nodes: list[str],\n        batch_size: int = 100,\n    ) -> list[dict]:\n        \"\"\"\n        Execute a paginated GraphQL query.\n\n        Parameters\n        ----------\n        query : str\n            The GraphQL query string.\n        variables : dict\n            A dictionary of variables to pass to the query.\n        path_to_nodes : list of str\n            The path to traverse the response data to extract the \"nodes\" list,\n            so the nodes can be combined from multiple paginated responses.\n        batch_size : int, optional\n            The number of items to fetch per batch, by default 100.\n\n        Returns\n        -------\n        list of dict\n            The combined results from all paginated responses.\n\n        \"\"\"\n        all_data = []\n        has_next_page = True\n        variables[\"batch\"] = batch_size\n        variables[\"endCursor\"] = None\n\n        while has_next_page:\n            response = self.execute_query(query, variables)\n            data = response[\"data\"]\n\n            # Traverse the data path to extract nodes\n            for key in path_to_nodes:\n                data = data[key]\n\n            all_data.extend(data[\"nodes\"])\n\n            # Handle pagination\n            page_info = data[\"pageInfo\"]\n            has_next_page = page_info[\"hasNextPage\"]\n            variables[\"endCursor\"] = page_info[\"endCursor\"]\n\n        return all_data"}
{"path":"api/src/api/schemas/__init__.py","language":"python","type":"code","directory":"api/src/api/schemas","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/integrations/github/getRoadmapData.graphql\nLanguage: graphql\nType: code\nDirectory: analytics/src/analytics/integrations/github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/github/getRoadmapData.graphql\nSize: 1.37 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/schemas/extension/__init__.py","language":"python","type":"code","directory":"api/src/api/schemas/extension","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/extension/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"fragment issueContent on Issue {\n  title\n  url\n  issueType {\n    name\n  }\n  # information about issue open/closed status\n  closed\n  createdAt\n  closedAt\n  # details about the parent issue\n  parent {\n    title\n    url\n  }\n}\n\nfragment iterationContent on ProjectV2ItemFieldIterationValue {\n  iterationId\n  title\n  startDate\n  duration\n}\n\nfragment singleSelectContent on ProjectV2ItemFieldSingleSelectValue {\n  optionId\n  name\n}"}
{"path":"api/src/api/schemas/extension/field_validators.py","language":"python","type":"code","directory":"api/src/api/schemas/extension","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/extension/field_validators.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/integrations/github/getSprintData.graphql\nLanguage: graphql\nType: code\nDirectory: analytics/src/analytics/integrations/github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/github/getSprintData.graphql\nSize: 1.43 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/schemas/extension/schema.py","language":"python","type":"code","directory":"api/src/api/schemas/extension","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/extension/schema.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"fragment issueContent on Issue {\n  title\n  url\n  issueType {\n    name\n  }\n  # information about issue open/closed status\n  closed\n  createdAt\n  closedAt\n  # details about the parent issue\n  parent {\n    title\n    url\n  }\n}\n\nfragment iterationContent on ProjectV2ItemFieldIterationValue {\n  iterationId\n  title\n  startDate\n  duration\n}\n\nfragment singleSelectContent on ProjectV2ItemFieldSingleSelectValue {\n  optionId\n  name\n}"}
{"path":"api/src/api/schemas/extension/schema_common.py","language":"python","type":"code","directory":"api/src/api/schemas/extension","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/extension/schema_common.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/integrations/github/main.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations/github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/github/main.py\nSize: 5.67 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/schemas/extension/schema_fields.py","language":"python","type":"code","directory":"api/src/api/schemas/extension","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/extension/schema_fields.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"TODO(widal001): 2025-01-04 Refactor and move this to src/analytics/etl/github when\nwe disable writing to disk in https://github.com/HHS/simpler-grants-gov/issues/3203\n\"\"\"\n\nimport json\nimport logging\nfrom pathlib import Path\n\nfrom pydantic import ValidationError\n\nfrom analytics.integrations.github.client import GitHubGraphqlClient\nfrom analytics.integrations.github.validation import ProjectItem\n\nlogger = logging.getLogger(__name__)\n\nPARENT_DIR = Path(__file__).resolve().parent\n\n\ndef transform_project_data(\n    raw_data: list[dict],\n    owner: str,\n    project: int,\n    excluded_types: tuple = (),  # By default include everything\n) -> list[dict]:\n    \"\"\"Pluck and reformat relevant fields for each item in the raw data.\"\"\"\n    transformed_data = []\n\n    for i, item in enumerate(raw_data):\n        try:\n            # Validate and parse the raw item\n            validated_item = ProjectItem.model_validate(item)\n\n            # Skip excluded issue types\n            if validated_item.content.issue_type.name in excluded_types:\n                continue\n\n            # Transform into flattened format\n            transformed = {\n                # project metadata\n                \"project_owner\": owner,\n                \"project_number\": project,\n                # issue metadata\n                \"issue_title\": validated_item.content.title,\n                \"issue_url\": validated_item.content.url,\n                \"issue_parent\": validated_item.content.parent.url,\n                \"issue_type\": validated_item.content.issue_type.name,\n                \"issue_status\": validated_item.status.name,\n                \"issue_is_closed\": validated_item.content.closed,\n                \"issue_opened_at\": validated_item.content.created_at,\n                \"issue_closed_at\": validated_item.content.closed_at,\n                \"issue_points\": validated_item.points.number,\n                # sprint metadata\n                \"sprint_id\": validated_item.sprint.iteration_id,\n                \"sprint_name\": validated_item.sprint.title,\n                \"sprint_start\": validated_item.sprint.start_date,\n                \"sprint_length\": validated_item.sprint.duration,\n                \"sprint_end\": validated_item.sprint.end_date,\n                # roadmap metadata\n                \"deliverable_pillar\": validated_item.pillar.name,\n                \"quad_id\": validated_item.quad.iteration_id,\n                \"quad_name\": validated_item.quad.title,\n                \"quad_start\": validated_item.quad.start_date,\n                \"quad_length\": validated_item.quad.duration,\n                \"quad_end\": validated_item.quad.end_date,\n            }\n            transformed_data.append(transformed)\n        except ValidationError as err:\n            logger.error(\"Error parsing row %d, skipped.\", i)  # noqa: TRY400\n            logger.debug(\"Error: %s\", err)\n            continue\n\n    return transformed_data\n\n\ndef export_sprint_data_to_file(\n    client: GitHubGraphqlClient,\n    owner: str,\n    project: int,\n    sprint_field: str,\n    points_field: str,\n    output_file: str,\n) -> None:\n    \"\"\"Export the issue and project data from a Sprint Board.\"\"\"\n    transformed_data = export_sprint_data_to_object(\n        client=client,\n        owner=owner,\n        project=project,\n        sprint_field=sprint_field,\n        points_field=points_field,\n    )\n\n    # Write output\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(transformed_data, f, indent=2)\n\n\ndef export_sprint_data_to_object(\n    client: GitHubGraphqlClient,\n    owner: str,\n    project: int,\n    sprint_field: str,\n    points_field: str,\n) -> list[dict]:\n    \"\"\"Export the issue and project data from a Sprint Board.\"\"\"\n    # Load query\n    query_path = PARENT_DIR / \"getSprintData.graphql\"\n    with open(query_path) as f:\n        query = f.read()\n\n    # Set query variables\n    variables = {\n        \"login\": owner,\n        \"project\": project,\n        \"sprintField\": sprint_field,\n        \"pointsField\": points_field,\n    }\n\n    # Execute query\n    data = client.execute_paginated_query(\n        query,\n        variables,\n        [\"organization\", \"projectV2\", \"items\"],\n    )\n\n    # Transform data\n    # And exclude deliverables if they appear on the sprint boards\n    # so that we use their status value from the roadmap board instead\n    return transform_project_data(\n        raw_data=data,\n        owner=owner,\n        project=project,\n        excluded_types=(\"Deliverable\",),\n    )\n\n\ndef export_roadmap_data_to_file(\n    client: GitHubGraphqlClient,\n    owner: str,\n    project: int,\n    quad_field: str,\n    pillar_field: str,\n    output_file: str,\n) -> None:\n    \"\"\"Export the epic and deliverable data from GitHub.\"\"\"\n    transformed_data = export_roadmap_data_to_object(\n        client=client,\n        owner=owner,\n        project=project,\n        quad_field=quad_field,\n        pillar_field=pillar_field,\n    )\n\n    # Write output\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(transformed_data, f, indent=2)\n\n\ndef export_roadmap_data_to_object(\n    client: GitHubGraphqlClient,\n    owner: str,\n    project: int,\n    quad_field: str,\n    pillar_field: str,\n) -> list[dict]:\n    \"\"\"Export the epic and deliverable data from GitHub.\"\"\"\n    # Load query\n    query_path = PARENT_DIR / \"getRoadmapData.graphql\"\n    with open(query_path) as f:\n        query = f.read()\n\n    # Set query variables\n    variables = {\n        \"login\": owner,\n        \"project\": project,\n        \"quadField\": quad_field,\n        \"pillarField\": pillar_field,\n    }\n\n    # Execute query\n    data = client.execute_paginated_query(\n        query,\n        variables,\n        [\"organization\", \"projectV2\", \"items\"],\n    )\n\n    # Transform data\n    return transform_project_data(data, owner, project)"}
{"path":"api/src/api/schemas/response_schema.py","language":"python","type":"code","directory":"api/src/api/schemas","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/response_schema.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/integrations/github/validation.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations/github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/github/validation.py\nSize: 4.42 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/schemas/search_schema.py","language":"python","type":"code","directory":"api/src/api/schemas","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/search_schema.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"# pylint: disable=no-self-argument\n# mypy: disable-error-code=\"literal-required\"\n# This mypy disable is needed so we can use constants for field aliases\n\nfrom datetime import datetime, timedelta\n\nfrom pydantic import BaseModel, Field, computed_field, model_validator\n\n# Declare constants for the fields that need to be aliased from the GitHub data\n# so that we only have to change these values in one place.\n#\n# We need to declare them at the module-level instead of class-level\n# because pydantic throws an error when using class level constants.\n\n# Issue content aliases\nISSUE_TYPE = \"issueType\"\nCLOSED_AT = \"closedAt\"\nCREATED_AT = \"createdAt\"\nPARENT = \"parent\"\n# Iteration aliases\nITERATION_ID = \"iterationId\"\nSTART_DATE = \"startDate\"\n# Single select aliases\nOPTION_ID = \"optionId\"\n\n\ndef safe_default_factory(data: dict, keys_to_replace: list[str]) -> dict:\n    \"\"\"\n    Replace keys that are explicitly set to None with an empty dict for default_factory.\n\n    We need to do this because if a key is present, but its value is None or null,\n    it will raise a Pydantic ValidationError rather than using the default_factory.\n    \"\"\"\n    for key in keys_to_replace:\n        if data.get(key) is None:\n            data[key] = {}\n    return data\n\n\n# #############################################\n# Issue content sub-schemas\n# #############################################\n\n\nclass IssueParent(BaseModel):\n    \"\"\"Schema for the parent issue of a sub-issue.\"\"\"\n\n    title: str | None = None\n    url: str | None = None\n\n\nclass IssueType(BaseModel):\n    \"\"\"Schema for the type of an issue.\"\"\"\n\n    name: str | None = None\n\n\nclass IssueContent(BaseModel):\n    \"\"\"Schema for core issue metadata.\"\"\"\n\n    # The fields that we're parsing from the raw GitHub output\n    title: str\n    url: str\n    closed: bool\n    created_at: str = Field(alias=CREATED_AT)\n    closed_at: str | None = Field(alias=CLOSED_AT, default=None)\n    issue_type: IssueType = Field(alias=ISSUE_TYPE, default_factory=IssueType)\n    parent: IssueParent = Field(default_factory=IssueParent)\n\n    @model_validator(mode=\"before\")\n    def replace_none_with_defaults(cls, values) -> dict:  # noqa: ANN001, N805\n        \"\"\"Replace keys that are set to None with default_factory instances.\"\"\"\n        # Replace None with default_factory instances\n        return safe_default_factory(values, [ISSUE_TYPE, PARENT])\n\n\n# #############################################\n# Project field sub-schemas\n# #############################################\n\n\nclass IterationValue(BaseModel):\n    \"\"\"Schema for iteration field values like Sprint or Quad.\"\"\"\n\n    iteration_id: str | None = Field(alias=ITERATION_ID, default=None)\n    title: str | None = None\n    start_date: str | None = Field(alias=START_DATE, default=None)\n    duration: int | None = None\n\n    @computed_field\n    def end_date(self) -> str | None:\n        \"\"\"Calculate the end date of the iteration.\"\"\"\n        if not self.start_date or not self.duration:\n            return None\n\n        start = datetime.strptime(self.start_date, \"%Y-%m-%d\")  # noqa: DTZ007\n        end = start + timedelta(days=self.duration)\n        return end.strftime(\"%Y-%m-%d\")\n\n\nclass SingleSelectValue(BaseModel):\n    \"\"\"Schema for single select field values like Status or Pillar.\"\"\"\n\n    option_id: str | None = Field(alias=OPTION_ID, default=None)\n    name: str | None = None\n\n\nclass NumberValue(BaseModel):\n    \"\"\"Schema for number field values like Points.\"\"\"\n\n    number: int | None = None\n\n\n# #############################################\n# Top-level project item schemas\n# #############################################\n\n\nclass ProjectItem(BaseModel):\n    \"\"\"Schema that combines fields from both RoadmapItem and SprintItem.\"\"\"\n\n    # Issue fields\n    content: IssueContent\n    status: SingleSelectValue = Field(default_factory=SingleSelectValue)\n    # Sprint fields\n    sprint: IterationValue = Field(default_factory=IterationValue)\n    points: NumberValue = Field(default_factory=NumberValue)\n    # Roadmap fields\n    quad: IterationValue = Field(default_factory=IterationValue)\n    pillar: SingleSelectValue = Field(default_factory=SingleSelectValue)\n\n    @model_validator(mode=\"before\")\n    def replace_none_with_defaults(cls, values) -> dict:  # noqa: ANN001, N805\n        \"\"\"Replace keys that are set to None with default_factory instances.\"\"\"\n        return safe_default_factory(\n            values,\n            [\"sprint\", \"points\", \"quad\", \"pillar\", \"status\"],\n        )"}
{"path":"api/src/api/users/README.md","language":"markdown","type":"code","directory":"api/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/users/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/integrations/slack.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/integrations\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/integrations/slack.py\nSize: 1.99 KB\nLast Modified: 2025-02-14T17:08:26.409Z"}
{"path":"api/src/api/users/__init__.py","language":"python","type":"code","directory":"api/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/users/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"import functools\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Any, Callable\n\nfrom slack_sdk import WebClient\nfrom slack_sdk.errors import SlackApiError\nfrom slack_sdk.web.slack_response import SlackResponse\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass FileMapping:\n    \"\"\"A dataclass that maps file path to file name.\"\"\"\n\n    path: str\n    name: str\n\n\ndef slack_api_error_handler(slackbot_api_call: Callable) -> Callable:\n    \"\"\"Wrap SlackBot methods with a try-except block.\"\"\"\n\n    @functools.wraps(slackbot_api_call)\n    def try_to_make_slackbot_api_call_and_catch_error(\n        *args: Any,  # noqa: ANN401\n        **kwargs: Any,  # noqa: ANN401\n    ) -> SlackResponse | None:\n        \"\"\"Try to make a slack API call, and print the error if it fails.\"\"\"\n        try:\n            return slackbot_api_call(*args, **kwargs)\n        except SlackApiError as e:\n            logger.info(e)\n            return None\n\n    return try_to_make_slackbot_api_call_and_catch_error\n\n\nclass SlackBot:\n    \"\"\"Interact with slack using a slack bot.\"\"\"\n\n    def __init__(self, client: WebClient) -> None:\n        \"\"\"Instantiate the SlackBot class.\"\"\"\n        self.client = client\n\n    @slack_api_error_handler\n    def fetch_slack_channel_info(self, channel_id: str) -> SlackResponse | None:\n        \"\"\"Get info about a slack channel.\"\"\"\n        # Call the conversations.info method using the WebClient\n        return self.client.conversations_info(channel=channel_id)\n\n    @slack_api_error_handler\n    def upload_files_to_slack_channel(\n        self,\n        channel_id: str,\n        files: list[FileMapping],\n        message: str,\n    ) -> SlackResponse | None:\n        \"\"\"Upload files to a slack channel with a message.\"\"\"\n        return self.client.files_upload_v2(\n            file_uploads=[{\"file\": f.path, \"title\": f.name} for f in files],\n            channel=channel_id,\n            initial_comment=message,\n        )"}
{"path":"api/src/api/users/user_blueprint.py","language":"python","type":"code","directory":"api/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/users/user_blueprint.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/logs/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/logs\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/__init__.py\nSize: 0.94 KB\nLast Modified: 2025-02-14T17:08:26.410Z"}
{"path":"api/src/api/users/user_routes.py","language":"python","type":"code","directory":"api/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/users/user_routes.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"There are two formatters for the log messages: human-readable and JSON.\nThe formatter that is used is determined by the environment variable\nLOG_FORMAT. If the environment variable is not set, the JSON formatter\nis used by default. See src.logging.formatters for more information.\n\nThe logger also adds a PII mask filter to the root logger. See\nsrc.logging.pii for more information.\n\nUsage:\n    import src.logging\n\n    with analytics.logs.init(\"program name\"):\n        ...\n\nOnce the module has been initialized, the standard logging module can be\nused to log messages:\n\nExample:\n-------\n    import logging\n\n    logger = logging.getLogger(__name__)\n    logger.info(\"message\")\n\n\"\"\"\n\nfrom analytics.logs.config import LoggingContext\n\n\ndef init(program_name: str) -> LoggingContext:\n    \"\"\"Initialize logging for the lifecycle of the application.\"\"\"\n    return LoggingContext(program_name)"}
{"path":"api/src/api/users/user_schemas.py","language":"python","type":"code","directory":"api/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/users/user_schemas.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/logs/app_logger.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/logs\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/app_logger.py\nSize: 1.95 KB\nLast Modified: 2025-02-14T17:08:26.410Z"}
{"path":"api/src/app.py","language":"python","type":"code","directory":"api/src","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/app.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"This module configures an application's logger to add extra data\nto all log messages. Application context data such as the app name.\n\nUsage:\n    import logging\n    import analytics.logs.app_logger as app_logger\n\n    logger = logging.getLogger(__name__)\n\n    app_logger.init_app(logging.root)\n\"\"\"\n\nimport logging\nimport os\n\nEXTRA_LOG_DATA_ATTR = \"extra_log_data\"\n\n_GLOBAL_LOG_CONTEXT: dict = {}\n\n\ndef init_app(app_logger: logging.Logger) -> None:\n    \"\"\"\n    Initialize the app logger.\n\n    Adds app context data to every log record using log filters.\n    https://docs.python.org/3/howto/logging-cookbook.html#using-filters-to-impart-contextual-information\n\n    Usage:\n        import logging\n        import analytics.logs.app_logger as app_logger\n\n        logger = logging.getLogger(__name__)\n\n        app_logger.init_app(logging.root)\n    \"\"\"\n    # Need to add filters to each of the handlers rather than to the logger itself, since\n    # messages are passed directly to the ancestor loggers` handlers bypassing any filters\n    # set on the ancestors.\n    # See https://docs.python.org/3/library/logging.html#logging.Logger.propagate\n    for handler in app_logger.handlers:\n        handler.addFilter(_add_global_context_info_to_log_record)\n\n    # Add some metadata to all log messages globally\n    add_extra_data_to_global_logs({\"environment\": os.environ.get(\"ENVIRONMENT\")})\n\n    app_logger.info(\"initialized logger\")\n\n\ndef add_extra_data_to_global_logs(\n    data: dict[str, str | int | float | bool | None],\n) -> None:\n    \"\"\"Add metadata to all logs for the rest of the lifecycle of this app process.\"\"\"\n    global _GLOBAL_LOG_CONTEXT  # noqa: PLW0602\n    _GLOBAL_LOG_CONTEXT.update(data)\n\n\ndef _add_global_context_info_to_log_record(record: logging.LogRecord) -> bool:\n    global _GLOBAL_LOG_CONTEXT  # noqa: PLW0602\n    record.__dict__ |= _GLOBAL_LOG_CONTEXT\n\n    return True"}
{"path":"api/src/app_config.py","language":"python","type":"code","directory":"api/src","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/app_config.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/logs/config.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/logs\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/config.py\nSize: 5.90 KB\nLast Modified: 2025-02-14T17:08:26.410Z"}
{"path":"api/src/auth/README.md","language":"markdown","type":"code","directory":"api/src/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.124Z","content":"import logging\nimport os\nimport platform\nimport pwd\nimport sys\nfrom contextlib import AbstractContextManager\nfrom types import TracebackType\nfrom typing import Any, cast\n\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nfrom analytics.logs import formatters, pii\n\nlogger = logging.getLogger(__name__)\n\n_original_argv = tuple(sys.argv)\n\n\nclass HumanReadableFormatterConfig(BaseSettings):\n    \"\"\"Configuration settings for the human-readable log formatter.\"\"\"\n\n    message_width: int = formatters.HUMAN_READABLE_FORMATTER_DEFAULT_MESSAGE_WIDTH\n\n\nclass LoggingConfig(BaseSettings):\n    \"\"\"Configuration settings for logging.\"\"\"\n\n    model_config = SettingsConfigDict(env_prefix=\"log_\", env_nested_delimiter=\"__\")\n\n    format: str = \"json\"\n    level: str = \"INFO\"\n    human_readable_formatter: HumanReadableFormatterConfig = (\n        HumanReadableFormatterConfig()\n    )\n\n\nclass LoggingContext(AbstractContextManager[None]):\n    \"\"\"\n    A context manager for handling setting up the logging stream.\n\n    To help facillitate being able to test logging, we need to be able\n    to easily create temporary output streams and then tear them down.\n\n    When this context manager is torn down, the stream handler created\n    with it will be removed.\n\n    For example:\n    ```py\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    with LoggingContext(\"example_program_name\"):\n        # This log message will go to stdout\n        logger.info(\"example log message\")\n\n    # This log message won't go to stdout as the\n    # handler will have been removed\n    logger.info(\"example log message\")\n    ```\n    Note that any other handlers added to the root logger won't be affected\n    and calling this multiple times before exit would result in duplicate logs.\n    \"\"\"\n\n    def __init__(self, program_name: str) -> None:\n        \"\"\"Initialize the logging context.\"\"\"\n        self._configure_logging()\n        log_program_info(program_name)\n\n    def __enter__(self) -> None:\n        \"\"\"Enter the context manager.\"\"\"\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        \"\"\"Exit the context manager.\"\"\"\n        # Remove the console handler to stop logs from being sent to stdout\n        # This is useful in the test suite, since multiple tests may initialize\n        # separate duplicate handlers. This allows for easier cleanup for each\n        # of those tests.\n        logging.root.removeHandler(self.console_handler)\n\n    def _configure_logging(self) -> None:\n        \"\"\"\n        Configure logging for the application.\n\n        Configures the root module logger to log to stdout.\n        Adds a PII mask filter to the root logger.\n        Also configures log levels third party packages.\n        \"\"\"\n        config = LoggingConfig()\n\n        # Loggers can be configured using config functions defined\n        # in logging.config or by directly making calls to the main API\n        # of the logging module (see https://docs.python.org/3/library/logging.config.html)\n        # We opt to use the main API using functions like `addHandler` which is\n        # non-destructive, i.e. it does not overwrite any existing handlers.\n        # In contrast, logging.config.dictConfig() would overwrite any existing loggers.\n        # This is important during testing, since fixtures like `caplog` add handlers that would\n        # get overwritten if we call logging.config.dictConfig() during the scope of the test.\n        self.console_handler = logging.StreamHandler(sys.stdout)\n        formatter = get_formatter(config)\n        self.console_handler.setFormatter(formatter)\n        self.console_handler.addFilter(pii.mask_pii)\n        logging.root.addHandler(self.console_handler)\n        logging.root.setLevel(config.level)\n\n        # Configure loggers for third party packages\n        logging.getLogger(\"sqlalchemy.pool\").setLevel(logging.INFO)\n        logging.getLogger(\"sqlalchemy.dialects.postgresql\").setLevel(logging.INFO)\n\n\ndef get_formatter(config: LoggingConfig) -> logging.Formatter:\n    \"\"\"\n    Return the formatter used by the root logger.\n\n    The formatter is determined by the environment variable LOG_FORMAT. If the\n    environment variable is not set, the JSON formatter is used by default.\n    \"\"\"\n    if config.format == \"human-readable\":\n        return get_human_readable_formatter(config.human_readable_formatter)\n    return formatters.JsonFormatter()\n\n\ndef log_program_info(program_name: str) -> None:\n    \"\"\"Log generic information about the software and hardware.\"\"\"\n    logger.info(\n        \"start %s: %s %s %s, hostname %s, pid %i, user %i(%s)\",\n        program_name,\n        platform.python_implementation(),\n        platform.python_version(),\n        platform.system(),\n        platform.node(),\n        os.getpid(),\n        os.getuid(),\n        pwd.getpwuid(os.getuid()).pw_name,\n        extra={\n            \"hostname\": platform.node(),\n            \"cpu_count\": os.cpu_count(),\n            # If mypy is run on a mac, it will throw a module has no attribute error, even though\n            # we never actually access it with the conditional.\n            #\n            # However, we can't just silence this error, because on linux (e.g. CI/CD) that will\n            # throw an unused â€œtype: ignoreâ€ comment error. Casting to Any instead ensures this\n            # passes regardless of where mypy is being run\n            \"cpu_usable\": (\n                len(cast(Any, os).sched_getaffinity(0))\n                if \"sched_getaffinity\" in dir(os)\n                else \"unknown\"\n            ),\n        },\n    )\n    logger.info(\"invoked as: %s\", \" \".join(_original_argv))\n\n\ndef get_human_readable_formatter(\n    config: HumanReadableFormatterConfig,\n) -> formatters.HumanReadableFormatter:\n    \"\"\"Return the human readable formatter used by the root logger.\"\"\"\n    return formatters.HumanReadableFormatter(message_width=config.message_width)"}
{"path":"api/src/auth/__init__.py","language":"python","type":"code","directory":"api/src/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/logs/decodelog.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/logs\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/decodelog.py\nSize: 3.56 KB\nLast Modified: 2025-02-14T17:08:26.410Z"}
{"path":"api/src/auth/api_jwt_auth.py","language":"python","type":"code","directory":"api/src/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/api_jwt_auth.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"Expects JSON log lines or `docker-compose log` output on stdin and outputs plain text lines on\nstdout.\n\nThis module intentionally has no dependencies outside the standard library so that it can be run\nas a script outside the virtual environment if needed.\n\"\"\"\n\nimport datetime\nimport json\nfrom typing import Mapping  # noqa:  UP035\n\nRED = \"\\033[31m\"\nGREEN = \"\\033[32m\"\nBLUE = \"\\033[34m\"\nORANGE = \"\\033[38;5;208m\"\nRESET = \"\\033[0m\"\nNO_COLOUR = \"\"\n\nDEFAULT_MESSAGE_WIDTH = 50\n\n\ndef process_line(line: str) -> str | None:\n    \"\"\"Process a line of the log and return the reformatted line.\"\"\"\n    line = line.rstrip()\n    if line and line[0] == \"{\":\n        # JSON format\n        return decode_json_line(line)\n    if \"| {\" in line:\n        # `docker-compose logs ...` format\n        return decode_json_line(line[line.find(\"| {\") + 2 :])\n    # Anything else is left alone\n    return line\n\n\ndef decode_json_line(line: str) -> str | None:\n    \"\"\"Decode a JSON log line and return the reformatted line.\"\"\"\n    try:\n        data = json.loads(line)\n    except json.decoder.JSONDecodeError:\n        return line\n\n    name = data.pop(\"name\", \"-\")\n    level = data.pop(\"levelname\", \"-\")\n    func_name = data.pop(\"funcName\", \"-\")\n    created = datetime.datetime.fromtimestamp(\n        float(data.pop(\"created\", 0)),\n        tz=datetime.timezone.utc,\n    )\n    message = data.pop(\"message\", \"-\")\n\n    if level == \"AUDIT\":\n        return None\n\n    return format_line(created, name, func_name, level, message, data)\n\n\ndef format_line(\n    created: datetime.datetime,\n    logger_name: str,\n    func_name: str,\n    level: str,\n    message: str,\n    extra: Mapping[str, str],\n    message_width: int = DEFAULT_MESSAGE_WIDTH,\n) -> str:\n    \"\"\"Format log fields as a coloured string.\"\"\"\n    logger_name_color = color_for_name(logger_name)\n    level_color = color_for_level(level)\n    return f\"{format_datetime(created)}  {colorize(logger_name.ljust(36), logger_name_color)} {func_name:<28} {colorize(level.ljust(8), level_color)} {colorize(message.ljust(message_width), level_color)} {colorize(format_extra(extra), BLUE)}\"  # noqa: E501\n\n\ndef colorize(text: str, color: str) -> str:\n    \"\"\"Util method for adding color to text.\"\"\"\n    return f\"{color}{text}{RESET}\"\n\n\ndef color_for_name(name: str) -> str:\n    \"\"\"Util method for mapping color of text based on name.\"\"\"\n    if name.startswith(\"src\"):\n        return GREEN\n    if name.startswith(\"sqlalchemy\"):\n        return ORANGE\n    return NO_COLOUR\n\n\ndef color_for_level(level: str) -> str:\n    \"\"\"Util method for mapping color of text based on log level severity.\"\"\"\n    if level in (\"WARNING\", \"ERROR\", \"CRITICAL\"):\n        return RED\n    return NO_COLOUR\n\n\ndef format_datetime(created: datetime.datetime) -> str:\n    \"\"\"Format datetime in output.\"\"\"\n    return created.time().isoformat(timespec=\"milliseconds\")\n\n\nEXCLUDE_EXTRA = {\n    \"args\",\n    \"created\",\n    \"entity.guid\",\n    \"entity.name\",\n    \"entity.type\",\n    \"exc_info\",\n    \"filename\",\n    \"funcName\",\n    \"levelname\",\n    \"levelno\",\n    \"lineno\",\n    \"message\",\n    \"module\",\n    \"msecs\",\n    \"msg\",\n    \"name\",\n    \"pathname\",\n    \"process\",\n    \"processName\",\n    \"relativeCreated\",\n    \"span.id\",\n    \"thread\",\n    \"threadName\",\n    \"trace.id\",\n    \"traceId\",\n}\n\n\ndef format_extra(data: Mapping[str, str]) -> str:\n    \"\"\"Format the extra json into human-readable text.\"\"\"\n    return \" \".join(\n        f\"{key}={value}\"\n        for key, value in data.items()\n        if key not in EXCLUDE_EXTRA and value is not None\n    )"}
{"path":"api/src/auth/api_key_auth.py","language":"python","type":"code","directory":"api/src/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/api_key_auth.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/logs/ecs_background_task.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/logs\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/ecs_background_task.py\nSize: 4.82 KB\nLast Modified: 2025-02-14T17:08:26.410Z"}
{"path":"api/src/auth/auth_errors.py","language":"python","type":"code","directory":"api/src/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/auth_errors.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"import contextlib\nimport logging\nimport os\nimport time\nimport uuid\nfrom functools import wraps\nfrom typing import Callable, Generator, ParamSpec, TypeVar  # noqa:  UP035\n\nimport requests\n\nfrom analytics.logs.app_logger import add_extra_data_to_global_logs\n\nlogger = logging.getLogger(__name__)\n\nP = ParamSpec(\"P\")\nT = TypeVar(\"T\")\n\n\ndef ecs_background_task(task_name: str) -> Callable[[Callable[P, T]], Callable[P, T]]:\n    \"\"\"\n    Decorate an ECS task to automatically add pre & post logging information.\n\n    This encapsulates the setup required by all ECS tasks, making it easy to:\n    - add new shared initialization steps for logging\n    - write new ECS task code without thinking about the boilerplate\n\n    Usage:\n\n        TASK_NAME = \"my-cool-task\"\n\n        @task_blueprint.cli.command(TASK_NAME, help=\"For running my cool task\")\n        @ecs_background_task(TASK_NAME)\n        @app_db.with_db_session()\n        def entrypoint(db_session: db.Session):\n            do_cool_stuff()\n\n    IMPORTANT: Do not specify this decorator before the task command.\n               CLI tooling rewrites your function to be a main function\n               and any decorators from before the \"task_blueprint.cli.command(...)\"\n               line are discarded.\n               See:\n               https://click.palletsprojects.com/en/8.1.x/quickstart/#basic-concepts-creating-a-command\n    \"\"\"\n\n    def decorator(f: Callable[P, T]) -> Callable[P, T]:\n        @wraps(f)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\n            with _ecs_background_task_impl(task_name):\n                return f(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\n@contextlib.contextmanager\ndef _ecs_background_task_impl(task_name: str) -> Generator[None, None, None]:\n    # The actual implementation, see the docs on the\n    # decorator method above for details on usage\n\n    start = time.perf_counter()\n    _add_log_metadata(task_name)\n\n    logger.info(\"Starting ECS task %s\", task_name)\n\n    try:\n        yield\n    except Exception:\n        # We want to make certain that any exception will always\n        # be logged as an error\n        # logger.exception is just an alias for logger.error(<msg>, exc_info=True)\n        logger.exception(\"ECS task failed\", extra={\"status\": \"error\"})\n        raise\n\n    end = time.perf_counter()\n    duration = round((end - start), 3)\n    logger.info(\n        \"Completed ECS task %s\",\n        task_name,\n        extra={\"ecs_task_duration_sec\": duration, \"status\": \"success\"},\n    )\n\n\ndef _add_log_metadata(task_name: str) -> None:\n    # Note we set an \"aws.ecs.task_name\" as well pulled from ECS\n    # which may be different as that value is set based on our infra setup\n    # while this one is just based on whatever we passed the @ecs_background_task decorator\n    add_extra_data_to_global_logs(\n        {\"task_name\": task_name, \"task_uuid\": str(uuid.uuid4())},\n    )\n    add_extra_data_to_global_logs(_get_ecs_metadata())\n\n\ndef _get_ecs_metadata() -> dict:\n    # Retrieve ECS metadata from an AWS-provided metadata URI.\n    # This URI is injected to all ECS tasks by AWS as an envar.\n    # See:\n    # https://docs.aws.amazon.com/AmazonECS/latest/userguide/task-metadata-endpoint-v4-fargate.html\n\n    ecs_metadata_uri = os.environ.get(\"ECS_CONTAINER_METADATA_URI_V4\")\n\n    if os.environ.get(\"ENVIRONMENT\", \"local\") == \"local\" or ecs_metadata_uri is None:\n        logger.info(\n            \"ECS metadata not available for local environments. \"\n            \"Run this task on ECS to see metadata.\",\n        )\n        return {}\n\n    task_metadata = requests.get(ecs_metadata_uri, timeout=1)  # 1sec timeout\n    logger.info(\"Retrieved task metadata from ECS\")\n    metadata_json = task_metadata.json()\n\n    ecs_task_name = metadata_json[\"Name\"]\n    ecs_task_id = metadata_json[\"Labels\"][\"com.amazonaws.ecs.task-arn\"].split(\"/\")[-1]\n    ecs_taskdef = \":\".join(\n        [\n            metadata_json[\"Labels\"][\"com.amazonaws.ecs.task-definition-family\"],\n            metadata_json[\"Labels\"][\"com.amazonaws.ecs.task-definition-version\"],\n        ],\n    )\n    cloudwatch_log_group = metadata_json[\"LogOptions\"][\"awslogs-group\"]\n    cloudwatch_log_stream = metadata_json[\"LogOptions\"][\"awslogs-stream\"]\n\n    # Step function only\n    sfn_execution_id = os.environ.get(\"SFN_EXECUTION_ID\")\n    sfn_id = sfn_execution_id.split(\":\")[-2] if sfn_execution_id is not None else None\n\n    return {\n        \"aws.ecs.task_name\": ecs_task_name,\n        \"aws.ecs.task_id\": ecs_task_id,\n        \"aws.ecs.task_definition\": ecs_taskdef,\n        # these will be added automatically by New Relic log ingester, but\n        # just to be sure and for non-log usages, explicitly declare them\n        \"aws.cloudwatch.log_group\": cloudwatch_log_group,\n        \"aws.cloudwatch.log_stream\": cloudwatch_log_stream,\n        \"aws.step_function.id\": sfn_id,\n    }"}
{"path":"api/src/auth/auth_utils.py","language":"python","type":"code","directory":"api/src/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/auth_utils.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/logs/formatters.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/logs\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/formatters.py\nSize: 3.48 KB\nLast Modified: 2025-02-14T17:08:26.410Z"}
{"path":"api/src/auth/jwt_user_http_token_auth.py","language":"python","type":"code","directory":"api/src/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/jwt_user_http_token_auth.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"This module defines two formatters, JsonFormatter for machine-readable logs to\nbe used in production, and HumanReadableFormatter for human readable logs to\nbe used used during development.\n\nSee https://docs.python.org/3/library/logging.html#formatter-objects\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import date, datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Callable, TypeVar\nfrom uuid import UUID\n\nfrom analytics.logs import decodelog\n\nT = TypeVar(\"T\")\n\n\ndef identity(obj: T) -> T:\n    \"\"\"Return an unmodified object.\"\"\"\n    return obj\n\n\n# Mapping of types to functions for conversion\n# when writing logs to JSON\nENCODERS_BY_TYPE: dict[type[Any], Callable[[Any], Any]] = {\n    # JSONEncoder handles these properly already:\n    # https://docs.python.org/3/library/json.html#json.JSONEncoder\n    str: identity,\n    int: identity,\n    float: identity,\n    bool: identity,\n    list: identity,\n    datetime: lambda d: d.isoformat(),\n    date: lambda d: d.isoformat(),\n    Enum: lambda e: e.value,\n    set: list,\n    # The fallback below would do these,\n    # but making it explicit that these\n    # types are supported for logging.\n    Decimal: str,\n    UUID: str,\n    Exception: str,\n}\n\n\ndef json_encoder(obj: Any) -> Any:  # noqa: ANN401\n    \"\"\"\n    Handle conversion of various types when logs are serialized.\n\n    If not specified will attempt to convert using str() on the object.\n\n    The recommended approach from the JSON docs\n    is to call the default method from JSONEncoder\n    to allow it to error anything not defined, we\n    choose not to do that as we want to give a best\n    effort for every value to be serialized for the logs\n    https://docs.python.org/3/library/json.html\n\n    If a field you are trying to log doesn't make sense\n    to format as a string then please add it above, but be\n    aware that the format needs to be parseable by whatever\n    tools you are using to ingest logs and metrics.\n    \"\"\"\n    _type = type(obj)\n    encode = ENCODERS_BY_TYPE.get(_type, str)\n\n    return encode(obj)\n\n\nclass JsonFormatter(logging.Formatter):\n    \"\"\"A logging formatter which formats each line as JSON.\"\"\"\n\n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"Format the record as JSON.\"\"\"\n        # logging.Formatter.format adds the `message` attribute to the LogRecord\n        # see https://github.com/python/cpython/blob/main/Lib/logging/__init__.py#L690-L720\n        super().format(record)\n\n        return json.dumps(record.__dict__, separators=(\",\", \":\"), default=json_encoder)\n\n\nHUMAN_READABLE_FORMATTER_DEFAULT_MESSAGE_WIDTH = decodelog.DEFAULT_MESSAGE_WIDTH\n\n\nclass HumanReadableFormatter(logging.Formatter):\n    \"\"\"A logging formatter which formats each line as color-code human-readable text.\"\"\"\n\n    message_width: int\n\n    def __init__(\n        self,\n        message_width: int = HUMAN_READABLE_FORMATTER_DEFAULT_MESSAGE_WIDTH,\n    ) -> None:\n        \"\"\"Create a human-readable formatter.\"\"\"\n        super().__init__()\n        self.message_width = message_width\n\n    def format(self, record: logging.LogRecord) -> str:\n        \"\"\"Format the log record to be human-readable with color-coding.\"\"\"\n        message = super().format(record)\n        return decodelog.format_line(\n            datetime.fromtimestamp(record.created, tz=timezone.utc),\n            record.name,\n            record.funcName,\n            record.levelname,\n            message,\n            record.__dict__,\n            message_width=self.message_width,\n        )"}
{"path":"api/src/auth/login_gov_jwt_auth.py","language":"python","type":"code","directory":"api/src/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/login_gov_jwt_auth.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/src/analytics/logs/pii.py\nLanguage: py\nType: code\nDirectory: analytics/src/analytics/logs\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/src/analytics/logs/pii.py\nSize: 3.16 KB\nLast Modified: 2025-02-14T17:08:26.410Z"}
{"path":"api/src/constants/__init__.py","language":"python","type":"code","directory":"api/src/constants","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/constants/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"This module defines a filter that can be attached to a logger to mask PII\nfrom log records. The filter is applied to all log records, and masks PII\nthat looks like social security numbers.\n\nYou can add the filter to a handler:\n\nExample:\n-------\n    import logging\n    import src.logging.pii as pii\n\n    handler = logging.StreamHandler()\n    handler.addFilter(pii.mask_pii)\n    logger = logging.getLogger(__name__)\n    logger.addHandler(handler)\n\nOr you can add the filter directly to a logger.\nIf adding the filter directly to a logger, take note that the filter\nwill not be called for child loggers.\nSee https://docs.python.org/3/library/logging.html#logging.Logger.propagate\n\nExample:\n-------\n    import logging\n    import src.logging.pii as pii\n\n    logger = logging.getLogger(__name__)\n    logger.addFilter(pii.mask_pii)\n\n\"\"\"\n\nimport logging\nimport re\nfrom typing import Any\n\n\ndef mask_pii(record: logging.LogRecord) -> bool:\n    \"\"\"Remove pii from log records.\"\"\"\n    # Loop through all entries in the record's __dict__\n    # attribute and mask any things that look like PII.\n    # We will mask positional args separately below.\n    record.__dict__ |= {\n        key: _mask_pii_for_key(key, value)\n        for key, value in record.__dict__.items()\n        if key != \"args\"  # Handle positional \"args\" separately\n    }\n\n    # record.__dict__[\"args\"] will contain positional arguments to logging calls.\n    # For example, a call like logger.info(\"%s %s\", \"foo\", \"bar\") will result in a LogRecord\n    # with record.__dict__[\"args\"] == (\"foo\", \"bar\")\n    # We want to mask the PII on each argument separately rather than trying to do a PII regex\n    # match on the entire args tuple.\n    args = record.__dict__[\"args\"]\n    record.__dict__[\"args\"] = tuple(map(_mask_pii, args))\n    return True\n\n\n# Regular expression to match a tax identifier (SSN), 9 digits with optional dashes.\n# Matches between word boundaries (\\b), except when:\n#  - Preceded by word character and dash (e.g. \"ip-10-11-12-134\")\n#  - Followed by a dot and digit, for decimal numbers (e.g. 999000000.5)\n# See https://docs.python.org/3/library/re.html#regular-expression-syntax\nTIN_RE = re.compile(\n    r\"\"\"\n        \\b          # word boundary\n        (?<!\\w-)    # not preceded by word character and dash\n        (\\d-?){8}   # digit then optional dash, 8 times\n        \\d          # last digit\n        \\b          # word boundary\n        (?!\\.\\d)    # not followed by decimal point and digit (for decimal numbers)\n    \"\"\",\n    re.ASCII | re.VERBOSE,\n)\n\nALLOW_NO_MASK = {\n    \"account_key\",\n    \"count\",\n    \"created\",\n    \"hostname\",\n    \"process\",\n    \"thread\",\n}\n\n\ndef _mask_pii_for_key(key: str, value: Any | None) -> Any | None:  # noqa: ANN401\n    \"\"\"\n    Mask the given value if it has the pattern of a tax identifier.\n\n    If its key is one of the allowed values, skip masking as we assume it is\n    something that looks like an SSN but is known to be safe (like a timestamp).\n    \"\"\"\n    if key in ALLOW_NO_MASK:\n        return value\n    return _mask_pii(value)\n\n\ndef _mask_pii(value: Any | None) -> Any | None:  # noqa: ANN401\n    if TIN_RE.search(str(value)):\n        return TIN_RE.sub(\"*********\", str(value))\n    return value"}
{"path":"api/src/constants/lookup_constants.py","language":"python","type":"code","directory":"api/src/constants","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/constants/lookup_constants.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/tests\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/__init__.py\nSize: 0.07 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/constants/schema.py","language":"python","type":"code","directory":"api/src/constants","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/constants/schema.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":""}
{"path":"api/src/data_migration/__init__.py","language":"python","type":"code","directory":"api/src/data_migration","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/assertions.py\nLanguage: py\nType: code\nDirectory: analytics/tests\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/assertions.py\nSize: 0.29 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/data_migration/command/__init__.py","language":"python","type":"code","directory":"api/src/data_migration/command","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/command/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"def assert_dict_contains(d: dict, expected: dict) -> None:\n    \"\"\"\n    Assert that d contains all the key-value pairs in expected.\n\n    Do this by checking to see if adding `expected` to `d` leaves `d` unchanged.\n    \"\"\"\n    assert d | expected == d"}
{"path":"api/src/data_migration/command/load_transform.py","language":"python","type":"code","directory":"api/src/data_migration/command","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/command/load_transform.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/conftest.py\nLanguage: py\nType: code\nDirectory: analytics/tests\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/conftest.py\nSize: 11.93 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/data_migration/data_migration_blueprint.py","language":"python","type":"code","directory":"api/src/data_migration","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/data_migration_blueprint.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"Visit pytest docs for more info:\nhttps://docs.pytest.org/en/7.1.x/reference/fixtures.html\n\"\"\"\nimport json\nimport logging\nimport uuid\nfrom pathlib import Path\n\nimport _pytest.monkeypatch\nimport boto3\nimport moto\nimport pandas as pd\nimport pytest\n\nfrom sqlalchemy import text  # isort: skip\nfrom analytics.datasets.issues import IssueMetadata, IssueType\nfrom analytics.integrations.etldb.etldb import EtlDb\n\nlogger = logging.getLogger(__name__)\n\n\n# skips the integration tests in tests/integrations/\n# to run the integration tests, invoke them directly: pytest tests/integrations/\ncollect_ignore = [\"integrations\"]\n\nDAY_0 = \"2023-10-31\"\nDAY_1 = \"2023-11-01\"\nDAY_2 = \"2023-11-02\"\nDAY_3 = \"2023-11-03\"\nDAY_4 = \"2023-11-04\"\nDAY_5 = \"2023-11-05\"\n\nLABEL_30K = \"deliverable: 30k ft\"\nLABEL_10K = \"deliverable: 10k ft\"\n\n\ndef pytest_addoption(parser: pytest.Parser):\n    \"\"\"Add a command line flag to collect tests that require a slack token.\"\"\"\n    parser.addoption(\n        \"--slack-token-set\",\n        action=\"store_true\",\n        default=False,\n        help=\"Run tests that require a slack token\",\n    )\n\n\nclass MockSlackbot:\n    \"\"\"Create a mock slackbot issue for unit tests.\"\"\"\n\n    def upload_files_to_slack_channel(\n        self,\n        channel_id: str,\n        files: list,\n        message: str,\n    ) -> None:\n        \"\"\"Stubs the corresponding method on the main SlackBot class.\"\"\"\n        assert isinstance(channel_id, str)\n        print(\"Fake posting the following files to Slack with this message:\")\n        print(message)\n        print(files)\n\n\n@pytest.fixture(name=\"mock_slackbot\")\ndef mock_slackbot_fixture():\n    \"\"\"Create a mock slackbot instance to stub post_to_slack() method.\"\"\"\n    return MockSlackbot()\n\n\ndef write_test_data_to_file(data: dict | list[dict], output_file: str | Path):\n    \"\"\"Write test JSON data to a file for use in a test.\"\"\"\n    parent_dir = Path(output_file).parent\n    parent_dir.mkdir(exist_ok=True, parents=True)\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(data, indent=2))\n\n\ndef json_issue_row(\n    issue: int,\n    labels: list[str] | None = None,\n    created_at: str | None = \"2023-11-01T00:00:00Z\",\n    closed_at: str | None = \"2023-11-01T00:00:00Z\",\n) -> dict:\n    \"\"\"Generate a row of JSON issue data for testing.\"\"\"\n    new_labels = (\n        [{\"name\": label, \"id\": hash(label)} for label in labels] if labels else []\n    )\n    return {\n        \"closedAt\": closed_at,\n        \"createdAt\": created_at,\n        \"number\": issue,\n        \"labels\": new_labels,\n        \"title\": f\"Issue {issue}\",\n    }\n\n\ndef json_roadmap_row(\n    issue: int,\n    deliverable: int,\n    status: str = \"In Progress\",\n    labels: list[str] | None = None,\n) -> dict:\n    \"\"\"\n    Generate a row of JSON roadmap data for testing.\n\n    This is the format returned by the export_project_data() command\n    \"\"\"\n    return {\n        \"assignees\": [\"mickeymouse\"],\n        \"content\": {\n            \"type\": \"Issue\",\n            \"body\": f\"Description of test deliverable {issue}\",\n            \"title\": f\"Deliverable {issue}\",\n            \"number\": issue,\n            \"repository\": \"HHS/simpler-grants-gov\",\n            \"url\": f\"https://github.com/HHS/simpler-grants-gov/issues/{issue}\",\n        },\n        \"id\": \"PVTI_lADOABZxns4ASDf3zgJhmCk\",\n        \"labels\": labels or [LABEL_30K],\n        \"linked pull requests\": [],\n        \"milestone\": {\n            \"title\": \"Sample milestone\",\n            \"description\": \"Deliverable for milestone\",\n            \"dueOn\": \"2023-10-20T00:00:00Z\",\n        },\n        \"repository\": \"https://github.com/HHS/simpler-grants-gov\",\n        \"status\": status,\n        \"deliverable\": f\"Deliverable {deliverable}\",\n        \"title\": f\"Deliverable {issue}\",\n    }\n\n\ndef json_sprint_row(\n    issue: int,\n    parent_number: int = -99,\n    sprint_name: str = \"Sprint 1\",\n    sprint_date: str = \"2023-11-01\",\n    status: str = \"Done\",\n    points: int = 5,\n    deliverable: int = 1,\n) -> dict:\n    \"\"\"\n    Generate a row of JSON sprint data for testing.\n\n    This is the format returned by the export_project_data() function.\n    \"\"\"\n    return {\n        \"assignees\": [\"mickeymouse\"],\n        \"content\": {\n            \"type\": \"Issue\",\n            \"body\": f\"Description of test issue {issue}\",\n            \"title\": f\"Issue {issue}\",\n            \"number\": issue,\n            \"repository\": \"HHS/simpler-grants-gov\",\n            \"url\": f\"https://github.com/HHS/simpler-grants-gov/issues/{issue}\",\n        },\n        \"id\": \"PVTI_lADOABZxns4ASDf3zgJhmCk\",\n        \"labels\": [\"topic: infra\", \"project: grants.gov\"],\n        \"linked pull requests\": [],\n        \"milestone\": {\n            \"title\": \"Sample milestone\",\n            \"description\": f\"30k ft deliverable: #{parent_number}\",\n            \"dueOn\": \"2023-10-20T00:00:00Z\",\n        },\n        \"repository\": \"https://github.com/HHS/simpler-grants-gov\",\n        \"sprint\": {\"title\": sprint_name, \"startDate\": sprint_date, \"duration\": 14},\n        \"status\": status,\n        \"story Points\": points,\n        \"deliverable\": f\"Deliverable {deliverable}\",\n        \"title\": f\"Issue {issue}\",\n    }\n\n\ndef sprint_row(\n    issue: int,\n    created: str = DAY_1,\n    closed: str | None = None,\n    status: str = \"In Progress\",\n    points: int | None = 1,\n    sprint: int = 1,\n    sprint_start: str = DAY_1,\n    sprint_length: int = 2,\n) -> dict:\n    \"\"\"Create a sample row of the SprintBoard dataset.\"\"\"\n    # create timestamp and time delta fields\n    sprint_start_ts = pd.Timestamp(sprint_start)\n    sprint_duration = pd.Timedelta(days=sprint_length)\n    sprint_end_ts = sprint_start_ts + sprint_duration\n    created_date = pd.Timestamp(created, tz=\"UTC\")\n    closed_date = pd.Timestamp(closed, tz=\"UTC\") if closed else None\n    # return the sample record\n    return {\n        \"issue_number\": issue,\n        \"issue_title\": f\"Issue {issue}\",\n        \"type\": \"issue\",\n        \"issue_body\": f\"Description of issue {issue}\",\n        \"status\": \"Done\" if closed else status,\n        \"assignees\": \"mickeymouse\",\n        \"labels\": [],\n        \"deliverable\": \"Deliverable 1\",\n        \"url\": f\"https://github.com/HHS/simpler-grants-gov/issues/{issue}\",\n        \"points\": points,\n        \"milestone\": \"Milestone 1\",\n        \"milestone_due_date\": sprint_end_ts,\n        \"milestone_description\": \"Milestone 1 description\",\n        \"sprint\": f\"Sprint {sprint}\",\n        \"sprint_start_date\": sprint_start_ts,\n        \"sprint_end_date\": sprint_end_ts,\n        \"sprint_duration\": sprint_duration,\n        \"created_date\": created_date,\n        \"closed_date\": closed_date,\n    }\n\n\ndef issue(  # pylint: disable=too-many-locals\n    issue: int,\n    kind: IssueType = IssueType.TASK,\n    owner: str = \"HHS\",\n    project: int = 1,\n    parent: str | None = None,\n    points: int | None = 1,\n    quad: str | None = None,\n    epic: str | None = None,\n    deliverable: str | None = None,\n    sprint: int = 1,\n    sprint_start: str = DAY_0,\n    sprint_length: int = 2,\n    created: str = DAY_0,\n    closed: str | None = None,\n) -> IssueMetadata:\n    \"\"\"Create a new issue.\"\"\"\n    # Create issue name\n    name = f\"{kind.value}{issue}\"\n    # Create sprint timestamp fields\n    sprint_name = f\"Sprint {sprint}\"\n    sprint_start_ts = pd.Timestamp(sprint_start)\n    sprint_duration = pd.Timedelta(days=sprint_length)\n    sprint_end_ts = sprint_start_ts + sprint_duration\n    return IssueMetadata(\n        # project metadata\n        project_owner=owner,\n        project_number=project,\n        # issue metadata\n        issue_title=name,\n        issue_type=kind.value,\n        issue_url=name,\n        issue_is_closed=bool(closed),\n        issue_opened_at=created,\n        issue_closed_at=closed,\n        issue_parent=parent,\n        issue_points=points,\n        # quad and epic metadata\n        quad_name=quad,\n        epic_title=epic,\n        epic_url=epic,\n        # deliverable metadata\n        deliverable_title=deliverable,\n        deliverable_url=deliverable,\n        # sprint metadata\n        sprint_id=sprint_name,\n        sprint_name=sprint_name,\n        sprint_start=sprint_start,\n        sprint_end=sprint_end_ts.strftime(\"%Y-%m-%d\"),\n    )\n\n\n####################\n# AWS Mock Fixtures\n####################\n\n\n@pytest.fixture(autouse=True)\ndef reset_aws_env_vars(monkeypatch: pytest.MonkeyPatch) -> None:\n    \"\"\"\n    Reset the aws env vars.\n\n    This will prevent you from accidentally connecting\n    to a real AWS account if you were doing some local testing.\n    \"\"\"\n    monkeypatch.setenv(\"AWS_ACCESS_KEY_ID\", \"testing\")\n    monkeypatch.setenv(\"AWS_SECRET_ACCESS_KEY\", \"testing\")\n    monkeypatch.setenv(\"AWS_SECURITY_TOKEN\", \"testing\")\n    monkeypatch.setenv(\"AWS_SESSION_TOKEN\", \"testing\")\n    monkeypatch.setenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n\n\n@pytest.fixture(autouse=True)\ndef use_cdn(monkeypatch: pytest.MonkeyPatch) -> None:\n    \"\"\"Set up CDN URL environment variable for tests.\"\"\"\n    monkeypatch.setenv(\"CDN_URL\", \"http://localhost:4566\")\n\n\n@pytest.fixture\ndef mock_s3() -> boto3.resource:\n    \"\"\"Instantiate an S3 bucket resource.\"\"\"\n    # https://docs.getmoto.org/en/stable/docs/configuration/index.html#whitelist-services\n    with moto.mock_aws(config={\"core\": {\"service_whitelist\": [\"s3\"]}}):\n        yield boto3.resource(\"s3\")\n\n\n@pytest.fixture\ndef mock_s3_bucket_resource(\n    mock_s3: boto3.resource,\n) -> boto3.resource(\"s3\").Bucket:\n    \"\"\"Create and return a mock S3 bucket resource.\"\"\"\n    bucket = mock_s3.Bucket(\"test_bucket\")\n    bucket.create()\n    return bucket\n\n\n@pytest.fixture\ndef mock_s3_bucket(mock_s3_bucket_resource: boto3.resource(\"s3\").Bucket) -> str:\n    \"\"\"Return name of mock S3 bucket.\"\"\"\n    return mock_s3_bucket_resource.name\n\n\n# From https://github.com/pytest-dev/pytest/issues/363\n@pytest.fixture(scope=\"session\")\ndef monkeypatch_session() -> pytest.MonkeyPatch:\n    \"\"\"\n    Create a monkeypatch instance.\n\n    This can be used to monkeypatch global environment, objects, and attributes\n    for the duration the test session.\n    \"\"\"\n    mpatch = _pytest.monkeypatch.MonkeyPatch()\n    yield mpatch\n    mpatch.undo()\n\n\n@pytest.fixture(scope=\"session\")\ndef test_schema() -> str:\n    \"\"\"Create a unique test schema.\"\"\"\n    return f\"test_schema_{uuid.uuid4().int}\"\n\n\n@pytest.fixture(scope=\"session\")\ndef create_test_db(test_schema: str) -> EtlDb:\n    \"\"\"\n    Create a temporary PostgreSQL schema.\n\n    This function creates schema and a database engine\n    that connects to that schema. Drops the schema after the context manager\n    exits.\n    \"\"\"\n    etldb_conn = EtlDb()\n\n    with etldb_conn.connection() as conn:\n\n        _create_schema(conn, test_schema)\n\n        _create_opportunity_table(conn, test_schema)\n        try:\n            yield etldb_conn\n\n        finally:\n            _drop_schema(conn, test_schema)\n\n\ndef _create_schema(conn: EtlDb.connection, schema: str) -> None:\n    \"\"\"Create a database schema.\"\"\"\n    db_test_user = \"app\"\n\n    with conn.begin():\n        conn.execute(\n            text(f\"CREATE SCHEMA IF NOT EXISTS {schema} AUTHORIZATION {db_test_user};\"),\n        )\n    logger.info(\"Created schema %s\", schema)\n\n\ndef _drop_schema(conn: EtlDb.connection, schema: str) -> None:\n    \"\"\"Drop a database schema.\"\"\"\n    with conn.begin():\n        conn.execute(text(f\"DROP SCHEMA {schema} CASCADE;\"))\n\n    logger.info(\"Dropped schema %s\", schema)\n\n\ndef _create_opportunity_table(conn: EtlDb.connection, schema: str) -> None:\n    \"\"\"Create opportunity tables.\"\"\"\n    with conn.begin():\n        conn.execute(text(f\"SET search_path TO {schema};\"))\n        # Get the path of the current file (test file)\n        test_file_path = Path(__file__).resolve()\n\n        # Construct the path to the SQL file\n        sql_file_path = (\n            test_file_path.parent.parent\n            / \"src\"\n            / \"analytics\"\n            / \"integrations\"\n            / \"etldb\"\n            / \"migrations\"\n            / \"versions\"\n            / \"0007_add_opportunity_tables.sql\"\n        )\n\n        with open(sql_file_path) as file:\n            create_table_commands = file.read()\n            conn.execute(text(create_table_commands))\n\n    logger.info(\"Created opportunity tables\")"}
{"path":"api/src/data_migration/load/__init__.py","language":"python","type":"code","directory":"api/src/data_migration/load","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/load/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/datasets/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/tests/datasets\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/datasets/__init__.py\nSize: 0.06 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/data_migration/load/load_oracle_data_task.py","language":"python","type":"code","directory":"api/src/data_migration/load","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/load/load_oracle_data_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":""}
{"path":"api/src/data_migration/load/sql.py","language":"python","type":"code","directory":"api/src/data_migration/load","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/load/sql.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/datasets/test_base.py\nLanguage: py\nType: code\nDirectory: analytics/tests/datasets\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/datasets/test_base.py\nSize: 1.22 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/data_migration/setup_foreign_tables.py","language":"python","type":"code","directory":"api/src/data_migration","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/setup_foreign_tables.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"from pathlib import Path  # noqa: I001\n\nimport pandas as pd\n\nfrom analytics.datasets.base import BaseDataset\n\nTEST_DATA = [\n    {\"Col A\": 1, \"Col b\": \"One\"},\n    {\"Col A\": 2, \"Col b\": \"Two\"},\n    {\"Col A\": 3, \"Col b\": \"Three\"},\n]\n\n\ndef test_to_and_from_csv(tmp_path: Path):\n    \"\"\"BaseDataset should write to csv with to_csv() and load from a csv with from_csv().\"\"\"\n    # setup - create sample dataframe and instantiate class\n    test_df = pd.DataFrame(TEST_DATA)\n    dataset_in = BaseDataset(test_df)\n    # setup - set output path and check that it doesn't exist\n    output_csv = tmp_path / \"dataset.csv\"\n    assert output_csv.exists() is False\n    # execution - write to csv and read from csv\n    dataset_in.to_csv(output_csv)\n    dataset_out = BaseDataset.from_csv(output_csv)\n    # validation - check that csv exists and that datasets match\n    assert output_csv.exists()\n    assert dataset_in.df.equals(dataset_out.df)\n\n\ndef test_to_and_from_dict():\n    \"\"\"BaseDataset should have same input and output with to_dict() and from_dict().\"\"\"\n    # execution\n    dict_in = TEST_DATA\n    dataset = BaseDataset.from_dict(TEST_DATA)\n    dict_out = dataset.to_dict()\n    # validation\n    assert dict_in == dict_out"}
{"path":"api/src/data_migration/transformation/__init__.py","language":"python","type":"code","directory":"api/src/data_migration/transformation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/datasets/test_etldb.py\nLanguage: py\nType: code\nDirectory: analytics/tests/datasets\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/datasets/test_etldb.py\nSize: 3.12 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/data_migration/transformation/subtask/__init__.py","language":"python","type":"code","directory":"api/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"from analytics.datasets.etl_dataset import EtlDataset\n\n\nclass TestEtlDataset:\n    \"\"\"Test EtlDataset methods.\"\"\"\n\n    TEST_FILE_1 = \"./tests/etldb_test_01.json\"\n\n    def test_load_from_json_files(self):\n        \"\"\"Class method should return the correctly transformed data.\"\"\"\n        dataset = EtlDataset.load_from_json_file(self.TEST_FILE_1)\n\n        row_count = dataset.df.shape[0]\n        col_count = dataset.df.shape[1]\n        assert row_count == 23\n        assert col_count == 27\n\n    def test_deliverable_fetchers(self):\n        \"\"\"Deliverable fetchers should return expected values.\"\"\"\n        dataset = EtlDataset.load_from_json_file(self.TEST_FILE_1)\n\n        unique_ghids = dataset.get_deliverable_ghids()\n        assert len(unique_ghids) == 4\n\n        ghid = unique_ghids[0]\n        assert ghid == \"HHS/simpler-grants-gov/issues/2200\"\n\n        deliverable = dataset.get_deliverable(ghid)\n        assert deliverable[\"deliverable_title\"] == \"Search *\"\n\n    def test_epic_fetchers(self):\n        \"\"\"Epic fetchers should return expected values.\"\"\"\n        dataset = EtlDataset.load_from_json_file(self.TEST_FILE_1)\n\n        unique_ghids = dataset.get_epic_ghids()\n        assert len(unique_ghids) == 6\n\n        ghid = unique_ghids[0]\n        assert ghid == \"HHS/simpler-grants-gov/issues/2719\"\n\n        epic = dataset.get_epic(ghid)\n        assert epic[\"epic_title\"] == \"Search API Engagement\"\n\n    def test_issue_fetchers(self):\n        \"\"\"Issue fetchers should return expected values.\"\"\"\n        dataset = EtlDataset.load_from_json_file(self.TEST_FILE_1)\n\n        unique_ghids = dataset.get_issue_ghids()\n        assert len(unique_ghids) == 22\n\n        ghid = unique_ghids[0]\n        assert ghid == \"HHS/simpler-grants-gov/issues/2763\"\n\n        issue = dataset.get_issue(ghid)\n        assert issue[\"issue_opened_at\"] == \"2024-11-07\"\n\n        rows = dataset.get_issues(ghid)\n        assert len(rows) == 2\n\n    def test_sprint_fetchers(self):\n        \"\"\"Deliverable fetchers should return expected values.\"\"\"\n        dataset = EtlDataset.load_from_json_file(self.TEST_FILE_1)\n\n        unique_ghids = dataset.get_sprint_ghids()\n        assert len(unique_ghids) == 4\n\n        ghid = unique_ghids[0]\n        assert ghid == \"b8f1831e\"\n\n        sprint = dataset.get_sprint(ghid)\n        assert sprint[\"sprint_name\"] == \"Sprint 1.3\"\n\n    def test_quad_fetchers(self):\n        \"\"\"Quad fetchers should return expected values.\"\"\"\n        dataset = EtlDataset.load_from_json_file(self.TEST_FILE_1)\n\n        unique_ghids = dataset.get_quad_ghids()\n        assert len(unique_ghids) == 1\n\n        ghid = unique_ghids[0]\n        assert ghid == \"93412e1c\"\n\n        quad = dataset.get_quad(ghid)\n        assert quad[\"quad_name\"] == \"Quad 1.1\"\n\n    def test_project_fetchers(self):\n        \"\"\"Project fetchers should return expected values.\"\"\"\n        dataset = EtlDataset.load_from_json_file(self.TEST_FILE_1)\n\n        unique_ghids = dataset.get_project_ghids()\n        assert len(unique_ghids) == 2\n\n        ghid = unique_ghids[0]\n        assert ghid == 17\n\n        project = dataset.get_project(ghid)\n        assert project[\"project_name\"] == \"HHS\""}
{"path":"api/src/data_migration/transformation/subtask/abstract_transform_subtask.py","language":"python","type":"code","directory":"api/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/abstract_transform_subtask.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/datasets/test_issues.py\nLanguage: py\nType: code\nDirectory: analytics/tests/datasets\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/datasets/test_issues.py\nSize: 2.67 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/data_migration/transformation/subtask/transform_agency.py","language":"python","type":"code","directory":"api/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_agency.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"import pandas as pd\nimport pytest\nfrom analytics.datasets.issues import (\n    GitHubIssues,\n)\n\nfrom tests.conftest import (\n    DAY_0,\n    DAY_1,\n    DAY_2,\n    DAY_3,\n    DAY_4,\n    DAY_5,\n    issue,\n)\n\n\nclass TestGetSprintNameFromDate:\n    \"\"\"Test the GitHubIssues.get_sprint_name_from_date() method.\"\"\"\n\n    @pytest.mark.parametrize(\n        (\"date\", \"expected\"),\n        [\n            (DAY_1, \"Sprint 1\"),\n            (DAY_2, \"Sprint 1\"),\n            (DAY_4, \"Sprint 2\"),\n            (DAY_5, \"Sprint 2\"),\n        ],\n    )\n    def test_return_name_if_matching_sprint_exists(self, date: str, expected: str):\n        \"\"\"Test that correct sprint is returned if date exists in a sprint.\"\"\"\n        # setup - create sample dataset\n        board_data = [\n            issue(issue=1, sprint=1, sprint_start=DAY_0, sprint_length=3),\n            issue(issue=2, sprint=1, sprint_start=DAY_0, sprint_length=3),\n            issue(issue=3, sprint=2, sprint_start=DAY_3, sprint_length=3),\n        ]\n        board_data = [i.__dict__ for i in board_data]\n        board = GitHubIssues.from_dict(board_data)\n        # validation\n        sprint_date = pd.Timestamp(date)\n        sprint_name = board.get_sprint_name_from_date(sprint_date)\n        assert sprint_name == expected\n\n    def test_return_none_if_no_matching_sprint(self):\n        \"\"\"The method should return None if no sprint contains the date.\"\"\"\n        # setup - create sample dataset\n        board_data = [\n            issue(issue=1, sprint=1, sprint_start=DAY_1),\n            issue(issue=2, sprint=2, sprint_start=DAY_4),\n        ]\n        board_data = [i.__dict__ for i in board_data]\n        board = GitHubIssues.from_dict(board_data)\n        # validation\n        bad_date = pd.Timestamp(\"1900-01-01\")\n        sprint_name = board.get_sprint_name_from_date(bad_date)\n        assert sprint_name is None\n\n    def test_return_previous_sprint_if_date_is_start_of_next_sprint(self):\n        \"\"\"\n        Test correct behavior for sprint end/start dates.\n\n        If date provided is both the the end of one sprint and the beginning of\n        another, then return the name of the sprint that just ended.\n        \"\"\"\n        # setup - create sample dataset\n        board_data = [\n            issue(issue=1, sprint=1, sprint_start=DAY_1, sprint_length=2),\n            issue(issue=2, sprint=2, sprint_start=DAY_3, sprint_length=2),\n        ]\n        board_data = [i.__dict__ for i in board_data]\n        board = GitHubIssues.from_dict(board_data)\n        # execution\n        bad_date = pd.Timestamp(DAY_3)  # end of sprint 1 and start of sprint 2\n        sprint_name = board.get_sprint_name_from_date(bad_date)\n        assert sprint_name == \"Sprint 1\""}
{"path":"api/src/data_migration/transformation/subtask/transform_applicant_type.py","language":"python","type":"code","directory":"api/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_applicant_type.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/etl/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/tests/etl\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/etl/__init__.py\nSize: 0.05 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/data_migration/transformation/subtask/transform_assistance_listing.py","language":"python","type":"code","directory":"api/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_assistance_listing.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":""}
{"path":"api/src/data_migration/transformation/subtask/transform_funding_category.py","language":"python","type":"code","directory":"api/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_funding_category.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/etl/test_github.py\nLanguage: py\nType: code\nDirectory: analytics/tests/etl\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/etl/test_github.py\nSize: 11.38 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/data_migration/transformation/subtask/transform_funding_instrument.py","language":"python","type":"code","directory":"api/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_funding_instrument.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"from pathlib import Path\nfrom unittest.mock import MagicMock\n\nimport pytest\nfrom analytics.datasets.issues import IssueType\nfrom analytics.datasets.utils import dump_to_json\nfrom analytics.etl.github import (\n    GitHubProjectConfig,\n    GitHubProjectETL,\n    InputFiles,\n    RoadmapConfig,\n    SprintBoardConfig,\n    get_parent_with_type,\n    populate_issue_lookup_table,\n)\nfrom analytics.integrations import github\n\nfrom tests.conftest import issue\n\n# ===========================================================\n# Fixtures\n# ===========================================================\n\n\n@pytest.fixture(name=\"config\")\ndef mock_config(tmp_path: Path) -> GitHubProjectConfig:\n    \"\"\"Fixture to create a sample configuration for testing.\"\"\"\n    return GitHubProjectConfig(\n        roadmap_project=RoadmapConfig(owner=\"test_owner\", project_number=1),\n        sprint_projects=[SprintBoardConfig(owner=\"test_owner\", project_number=2)],\n        temp_dir=str(tmp_path),\n        output_file=str(tmp_path / \"test_output.json\"),\n    )\n\n\n@pytest.fixture(name=\"etl\")\ndef mock_etl(config: GitHubProjectConfig):\n    \"\"\"Fixture to initialize the ETL pipeline.\"\"\"\n    return GitHubProjectETL(config)\n\n\n@pytest.fixture(name=\"sprint_file\")\ndef mock_sprint_data_file(config: GitHubProjectConfig) -> str:\n    \"\"\"Create a path to a JSON file with mock sprint data exported from GitHub.\"\"\"\n    # Arrange - create dummy sprint data\n    proj_number = config.sprint_projects[0].project_number\n    sprint_file = str(Path(config.temp_dir) / f\"sprint-data-{proj_number}.json\")\n    sprint_data = [\n        issue(issue=1, kind=IssueType.TASK, parent=\"Epic3\", points=2),\n        issue(issue=2, kind=IssueType.TASK, parent=\"Epic4\", points=1),\n    ]\n    roadmap_data = [i.model_dump() for i in sprint_data]\n    dump_to_json(sprint_file, roadmap_data)\n    return sprint_file\n\n\n@pytest.fixture(name=\"roadmap_file\")\ndef mock_roadmap_data_file(config: GitHubProjectConfig) -> str:\n    \"\"\"Create a path to a JSON file with mock sprint data exported from GitHub.\"\"\"\n    roadmap_file = str(Path(config.temp_dir) / \"roadmap-data.json\")\n    roadmap_data = [\n        issue(issue=3, kind=IssueType.EPIC, parent=\"Deliverable5\"),\n        issue(issue=4, kind=IssueType.EPIC, parent=\"Deliverable6\"),\n        issue(issue=5, kind=IssueType.DELIVERABLE, quad=\"quad1\"),\n    ]\n    roadmap_data = [i.model_dump() for i in roadmap_data]\n    dump_to_json(roadmap_file, roadmap_data)\n    return roadmap_file\n\n\n# ===========================================================\n# Test ETL class\n# ===========================================================\n\n\nclass TestGitHubProjectETL:\n    \"\"\"Tests the GitHubProjectETL class.\"\"\"\n\n    def test_extract(\n        self,\n        monkeypatch: pytest.MonkeyPatch,\n        etl: GitHubProjectETL,\n    ):\n        \"\"\"Test the extract step by mocking export functions.\"\"\"\n        mock_export_roadmap_data_to_file = MagicMock()\n        mock_export_sprint_data_to_file = MagicMock()\n        monkeypatch.setattr(\n            etl,\n            \"_export_roadmap_data_to_file\",\n            mock_export_roadmap_data_to_file,\n        )\n        monkeypatch.setattr(\n            etl,\n            \"_export_sprint_data_to_file\",\n            mock_export_sprint_data_to_file,\n        )\n\n        # Run the extract method\n        etl.extract()\n\n        # Assert roadmap export was called with expected arguments\n        roadmap = etl.config.roadmap_project\n        mock_export_roadmap_data_to_file.assert_called_once_with(\n            roadmap=roadmap,\n            output_file_path=str(Path(etl.config.temp_dir) / \"roadmap-data.json\"),\n        )\n\n        # Assert sprint export was called with expected arguments\n        sprint_board = etl.config.sprint_projects[0]\n        mock_export_sprint_data_to_file.assert_called_once_with(\n            sprint_board=sprint_board,\n            output_file_path=str(\n                Path(etl.config.temp_dir)\n                / f\"sprint-data-{sprint_board.project_number}.json\",\n            ),\n        )\n\n        # Verify transient files were set correctly\n        assert len(etl._transient_files) == 1\n        assert etl._transient_files[0].roadmap.endswith(\"roadmap-data.json\")\n        assert etl._transient_files[0].sprint.endswith(\n            f\"sprint-data-{sprint_board.project_number}.json\",\n        )\n\n    def test_transform(\n        self,\n        etl: GitHubProjectETL,\n        sprint_file: str,\n        roadmap_file: str,\n    ):\n        \"\"\"Test the transform step by mocking GitHubIssues.load_from_json_files.\"\"\"\n        # Arrange\n        output_data = [\n            issue(\n                issue=1,\n                points=2,\n                parent=\"Epic3\",\n                deliverable=\"Deliverable5\",\n                quad=\"quad1\",\n                epic=\"Epic3\",\n            ),\n            issue(\n                issue=2,\n                points=1,\n                parent=\"Epic4\",\n                deliverable=None,\n                quad=None,\n                epic=\"Epic4\",\n            ),\n        ]\n        wanted = [i.model_dump() for i in output_data]\n        etl._transient_files = [InputFiles(roadmap=roadmap_file, sprint=sprint_file)]\n        # Act\n        etl.transform()\n        # Assert\n        assert etl.dataset.to_dict() == wanted\n\n    def test_load(self, etl: GitHubProjectETL):\n        \"\"\"Test the load step by mocking the to_json method.\"\"\"\n        mock_to_json = MagicMock()\n        etl.dataset = MagicMock()\n        etl.dataset.to_json = mock_to_json\n\n        # Run the load method\n        etl.write_to_file()\n\n        # Check if to_json was called with the correct output file\n        mock_to_json.assert_called_once_with(etl.config.output_file)\n\n    def test_run(\n        self,\n        monkeypatch: pytest.MonkeyPatch,\n        etl: GitHubProjectETL,\n        sprint_file: str,\n        roadmap_file: str,\n    ):\n        \"\"\"Test the entire ETL pipeline by verifying method calls in run.\"\"\"\n        # Arrange - Mock the export private methods\n        monkeypatch.setattr(github, \"export_roadmap_data_to_file\", MagicMock())\n        monkeypatch.setattr(github, \"export_sprint_data_to_file\", MagicMock())\n        # Arrange - specify the output wanted\n        output_data = [\n            issue(\n                issue=1,\n                points=2,\n                parent=\"Epic3\",\n                deliverable=\"Deliverable5\",\n                quad=\"quad1\",\n                epic=\"Epic3\",\n            ),\n            issue(\n                issue=2,\n                points=1,\n                parent=\"Epic4\",\n                deliverable=None,\n                quad=None,\n                epic=\"Epic4\",\n            ),\n        ]\n        dataset_wanted = [i.model_dump() for i in output_data]\n        files_wanted = [InputFiles(roadmap=roadmap_file, sprint=sprint_file)]\n        # Act - run the ETL\n        etl.run()\n        # Assert\n        assert etl._transient_files == files_wanted\n        assert etl.dataset.to_dict() == dataset_wanted\n\n\n# ===========================================================\n# Test ETL helper functions\n# ===========================================================\n\n\nclass TestPopulateLookupTable:\n    \"\"\"Test the populate_lookup_table() function.\"\"\"\n\n    def test_drop_issues_with_validation_errors(self):\n        \"\"\"Issues with validation errors should be excluded from the lookup table.\"\"\"\n        # Arrange\n        test_data = [\n            issue(issue=1).model_dump(),\n            issue(issue=2).model_dump(),\n            {\n                \"issue_url\": \"bad_issue\",\n                \"issue_points\": \"foo\",\n            },  # missing required field and wrong type for points\n        ]\n        wanted = 2\n        # Act\n        got = populate_issue_lookup_table(lookup={}, issues=test_data)\n        # Assert\n        assert len(got) == wanted\n        assert \"bad_issue\" not in got\n\n\nclass TestGetParentWithType:\n    \"\"\"Test the get_parent_with_type() method.\"\"\"\n\n    def test_return_epic_that_is_direct_parent_of_issue(self):\n        \"\"\"Return the correct epic for an issue that is one level down.\"\"\"\n        # Arrange\n        task = \"Task1\"\n        epic = \"Epic1\"\n        lookup = {\n            task: issue(issue=1, kind=IssueType.TASK, parent=epic),\n            epic: issue(issue=2, kind=IssueType.EPIC, parent=None),\n        }\n        wanted = lookup[epic]\n        # Act\n        got = get_parent_with_type(\n            child_url=task,\n            lookup=lookup,\n            type_wanted=IssueType.EPIC,\n        )\n        # Assert\n        assert got == wanted\n\n    def test_return_correct_deliverable_that_is_grandparent_of_issue(self):\n        \"\"\"Return the correct deliverable for an issue that is two levels down.\"\"\"\n        # Arrange\n        task = \"Task1\"\n        epic = \"Epic2\"\n        deliverable = \"Deliverable3\"\n        lookup = {\n            task: issue(issue=1, kind=IssueType.TASK, parent=epic),\n            epic: issue(issue=2, kind=IssueType.EPIC, parent=deliverable),\n            deliverable: issue(issue=3, kind=IssueType.DELIVERABLE, parent=None),\n        }\n        wanted = lookup[deliverable]\n        # Act\n        got = get_parent_with_type(\n            child_url=task,\n            lookup=lookup,\n            type_wanted=IssueType.DELIVERABLE,\n        )\n        # Assert\n        assert got == wanted\n\n    def test_return_none_if_issue_has_no_parent(self):\n        \"\"\"Return None if the input issue has no parent.\"\"\"\n        # Arrange\n        task = \"task\"\n        lookup = {\n            task: issue(issue=1, kind=IssueType.TASK, parent=None),\n        }\n        wanted = None\n        # Act\n        got = get_parent_with_type(\n            child_url=task,\n            lookup=lookup,\n            type_wanted=IssueType.DELIVERABLE,\n        )\n        # Assert\n        assert got == wanted\n\n    def test_return_none_if_parents_form_a_cycle(self):\n        \"\"\"Return None if the issue hierarchy forms a cycle.\"\"\"\n        # Arrange\n        task = \"Task1\"\n        parent = \"Task2\"\n        lookup = {\n            task: issue(issue=1, kind=IssueType.TASK, parent=\"parent\"),\n            parent: issue(issue=2, kind=IssueType.TASK, parent=task),\n        }\n        wanted = None\n        # Act\n        got = get_parent_with_type(\n            child_url=task,\n            lookup=lookup,\n            type_wanted=IssueType.DELIVERABLE,\n        )\n        # Assert\n        assert got == wanted\n\n    def test_return_none_if_deliverable_is_not_found_in_parents(self):\n        \"\"\"Return None if the desired type (e.g. epic) isn't found in the list of parents.\"\"\"\n        # Arrange\n        task = \"Task1\"\n        parent = \"Task2\"\n        epic = \"Epic3\"\n        lookup = {\n            task: issue(issue=1, kind=IssueType.TASK, parent=parent),\n            parent: issue(issue=2, kind=IssueType.TASK, parent=epic),\n            epic: issue(issue=3, kind=IssueType.EPIC, parent=task),\n        }\n        wanted = None\n        # Act\n        got = get_parent_with_type(\n            child_url=task,\n            lookup=lookup,\n            type_wanted=IssueType.DELIVERABLE,\n        )\n        # Assert\n        assert got == wanted\n\n    def test_raise_value_error_if_child_url_not_in_lookup(self):\n        \"\"\"Raise a value error if the child_url isn't found in lookup table.\"\"\"\n        # Arrange\n        task = \"Task1\"\n        lookup = {\n            task: issue(issue=1, kind=IssueType.TASK),\n        }\n        # Act\n        with pytest.raises(ValueError, match=\"Lookup doesn't contain\"):\n            get_parent_with_type(\n                child_url=\"fake\",\n                lookup=lookup,\n                type_wanted=IssueType.DELIVERABLE,\n            )"}
{"path":"api/src/data_migration/transformation/subtask/transform_opportunity.py","language":"python","type":"code","directory":"api/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_opportunity.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/etl/test_utils.py\nLanguage: py\nType: code\nDirectory: analytics/tests/etl\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/etl/test_utils.py\nSize: 1.61 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/data_migration/transformation/subtask/transform_opportunity_attachment.py","language":"python","type":"code","directory":"api/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_opportunity_attachment.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"import json  # noqa: I001\nfrom pathlib import Path\n\nimport pytest\nfrom pydantic import BaseModel, ValidationError\n\nfrom analytics.etl.utils import load_config\n\n\nclass MockConfig(BaseModel):\n    \"\"\"Mock config class.\"\"\"\n\n    param1: str\n    param2: str\n\n\n@pytest.fixture(name=\"valid_config_file\")\ndef mock_valid_config_file(tmp_path: Path) -> Path:\n    \"\"\"Path to a valid config file.\"\"\"\n    # Create a temporary file with valid JSON data\n    config_data = {\"param1\": \"foo\", \"param2\": \"bar\"}\n    config_file = tmp_path / \"valid_config.json\"\n    config_file.write_text(json.dumps(config_data))\n    return config_file\n\n\n@pytest.fixture(name=\"invalid_config_file\")\ndef mock_invalid_config_file(tmp_path: Path) -> Path:\n    \"\"\"Path to an invalid config file.\"\"\"\n    # Create a temporary file with invalid JSON data\n    config_data = {\n        \"database_url\": \"sqlite:///test.db\",\n        # Missing the required 'api_key' field\n    }\n    config_file = tmp_path / \"invalid_config.json\"\n    config_file.write_text(json.dumps(config_data))\n    return config_file\n\n\ndef test_load_valid_config(valid_config_file: Path):\n    \"\"\"Valid config should load successfully.\"\"\"\n    # Test that a valid config file loads successfully\n    config = load_config(valid_config_file, MockConfig)\n    assert config.param1 == \"foo\"\n    assert config.param2 == \"bar\"\n\n\ndef test_load_invalid_config(invalid_config_file: Path):\n    \"\"\"Invalid config should raise an error.\"\"\"\n    # Test that an invalid config file raises a ValidationError\n    with pytest.raises(ValidationError):\n        load_config(invalid_config_file, MockConfig)"}
{"path":"api/src/data_migration/transformation/subtask/transform_opportunity_summary.py","language":"python","type":"code","directory":"api/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_opportunity_summary.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/etldb_test_01.json\nLanguage: json\nType: code\nDirectory: analytics/tests\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/etldb_test_01.json\nSize: 23.48 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/data_migration/transformation/transform_constants.py","language":"python","type":"code","directory":"api/src/data_migration/transformation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/transform_constants.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":""}
{"path":"api/src/data_migration/transformation/transform_oracle_data_task.py","language":"python","type":"code","directory":"api/src/data_migration/transformation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/transform_oracle_data_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/integrations/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/tests/integrations\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/__init__.py\nSize: 0.19 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/data_migration/transformation/transform_util.py","language":"python","type":"code","directory":"api/src/data_migration/transformation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/transform_util.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"NOTE: These tests are excluded from pytest collection by default and will only\nbe executed when they are called directly: `pytest tests/integrations/`\n\"\"\""}
{"path":"api/src/db/__init__.py","language":"python","type":"code","directory":"api/src/db","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/integrations/extracts/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/tests/integrations/extracts\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/__init__.py\nSize: 0.03 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/db/migrations/__init__.py","language":"python","type":"code","directory":"api/src/db/migrations","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":""}
{"path":"api/src/db/migrations/alembic.ini","language":"unknown","type":"code","directory":"api/src/db/migrations","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/alembic.ini","size":0,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/integrations/extracts/opportunity_tables_test_files/current_opportunity_summary.csv\nLanguage: csv\nType: code\nDirectory: analytics/tests/integrations/extracts/opportunity_tables_test_files\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/opportunity_tables_test_files/current_opportunity_summary.csv\nSize: 2.32 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/db/migrations/env.py","language":"python","type":"code","directory":"api/src/db/migrations","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/env.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":""}
{"path":"api/src/db/migrations/run.py","language":"python","type":"code","directory":"api/src/db/migrations","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/run.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/integrations/extracts/opportunity_tables_test_files/lk_opportunity_category.csv\nLanguage: csv\nType: code\nDirectory: analytics/tests/integrations/extracts/opportunity_tables_test_files\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/opportunity_tables_test_files/lk_opportunity_category.csv\nSize: 0.45 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/db/migrations/script.py.mako","language":"unknown","type":"code","directory":"api/src/db/migrations","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/script.py.mako","size":0,"lastModified":"2025-02-14T17:08:31.124Z","content":""}
{"path":"api/src/db/migrations/setup_local_postgres_db.py","language":"python","type":"code","directory":"api/src/db/migrations","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/setup_local_postgres_db.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/integrations/extracts/opportunity_tables_test_files/lk_opportunity_status.csv\nLanguage: csv\nType: code\nDirectory: analytics/tests/integrations/extracts/opportunity_tables_test_files\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/opportunity_tables_test_files/lk_opportunity_status.csv\nSize: 0.36 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/db/migrations/versions/2023_08_01_base_migration.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2023_08_01_base_migration.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":""}
{"path":"api/src/db/migrations/versions/2023_08_10_default_table_privileges.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2023_08_10_default_table_privileges.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/integrations/extracts/opportunity_tables_test_files/opportunity.csv\nLanguage: csv\nType: code\nDirectory: analytics/tests/integrations/extracts/opportunity_tables_test_files\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/opportunity_tables_test_files/opportunity.csv\nSize: 6.09 KB\nLast Modified: 2025-02-14T17:08:26.429Z"}
{"path":"api/src/db/migrations/versions/2023_10_18_basic_opportunity_table.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2023_10_18_basic_opportunity_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":""}
{"path":"api/src/db/migrations/versions/2023_11_27_rename_opportunity_table_prior_to_real_.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2023_11_27_rename_opportunity_table_prior_to_real_.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":"File: analytics/tests/integrations/extracts/opportunity_tables_test_files/opportunity_summary.csv\nLanguage: csv\nType: code\nDirectory: analytics/tests/integrations/extracts/opportunity_tables_test_files\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/opportunity_tables_test_files/opportunity_summary.csv\nSize: 52.32 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2023_12_11_add_rest_of_opportunity_table.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2023_12_11_add_rest_of_opportunity_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.124Z","content":""}
{"path":"api/src/db/migrations/versions/2024_01_29_add_topportunity_table_for_transfer.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_01_29_add_topportunity_table_for_transfer.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/integrations/extracts/test_load_opportunity_data.py\nLanguage: py\nType: code\nDirectory: analytics/tests/integrations/extracts\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/extracts/test_load_opportunity_data.py\nSize: 2.83 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_02_02_add_opportunity_category_table.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_02_02_add_opportunity_category_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"# pylint: disable=W0613,W0621\nimport os\nimport pathlib\n\nimport boto3\nimport pytest\n\nfrom sqlalchemy import text  # isort: skip\nfrom analytics.integrations.etldb.etldb import EtlDb\nfrom analytics.integrations.extracts.load_opportunity_data import (\n    extract_copy_opportunity_data,\n)\n\ntest_folder_path = (\n    pathlib.Path(__file__).parent.resolve() / \"opportunity_tables_test_files\"\n)\n\n\n@pytest.fixture\ndef upload_opportunity_tables_s3(\n    mock_s3_bucket_resource: boto3.resource(\"s3\").Bucket,\n) -> int:\n    \"\"\"Upload test files to mockS3.\"\"\"\n    for root, _, files in os.walk(test_folder_path):\n        root_path = pathlib.Path(root)\n        for file in files:\n            file_path = root_path / file\n            object_key = (\n                f\"public-extracts/{os.path.relpath(file_path, test_folder_path)}\"\n            )\n            with open(file_path, \"rb\") as data:\n                mock_s3_bucket_resource.upload_fileobj(data, object_key)\n\n    s3_files = list(mock_s3_bucket_resource.objects.all())\n\n    return len(s3_files)\n\n\ndef test_extract_copy_opportunity_data(\n    create_test_db: EtlDb,\n    test_schema: str,\n    upload_opportunity_tables_s3: int,\n    monkeypatch: pytest.MonkeyPatch,\n    monkeypatch_session: pytest.MonkeyPatch,\n    mock_s3_bucket: str,\n):\n    \"\"\"Test files are uploaded to mocks3 and all records are in test schema.\"\"\"\n    monkeypatch.setenv(\"DB_SCHEMA\", test_schema)\n    monkeypatch_session.setenv(\n        \"API_ANALYTICS_DB_EXTRACTS_PATH\",\n        f\"S3://{mock_s3_bucket}/public-extracts\",\n    )\n    test_db_conn = create_test_db\n    extract_copy_opportunity_data()\n    conn = test_db_conn.connection()\n\n    # Verify that the data was inserted into the database\n    with conn.begin():\n        lk_opp_sts_result = conn.execute(\n            text(\"SELECT COUNT(*) FROM lk_opportunity_status ;\"),\n        )\n        lk_opp_ctgry_result = conn.execute(\n            text(\"SELECT COUNT(*) FROM lk_opportunity_category ;\"),\n        )\n        opp_result = conn.execute(\n            text(\"SELECT COUNT(*) FROM opportunity ;\"),\n        )\n        opp_smry_result = conn.execute(\n            text(\"SELECT COUNT(*) FROM opportunity_summary ;\"),\n        )\n\n        curr_opp_smry_result = conn.execute(\n            text(\"SELECT COUNT(*) FROM current_opportunity_summary ;\"),\n        )\n\n        # test all test_files were upload to mocks3 bucket\n        assert upload_opportunity_tables_s3 == 5\n\n        # test table records were inserted for each table\n        assert lk_opp_sts_result.fetchone()[0] == 4\n        assert lk_opp_ctgry_result.fetchone()[0] == 5\n        assert opp_result.fetchone()[0] == 37\n        assert opp_smry_result.fetchone()[0] == 32\n        assert curr_opp_smry_result.fetchone()[0] == 32\n\n    # running again to verify that it does not break on the next call\n    extract_copy_opportunity_data()"}
{"path":"api/src/db/migrations/versions/2024_02_07_add_expanded_opportunity_models.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_02_07_add_expanded_opportunity_models.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/integrations/github/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/tests/integrations/github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/github/__init__.py\nSize: 0.04 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_02_12_create_dms_exceptions_table.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_02_12_create_dms_exceptions_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":""}
{"path":"api/src/db/migrations/versions/2024_02_21_remove_dms_exceptions_table.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_02_21_remove_dms_exceptions_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/integrations/github/test_client.py\nLanguage: py\nType: code\nDirectory: analytics/tests/integrations/github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/github/test_client.py\nSize: 3.63 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_03_07_drop_tables_to_remake.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_03_07_drop_tables_to_remake.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"from unittest.mock import Mock, patch\n\nimport pytest\nfrom analytics.integrations.github.client import GitHubGraphqlClient, GraphqlError\n\n\n@pytest.fixture(name=\"client\")\ndef mock_client() -> GitHubGraphqlClient:\n    \"\"\"Fixture to initialize GitHubGraphqlClient with a mock token.\"\"\"\n    return GitHubGraphqlClient()\n\n\n@pytest.fixture(name=\"sample_query\")\ndef mock_query() -> str:\n    \"\"\"Fixture for a sample GraphQL query.\"\"\"\n    return \"\"\"\n    query($login: String!, $first: Int!, $after: String) {\n      user(login: $login) {\n        repositories(first: $first, after: $after) {\n          pageInfo {\n            hasNextPage\n            endCursor\n          }\n          nodes {\n            name\n          }\n        }\n      }\n    }\n    \"\"\"\n\n\n@patch(\"requests.post\")  # Mocks the requests.post() method\ndef test_paginated_query_success(\n    mock_post: Mock,\n    client: GitHubGraphqlClient,\n    sample_query: str,\n) -> None:\n    \"\"\"Test successfully making a paginated call and extracting data.\"\"\"\n    # Arrange - Mock the response from requests.post()\n    mock_response = {\n        \"data\": {\n            \"user\": {\n                \"repositories\": {\n                    \"nodes\": [{\"name\": \"repo1\"}],\n                    \"pageInfo\": {\"hasNextPage\": False, \"endCursor\": None},\n                },\n            },\n        },\n    }\n    mock_post.return_value = Mock(\n        status_code=200,\n        json=Mock(return_value=mock_response),\n    )\n\n    # Act - Set\n    variables: dict[str, str] = {\"login\": \"octocat\"}\n    path_to_nodes: list[str] = [\"user\", \"repositories\"]\n    result: list[dict[str, str]] = client.execute_paginated_query(\n        sample_query,\n        variables,\n        path_to_nodes,\n    )\n\n    assert result == [{\"name\": \"repo1\"}]\n\n\n@patch(\"requests.post\")\ndef test_invalid_path_to_nodes(\n    mock_post: Mock,\n    client: GitHubGraphqlClient,\n    sample_query: str,\n) -> None:\n    \"\"\"Test catching an error if the path_to_nodes is incorrect.\"\"\"\n    # Arrange - Mock the response from requests.post()\n    mock_response = {\n        \"data\": {\n            \"user\": {\n                \"repositories\": {\n                    \"nodes\": [{\"name\": \"repo1\"}],\n                    \"pageInfo\": {\"hasNextPage\": False, \"endCursor\": None},\n                },\n            },\n        },\n    }\n    mock_post.return_value = Mock(\n        status_code=200,\n        json=Mock(return_value=mock_response),\n    )\n\n    # Arrange - Set variables and incorrect path to nodes\n    variables: dict[str, str] = {\"login\": \"octocat\"}\n    path_to_nodes: list[str] = [\"user\", \"invalid_path\"]\n\n    # Assert - Check that the incorrect path raises a KeyError\n    with pytest.raises(KeyError):\n        client.execute_paginated_query(sample_query, variables, path_to_nodes)\n\n\n@patch(\"requests.post\")\ndef test_graphql_error(\n    mock_post: Mock,\n    client: GitHubGraphqlClient,\n    sample_query: str,\n) -> None:\n    \"\"\"Test raising a GraphqlError if errors are present in the response.\"\"\"\n    # Arrange - Mock the response from requests.post() to include an error\n    mock_post.return_value = Mock(\n        status_code=200,\n        json=Mock(return_value={\"errors\": [{\"message\": \"Test GitHub error\"}]}),\n    )\n\n    # Arrange - Set the variables and path to nodes in the response body\n    variables: dict[str, str] = {\"login\": \"octocat\"}\n    path_to_nodes: list[str] = [\"user\", \"repositories\"]\n\n    # Assert - Check that GraphqlError was raised\n    with pytest.raises(GraphqlError) as excinfo:\n        client.execute_paginated_query(sample_query, variables, path_to_nodes)\n\n    # Assert - Check that it contains the error message from the mock response\n    assert \"Test GitHub error\" in str(excinfo.value)"}
{"path":"api/src/db/migrations/versions/2024_03_07_updates_for_summary_tables.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_03_07_updates_for_summary_tables.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/integrations/github/test_validation.py\nLanguage: py\nType: code\nDirectory: analytics/tests/integrations/github\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/github/test_validation.py\nSize: 8.82 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_03_12_add_indexes_for_search.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_03_12_add_indexes_for_search.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"import pytest  # noqa: I001\nfrom pydantic import ValidationError\nfrom analytics.integrations.github.validation import (\n    IssueContent,\n    IterationValue,\n    NumberValue,\n    ProjectItem,\n    SingleSelectValue,\n)\n\n# #############################################\n# Test data constants\n# #############################################\n\nVALID_ISSUE_CONTENT = {\n    \"title\": \"Test Issue\",\n    \"url\": \"https://github.com/test/repo/issues/1\",\n    \"closed\": True,\n    \"createdAt\": \"2024-01-01T00:00:00Z\",\n    \"closedAt\": \"2024-01-02T00:00:00Z\",\n    \"parent\": {\n        \"title\": \"Test Parent\",\n        \"url\": \"https://github.com/test/repo/issues/2\",\n    },\n    \"issueType\": {\n        \"name\": \"Bug\",\n    },\n}\n\nVALID_ITERATION_VALUE = {\n    \"iterationId\": \"123\",\n    \"title\": \"Sprint 1\",\n    \"startDate\": \"2024-01-01\",\n    \"duration\": 14,\n}\n\nVALID_SINGLE_SELECT = {\n    \"optionId\": \"456\",\n    \"name\": \"In Progress\",\n}\n\n\n# #############################################\n# Project items tests\n# #############################################\n\n\nclass TestProjectItems:\n    \"\"\"Test cases for project item schemas.\"\"\"\n\n    def test_fully_populated(self) -> None:\n        \"\"\"Test validating a fully populated project item.\"\"\"\n        data = {\n            \"content\": VALID_ISSUE_CONTENT,\n            \"status\": VALID_SINGLE_SELECT,\n            \"sprint\": VALID_ITERATION_VALUE,\n            \"points\": {\"number\": 5},\n            \"quad\": VALID_ITERATION_VALUE,\n            \"pillar\": VALID_SINGLE_SELECT,\n        }\n        item = ProjectItem.model_validate(data)\n        # Check issue content\n        assert item.content.title == \"Test Issue\"\n        assert item.status.name == \"In Progress\"\n        # Check sprint fields\n        assert item.sprint.title == \"Sprint 1\"\n        assert item.points.number == 5\n        # Check roadmap fields\n        assert item.quad.title == \"Sprint 1\"\n        assert item.pillar.name == \"In Progress\"\n\n    def test_minimal(self) -> None:\n        \"\"\"Test validating a project item with only required fields.\"\"\"\n        data = {\n            \"content\": VALID_ISSUE_CONTENT,\n        }\n        item = ProjectItem.model_validate(data)\n        # Check status defaults\n        assert item.status.name is None\n        assert item.status.option_id is None\n        # Check sprint defaults\n        assert item.sprint.title is None\n        assert item.sprint.iteration_id is None\n        assert item.points.number is None\n        # Check roadmap defaults\n        assert item.quad.title is None\n        assert item.quad.iteration_id is None\n        assert item.pillar.name is None\n        assert item.pillar.option_id is None\n\n    def test_with_nulls(self) -> None:\n        \"\"\"Test validating a project item with null values explicitly set.\"\"\"\n        data = {\n            \"content\": {\n                \"title\": \"Test Issue\",\n                \"url\": \"https://github.com/test/repo/issues/1\",\n                \"closed\": True,\n                \"createdAt\": \"2024-01-01T00:00:00Z\",\n                \"closedAt\": \"2024-01-02T00:00:00Z\",\n                \"type\": None,\n                \"parent\": None,\n            },\n            \"status\": None,\n            \"sprint\": None,\n            \"points\": None,\n            \"quad\": None,\n            \"pillar\": None,\n        }\n        item = ProjectItem.model_validate(data)\n        # Check status defaults\n        assert item.status.name is None\n        assert item.status.option_id is None\n        # Check sprint defaults\n        assert item.sprint.title is None\n        assert item.sprint.iteration_id is None\n        assert item.points.number is None\n        # Check roadmap defaults\n        assert item.quad.title is None\n        assert item.quad.iteration_id is None\n        assert item.pillar.name is None\n        assert item.pillar.option_id is None\n\n\n# #############################################\n# Issue content tests\n# #############################################\n\n\nclass TestIssueContent:\n    \"\"\"Test cases for issue content schemas.\"\"\"\n\n    def test_fully_populated(self) -> None:\n        \"\"\"Test validating a fully populated issue content.\"\"\"\n        issue = IssueContent.model_validate(VALID_ISSUE_CONTENT)\n        assert issue.title == \"Test Issue\"\n        assert issue.url == \"https://github.com/test/repo/issues/1\"\n        assert issue.closed is True\n        assert issue.parent.title == \"Test Parent\"\n        assert issue.issue_type.name == \"Bug\"\n\n    def test_minimal(self) -> None:\n        \"\"\"Test validating an issue content with only required fields.\"\"\"\n        minimal_content = {\n            \"title\": \"Test Issue\",\n            \"url\": \"https://github.com/test/repo/issues/1\",\n            \"closed\": False,\n            \"createdAt\": \"2024-01-01T00:00:00Z\",\n        }\n        issue = IssueContent.model_validate(minimal_content)\n        assert issue.closed_at is None\n        assert issue.parent.title is None\n        assert issue.parent.url is None\n        assert issue.issue_type.name is None\n\n    def test_with_nulls(self) -> None:\n        \"\"\"Test validating an issue content with null values.\"\"\"\n        data = {\n            \"title\": \"Test Issue\",\n            \"url\": \"https://github.com/test/repo/issues/1\",\n            \"closed\": True,\n            \"createdAt\": \"2024-01-01T00:00:00Z\",\n            \"closedAt\": None,\n            \"type\": None,\n            \"parent\": None,\n        }\n        issue = IssueContent.model_validate(data)\n        assert issue.title == \"Test Issue\"\n        assert issue.closed_at is None\n        assert issue.issue_type.name is None\n        assert issue.parent.title is None\n        assert issue.parent.url is None\n\n    def test_missing_title_raises_error(self) -> None:\n        \"\"\"Test that validation fails when title is missing.\"\"\"\n        with pytest.raises(ValidationError):\n            IssueContent.model_validate(\n                {\n                    \"url\": \"https://github.com/test/repo/issues/1\",\n                    \"closed\": True,\n                    \"createdAt\": \"2024-01-01T00:00:00Z\",\n                },\n            )\n\n    def test_missing_url_raises_error(self) -> None:\n        \"\"\"Test that validation fails when url is missing.\"\"\"\n        with pytest.raises(ValidationError):\n            IssueContent.model_validate(\n                {\n                    \"title\": \"Test Issue\",\n                    \"closed\": True,\n                    \"createdAt\": \"2024-01-01T00:00:00Z\",\n                },\n            )\n\n    def test_missing_closed_raises_error(self) -> None:\n        \"\"\"Test that validation fails when closed is missing.\"\"\"\n        with pytest.raises(ValidationError):\n            IssueContent.model_validate(\n                {\n                    \"title\": \"Test Issue\",\n                    \"url\": \"https://github.com/test/repo/issues/1\",\n                    \"createdAt\": \"2024-01-01T00:00:00Z\",\n                },\n            )\n\n    def test_missing_created_at_raises_error(self) -> None:\n        \"\"\"Test that validation fails when createdAt is missing.\"\"\"\n        with pytest.raises(ValidationError):\n            IssueContent.model_validate(\n                {\n                    \"title\": \"Test Issue\",\n                    \"url\": \"https://github.com/test/repo/issues/1\",\n                    \"closed\": True,\n                },\n            )\n\n\n# #############################################\n# Project field tests\n# #############################################\n\n\nclass TestProjectFields:\n    \"\"\"Test cases for project field schemas.\"\"\"\n\n    def test_iteration_value_fully_populated(self) -> None:\n        \"\"\"Test validating a fully populated iteration value.\"\"\"\n        iteration = IterationValue.model_validate(VALID_ITERATION_VALUE)\n        assert iteration.iteration_id == \"123\"\n        assert iteration.title == \"Sprint 1\"\n        assert iteration.start_date == \"2024-01-01\"\n        assert iteration.duration == 14\n        assert iteration.end_date == \"2024-01-15\"\n\n    def test_iteration_value_with_empty_data(self) -> None:\n        \"\"\"Test validating an iteration value with empty data.\"\"\"\n        iteration = IterationValue.model_validate({})\n        assert iteration.iteration_id is None\n        assert iteration.title is None\n        assert iteration.start_date is None\n        assert iteration.duration is None\n        assert iteration.end_date is None\n\n    def test_single_select_value_fully_populated(self) -> None:\n        \"\"\"Test validating a fully populated single select value.\"\"\"\n        select = SingleSelectValue.model_validate(VALID_SINGLE_SELECT)\n        assert select.option_id == \"456\"\n        assert select.name == \"In Progress\"\n\n    def test_number_value_with_number(self) -> None:\n        \"\"\"Test validating number value with a number.\"\"\"\n        with_value = NumberValue.model_validate({\"number\": 5})\n        assert with_value.number == 5\n\n    def test_number_value_with_empty_data(self) -> None:\n        \"\"\"Test validating number value with empty data.\"\"\"\n        without_value = NumberValue.model_validate({})\n        assert without_value.number is None"}
{"path":"api/src/db/migrations/versions/2024_04_16_make_revision_number_nullable.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_04_16_make_revision_number_nullable.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/integrations/test_etldb.py\nLanguage: py\nType: code\nDirectory: analytics/tests/integrations\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/test_etldb.py\nSize: 3.02 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_04_19_fix_column_type_and_add_version_number.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_04_19_fix_column_type_and_add_version_number.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"import sqlalchemy\nfrom analytics.integrations.etldb.etldb import EtlDb\n\n\nclass TestEtlDb:\n    \"\"\"Test EtlDb methods.\"\"\"\n\n    TEST_FILE_1 = \"./tests/etldb_test_01.json\"\n\n    def test_instantiate_with_effective_date(self):\n        \"\"\"Class method should return the correctly instantiated object.\"\"\"\n        effective_date = \"2024-11-18\"\n        etldb = EtlDb(effective_date)\n\n        assert etldb.effective_date == effective_date\n\n    def test_database_connection(self):\n        \"\"\"Class method should return database connection object.\"\"\"\n        etldb = EtlDb()\n        connection = etldb.connection()\n\n        assert isinstance(connection, sqlalchemy.Connection)\n\n    def test_schema_versioning(self):\n        \"\"\"Class methods should return appropriate values.\"\"\"\n        etldb = EtlDb()\n        has_versioning = etldb.schema_versioning_exists()\n        current_version = etldb.get_schema_version()\n\n        if has_versioning:\n            assert current_version >= 2\n        else:\n            assert current_version <= 1\n\n    def test_set_version_number(self):\n        \"\"\"Class method should successfully update version.\"\"\"\n        etldb = EtlDb()\n        if not etldb.schema_versioning_exists():\n            return\n\n        original_version = etldb.get_schema_version()\n        next_version = original_version + 1\n        result = etldb.set_schema_version(next_version)\n\n        assert result is True\n        assert etldb.get_schema_version() == next_version\n\n        # revert to keep testing env in same state\n        etldb.revert_to_schema_version(original_version)\n\n    def test_set_bad_version_number(self):\n        \"\"\"Class method should not update version.\"\"\"\n        etldb = EtlDb()\n        if not etldb.schema_versioning_exists():\n            return\n\n        current_version = etldb.get_schema_version()\n        previous_version = current_version - 1\n        result = etldb.set_schema_version(previous_version)\n\n        assert result is False\n        assert etldb.get_schema_version() == current_version\n\n    def test_revert_to_version_number(self):\n        \"\"\"Class method should successfully update version.\"\"\"\n        etldb = EtlDb()\n        if not etldb.schema_versioning_exists():\n            return\n\n        original_version = etldb.get_schema_version()\n        previous_version = original_version - 1\n        result = etldb.revert_to_schema_version(previous_version)\n\n        assert result is True\n        assert etldb.get_schema_version() == previous_version\n\n        # revert the revert to keep testing env in same state\n        etldb.set_schema_version(original_version)\n\n    def test_revert_to_bad_version_number(self):\n        \"\"\"Class method should successfully update version.\"\"\"\n        etldb = EtlDb()\n        if not etldb.schema_versioning_exists():\n            return\n\n        original_version = etldb.get_schema_version()\n        previous_version = -99\n        result = etldb.revert_to_schema_version(previous_version)\n\n        assert result is False\n        assert etldb.get_schema_version() == original_version"}
{"path":"api/src/db/migrations/versions/2024_04_24_add_staging_tables.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_04_24_add_staging_tables.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/integrations/test_slack.py\nLanguage: py\nType: code\nDirectory: analytics/tests/integrations\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/integrations/test_slack.py\nSize: 1.85 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_05_01_add_created_at_updated_at_and_deleted_.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_05_01_add_created_at_updated_at_and_deleted_.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"from pathlib import Path  # noqa: I001\n\nimport pytest\nfrom slack_sdk import WebClient\n\nfrom analytics.integrations.slack import FileMapping, SlackBot\nfrom config import get_db_settings\n\nsettings = get_db_settings()\nclient = WebClient(token=settings.slack_bot_token)\n\n\n@pytest.fixture(name=\"slackbot\")\ndef mock_slackbot() -> SlackBot:\n    \"\"\"Create a SlackBot instance for testing.\"\"\"\n    return SlackBot(client=client)\n\n\nslack_token_required = pytest.mark.skipif(\n    \"not config.getoption('--slack-token-set')\",\n    reason=\"requires Slack token\",\n)\n\n\n@slack_token_required\ndef test_fetch_slack_channels(slackbot: SlackBot):\n    \"\"\"The fetch_slack_channels() function should execute correctly.\"\"\"\n    result = slackbot.fetch_slack_channel_info(\n        channel_id=settings.reporting_channel_id,\n    )\n    assert result[\"ok\"] is True\n    assert result[\"channel\"][\"name\"] == \"z_bot-analytics-ci-test\"\n\n\n@slack_token_required\ndef test_upload_files_to_slack_channel(slackbot: SlackBot):\n    \"\"\"The upload_files_to_slack_channel() function should execute correctly.\"\"\"\n    # setup - create test files to upload\n    files = [\n        FileMapping(path=\"data/test1.txt\", name=\"test1.txt\"),\n        FileMapping(path=\"data/test2.txt\", name=\"test2.txt\"),\n    ]\n    for _file in files:\n        test_dir = Path(_file.path).parent\n        test_dir.mkdir(exist_ok=True, parents=True)\n        with open(_file.path, \"w\", encoding=\"utf-8\") as f:\n            f.write(_file.name)\n    # execution - run the upload\n    var = \"variable\"\n    markdown = f\"\"\"\n*Bolded text {var}* :thread:\n\nâ€¢ *Bullet 1:* Text 1\nâ€¢ *Bullet 2:* Text 2\n\"\"\"\n    result = slackbot.upload_files_to_slack_channel(\n        files=files,\n        channel_id=settings.reporting_channel_id,\n        message=markdown,\n    )\n    assert result[\"ok\"] is True\n    assert result[\"files\"] is not None"}
{"path":"api/src/db/migrations/versions/2024_05_02_add_unique_constraint_for_summary.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_05_02_add_unique_constraint_for_summary.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/logs/__init__.py\nLanguage: py\nType: code\nDirectory: analytics/tests/logs\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/logs/__init__.py\nSize: 0.05 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_05_07_add_unique_constraint_link_table_legacy_.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_05_07_add_unique_constraint_link_table_legacy_.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":""}
{"path":"api/src/db/migrations/versions/2024_05_09_add_transformation_notes_to_staging_.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_05_09_add_transformation_notes_to_staging_.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/logs/test_ecs_background_task.py\nLanguage: py\nType: code\nDirectory: analytics/tests/logs\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/logs/test_ecs_background_task.py\nSize: 2.43 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_07_08_add_agency_related_tables.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_07_08_add_agency_related_tables.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"import logging\nimport time\n\nimport pytest\nfrom analytics.logs import init\nfrom analytics.logs.app_logger import add_extra_data_to_global_logs, init_app\nfrom analytics.logs.ecs_background_task import ecs_background_task\n\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef setup_logging():\n    \"\"\"Fixture to setup logging for the ecs_background_task tests.\"\"\"\n    with init(\"ecs_background_task_tests\"):\n        yield init_app(logging.root)\n\n\ndef test_ecs_background_task(caplog: pytest.LogCaptureFixture):\n    \"\"\"Test the ecs_background_task in a normal run.\"\"\"\n    # We pull in the app so its initialized\n    # Global logging params like the task name are stored on the app\n    caplog.set_level(logging.INFO)\n\n    @ecs_background_task(task_name=\"my_test_task_name\")\n    def my_test_func(param1: int, param2: int) -> int:\n        # Add a brief sleep so that we can test the duration logic\n        time.sleep(0.2)  # 0.2s\n        add_extra_data_to_global_logs({\"example_param\": 12345})\n\n        return param1 + param2\n\n    # Verify the function works uneventfully\n    assert my_test_func(1, 2) == 3\n\n    for record in caplog.records:\n        extra = record.__dict__\n        assert extra[\"task_name\"] == \"my_test_task_name\"\n\n    last_record = caplog.records[-1].__dict__\n    # Make sure the ECS task duration was tracked\n    allowed_error = 0.1\n    assert last_record[\"ecs_task_duration_sec\"] == pytest.approx(0.2, abs=allowed_error)\n    # Make sure the extra we added was put in this automatically\n    assert last_record[\"example_param\"] == 12345\n    assert last_record[\"message\"] == \"Completed ECS task my_test_task_name\"\n\n\ndef test_ecs_background_task_when_erroring(caplog: pytest.LogCaptureFixture):\n    \"\"\"Test the ecs_background_task when a task errors.\"\"\"\n    caplog.set_level(logging.INFO)\n\n    @ecs_background_task(task_name=\"my_error_test_task_name\")\n    def my_test_error_func() -> None:\n        add_extra_data_to_global_logs({\"another_param\": \"hello\"})\n\n        msg = \"I am an error\"\n        raise ValueError(msg)\n\n    with pytest.raises(ValueError, match=\"I am an error\"):\n        my_test_error_func()\n\n    for record in caplog.records:\n        extra = record.__dict__\n        assert extra[\"task_name\"] == \"my_error_test_task_name\"\n\n    last_record = caplog.records[-1].__dict__\n\n    assert last_record[\"another_param\"] == \"hello\"\n    assert last_record[\"levelname\"] == \"ERROR\"\n    assert last_record[\"message\"] == \"ECS task failed\""}
{"path":"api/src/db/migrations/versions/2024_10_01_make_number_of_awards_a_big_int.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_01_make_number_of_awards_a_big_int.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/logs/test_formatters.py\nLanguage: py\nType: code\nDirectory: analytics/tests/logs\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/logs/test_formatters.py\nSize: 3.08 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_10_16_add_opportunity_attachment_tables.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_16_add_opportunity_attachment_tables.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"import json\nimport logging\nimport re\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom uuid import uuid4\n\nimport pytest\nfrom analytics.logs import formatters\n\nfrom tests.assertions import assert_dict_contains\n\n\ndef test_json_formatter(capsys: pytest.CaptureFixture):\n    \"\"\"Test that the JSON formatter can handle several common types.\"\"\"\n    logger = logging.getLogger(\"test_json_formatter\")\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(formatters.JsonFormatter())\n    logger.addHandler(console_handler)\n\n    datetime_now = datetime.now(tz=timezone.utc)\n    date_now = datetime_now.date()\n    decimal_field = Decimal(\"12.34567\")\n    uuid_field = uuid4()\n    set_field = {uuid4(), uuid4()}\n    list_field = [1, 2, 3, 4]\n    exception_field = ValueError(\"my exception message\")\n    logger.warning(\n        \"hello %s\",\n        \"interpolated_string\",\n        extra={\n            \"foo\": \"bar\",\n            \"int_field\": 5,\n            \"bool_field\": True,\n            \"none_field\": None,\n            \"datetime_field\": datetime_now,\n            \"date_field\": date_now,\n            \"decimal_field\": decimal_field,\n            \"uuid_field\": uuid_field,\n            \"set_field\": set_field,\n            \"list_field\": list_field,\n            \"exception_field\": exception_field,\n        },\n    )\n\n    json_record = json.loads(capsys.readouterr().err)\n\n    expected = {\n        \"name\": \"test_json_formatter\",\n        \"message\": \"hello interpolated_string\",\n        \"msg\": \"hello %s\",\n        \"levelname\": \"WARNING\",\n        \"levelno\": 30,\n        \"filename\": \"test_formatters.py\",\n        \"module\": \"test_formatters\",\n        \"funcName\": \"test_json_formatter\",\n        \"foo\": \"bar\",\n        \"int_field\": 5,\n        \"bool_field\": True,\n        \"none_field\": None,\n        \"datetime_field\": datetime_now.isoformat(),\n        \"date_field\": date_now.isoformat(),\n        \"decimal_field\": str(decimal_field),\n        \"uuid_field\": str(uuid_field),\n        \"set_field\": [str(u) for u in set_field],\n        \"list_field\": list_field,\n        \"exception_field\": str(exception_field),\n    }\n    assert_dict_contains(json_record, expected)\n    logger.removeHandler(console_handler)\n\n\ndef test_human_readable_formatter(capsys: pytest.CaptureFixture):\n    \"\"\"Test that the human-readable formatter can handle several common types.\"\"\"\n    logger = logging.getLogger(\"test_human_readable_formatter\")\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(formatters.HumanReadableFormatter())\n    logger.addHandler(console_handler)\n\n    logger.warning(\"hello %s\", \"interpolated_string\", extra={\"foo\": \"bar\"})\n\n    text = capsys.readouterr().err\n    created_time = text[:12]\n    rest = text[12:]\n    assert re.match(r\"^\\d{2}:\\d{2}:\\d{2}\\.\\d{3}\", created_time)\n    assert (\n        rest == \"  test_human_readable_formatter       \\x1b[0m \"\n        \"test_human_readable_formatter \"\n        \"\\x1b[31mWARNING \\x1b[0m \\x1b[31m\"\n        \"hello interpolated_string                         \"\n        \"\\x1b[0m \\x1b[34mfoo=bar\\x1b[0m\\n\"\n    )\n    logger.removeHandler(console_handler)"}
{"path":"api/src/db/migrations/versions/2024_10_17_add_top_level_agency_field.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_17_add_top_level_agency_field.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/logs/test_logging.py\nLanguage: py\nType: code\nDirectory: analytics/tests/logs\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/logs/test_logging.py\nSize: 3.52 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_10_17_legacy_null_agency.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_17_legacy_null_agency.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"import logging\nimport re\n\nimport pytest\nfrom analytics.logs import formatters, init\n\nfrom tests.assertions import assert_dict_contains\n\n\n@pytest.fixture\ndef init_test_logger(caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch):\n    \"\"\"Fixture to setup a logger for tests.\"\"\"\n    caplog.set_level(logging.DEBUG)\n    monkeypatch.setenv(\"LOG_FORMAT\", \"human-readable\")\n    with init(\"test_logging\"):\n        yield\n\n\n@pytest.mark.parametrize(\n    (\"log_format\", \"expected_formatter\"),\n    [\n        (\"human-readable\", formatters.HumanReadableFormatter),\n        (\"json\", formatters.JsonFormatter),\n    ],\n)\ndef test_init(\n    caplog: pytest.LogCaptureFixture,\n    monkeypatch: pytest.MonkeyPatch,\n    log_format: str,\n    expected_formatter: logging.Formatter,\n):\n    \"\"\"Test to verify behavior of initializing the logger.\"\"\"\n    caplog.set_level(logging.DEBUG)\n    monkeypatch.setenv(\"LOG_FORMAT\", log_format)\n\n    with init(\"test_logging\"):\n        records = caplog.records\n        assert len(records) == 2\n        assert re.match(\n            r\"^start test_logging: \\w+ [0-9.]+ \\w+, hostname \\S+, pid \\d+, user \\d+\\([\\w\\.]+\\)\",\n            records[0].message,\n        )\n        assert re.match(r\"^invoked as:\", records[1].message)\n\n        formatter_types = [type(handler.formatter) for handler in logging.root.handlers]\n        assert expected_formatter in formatter_types\n\n\ndef test_log_exception(\n    init_test_logger: None,  # noqa: ARG001\n    caplog: pytest.LogCaptureFixture,\n):\n    \"\"\"Test to verify behavior of logging exceptions.\"\"\"\n    logger = logging.getLogger(__name__)\n\n    try:\n        msg = \"example exception\"\n        raise Exception(msg)  # noqa: TRY301, TRY002\n    except Exception:\n        logger.exception(\n            \"test log message %s\",\n            \"example_arg\",\n            extra={\"key1\": \"value1\", \"key2\": \"value2\"},\n        )\n\n    last_record: logging.LogRecord = caplog.records[-1]\n\n    assert last_record.message == \"test log message example_arg\"\n    assert last_record.funcName == \"test_log_exception\"\n    assert last_record.threadName == \"MainThread\"\n    assert last_record.exc_text.startswith(\"Traceback (most recent call last)\")\n    assert last_record.exc_text.endswith(\"Exception: example exception\")\n    assert last_record.__dict__[\"key1\"] == \"value1\"\n    assert last_record.__dict__[\"key2\"] == \"value2\"\n\n\n@pytest.mark.parametrize(\n    (\"args\", \"extra\", \"expected\"),\n    [\n        pytest.param(\n            (\"ssn: 123456789\",),\n            None,\n            {\"message\": \"ssn: *********\"},\n            id=\"pii in msg\",\n        ),\n        pytest.param(\n            (\"pii\",),\n            {\"foo\": \"bar\", \"tin\": \"123456789\", \"dashed-ssn\": \"123-45-6789\"},\n            {\n                \"message\": \"pii\",\n                \"foo\": \"bar\",\n                \"tin\": \"*********\",\n                \"dashed-ssn\": \"*********\",\n            },\n            id=\"pii in extra\",\n        ),\n        pytest.param(\n            (\"%s %s\", \"text\", \"123456789\"),\n            None,\n            {\"message\": \"text *********\"},\n            id=\"pii in interpolation args\",\n        ),\n    ],\n)\ndef test_mask_pii(\n    init_test_logger: None,  # noqa: ARG001\n    caplog: pytest.LogCaptureFixture,\n    args: list,\n    extra: dict,\n    expected: dict,\n):\n    \"\"\"Test to verify PII is masked properly in logs.\"\"\"\n    logger = logging.getLogger(__name__)\n\n    logger.info(*args, extra=extra)\n\n    assert len(caplog.records) == 1\n    assert_dict_contains(caplog.records[0].__dict__, expected)"}
{"path":"api/src/db/migrations/versions/2024_10_28_add_opportunity_search_index_queue_table.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_28_add_opportunity_search_index_queue_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/logs/test_pii.py\nLanguage: py\nType: code\nDirectory: analytics/tests/logs\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/logs/test_pii.py\nSize: 1.07 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_10_28_add_opportunity_table_triggers.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_28_add_opportunity_table_triggers.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"from typing import Any\n\nimport pytest\nfrom analytics.logs import pii\n\n\n@pytest.mark.parametrize(\n    (\"value\", \"expected\"),\n    [\n        (\"\", \"\"),\n        (\"1234\", \"1234\"),\n        (1234, 1234),\n        (None, None),\n        (\n            \"hostname ip-10-11-12-134.ec2.internal\",\n            \"hostname ip-10-11-12-134.ec2.internal\",\n        ),\n        ({}, {}),\n        (\"123456789\", \"*********\"),\n        (123456789, \"*********\"),\n        (\"123-45-6789\", \"*********\"),\n        (\"123456789 test\", \"********* test\"),\n        (\"test 123456789\", \"test *********\"),\n        (\"test 123456789 test\", \"test ********* test\"),\n        (\"test=999000000.\", \"test=*********.\"),\n        (\"test=999000000,\", \"test=*********,\"),\n        (999000000.5, 999000000.5),\n        ({\"a\": \"x\", \"b\": \"999000000\"}, \"{'a': 'x', 'b': '*********'}\"),\n    ],\n)\ndef test_mask_pii(value: Any | None, expected: Any | None):  # noqa: ANN401\n    \"\"\"Test mask_pii logic with various input values.\"\"\"\n    assert pii._mask_pii(value) == expected  # noqa: SLF001"}
{"path":"api/src/db/migrations/versions/2024_10_31_remove_has_update_column_from_.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_31_remove_has_update_column_from_.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/ruff.toml\nLanguage: toml\nType: code\nDirectory: analytics/tests\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/ruff.toml\nSize: 0.30 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_11_01_rename_agency_column.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_11_01_rename_agency_column.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"[lint]\nignore = [\n  \"ANN201\", # missing return type\n  \"PLR2004\", # magic value used in comparison instead of constant\n  \"S101\", # Use of `assert` detected\n  \"T201\",    # use of `print` detected\n]"}
{"path":"api/src/db/migrations/versions/2024_11_12_add_basic_user_tables.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_11_12_add_basic_user_tables.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/test_cli.py\nLanguage: py\nType: code\nDirectory: analytics/tests\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/test_cli.py\nSize: 3.85 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_11_15_add_metadata_extract_table.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_11_15_add_metadata_extract_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"from dataclasses import dataclass  # noqa: I001\nfrom pathlib import Path\n\nimport pytest\nfrom typer.testing import CliRunner\n\nfrom analytics.cli import app\nfrom tests.conftest import (\n    json_issue_row,\n    json_sprint_row,\n    issue,\n    write_test_data_to_file,\n)\n\nrunner = CliRunner()\n\n\n@dataclass\nclass MockFiles:\n    \"\"\"Store paths to stub files for testing.\"\"\"\n\n    issue_file: Path\n    sprint_file: Path\n    delivery_file: Path\n\n\n@pytest.fixture(name=\"mock_files\")\ndef test_file_fixtures(tmp_path: Path) -> MockFiles:\n    \"\"\"Create test files to use in unit tests.\"\"\"\n    # set paths to test files\n    issue_file = tmp_path / \"data\" / \"issue-data.json\"\n    sprint_file = tmp_path / \"data\" / \"sprint-data.json\"\n    delivery_file = tmp_path / \"data\" / \"delivery-data.json\"\n    # create test data\n    sprint_data = [json_sprint_row(issue=1, parent_number=2)]\n    issue_data = [\n        json_issue_row(issue=1, labels=[\"task\"]),\n        json_issue_row(issue=2, labels=[\"deliverable: 30k ft\"]),\n    ]\n    delivery_data = [\n        issue(issue=1).model_dump(),\n        issue(issue=2).model_dump(),\n    ]\n    # write test data to json files\n    write_test_data_to_file(issue_data, issue_file)\n    write_test_data_to_file({\"items\": sprint_data}, sprint_file)\n    write_test_data_to_file(delivery_data, delivery_file)\n    # confirm the data was written\n    assert issue_file.exists()\n    assert sprint_file.exists()\n    # return paths to output files\n    return MockFiles(\n        issue_file=issue_file,\n        sprint_file=sprint_file,\n        delivery_file=delivery_file,\n    )\n\n\nclass TestEtlEntryPoint:\n    \"\"\"Test the etl entry point.\"\"\"\n\n    TEST_FILE_1 = \"./tests/etldb_test_01.json\"\n    EFFECTIVE_DATE = \"2024-10-07\"\n\n    def test_init_db(self):\n        \"\"\"Test the db initialization command.\"\"\"\n        # setup - create command\n        command = [\n            \"etl\",\n            \"db_migrate\",\n        ]\n        # execution\n        result = runner.invoke(app, command)\n        print(result.stdout)\n        # validation - check there wasn't an error\n        assert result.exit_code == 0\n        assert \"initializing database\" in result.stdout\n        assert \"done\" in result.stdout\n\n    def test_transform_and_load_with_valid_parameters(self):\n        \"\"\"Test the transform and load command.\"\"\"\n        # setup - create command\n        command = [\n            \"etl\",\n            \"transform_and_load\",\n            \"--issue-file\",\n            self.TEST_FILE_1,\n            \"--effective-date\",\n            str(self.EFFECTIVE_DATE),\n        ]\n        # execution\n        result = runner.invoke(app, command)\n        print(result.stdout)\n        # validation - check there wasn't an error\n        assert result.exit_code == 0\n        assert (\n            f\"running transform and load with effective date {self.EFFECTIVE_DATE}\"\n            in result.stdout\n        )\n        assert \"project row(s) processed: 2\" in result.stdout\n        assert \"quad row(s) processed: 1\" in result.stdout\n        assert \"deliverable row(s) processed: 4\" in result.stdout\n        assert \"sprint row(s) processed: 4\" in result.stdout\n        assert \"epic row(s) processed: 6\" in result.stdout\n        assert \"issue row(s) processed: 22\" in result.stdout\n        assert \"transform and load is done\" in result.stdout\n\n    def test_transform_and_load_with_malformed_effective_date_parameter(self):\n        \"\"\"Test the transform and load command.\"\"\"\n        # setup - create command\n        command = [\n            \"etl\",\n            \"transform_and_load\",\n            \"--issue-file\",\n            self.TEST_FILE_1,\n            \"--effective-date\",\n            \"2024-Oct-07\",\n        ]\n        # execution\n        result = runner.invoke(app, command)\n        print(result.stdout)\n        # validation - check there wasn't an error\n        assert result.exit_code == 0\n        assert \"FATAL ERROR: malformed effective date\" in result.stdout"}
{"path":"api/src/db/migrations/versions/2024_11_18_user_session_table.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_11_18_user_session_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: analytics/tests/test_version.py\nLanguage: py\nType: code\nDirectory: analytics/tests\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/analytics/tests/test_version.py\nSize: 0.22 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_12_04_login_gov_state.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_12_04_login_gov_state.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"import analytics\n\n\ndef test_package_import_and_name():\n    \"\"\"The package should be imported and named correctly.\"\"\"\n    assert analytics.__name__ == \"analytics\""}
{"path":"api/src/db/migrations/versions/2024_12_19_add_attachment_table_staging_and_foreign.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_12_19_add_attachment_table_staging_and_foreign.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/Dockerfile\nLanguage: unknown\nType: code\nDirectory: api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/Dockerfile\nSize: 4.39 KB\nLast Modified: 2025-02-14T17:08:26.430Z"}
{"path":"api/src/db/migrations/versions/2024_12_19_create_user_saved_opportunity_table.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_12_19_create_user_saved_opportunity_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"# The build stage that will be used to deploy to the various environments\n# needs to be called `release` in order to integrate with the repo's\n# top-level Makefile\nFROM python:3.13-slim AS base\n# See /documentation/api/package-depedency-management.md#Upgrading Python\n# for details on upgrading your Python version\n\n# Install poetry, the package manager.\n# https://python-poetry.org\nRUN pip install --no-cache-dir poetry==1.5\n\nRUN apt-get update \\\n    # Remove existing packages before installing their never versions\n    && apt-get remove --yes \\\n        build-essential \\\n        libpq-dev \\\n        postgresql \\\n        wget \\\n    # Install security updates\n    # https://pythonspeed.com/articles/security-updates-in-docker/\n    && apt-get upgrade --yes \\\n    && apt-get install --no-install-recommends --yes \\\n        build-essential \\\n        libpq-dev \\\n        postgresql \\\n        wget \\\n        libtasn1-6 \\\n    # Reduce the image size by clear apt cached lists\n    # Complies with https://github.com/codacy/codacy-hadolint/blob/master/codacy-hadolint/docs/description/DL3009.md\n    && rm -fr /var/lib/apt/lists/* \\\n    && rm /etc/ssl/private/ssl-cert-snakeoil.key\n\nARG RUN_UID\nARG RUN_USER\n\n# The following logic creates the RUN_USER home directory and the directory where\n# we will be storing the application in the image. This runs when the user is not root\nRUN : \"${RUN_USER:?RUN_USER and RUN_UID need to be set and non-empty.}\" && \\\n    [ \"${RUN_USER}\" = \"root\" ] || \\\n    (useradd --create-home --create --user-group --home \"/home/${RUN_USER}\" --uid ${RUN_UID} \"${RUN_USER}\" \\\n    && mkdir /api \\\n    && chown -R ${RUN_UID} \"/home/${RUN_USER}\" /api)\n\n#-----------\n# Dev image\n#-----------\n\nFROM base AS dev\nARG RUN_USER\n\n# In between ARG RUN_USER and USER ${RUN_USER}, the user is still root\n# If there is anything that needs to be ran as root, this is the spot\n\n# Install graphviz which is used to generate ERD diagrams\nRUN apt-get update && apt-get install --no-install-recommends --yes graphviz\n\nUSER ${RUN_USER}\nWORKDIR /api\n\nCOPY pyproject.toml poetry.lock ./\n# Explicitly create a new virtualenv to avoid getting overridden by mounted .venv folders\nRUN poetry config virtualenvs.in-project false && poetry env use python\n# Install all dependencies including dev dependencies\nRUN poetry install --no-root --with dev\n\nCOPY . /api\n\n# Set the host to 0.0.0.0 to make the server available external\n# to the Docker container that it's running in.\nENV HOST=0.0.0.0\n\n# Run the application.\nCMD [\"poetry\", \"run\", \"python\", \"-m\", \"src\"]\n\n#---------\n# Release\n#---------\n\nFROM base AS release\nARG RUN_USER\n\n# Gunicorn requires this workaround to create writable temporary directory in\n# our readonly root file system. https://github.com/aws/containers-roadmap/issues/736\nRUN mkdir -p /tmp\nVOLUME [\"/tmp\"]\n\n# TODO(https://github.com/navapbc/template-application-flask/issues/23) Productionize the Docker image\n\nWORKDIR /api\n\nCOPY . /api\n\n# Remove any existing virtual environments that might exist. This\n# might happen if testing out building the release image from a local machine\n# that has a virtual environment within the project api folder.\nRUN rm -fr /api/.venv\n\n# Set virtualenv location to be in project to be easy to find\n# This will create a virtualenv in /api/.venv/\n# See https://python-poetry.org/docs/configuration/#virtualenvsin-project\n# See https://python-poetry.org/docs/configuration/#using-environment-variables\nENV POETRY_VIRTUALENVS_IN_PROJECT=true\n\n# Install production runtime dependencies only\nRUN poetry install --no-root --only main\n\n# Build the application binary (python wheel) defined in pyproject.toml\n# Note that this will only copy over python files, and files stated in the\n# include section in pyproject.toml. Also note that if you change the name or\n# version section in pyproject.toml, you will need to change the dist/... to match\n# or the application will not build\nRUN poetry build --format wheel && poetry run pip install 'dist/simpler_grants_gov_api-0.1.0-py3-none-any.whl'\n\n# Add project's virtual env to the PATH so we can directly run poetry scripts\n# defiend in pyproject.toml\nENV PATH=\"/api/.venv/bin:$PATH\"\n\n# Set the host to 0.0.0.0 to make the server available external\n# to the Docker container that it's running in.\nENV HOST=0.0.0.0\n\nUSER ${RUN_USER}\n\n# Run the application.\nCMD [\"poetry\", \"run\", \"gunicorn\", \"src.app:create_app()\"]"}
{"path":"api/src/db/migrations/versions/2025_01_06_remove_transfertopportunity_models.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_01_06_remove_transfertopportunity_models.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/Makefile\nLanguage: unknown\nType: code\nDirectory: api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/Makefile\nSize: 11.42 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/migrations/versions/2025_01_08_create_saved_searches_table.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_01_08_create_saved_searches_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"APP_NAME := grants-api\n\n# Colors for output, can be used as:\n# \techo -e \"this text is the default color $(RED) this text is red $(NO_COLOR) everything here is the default color again\"\nRED := \\033[0;31m\nNO_COLOR := \\033[0m\n\n# Adding this to the end of a script that outputs JSON will convert\n# it to a readable format with timestamps and color-coding.\n#\n# Note that you can also change the LOG_FORMAT env var to switch\n# between JSON & human readable format. This is left in place\n# in the event JSON is output from a process we don't log.\nDECODE_LOG := 2>&1 | python3 -u src/logging/util/decodelog.py\n\n# Required for CI flags below to work properly\nSHELL = /bin/bash -o pipefail\n\n# The APP_DIR variable is the path from the root of the repository to this Makefile.\n# This variable is used to display errors from MyPy in the 'Files Changed'\n# section of a pull request. If this is set to the incorrect value, you won't be able\n# to see the errors on the correct files in that section\nAPP_DIR := api\nifdef CI\n DOCKER_EXEC_ARGS := -T -e CI -e PYTEST_ADDOPTS=\"--color=yes\"\n MYPY_FLAGS := --no-pretty\n MYPY_POSTPROC := | perl -pe \"s/^(.+):(\\d+):(\\d+): error: (.*)/::warning file=$(APP_DIR)\\/\\1,line=\\2,col=\\3::\\4/\"\nendif\n\n# By default, all python/poetry commands will run inside of the docker container\n# if you wish to run this natively, add PY_RUN_APPROACH=local to your environment vars\n# You can set this by either running `export PY_RUN_APPROACH=local` in your shell or add\n# it to your ~/.zshrc file (and run `source ~/.zshrc`)\nifeq \"$(PY_RUN_APPROACH)\" \"local\"\nPY_RUN_CMD := poetry run\nelse\nPY_RUN_CMD := docker compose run $(DOCKER_EXEC_ARGS) --rm $(APP_NAME) poetry run\nendif\n\nFLASK_CMD := $(PY_RUN_CMD) flask\n\n# Docker user configuration\n# This logic is to avoid issues with permissions and mounting local volumes,\n# which should be owned by the same UID for Linux distros. Mac OS can use root,\n# but it is best practice to run things as with least permission where possible\n\n# Can be set by adding user=<username> and/ or uid=<id> after the make command\n# If variables are not set explicitly: try looking up values from current\n# environment, otherwise fixed defaults.\n# uid= defaults to 0 if user= set (which makes sense if user=root, otherwise you\n# probably want to set uid as well).\nifeq ($(user),)\nRUN_USER ?= $(or $(strip $(USER)),nodummy)\nRUN_UID ?= $(or $(strip $(shell id -u)),4000)\nelse\nRUN_USER = $(user)\nRUN_UID = $(or $(strip $(uid)),0)\nendif\n\nexport RUN_USER\nexport RUN_UID\n\nrelease-build:\n\tdocker buildx build \\\n\t\t--target release \\\n\t\t--platform=linux/amd64 \\\n\t\t--build-arg RUN_USER=$(RUN_USER) \\\n\t\t--build-arg RUN_UID=$(RUN_UID) \\\n\t\t$(OPTS) \\\n\t\t.\n\n##################################################\n# Local Development Environment Setup\n##################################################\n\nsetup-local:\n\t# Configure poetry to use virtualenvs in the project directory\n\tpoetry config virtualenvs.in-project true\n\n\t# Install dependencies\n\tpoetry install --no-root --all-extras --with dev\n\nsetup-env-override-file:\n\t./bin/setup-env-override-file.sh $(args)\n\n##################################################\n# API Build & Run\n##################################################\n\nbuild:\n\tdocker compose build\n\nstart: ## Start the API\n\tdocker compose up --detach\n\nstart-debug:\n\tdocker compose -f docker-compose.yml -f docker-compose.debug.yml up --detach\n\nrun-logs: start ## Start the API and follow the logs\n\tdocker compose logs --follow --no-color $(APP_NAME)\n\ninit: setup-env-override-file build init-db init-opensearch init-localstack\n\nclean-volumes: ## Remove project docker volumes - which includes the DB, and OpenSearch state\n\tdocker compose down --volumes\n\nvolume-recreate: clean-volumes init ## Destroy current volumes, setup new ones - will remove all existing data\n\nstop:\n\tdocker compose down\n\ncheck: format-check lint db-check-migrations test\n\nremake-backend: volume-recreate db-seed-local populate-search-opportunities ## Completely recreate API services, load data into the DB and search index\n\n##################################################\n# DB & migrations\n##################################################\n\n#########################\n# DB running / setup\n#########################\n\n# Docker starts the image for the DB but it's not quite\n# ready to accept connections so we add a brief wait script\ninit-db: start-db setup-postgres-db db-migrate\n\nstart-db:\n\tdocker compose up --detach grants-db\n\t./bin/wait-for-local-db.sh\n\n#########################\n# DB Migrations\n#########################\n\nalembic_config := ./src/db/migrations/alembic.ini\nalembic_cmd := $(PY_RUN_CMD) alembic --config $(alembic_config)\n\ndb-migrate: ## Apply pending migrations to db\n\t$(PY_RUN_CMD) db-migrate\n\ndb-migrate-down: ## Rollback last migration in db\n\t$(PY_RUN_CMD) db-migrate-down\n\ndb-migrate-down-all: ## Rollback all migrations\n\t$(PY_RUN_CMD) db-migrate-down-all\n\ncheck-migrate-msg:\nifndef MIGRATE_MSG\n\t$(error MIGRATE_MSG is undefined)\nendif\n\ndb-migrate-create: check-migrate-msg ## Create database migration with description MIGRATE_MSG\n\t$(alembic_cmd) revision --autogenerate -m \"$(MIGRATE_MSG)\"\n\nMIGRATE_MERGE_MSG := Merge multiple heads\ndb-migrate-merge-heads: ## Create a new migration that depends on all existing `head`s\n\t$(alembic_cmd) merge heads -m \"$(MIGRATE_MERGE_MSG)\" $(args)\n\ndb-migrate-current: ## Show current revision for a database\n\t$(alembic_cmd) current $(args)\n\ndb-migrate-history: ## Show migration history\n\t$(alembic_cmd) history $(args)\n\ndb-migrate-heads: ## Show migrations marked as a head\n\t$(alembic_cmd) heads $(args)\n\ndb-seed-local: ## Generate records into your local database\n\t$(PY_RUN_CMD) db-seed-local $(args)\n\ndb-check-migrations: ## Verify the DB schema matches the DB migrations generated\n\t$(alembic_cmd) check || (echo -e \"\\n$(RED)Migrations are not up-to-date, make sure you generate migrations by running 'make db-migrate-create <msg>'$(NO_COLOR)\"; exit 1)\n\ncreate-erds: # Create ERD diagrams for our DB schema\n\t$(PY_RUN_CMD) create-erds\n\tmv bin/*.png ../documentation/api/database/erds\n\nsetup-postgres-db: ## Does any initial setup necessary for our local database to work\n\t$(PY_RUN_CMD) setup-postgres-db\n\n##################################################\n# Opensearch\n##################################################\n\ninit-opensearch: start-opensearch ## Start the opensearch service locally\n\nstart-opensearch:\n\tdocker compose up --detach opensearch-node\n\tdocker compose up --detach opensearch-dashboards\n\t./bin/wait-for-local-opensearch.sh\n\n##################################################\n# Localstack\n##################################################\n\ninit-localstack: start-localstack setup-localstack ## Start localstack (local s3) and setup buckets\n\nstart-localstack:\n\tdocker compose up --detach localstack\n\nsetup-localstack:\n\t$(PY_RUN_CMD) setup-localstack\n\n##################################################\n# Mock Oauth Server\n##################################################\n\ninit-mock-oauth2: start-mock-oauth2\n\nstart-mock-oauth2:\n\tdocker compose up --detach mock-oauth2-server\n\n##################################################\n# Testing\n##################################################\n\ntest: ## Run all tests except for audit logging tests\n\t$(PY_RUN_CMD) pytest -m \"not audit\" $(args)\n\ntest-audit: ## Run audit logging tests\n\t$(PY_RUN_CMD) pytest -m \"audit\" $(args)\n\ntest-watch: ## Run tests continually and watch for changes\n\t$(PY_RUN_CMD) pytest-watch --clear $(args)\n\ntest-coverage: ## Run tests and generate coverage report\n\t$(PY_RUN_CMD) coverage run --branch --source=src -m pytest -m \"not audit\" $(args)\n\t$(PY_RUN_CMD) coverage run --data-file=.coverage.audit --branch --source=src -m pytest -m \"audit\" $(args)\n\t$(PY_RUN_CMD) coverage combine --data-file=.coverage --append\n\t$(PY_RUN_CMD) coverage report\n\ntest-coverage-report: ## Open HTML test coverage report\n\t$(PY_RUN_CMD) coverage html --directory .coverage_report\n\topen .coverage_report/index.html\n\n##################################################\n# Formatting and linting\n##################################################\n\nformat: ## Format files\n\t$(PY_RUN_CMD) isort --atomic src tests bin\n\t$(PY_RUN_CMD) black src tests bin\n\nformat-check: ## Check file formatting\n\t$(PY_RUN_CMD) isort --atomic --check-only src tests bin\n\t$(PY_RUN_CMD) black --check src tests bin\n\nlint: lint-py ## Lint\n\nlint-py: lint-ruff lint-mypy\n\nlint-ruff:\n\t$(PY_RUN_CMD) ruff check src tests bin\n\nlint-mypy:\n\t$(PY_RUN_CMD) mypy --show-error-codes $(MYPY_FLAGS) src bin $(MYPY_POSTPROC)\n\nlint-security: # https://bandit.readthedocs.io/en/latest/index.html\n\t$(PY_RUN_CMD) bandit -c pyproject.toml -r . --number 3 --skip B101 -ll -x ./.venv\n\n\n##################################################\n# CLI Commands\n##################################################\n\ncmd: ## Run Flask app CLI command (Run `make cli args=\"--help\"` to see list of CLI commands)\n\t$(FLASK_CMD) $(args)\n\n# Set init-db as pre-requisite since there seems to be a race condition\n# where the DB can't yet receive connections if it's starting from a\n# clean state (e.g. after make stop, make clean-volumes, make openapi-spec)\nopenapi-spec: init-db ## Generate OpenAPI spec\n\t$(FLASK_CMD) spec --format yaml --output ./openapi.generated.yml\n\n\ncopy-oracle-data:\n\t$(FLASK_CMD) data-migration copy-oracle-data\n\nsetup-foreign-tables:\n\t$(FLASK_CMD) data-migration setup-foreign-tables\n\nseed-local-legacy-tables:\n\t$(PY_RUN_CMD) python3 -m tests.lib.seed_local_legacy_tables\n\npopulate-search-opportunities: ## Load opportunities from the DB into the search index, run \"make db-seed-local\" first to populate your database\n\t$(FLASK_CMD) load-search-data load-opportunity-data $(args)\n\n##################################################\n# Miscellaneous Utilities\n##################################################\n\nlogin: start ## Start shell in running container\n\tdocker exec -it $(APP_NAME) bash\n\nDB_URI := postgresql://$(DB_USER)@$(DB_HOST):$(DB_PORT)/$(DB_NAME)?options=-csearch_path%3dapi,legacy,staging\nlogin-db: ## Start psql with project environment variables\n\tPGPASSWORD=$$DB_PASSWORD psql $(DB_URI)\n\nconsole: ## Start interactive Python console\n\t$(PY_RUN_CMD) python3 -i -m tool.console.interactive\n\nhelp: ## Prints the help documentation and info about each command\n\t@grep -E '^[/a-zA-Z0-9_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-30s\\033[0m %s\\n\", $$1, $$2}'\n\n##################################################\n# Load testing\n##################################################\n\nload-test-local: # Load test the local environment at localhost:3000\n\tartillery run artillery-load-test.yml\n\nload-test-dev: # Load test the dev environment in aws\n\t$(eval API_AUTH_TOKEN := $(shell aws ssm get-parameter --name /api/dev/api-auth-token --query Parameter.Value --with-decryption --output text | cut -d',' -f1))\n\tenv API_AUTH_TOKEN=$(API_AUTH_TOKEN) artillery run -e dev artillery-load-test.yml\n\nload-test-staging: # Load test the staging environment in aws\n\t$(eval API_AUTH_TOKEN := $(shell aws ssm get-parameter --name /api/staging/api-auth-token --query Parameter.Value --with-decryption --output text | cut -d',' -f1))\n\tenv API_AUTH_TOKEN=$(API_AUTH_TOKEN) artillery run -e staging artillery-load-test.yml\n\nload-test-prod: # Load test the production environment in aws. Please test responsibly\n\t$(eval API_AUTH_TOKEN := $(shell aws ssm get-parameter --name /api/prod/api-auth-token --query Parameter.Value --with-decryption --output text | cut -d',' -f1))\n\tenv API_AUTH_TOKEN=$(API_AUTH_TOKEN) artillery run -e prod artillery-load-test.yml"}
{"path":"api/src/db/migrations/versions/2025_01_16_rename_tables_and_create_job_table.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_01_16_rename_tables_and_create_job_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/README.md\nLanguage: md\nType: code\nDirectory: api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/README.md\nSize: 2.04 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/migrations/versions/2025_01_22_notification_table.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_01_22_notification_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"## Introduction\n\nThis is the API layer. It includes a few separate components:\n\n* The REST API\n* Backend & utility scripts\n\n## Project Directory Structure\n\n```text\nroot\nâ”œâ”€â”€ api\nâ”‚   â””â”€â”€ src\nâ”‚       â””â”€â”€ auth                Authentication code for API\nâ”‚       â””â”€â”€ db\nâ”‚           â””â”€â”€ models          DB model definitions\nâ”‚           â””â”€â”€ migrations      DB migration configs\nâ”‚               â””â”€â”€ versions    The DB migrations\nâ”‚       â””â”€â”€ logging\nâ”‚       â””â”€â”€ route               API route definitions\nâ”‚           â””â”€â”€ handler         API route implementations\nâ”‚       â””â”€â”€ scripts             Backend scripts that run separate from the application\n|       â””â”€â”€ services            Methods for service layer\nâ”‚       â””â”€â”€ util                Utility methods and classes useful to most areas of the code\nâ”‚\nâ”‚   â””â”€â”€ tests\nâ”‚   â””â”€â”€ local.env           Environment variable configuration for local files\nâ”‚   â””â”€â”€ Makefile            Frequently used CLI commands for docker and utilities\nâ”‚   â””â”€â”€ pyproject.toml      Python project configuration file\nâ”‚   â””â”€â”€ setup.cfg           Python config for tools that don't support pyproject.toml yet\nâ”‚   â””â”€â”€ Dockerfile          Docker build file for project\nâ”‚\nâ””â”€â”€ docker-compose.yml  Config file for docker compose tool, used for local development\n```\n\n## Local Development\n\nSee [development.md](../documentation/api/development.md) for installation and development instructions.\n\n## Technical Information\n\n* [API Technical Overview](../documentation/api/technical-overview.md)\n* [Database Management](../documentation/api/database/database-management.md)\n* [Formatting and Linting](../documentation/api/formatting-and-linting.md)\n* [Writing Tests](../documentation/api/writing-tests.md)\n* [Logging configuration](../documentation/api/monitoring-and-observability/logging-configuration.md)\n* [Logging conventions](../documentation/api/monitoring-and-observability/logging-conventions.md)"}
{"path":"api/src/db/migrations/versions/2025_01_24_add_last_notified_at_to_user_saved_.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_01_24_add_last_notified_at_to_user_saved_.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/artillery-load-test.yml\nLanguage: yml\nType: code\nDirectory: api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/artillery-load-test.yml\nSize: 1.37 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/migrations/versions/2025_01_28_add_searched_opportunity_ids_and_last_.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_01_28_add_searched_opportunity_ids_and_last_.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":""}
{"path":"api/src/db/migrations/versions/2025_02_07_remove_opportunity_attachment_type.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_02_07_remove_opportunity_attachment_type.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/bin/__init__.py\nLanguage: py\nType: code\nDirectory: api/bin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/migrations/versions/2025_02_07_remove_opportunity_attachment_type_.py","language":"python","type":"code","directory":"api/src/db/migrations/versions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_02_07_remove_opportunity_attachment_type_.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":""}
{"path":"api/src/db/models/__init__.py","language":"python","type":"code","directory":"api/src/db/models","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/bin/create_erds.py\nLanguage: py\nType: code\nDirectory: api/bin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/create_erds.py\nSize: 1.16 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/agency_models.py","language":"python","type":"code","directory":"api/src/db/models","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/agency_models.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"from eralchemy import render_er\nfrom sqlalchemy import MetaData\n\nimport src.logging\nfrom src.db.models.base import ApiSchemaTable\nfrom src.db.models.staging.staging_base import StagingBase\n\nlogger = logging.getLogger(__name__)\n\n# Construct the path to the folder this file is within\n# This gets an absolute path so that where you run the script from won't matter\n# and should always resolve to the app/erds folder\nERD_FOLDER = pathlib.Path(__file__).parent.resolve()\n\n# If we want to generate separate files for more specific groups, we can set that up here\nSTAGING_BASE_METADATA = StagingBase.metadata\nAPI_BASE_METADATA = ApiSchemaTable.metadata\n\n\ndef create_erds(metadata: MetaData, file_name: str) -> None:\n    logger.info(\"Generating ERD diagrams for %s\", file_name)\n\n    png_file_path = str(ERD_FOLDER / f\"{file_name}.png\")\n    render_er(metadata, png_file_path)\n\n\ndef main() -> None:\n    with src.logging.init(__package__):\n        logger.info(\"Generating ERD diagrams\")\n\n        create_erds(STAGING_BASE_METADATA, \"staging-schema\")\n        create_erds(API_BASE_METADATA, \"api-schema\")"}
{"path":"api/src/db/models/base.py","language":"python","type":"code","directory":"api/src/db/models","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/base.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/bin/execute_sql_rds.sh\nLanguage: sh\nType: code\nDirectory: api/bin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/execute_sql_rds.sh\nSize: 3.20 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/extract_models.py","language":"python","type":"code","directory":"api/src/db/models","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/extract_models.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"set -o errexit -o pipefail\n\nPROGRAM_NAME=$(basename \"$0\")\n\nCYAN='\\033[96m'\nGREEN='\\033[92m'\nRED='\\033[01;31m'\nEND='\\033[0m'\n\nCLUSTER=api-dev\n\nUSAGE=\"Usage: $PROGRAM_NAME [OPTION]\n\n  --multiple         one SQL statement per input line (otherwise expects a single multi-line statement)\n  --cluster=CLUSTER  target RDS cluster (default $CLUSTER)\n\"\n\n\nmain() {\n  cluster=\"$CLUSTER\"\n  parse_arguments \"$@\"\n  print_log \"using cluster $cluster\"\n  read_cluster_arns\n  create_temporary_directory\n\n  # Note that to use jtbl, it needs to be installed directly\n  # by the user with pip - if we wanted it to work with our poetry\n  # setup we'd have to run many of these commands via poetry\n  if ! command -v jtbl 2>&1 >/dev/null\n  then\n    printf \"\\n${RED}jtbl command not found${END} - please install before running: https://github.com/kellyjonbrazil/jtbl \\n\\n\"\n    exit 1\n  fi\n\n  count=1\n  if [ $multiple ]\n  then\n    while read line\n    do\n      execute_statement \"$line\"\n      count=$((count + 1))\n    done\n  else\n    execute_statement \"$(cat)\"\n  fi\n}\n\n\nparse_arguments() {\n  for arg in \"$@\"\n  do\n    if [ \"$arg\" == \"--multiple\" ]; then\n      print_log \"multiple mode enabled (one statement per input line)\"\n      multiple=1\n    elif [[ \"$arg\" =~ ^--cluster=(.*)$ ]]; then\n      cluster=\"${BASH_REMATCH[1]}\"\n    else\n      echo \"$USAGE\"\n      exit 1\n    fi\n  done\n}\n\n\nread_cluster_arns() {\n  resource_arn=$(aws rds describe-db-clusters --db-cluster-identifier=\"$cluster\" \\\n                     --query='DBClusters[0].DBClusterArn' --output=text)\n  secret_arn=$(aws rds describe-db-clusters --db-cluster-identifier=\"$cluster\" \\\n                   --query='DBClusters[0].MasterUserSecret.SecretArn' --output=text)\n  print_log \"database resource $resource_arn\"\n}\n\n\ncreate_temporary_directory() {\n  tmp_dir=\"/tmp/execute_sql_rds/execute_sql_rds.$(date \"+%Y-%m-%d_%H:%M:%S\")\"\n  mkdir -m \"u=rwx,g=,o=\" -p \"$tmp_dir\"\n  print_log \"temporary directory $tmp_dir\"\n}\n\n\nexecute_statement() {\n  print_log \"$1\"\n  result_path=\"$tmp_dir/raw_result_$count.json\"\n  json_result_path=\"$tmp_dir/result_$count.json\"\n  csv_result_path=\"$tmp_dir/result_$count.csv\"\n\n  aws rds-data execute-statement \\\n      --resource-arn \"$resource_arn\" \\\n      --database \"app\" \\\n      --secret-arn \"$secret_arn\" \\\n      --sql \"$1\" \\\n      --continue-after-timeout \\\n      --format-records-as JSON \\\n      >\"$result_path\"\n\n  if grep formattedRecords \"$result_path\" >/dev/null\n  then\n    # Print a pretty table to the user\n    jq -r .formattedRecords \"$result_path\" | jtbl --truncate --markdown\n    # Pull the results out and write to a CSV + JSON\n    jq -r .formattedRecords \"$result_path\" | jtbl --csv > $csv_result_path\n    jq -r .formattedRecords \"$result_path\" > $json_result_path\n    print_log \"----\"\n    print_log \"Output written to $tmp_dir/\"\n  else\n    cat \"$result_path\"\n  fi\n}\n\n\n# Utility functions\nprint_log() {\n  printf \"$CYAN%s $GREEN%s: $END%s\\\\n\" \"$(date \"+%Y-%m-%d %H:%M:%S\")\" \"$PROGRAM_NAME\" \"$*\"\n}\n\n# Entry point\nmain \"$@\""}
{"path":"api/src/db/models/foreign/__init__.py","language":"python","type":"code","directory":"api/src/db/models/foreign","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/bin/setup-env-override-file.sh\nLanguage: sh\nType: code\nDirectory: api/bin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/setup-env-override-file.sh\nSize: 2.76 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/foreign/attachment.py","language":"python","type":"code","directory":"api/src/db/models/foreign","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/attachment.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"set -o errexit -o pipefail\n\nPROGRAM_NAME=$(basename \"$0\")\n\nCYAN='\\033[96m'\nGREEN='\\033[92m'\nRED='\\033[01;31m'\nEND='\\033[0m'\n\nUSAGE=\"Usage: $PROGRAM_NAME [OPTION]\n\n  --recreate         Recreate the override.env file, fully overwriting any existing file\n\"\n\nmain() {\n  print_log \"Running $PROGRAM_NAME\"\n\n  for arg in \"$@\"\n  do\n    if [ \"$arg\" == \"--recreate\" ]; then\n      recreate=1\n    else\n      echo \"$USAGE\"\n      exit 1\n    fi\n  done\n\n  OVERRIDE_FILE=\"override.env\"\n\n  if [ -f \"$OVERRIDE_FILE\" ] ; then\n    if [ $recreate ] ; then\n      print_log \"Recreating existing override.env file\"\n    else\n      print_log \"override.env already exists, not recreating\"\n      exit 0\n    fi\n  fi\n\n  # Delete any key files that may be leftover from a prior run\n  cleanup_files\n\n  # Generate RSA keys\n  # note ssh-keygen generates a different format for\n  # the public key so we run it through openssl to fix it\n  ssh-keygen -t rsa -b 2048 -m PEM -N '' -f tmp_jwk.key 2>&1 >/dev/null\n  openssl rsa -in tmp_jwk.key -pubout -outform PEM -out tmp_jwk.pub\n\n  PUBLIC_KEY=`cat tmp_jwk.pub`\n  PRIVATE_KEY=`cat tmp_jwk.key`\n\n  cat > $OVERRIDE_FILE <<EOF\n# override.env\n#\n# Any environment variables written to this file\n# will take precedence over those defined in local.env\n#\n# This file will not be checked into github and it is safe\n# to store secrets here, however you should still follow caution\n# with using any secrets locally if they cause the app to interact\n# with external systems.\n#\n# This file was generated by running:\n#    make setup-env-override-file\n#\n# Which runs as part of our \"make init\" flow.\n#\n# If you would like to re-generate this file, please run:\n#    make setup-env-override-file args=\"--recreate\"\n#\n# Note that this will completely erase any existing configuration you may have\n\n############################\n# Authentication\n############################\n\nAPI_JWT_PRIVATE_KEY=\"$PRIVATE_KEY\"\n\nAPI_JWT_PUBLIC_KEY=\"$PUBLIC_KEY\"\n\n# The local mock doesn't check the key used\n# for token auth so just re-use the other private key\nLOGIN_GOV_CLIENT_ASSERTION_PRIVATE_KEY=\"$PRIVATE_KEY\"\nEOF\n\n\n  print_log \"Created new override.env\"\n\n  # Cleanup all keys generated in this run\n  cleanup_files\n}\n\n# Cleanup a single file if it exists\ncleanup_file()\n{\n  FILE=$1\n  shift;\n\n  if [ -f \"$FILE\" ] ; then\n    rm \"$FILE\"\n  fi\n}\n\n# Cleanup all miscellaneous keys generated\ncleanup_files()\n{\n  cleanup_file tmp_jwk.key\n  cleanup_file tmp_jwk.pub\n  cleanup_file tmp_jwk.key.pub\n}\n\nprint_log() {\n  printf \"$CYAN%s $GREEN%s: $END%s\\\\n\" \"$(date \"+%Y-%m-%d %H:%M:%S\")\" \"$PROGRAM_NAME\" \"$*\"\n}\n\n# Entry point\nmain \"$@\""}
{"path":"api/src/db/models/foreign/dialect.py","language":"python","type":"code","directory":"api/src/db/models/foreign","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/dialect.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/bin/setup_localstack.py\nLanguage: py\nType: code\nDirectory: api/bin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/setup_localstack.py\nSize: 1.95 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/foreign/forecast.py","language":"python","type":"code","directory":"api/src/db/models/foreign","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/forecast.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"import boto3\nimport botocore.client\nimport botocore.exceptions\n\nimport src.logging\nfrom src.adapters.aws import S3Config, get_s3_client\nfrom src.util import file_util\nfrom src.util.local import error_if_not_local\n\nlogger = logging.getLogger(__name__)\n\n\ndef does_s3_bucket_exist(s3_client: botocore.client.BaseClient, bucket_name: str) -> bool:\n    try:\n        s3_client.head_bucket(Bucket=bucket_name)\n        return True\n    except botocore.exceptions.ClientError as e:\n        # We'll assume that if the error code is a 404 that means\n        # it could not find the bucket and thus it needs to be created\n        # as there are not more specific errors than this available\n        error_code = e.response.get(\"Error\", {}).get(\"Code\")\n        if error_code != \"404\":\n            raise e\n\n    return False\n\n\ndef create_bucket_if_not_exists(s3_client: botocore.client.BaseClient, bucket_name: str) -> None:\n    if not does_s3_bucket_exist(s3_client, bucket_name):\n        logger.info(\"Creating S3 bucket %s\", bucket_name)\n        s3_client.create_bucket(Bucket=bucket_name)\n    else:\n        logger.info(\"S3 bucket %s already exists - skipping creation\", bucket_name)\n\n\ndef setup_s3() -> None:\n    s3_config = S3Config()\n    # This is only used locally - to avoid any accidental running of commands here\n    # against a real AWS account (ie. you've authed in your local terminal where you're running this)\n    # we'll override the access keys explicitly.\n    s3_client = get_s3_client(\n        s3_config, boto3.Session(aws_access_key_id=\"NO_CREDS\", aws_secret_access_key=\"NO_CREDS\")\n    )\n\n    create_bucket_if_not_exists(\n        s3_client, file_util.get_s3_bucket(s3_config.public_files_bucket_path)\n    )\n    create_bucket_if_not_exists(\n        s3_client, file_util.get_s3_bucket(s3_config.draft_files_bucket_path)\n    )\n\n\ndef main() -> None:\n    with src.logging.init(\"setup_localstack\"):\n        error_if_not_local()\n        setup_s3()\n\n\nif __name__ == \"__main__\":\n    main()"}
{"path":"api/src/db/models/foreign/foreignbase.py","language":"python","type":"code","directory":"api/src/db/models/foreign","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/foreignbase.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/bin/sql/select_from_foreign_table.sql\nLanguage: sql\nType: code\nDirectory: api/bin/sql\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/sql/select_from_foreign_table.sql\nSize: 1.37 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/foreign/opportunity.py","language":"python","type":"code","directory":"api/src/db/models/foreign","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/opportunity.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":""}
{"path":"api/src/db/models/foreign/synopsis.py","language":"python","type":"code","directory":"api/src/db/models/foreign","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/synopsis.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/bin/sql/table_list.sql\nLanguage: sql\nType: code\nDirectory: api/bin/sql\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/sql/table_list.sql\nSize: 0.88 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/foreign/tgroups.py","language":"python","type":"code","directory":"api/src/db/models/foreign","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/tgroups.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":""}
{"path":"api/src/db/models/legacy_mixin/__init__.py","language":"python","type":"code","directory":"api/src/db/models/legacy_mixin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/legacy_mixin/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/bin/wait-for-api.sh\nLanguage: sh\nType: code\nDirectory: api/bin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/wait-for-api.sh\nSize: 0.67 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/legacy_mixin/forecast_mixin.py","language":"python","type":"code","directory":"api/src/db/models/legacy_mixin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/legacy_mixin/forecast_mixin.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"set -e\n\n# Color formatting for readability\nGREEN='\\033[0;32m'\nRED='\\033[0;31m'\nNO_COLOR='\\033[0m'\n\nMAX_WAIT_TIME=800 # seconds, adjust as necessary\nWAIT_TIME=0\n\necho \"Waiting for API server to become ready...\"\n\n# Use curl to check the API server health endpoint\nuntil curl --output /dev/null --silent --head --fail http://localhost:8080/health;\ndo\n  printf '.'\n  sleep 5\n\n  WAIT_TIME=$(($WAIT_TIME + 5))\n  if [ $WAIT_TIME -gt $MAX_WAIT_TIME ]\n  then\n    echo -e \"${RED}ERROR: API server did not become ready within ${MAX_WAIT_TIME} seconds.${NO_COLOR}\"\n    exit 1\n  fi\ndone\n\necho -e \"${GREEN}API server is ready after ~${WAIT_TIME} seconds.${NO_COLOR}\""}
{"path":"api/src/db/models/legacy_mixin/opportunity_mixin.py","language":"python","type":"code","directory":"api/src/db/models/legacy_mixin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/legacy_mixin/opportunity_mixin.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/bin/wait-for-local-db.sh\nLanguage: sh\nType: code\nDirectory: api/bin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/wait-for-local-db.sh\nSize: 0.74 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/legacy_mixin/synopsis_mixin.py","language":"python","type":"code","directory":"api/src/db/models/legacy_mixin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/legacy_mixin/synopsis_mixin.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"set -e\n\n# Color formatting\nRED='\\033[0;31m'\nNO_COLOR='\\033[0m'\n\nMAX_WAIT_TIME=30 # seconds\nWAIT_TIME=0\n\n# Use pg_isready to wait for the DB to be ready to accept connections\n# We check every 3 seconds and consider it failed if it gets to 30+\n# https://www.postgresql.org/docs/current/app-pg-isready.html\nuntil pg_isready -h localhost -d grants-db -q;\ndo\n  echo \"waiting on Postgres DB to initialize...\"\n  sleep 3\n\n  WAIT_TIME=$(($WAIT_TIME+3))\n  if [ $WAIT_TIME -gt $MAX_WAIT_TIME ]\n  then\n    echo -e \"${RED}ERROR: Database appears to not be starting up, running \\\"docker logs grants-db\\\" to troubleshoot.${NO_COLOR}\"\n    docker logs grants-db\n    exit 1\n  fi\ndone\n\necho \"Postgres DB is ready after ~${WAIT_TIME} seconds\""}
{"path":"api/src/db/models/legacy_mixin/tgroups_mixin.py","language":"python","type":"code","directory":"api/src/db/models/legacy_mixin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/legacy_mixin/tgroups_mixin.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/bin/wait-for-local-opensearch.sh\nLanguage: sh\nType: code\nDirectory: api/bin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/bin/wait-for-local-opensearch.sh\nSize: 0.69 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/lookup/__init__.py","language":"python","type":"code","directory":"api/src/db/models/lookup","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/lookup/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"set -e\n\n# Color formatting\nRED='\\033[0;31m'\nNO_COLOR='\\033[0m'\n\nMAX_WAIT_TIME=30 # seconds\nWAIT_TIME=0\n\n# Curl the healthcheck endpoint of the local opensearch\n# until it returns a success response\nuntil curl --output /dev/null --silent http://localhost:9200/_cluster/health;\ndo\n  echo \"waiting on OpenSearch to initialize...\"\n  sleep 3\n\n  WAIT_TIME=$(($WAIT_TIME+3))\n  if [ $WAIT_TIME -gt $MAX_WAIT_TIME ]\n  then\n    echo -e \"${RED}ERROR: OpenSearch appears to not be starting up, running \\\"docker logs opensearch-node\\\" to troubleshoot.${NO_COLOR}\"\n    docker logs opensearch-node\n    exit 1\n  fi\ndone\n\necho \"OpenSearch is ready after ~${WAIT_TIME} seconds\""}
{"path":"api/src/db/models/lookup/lookup.py","language":"python","type":"code","directory":"api/src/db/models/lookup","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/lookup/lookup.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/docker-compose.debug.yml\nLanguage: yml\nType: code\nDirectory: api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/docker-compose.debug.yml\nSize: 0.41 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/lookup/lookup_registry.py","language":"python","type":"code","directory":"api/src/db/models/lookup","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/lookup/lookup_registry.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":""}
{"path":"api/src/db/models/lookup/lookup_table.py","language":"python","type":"code","directory":"api/src/db/models/lookup","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/lookup/lookup_table.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/docker-compose.yml\nLanguage: yml\nType: code\nDirectory: api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/docker-compose.yml\nSize: 3.85 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/lookup/sync_lookup_values.py","language":"python","type":"code","directory":"api/src/db/models/lookup","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/lookup/sync_lookup_values.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"opensearch-node:\n    build:\n      context: .\n      dockerfile: opensearch/Dockerfile\n    container_name: opensearch-node\n    environment:\n      - cluster.name=opensearch-cluster # Name the cluster\n      - node.name=opensearch-node # Name the node that will run in this container\n      - discovery.type=single-node # Nodes to look for when discovering the cluster\n      - bootstrap.memory_lock=true # Disable JVM heap memory swapping\n      - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m\" # Set min and max JVM heap sizes to at least 50% of system RAM\n      - DISABLE_INSTALL_DEMO_CONFIG=true # Prevents execution of bundled demo script which installs demo certificates and security configurations to OpenSearch\n      - DISABLE_SECURITY_PLUGIN=true # Disables Security plugin\n    ulimits:\n      memlock:\n        soft: -1 # Set memlock to unlimited (no soft or hard limit)\n        hard: -1\n      nofile:\n        soft: 65536 # Maximum number of open files for the opensearch user - set to at least 65536\n        hard: 65536\n    volumes:\n      - opensearch-data:/usr/share/opensearch/data # Creates volume called opensearch-data and mounts it to the container\n    ports:\n      - 9200:9200 # REST API\n      - 9600:9600 # Performance Analyzer\n    networks:\n      - default\n\n  opensearch-dashboards:\n    image: opensearchproject/opensearch-dashboards:latest\n    container_name: opensearch-dashboards\n    ports:\n      - 5601:5601 # Map host port 5601 to container port 5601\n    expose:\n      - \"5601\" # Expose port 5601 for web access to OpenSearch Dashboards\n    environment:\n      - 'OPENSEARCH_HOSTS=[\"http://opensearch-node:9200\"]'\n      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true # disables security dashboards plugin in OpenSearch Dashboards\n    networks:\n      - default\n\n  localstack:\n    container_name: \"${LOCALSTACK_DOCKER_NAME:-localstack-main}\"\n    image: localstack/localstack\n    ports:\n      - \"127.0.0.1:4566:4566\" # LocalStack Gateway\n      - \"127.0.0.1:4510-4559:4510-4559\" # external services port range\n    environment:\n      # LocalStack configuration: https://docs.localstack.cloud/references/configuration/\n      - DEBUG=${DEBUG:-0}\n      # To improve startup time, only add services we use\n      - SERVICES=s3\n      - EAGER_SERVICES_LOADING=1\n    volumes:\n      - \"${LOCALSTACK_VOLUME_DIR:-./volume}:/var/lib/localstack\"\n      - \"/var/run/docker.sock:/var/run/docker.sock\"\n    networks:\n      - default\n\n  grants-api:\n    build:\n      context: .\n      target: dev\n      args:\n        - RUN_UID=${RUN_UID:-4000}\n        - RUN_USER=${RUN_USER:-api}\n    command:\n      [\n        \"poetry\",\n        \"run\",\n        \"flask\",\n        \"--app\",\n        \"src.app\",\n        \"run\",\n        \"--host\",\n        \"0.0.0.0\",\n        \"--port\",\n        \"8080\",\n        \"--reload\",\n      ]\n    container_name: grants-api\n    env_file:\n      - path: ./local.env\n        required: true\n      - path: ./override.env\n        required: false\n    ports:\n      - 8080:8080\n    volumes:\n      - .:/api\n    depends_on:\n      - grants-db\n      - opensearch-node\n      - localstack\n      - mock-oauth2-server\n    networks:\n      - grants_backend\n      - default\n\n  mock-oauth2-server:\n    image: ghcr.io/navikt/mock-oauth2-server:2.1.10\n    ports:\n      - \"5001:5001\"\n    environment:\n      LOG_LEVEL: \"debug\"\n      SERVER_PORT: 5001\n      JSON_CONFIG_PATH: /app/config.json\n    networks:\n      - grants_backend\n      - default\n    volumes:\n      - ./mock-oauth/config.json:/app/config.json\n      - ./mock-oauth/mock-server-key.json:/app/resources/mock-oauth2-server-keys.json\n\nvolumes:\n  grantsdbdata:\n  opensearch-data:\n\nnetworks:\n  grants_backend:\n    driver: bridge"}
{"path":"api/src/db/models/lookup_models.py","language":"python","type":"code","directory":"api/src/db/models","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/lookup_models.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/gunicorn.conf.py\nLanguage: py\nType: code\nDirectory: api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/gunicorn.conf.py\nSize: 1.00 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/opportunity_models.py","language":"python","type":"code","directory":"api/src/db/models","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/opportunity_models.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"Attributes:\n    bind(str): The socket to bind. Formatted as '0.0.0.0:$PORT'.\n    workers(int): The number of worker processes for handling requests.\n    threads(int): The number of threads per worker for handling requests.\n\nFor more information, see https://docs.gunicorn.org/en/stable/configure.html\n\"\"\"\n\nimport os\n\nfrom src.app_config import AppConfig\n\napp_config = AppConfig()\n\nbind = app_config.host + ':' + str(app_config.port)\n# Calculates the number of usable cores and doubles it. Recommended number of workers per core is two.\n# https://docs.gunicorn.org/en/latest/design.html#how-many-workers\n# We use 'os.sched_getaffinity(pid)' not 'os.cpu_count()' because it returns only allowable CPUs.\n# os.sched_getaffinity(pid): Return the set of CPUs the process with PID pid is restricted to.\n# os.cpu_count(): Return the number of CPUs in the system.\n\nworkers = (len(os.sched_getaffinity(0)) * 2) + 1\nthreads = 4"}
{"path":"api/src/db/models/staging/__init__.py","language":"python","type":"code","directory":"api/src/db/models/staging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"File: api/local.env\nLanguage: env\nType: code\nDirectory: api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/local.env\nSize: 4.12 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/staging/attachment.py","language":"python","type":"code","directory":"api/src/db/models/staging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/attachment.py","size":1631931,"lastModified":"2025-02-14T17:08:31.125Z","content":"ENVIRONMENT=local\nPORT=8080\n\nPERSIST_AUTHORIZATION_OPENAPI=TRUE\n\n# Python path needs to be specified\n# for pytest to find the implementation code\nPYTHONPATH=/api/\n\n# PY_RUN_APPROACH=python OR docker\n# Set this in your environment\n# to modify how the Makefile runs\n# commands that can run in or out\n# of the Docker container - defaults to outside\n\nFLASK_APP=src.app:create_app\n\n############################\n# Logging\n############################\n\n# Can be \"human-readable\" OR \"json\"\nLOG_FORMAT=human-readable\n\n# Set log level. Valid values are DEBUG, INFO, WARNING, CRITICAL\n# LOG_LEVEL=INFO\n\n# Enable/disable audit logging. Valid values are TRUE, FALSE\nLOG_ENABLE_AUDIT=FALSE\n\n# Change the message length for the human readable formatter\n# LOG_HUMAN_READABLE_FORMATTER__MESSAGE_WIDTH=50\n\nLOG_LEVEL_OVERRIDES=smart_open.s3=ERROR\n\n############################\n# Authentication\n############################\n# The auth token used by the local endpoints\nAPI_AUTH_TOKEN=LOCAL_AUTH_12345678,LOCAL_AUTH_87654321,LOCAL_1234\n\nLOGIN_GOV_CLIENT_ID=local_mock_client_id\n\nLOGIN_GOV_JWK_ENDPOINT=http://host.docker.internal:5001/issuer1/jwks\nLOGIN_GOV_AUTH_ENDPOINT=http://localhost:5001/issuer1/authorize\nLOGIN_GOV_TOKEN_ENDPOINT=http://host.docker.internal:5001/issuer1/token\nLOGIN_GOV_ENDPOINT=http://host.docker.internal:5001/issuer1\n\nLOGIN_FINAL_DESTINATION=http://localhost:8080/v1/users/login/result\n\n# These should be set in your override.env file\n# which can be created by running `make setup-env-override-file`\nAPI_JWT_PRIVATE_KEY=\nAPI_JWT_PUBLIC_KEY=\nLOGIN_GOV_CLIENT_ASSERTION_PRIVATE_KEY=\n\nENABLE_AUTH_ENDPOINT=TRUE\n\n############################\n# DB Environment Variables\n############################\n\n# These are used by the Postgres image to create the admin user\nPOSTGRES_USER=app\nPOSTGRES_PASSWORD=secret123\n\n# Set DB_HOST to localhost if accessing a non-dockerized database\nDB_HOST=grants-db\nDB_NAME=app\nDB_USER=app\nDB_PASSWORD=secret123\nDB_SSL_MODE=allow\n\n# When an error occurs with a SQL query,\n# whether or not to hide the parameters which\n# could contain sensitive information.\nHIDE_SQL_PARAMETER_LOGS=TRUE\n\n############################\n# Opensearch Environment Variables\n############################\n\nSEARCH_ENDPOINT=opensearch-node\nSEARCH_PORT=9200\nSEARCH_USE_SSL=FALSE\nSEARCH_VERIFY_CERTS=FALSE\n\n############################\n# Localstack\n############################\n\n# If you want to connect to localstack outside of docker\n# use localhost:4566 instead\nS3_ENDPOINT_URL=http://localstack:4566\n\n############################\n# S3\n############################\n\n# Our terraform sets these as s3 paths, so include s3:// on the bucket name\nPUBLIC_FILES_BUCKET=s3://local-mock-public-bucket\nDRAFT_FILES_BUCKET=s3://local-mock-draft-bucket\n\n# This env var is used to set local AWS credentials\nIS_LOCAL_AWS=1\n\n############################\n# Feature Flags\n############################\nENABLE_OPPORTUNITY_LOG_MSG=false\nENABLE_OPPORTUNITY_ATTACHMENT_PIPELINE=true\n\n############################\n# Endpoint Configuration\n############################\n# Nothing needs to be configured at the moment\n\n############################\n# Script Configuration\n############################\n\n# For the script to setup the foreign data tables\n# this env var overrides it so the script generates normal\n# tables that don't need to connect to an Oracle database\nIS_LOCAL_FOREIGN_TABLE=true\n\n############################\n# Task Configuration\n############################\n\n# File path for the export_opportunity_data task\nPUBLIC_FILES_OPPORTUNITY_DATA_EXTRACTS_PATH=/tmp\n\n# File path for the create-analytics-db-csvs task\nAPI_ANALYTICS_DB_EXTRACTS_PATH=/tmp\n\n############################\n# Deploy Metadata\n############################\n\n# These params are set/updated when we deploy the API\n# and are used to add metadata info in various places\n# For local development, just define static values\n\nDEPLOY_GITHUB_REF=main\nDEPLOY_GITHUB_SHA=ffaca647223e0b6e54344122eefa73401f5ec131\nDEPLOY_TIMESTAMP=2024-12-02T21:25:18Z\nDEPLOY_WHOAMI=local-developer\n\nCDN_URL=http://localhost:4566/local-mock-public-bucket"}
{"path":"api/src/db/models/staging/forecast.py","language":"python","type":"code","directory":"api/src/db/models/staging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/forecast.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/mock-oauth/config.json\nLanguage: json\nType: code\nDirectory: api/mock-oauth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/mock-oauth/config.json\nSize: 0.44 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/staging/opportunity.py","language":"python","type":"code","directory":"api/src/db/models/staging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/opportunity.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":""}
{"path":"api/src/db/models/staging/staging_base.py","language":"python","type":"code","directory":"api/src/db/models/staging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/staging_base.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/mock-oauth/mock-server-key.json\nLanguage: json\nType: code\nDirectory: api/mock-oauth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/mock-oauth/mock-server-key.json\nSize: 1.75 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/staging/synopsis.py","language":"python","type":"code","directory":"api/src/db/models/staging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/synopsis.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":""}
{"path":"api/src/db/models/staging/tgroups.py","language":"python","type":"code","directory":"api/src/db/models/staging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/tgroups.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/newrelic.ini\nLanguage: ini\nType: code\nDirectory: api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/newrelic.ini\nSize: 12.11 KB\nLast Modified: 2025-02-14T17:08:26.431Z"}
{"path":"api/src/db/models/task_models.py","language":"python","type":"code","directory":"api/src/db/models","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/task_models.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"#\n# This file configures the New Relic Python Agent.\n#\n# The path to the configuration file should be supplied to the function\n# newrelic.agent.initialize() when the agent is being initialized.\n#\n# The configuration file follows a structure similar to what you would\n# find for Microsoft Windows INI files. For further information on the\n# configuration file format see the Python ConfigParser documentation at:\n#\n#    http://docs.python.org/library/configparser.html\n#\n# For further discussion on the behaviour of the Python agent that can\n# be configured via this configuration file see:\n#\n#    https://docs.newrelic.com/docs/apm/agents/python-agent/configuration/python-agent-configuration/\n#\n\n# ---------------------------------------------------------------------------\n\n# Here are the settings that are common to all environments.\n\n[newrelic]\n\n# You must specify the license key associated with your New\n# Relic account. This key binds the Python Agent's data to your\n# account in the New Relic service. For more information on\n# storing and generating license keys, see\n# https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/#ingest-license-key\n# license_key = # Supplied through NEW_RELIC_LICENSE_KEY by AWS SSM.\n\n# The application name. Set this to be the name of your\n# application as you would like it to show up in New Relic UI.\n# The UI will then auto-map instances of your application into a\n# entry on your home dashboard page. You can also specify multiple\n# app names to group your aggregated data. For further details,\n# please see:\n# https://docs.newrelic.com/docs/apm/agents/manage-apm-agents/app-naming/use-multiple-names-app/\n# app_name = # Parameterized by environment at the end of this file.\n\n# When \"true\", the agent collects performance data about your\n# application and reports this data to the New Relic UI at\n# newrelic.com. This global switch is normally overridden for\n# each environment below.\nmonitor_mode = false\n\n# Sets the name of a file to log agent messages to. Whatever you\n# set this to, you must ensure that the permissions for the\n# containing directory and the file itself are correct, and\n# that the user that your web application runs as can write out\n# to the file. If not able to out a log file, it is also\n# possible to say \"stderr\" and output to standard error output.\n# This would normally result in output appearing in your web\n# server log.\n# TODO: Figure out where (in CloudWatch or Splunk) New Relic agent messages may need to be sent.\nlog_file = stderr\n\n# Sets the level of detail of messages sent to the log file, if\n# a log file location has been provided. Possible values, in\n# increasing order of detail, are: \"critical\", \"error\", \"warning\",\n# \"info\" and \"debug\". When reporting any agent issues to New\n# Relic technical support, the most useful setting for the\n# support engineers is \"debug\". However, this can generate a lot\n# of information very quickly, so it is best not to keep the\n# agent at this level for longer than it takes to reproduce the\n# problem you are experiencing.\nlog_level = info\n\n# High Security Mode enforces certain security settings, and prevents\n# them from being overridden, so that no sensitive data is sent to New\n# Relic. Enabling High Security Mode means that request parameters are\n# not collected and SQL can not be sent to New Relic in its raw form.\n# To activate High Security Mode, it must be set to 'true' in this\n# local .ini configuration file AND be set to 'true' in the\n# server-side configuration in the New Relic user interface. For\n# details, see\n# https://docs.newrelic.com/docs/subscriptions/high-security\nhigh_security = false\n\n# The Python Agent will attempt to connect directly to the New\n# Relic service. If there is an intermediate firewall between\n# your host and the New Relic service that requires you to use a\n# HTTP proxy, then you should set both the \"proxy_host\" and\n# \"proxy_port\" settings to the required values for the HTTP\n# proxy. The \"proxy_user\" and \"proxy_pass\" settings should\n# additionally be set if proxy authentication is implemented by\n# the HTTP proxy. The \"proxy_scheme\" setting dictates what\n# protocol scheme is used in talking to the HTTP proxy. This\n# would normally always be set as \"http\" which will result in the\n# agent then using a SSL tunnel through the HTTP proxy for end to\n# end encryption.\n# proxy_scheme = http\n# proxy_host = hostname\n# proxy_port = 8080\n# proxy_user =\n# proxy_pass =\n\n# Capturing request parameters is off by default. To enable the\n# capturing of request parameters, first ensure that the setting\n# \"attributes.enabled\" is set to \"true\" (the default value), and\n# then add \"request.parameters.*\" to the \"attributes.include\"\n# setting. For details about attributes configuration, please\n# consult the documentation.\n# TODO: Figure out if add'l attrs will be important to capture (e.g. in events or transaction traces) later on.\n# attributes.enabled = true\n# attributes.include = request.parameters.*\n\n# The transaction tracer captures deep information about slow\n# transactions and sends this to the UI on a periodic basis. The\n# transaction tracer is enabled by default. Set this to \"false\"\n# to turn it off.\ntransaction_tracer.enabled = true\n\n# Threshold in seconds for when to collect a transaction trace.\n# When the response time of a controller action exceeds this\n# threshold, a transaction trace will be recorded and sent to\n# the UI. Valid values are any positive float value, or (default)\n# \"apdex_f\", which will use the threshold for a dissatisfying\n# Apdex controller action - four times the Apdex T value.\ntransaction_tracer.transaction_threshold = apdex_f\n\n# When the transaction tracer is on, SQL statements can\n# optionally be recorded. The recorder has three modes, \"off\"\n# which sends no SQL, \"raw\" which sends the SQL statement in its\n# original form, and \"obfuscated\", which strips out numeric and\n# string literals.\ntransaction_tracer.record_sql = obfuscated\n\n# Threshold in seconds for when to collect stack trace for a SQL\n# call. In other words, when SQL statements exceed this\n# threshold, then capture and send to the UI the current stack\n# trace. This is helpful for pinpointing where long SQL calls\n# originate from in an application.\ntransaction_tracer.stack_trace_threshold = 0.5\n\n# Determines whether the agent will capture query plans for slow\n# SQL queries. Only supported in MySQL and PostgreSQL. Set this\n# to \"false\" to turn it off.\ntransaction_tracer.explain_enabled = true\n\n# Threshold for query execution time below which query plans\n# will not not be captured. Relevant only when \"explain_enabled\"\n# is true.\ntransaction_tracer.explain_threshold = 0.5\n\n# Space separated list of function or method names in form\n# 'module:function' or 'module:class.function' for which\n# additional function timing instrumentation will be added.\ntransaction_tracer.function_trace =\n\n# The error collector captures information about uncaught\n# exceptions or logged exceptions and sends them to UI for\n# viewing. The error collector is enabled by default. Set this\n# to \"false\" to turn it off. For more details on errors, see\n# https://docs.newrelic.com/docs/apm/agents/manage-apm-agents/agent-data/manage-errors-apm-collect-ignore-or-mark-expected/\nerror_collector.enabled = true\n\n# To stop specific errors from reporting to the UI, set this to\n# a space separated list of the Python exception type names to\n# ignore. The exception name should be of the form 'module:class'.\n#\n# Explicitly not on the list for now:\n# - pydantic.error_wrappers:ValidationError (These seem like real coding issues)\n#\n# Note that most of these except for UnsupportedMediaTypeProblem are 400 responses.\n#\nerror_collector.ignore_classes =\n\n# Expected errors are reported to the UI but will not affect the\n# Apdex or error rate. To mark specific errors as expected, set this\n# to a space separated list of the Python exception type names to\n# expected. The exception name should be of the form 'module:class'.\nerror_collector.expected_classes =\n\n# Status codes ignored by default: 100-102 200-208 226 300-308 404\n\n# Addtional status codes to ignore reporting:\n# 401: Unauthorized - Invalid or missing JWT\n# 402: Payment Required - Employer does not have withholding data\n# 403: Forbidden - User does not have access to endpoint or resource\n# 405: Method Not Allowed\n# 406: Not Acceptable - Invalid Accept header (API does not support it)\n# 415: Unsupported Media Type - Invalid media types for upload\n# 503: Service Unavailable - Temporary service unavailability. High volume should trigger a New Relic alarm.\n#\n# Status codes that we may want to ignore in the future:\n#\n# - 400: Bad Request - Validation and extra parameter errors. Unclear if we want to catch pydantic ValidationErrors here so we're keeping them for now. Instead, selectively ignore specific error classes above.\n# - 422: Unprocessable Entity - Haven't seen these yet so we'll report them.\n# - 504: Gateway Timeout - We do not expect any 504s to be thrown from the API server itself. This should come from the API Gateway that sits in front of it.\n#\nerror_collector.ignore_status_codes = 401 402 403 404 405 406 415 503\n\n# Browser monitoring is the Real User Monitoring feature of the UI.\n# For those Python web frameworks that are supported, this\n# setting enables the auto-insertion of the browser monitoring\n# JavaScript fragments.\nbrowser_monitoring.auto_instrument = false\n\n# A thread profiling session can be scheduled via the UI when\n# this option is enabled. The thread profiler will periodically\n# capture a snapshot of the call stack for each active thread in\n# the application to construct a statistically representative\n# call tree. For more details on the thread profiler tool, see\n# https://docs.newrelic.com/docs/apm/apm-ui-pages/events/thread-profiler-tool/\nthread_profiler.enabled = true\n\n# Your application deployments can be recorded through the\n# New Relic REST API. To use this feature provide your API key\n# below then use the `newrelic-admin record-deploy` command.\n# api_key =\n\n# Distributed tracing lets you see the path that a request takes\n# through your distributed system. For more information, please\n# consult our distributed tracing planning guide.\n# https://docs.newrelic.com/docs/transition-guide-distributed-tracing\ndistributed_tracing.enabled = true\n\n# When storing errors in database, distributed tracing solution captures the database query\n# and sends the full, unscrubbed message to New Relic. This enablement will ensure that\n# no PII data is captured in messages of new relic.\nstrip_exception_messages.enabled = true\n\n# Tell New Relic to include the full JSON / extra info from our logs\n# https://docs.newrelic.com/docs/apm/agents/python-agent/configuration/python-agent-configuration/#application_logging.forwarding.context_data.enabled\napplication_logging.forwarding.context_data.enabled = true\n\n# ---------------------------------------------------------------------------\n\n#\n# The application environments. These are specific settings which\n# override the common environment settings. The settings related to a\n# specific environment will be used when the environment argument to the\n# newrelic.agent.initialize() function has been defined to be either\n# \"local\", \"stage\", \"performance\", \"training\", \"prod\", or \"uat\".\n#\n\n[newrelic:local]\n# Don't turn on data reporting by default when running the API locally.\n#\n# To enable New Relic locally, set the following variables:\n# - developer mode: false\n# - monitor_mode: true\n# - license_key: retrieved from New Relic here: https://one.newrelic.com/launcher/api-keys-ui.launcher\n#\n# NOTE: DO NOT COMMIT THE LICENSE KEY IN A GIT COMMIT.\n#\n# Less scary note: do not wrap your license key in quotes, it should look like this:\n#   license_key=1234abcd\n#\napp_name = SIMPLER-GRANTS-API-LOCAL\ndeveloper_mode = true\nmonitor_mode = false\nlicense_key=replace_me\n\napplication_logging.enabled = false\napplication_logging.forwarding.enabled = false\napplication_logging.local_decorating.enabled = false\n\n[newrelic:staging]\napp_name = SIMPLER-GRANTS-API-STAGING\nmonitor_mode = true\n\n[newrelic:prod]\napp_name = SIMPLER-GRANTS-API-PROD\nmonitor_mode = true\n\n[newrelic:dev]\napp_name = SIMPLER-GRANTS-API-DEV\nmonitor_mode = true\n\n# ---------------------------------------------------------------------------"}
{"path":"api/src/db/models/user_models.py","language":"python","type":"code","directory":"api/src/db/models","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/user_models.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/openapi.generated.yml\nLanguage: yml\nType: code\nDirectory: api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/openapi.generated.yml\nSize: 67.01 KB\nLast Modified: 2025-02-14T17:08:26.432Z"}
{"path":"api/src/logging/__init__.py","language":"python","type":"code","directory":"api/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"Back end API for simpler.grants.gov.\n\n\n    This API is an ALPHA VERSION! Its current form is primarily for testing and feedback.\n    Features are still under heavy development, and subject to change. Not for production\n    use.\n\n\n    See [Release Phases](https://github.com/github/roadmap?tab=readme-ov-file#release-phases)\n    for further details.\n\n    '\n  contact:\n    name: Simpler Grants.gov\n    url: https://simpler.grants.gov/\n    email: simpler@grants.gov\n  title: Simpler Grants API\n  version: v0\ntags:\n- name: Health\n- name: Opportunity v1\n- name: Extract v1\n- name: Agency v1\n- name: User v1\nservers: .\npaths:\n  /health:\n    get:\n      parameters: []\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/HealthcheckResponse'\n          description: Successful response\n        '503':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Service Unavailable\n      tags:\n      - Health\n      summary: Health\n  /v1/extracts:\n    post:\n      parameters: []\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ExtractMetadataListResponse'\n          description: Successful response\n        '422':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Validation error\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n      tags:\n      - Extract v1\n      summary: Extract Metadata Get\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/ExtractMetadataRequest'\n            examples:\n              example1:\n                summary: No filters\n                value:\n                  pagination:\n                    sort_order:\n                    - order_by: created_at\n                      sort_direction: descending\n                    page_offset: 1\n                    page_size: 25\n      security:\n      - ApiKeyAuth: []\n  /v1/agencies:\n    post:\n      parameters: []\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/AgencyListResponse'\n          description: Successful response\n        '422':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Validation error\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n      tags:\n      - Agency v1\n      summary: Agencies Get\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/AgencyListRequest'\n            examples:\n              example1:\n                summary: No filters\n                value:\n                  pagination:\n                    sort_order:\n                    - order_by: created_at\n                      sort_direction: descending\n                    page_offset: 1\n                    page_size: 25\n      security:\n      - ApiKeyAuth: []\n  /v1/users/login:\n    get:\n      parameters: []\n      responses:\n        '302':\n          content:\n            application/json:\n              schema: {}\n          description: Found\n      tags:\n      - User v1\n      summary: User Login\n      description: '\n\n        To use this endpoint, click [this link](/v1/users/login) which will redirect\n\n        you to an OAuth provider where you can sign into an account.\n\n\n        Do not try to use the execute option below as OpenAPI will not redirect your\n        browser for you.\n\n\n        The token you receive can then be set to the X-SGG-Token header for authenticating\n        with endpoints.\n\n        '\n  /v1/users/token/logout:\n    post:\n      parameters: []\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserTokenLogoutResponse'\n          description: Successful response\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n      tags:\n      - User v1\n      summary: User Token Logout\n      security:\n      - ApiJwtAuth: []\n  /v1/users/token/refresh:\n    post:\n      parameters: []\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserTokenRefreshResponse'\n          description: Successful response\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n      tags:\n      - User v1\n      summary: User Token Refresh\n      security:\n      - ApiJwtAuth: []\n  /v1/opportunities/search:\n    post:\n      parameters: []\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/OpportunitySearchResponseV1'\n            application/octet-stream: {}\n          description: Successful response\n        '422':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Validation error\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n      tags:\n      - Opportunity v1\n      summary: Opportunity Search\n      description: '\n\n        __ALPHA VERSION__\n\n\n        This endpoint in its current form is primarily for testing and feedback.\n\n\n        Features in this endpoint are still under heavy development, and subject to\n        change. Not for production use.\n\n\n        See [Release Phases](https://github.com/github/roadmap?tab=readme-ov-file#release-phases)\n        for further details.\n\n        '\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/OpportunitySearchRequestV1'\n            examples:\n              example1:\n                summary: No filters\n                value:\n                  pagination:\n                    sort_order:\n                    - order_by: opportunity_id\n                      sort_direction: ascending\n                    page_offset: 1\n                    page_size: 25\n              example2:\n                summary: All filters\n                value:\n                  query: research\n                  filters:\n                    agency:\n                      one_of:\n                      - USAID\n                      - DOC\n                    applicant_type:\n                      one_of:\n                      - state_governments\n                      - county_governments\n                      - individuals\n                    funding_category:\n                      one_of:\n                      - recovery_act\n                      - arts\n                      - natural_resources\n                    funding_instrument:\n                      one_of:\n                      - cooperative_agreement\n                      - grant\n                    opportunity_status:\n                      one_of:\n                      - forecasted\n                      - posted\n                    post_date:\n                      start_date: '2024-01-01'\n                      end_date: '2024-02-01'\n                    close_date:\n                      start_date: '2024-01-01'\n                  pagination:\n                    sort_order:\n                    - order_by: opportunity_id\n                      sort_direction: ascending\n                    page_offset: 1\n                    page_size: 25\n              example3:\n                summary: Query & opportunity_status filters\n                value:\n                  query: research\n                  filters:\n                    opportunity_status:\n                      one_of:\n                      - forecasted\n                      - posted\n                  pagination:\n                    sort_order:\n                    - order_by: opportunity_id\n                      sort_direction: ascending\n                    page_offset: 1\n                    page_size: 25\n              example4:\n                summary: CSV file response\n                value:\n                  format: csv\n                  filters:\n                    opportunity_status:\n                      one_of:\n                      - forecasted\n                      - posted\n                  pagination:\n                    sort_order:\n                    - order_by: opportunity_id\n                      sort_direction: ascending\n                    page_offset: 1\n                    page_size: 100\n              example5:\n                summary: Filter by award fields\n                value:\n                  filters:\n                    expected_number_of_awards:\n                      min: 5\n                    award_floor:\n                      min: 10000\n                    award_ceiling:\n                      max: 1000000\n                    estimated_total_program_funding:\n                      min: 100000\n                      max: 250000\n                  pagination:\n                    sort_order:\n                    - order_by: opportunity_id\n                      sort_direction: ascending\n                    page_offset: 1\n                    page_size: 25\n              example6:\n                summary: Filter by assistance listing numbers\n                value:\n                  filters:\n                    assistance_listing_number:\n                      one_of:\n                      - '43.001'\n                      - '47.049'\n                  pagination:\n                    sort_order:\n                    - order_by: opportunity_id\n                      sort_direction: ascending\n                    page_offset: 1\n                    page_size: 25\n              example7:\n                summary: Primary sort agency_code desc, secondary sort opportunity_id\n                  asc\n                value:\n                  pagination:\n                    page_offset: 1\n                    page_size: 25\n                    sort_order:\n                    - order_by: agency_code\n                      sort_direction: descending\n                    - order_by: opportunity_id\n                      sort_direction: ascending\n      security:\n      - ApiKeyAuth: []\n  /v1/users/{user_id}:\n    get:\n      parameters:\n      - in: path\n        name: user_id\n        schema:\n          type: string\n        required: true\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserGetResponse'\n          description: Successful response\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n        '404':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Not found\n      tags:\n      - User v1\n      summary: User Get\n      security:\n      - ApiJwtAuth: []\n  /v1/opportunities/{opportunity_id}:\n    get:\n      parameters:\n      - in: path\n        name: opportunity_id\n        schema:\n          type: integer\n        required: true\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/OpportunityGetResponseV1'\n          description: Successful response\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n        '404':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Not found\n      tags:\n      - Opportunity v1\n      summary: Opportunity Get\n      description: '\n\n        __ALPHA VERSION__\n\n\n        This endpoint in its current form is primarily for testing and feedback.\n\n\n        Features in this endpoint are still under heavy development, and subject to\n        change. Not for production use.\n\n\n        See [Release Phases](https://github.com/github/roadmap?tab=readme-ov-file#release-phases)\n        for further details.\n\n        '\n      security:\n      - ApiKeyAuth: []\n  /v1/users/{user_id}/saved-searches:\n    post:\n      parameters:\n      - in: path\n        name: user_id\n        schema:\n          type: string\n        required: true\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserSaveSearchResponse'\n          description: Successful response\n        '422':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Validation error\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n        '404':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Not found\n      tags:\n      - User v1\n      summary: User Save Search\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/UserSaveSearchRequest'\n      security:\n      - ApiJwtAuth: []\n  /v1/users/{user_id}/saved-opportunities:\n    get:\n      parameters:\n      - in: path\n        name: user_id\n        schema:\n          type: string\n        required: true\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserSavedOpportunitiesResponse'\n          description: Successful response\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n        '404':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Not found\n      tags:\n      - User v1\n      summary: User Get Saved Opportunities\n      security:\n      - ApiJwtAuth: []\n    post:\n      parameters:\n      - in: path\n        name: user_id\n        schema:\n          type: string\n        required: true\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserSaveOpportunityResponse'\n          description: Successful response\n        '422':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Validation error\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n        '404':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Not found\n      tags:\n      - User v1\n      summary: User Save Opportunity\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/UserSaveOpportunityRequest'\n      security:\n      - ApiJwtAuth: []\n  /v1/users/{user_id}/saved-searches/list:\n    post:\n      parameters:\n      - in: path\n        name: user_id\n        schema:\n          type: string\n        required: true\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserSavedSearchesResponse'\n          description: Successful response\n        '422':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Validation error\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n        '404':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Not found\n      tags:\n      - User v1\n      summary: User Get Saved Searches\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/UserSavedSearchesRequest'\n      security:\n      - ApiJwtAuth: []\n  /v1/users/{user_id}/saved-searches/{saved_search_id}:\n    put:\n      parameters:\n      - in: path\n        name: user_id\n        schema:\n          type: string\n        required: true\n      - in: path\n        name: saved_search_id\n        schema:\n          type: string\n        required: true\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserUpdateSavedSearchResponse'\n          description: Successful response\n        '422':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Validation error\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n        '404':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Not found\n      tags:\n      - User v1\n      summary: User Update Saved Search\n      requestBody:\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/UserUpdateSavedSearchRequest'\n      security:\n      - ApiJwtAuth: []\n    delete:\n      parameters:\n      - in: path\n        name: user_id\n        schema:\n          type: string\n        required: true\n      - in: path\n        name: saved_search_id\n        schema:\n          type: string\n        required: true\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserDeleteSavedSearchResponse'\n          description: Successful response\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n        '404':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Not found\n      tags:\n      - User v1\n      summary: User Delete Saved Search\n      security:\n      - ApiJwtAuth: []\n  /v1/users/{user_id}/saved-opportunities/{opportunity_id}:\n    delete:\n      parameters:\n      - in: path\n        name: user_id\n        schema:\n          type: string\n        required: true\n      - in: path\n        name: opportunity_id\n        schema:\n          type: integer\n        required: true\n      responses:\n        '200':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserDeleteSavedOpportunityResponse'\n          description: Successful response\n        '401':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Authentication error\n        '404':\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorResponse'\n          description: Not found\n      tags:\n      - User v1\n      summary: User Delete Saved Opportunity\n      security:\n      - ApiJwtAuth: []\nopenapi: 3.1.0\ncomponents:\n  schemas:\n    HealthcheckMetadata:\n      type: object\n      properties:\n        commit_sha:\n          type: string\n          description: The github commit sha for the latest deployed commit\n          example: ffaca647223e0b6e54344122eefa73401f5ec131\n        commit_link:\n          type: string\n          description: A github link to the latest deployed commit\n          example: https://github.com/HHS/simpler-grants-gov/commit/main\n        release_notes_link:\n          type: string\n          description: A github link to the release notes - direct if the latest deploy\n            was a release\n          example: https://github.com/HHS/simpler-grants-gov/releases\n        last_deploy_time:\n          type: string\n          format: date-time\n          description: Latest deploy time in US/Eastern timezone\n        deploy_whoami:\n          type: string\n          description: The latest user to deploy the application\n          example: runner\n    HealthcheckResponse:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/HealthcheckMetadata'\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    ValidationIssue:\n      type: object\n      properties:\n        type:\n          type: string\n          description: The type of error\n          example: invalid\n        message:\n          type: string\n          description: The message to return\n          example: Not a valid string.\n        field:\n          type: string\n          description: The field that failed\n          example: summary.summary_description\n    ErrorResponse:\n      type: object\n      properties:\n        data:\n          description: Additional data that might be useful in resolving an error\n            (see specific endpoints for details, this is used infrequently)\n          example: {}\n        message:\n          type: string\n          description: General description of the error\n          example: Error\n        status_code:\n          type: integer\n          description: The HTTP status code of the error\n        errors:\n          type: array\n          example: []\n          items:\n            type:\n            - object\n            allOf:\n            - $ref: '#/components/schemas/ValidationIssue'\n        internal_request_id:\n          type: string\n          description: An internal tracking ID\n          example: 550e8400-e29b-41d4-a716-446655440000\n    DateRange:\n      type: object\n      properties:\n        start_date:\n          type:\n          - string\n          - 'null'\n          format: date\n        end_date:\n          type:\n          - string\n          - 'null'\n          format: date\n      required:\n      - end_date\n      - start_date\n    ExtractMetadataFilterV1:\n      type: object\n      properties:\n        extract_type:\n          description: The type of extract to filter by\n          example: opportunities_json\n          enum:\n          - opportunities_json\n          - opportunities_csv\n          type:\n          - string\n          - 'null'\n          - 'null'\n        created_at:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/DateRange'\n    SortOrderExtractMetadataPaginationV1:\n      type: object\n      properties:\n        order_by:\n          type: string\n          enum:\n          - created_at\n          description: The field to sort the response by\n        sort_direction:\n          description: Whether to sort the response ascending or descending\n          enum:\n          - ascending\n          - descending\n          type:\n          - string\n      required:\n      - order_by\n      - sort_direction\n    ExtractMetadataPaginationV1:\n      type: object\n      properties:\n        sort_order:\n          type: array\n          minItems: 1\n          maxItems: 5\n          description: The list of sorting rules\n          items:\n            type:\n            - object\n            allOf:\n            - $ref: '#/components/schemas/SortOrderExtractMetadataPaginationV1'\n        page_size:\n          type: integer\n          minimum: 1\n          maximum: 5000\n          description: The size of the page to fetch\n          example: 25\n        page_offset:\n          type: integer\n          minimum: 1\n          description: The page number to fetch, starts counting from 1\n          example: 1\n      required:\n      - page_offset\n      - page_size\n      - sort_order\n    ExtractMetadataRequest:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          description: The REST resource object\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n        filters:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/ExtractMetadataFilterV1'\n        pagination:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/ExtractMetadataPaginationV1'\n      required:\n      - pagination\n    SortOrder:\n      type: object\n      properties:\n        order_by:\n          type: string\n          description: The field that the records were sorted by\n          example: id\n        sort_direction:\n          description: The direction the records are sorted\n          enum:\n          - ascending\n          - descending\n          type:\n          - string\n    PaginationInfo:\n      type: object\n      properties:\n        page_offset:\n          type: integer\n          description: The page number that was fetched\n          example: 1\n        page_size:\n          type: integer\n          description: The size of the page fetched\n          example: 25\n        total_records:\n          type: integer\n          description: The total number of records fetchable\n          example: 42\n        total_pages:\n          type: integer\n          description: The total number of pages that can be fetched\n          example: 2\n        sort_order:\n          type: array\n          description: The sort order passed in originally\n          items:\n            type:\n            - object\n            allOf:\n            - $ref: '#/components/schemas/SortOrder'\n    ExtractMetadataResponse:\n      type: object\n      properties:\n        download_path:\n          type: string\n          description: The file's download path\n        file_size_bytes:\n          type: integer\n          description: The size of the file in bytes\n          example: 1024\n        created_at:\n          type: string\n          format: date-time\n          readOnly: true\n        updated_at:\n          type: string\n          format: date-time\n          readOnly: true\n        extract_metadata_id:\n          type: integer\n          description: The ID of the extract metadata\n          example: 1\n        extract_type:\n          type: string\n          description: The type of extract\n          example: opportunity_data_extract\n    ExtractMetadataListResponse:\n      type: object\n      properties:\n        pagination_info:\n          description: The pagination information for paginated endpoints\n          type: &id001\n          - object\n          allOf:\n          - $ref: '#/components/schemas/PaginationInfo'\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          type: array\n          description: A list of extract metadata records\n          items:\n            type:\n            - object\n            allOf:\n            - $ref: '#/components/schemas/ExtractMetadataResponse'\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    AgencyFilterV1:\n      type: object\n      properties:\n        agency_id:\n          type: integer\n    SortOrderAgencyPaginationV1:\n      type: object\n      properties:\n        order_by:\n          type: string\n          enum:\n          - agency_code\n          - agency_name\n          - created_at\n          description: The field to sort the response by\n        sort_direction:\n          description: Whether to sort the response ascending or descending\n          enum:\n          - ascending\n          - descending\n          type:\n          - string\n      required:\n      - order_by\n      - sort_direction\n    AgencyPaginationV1:\n      type: object\n      properties:\n        sort_order:\n          type: array\n          default:\n          - order_by: agency_code\n            sort_direction: ascending\n          minItems: 1\n          maxItems: 5\n          description: The list of sorting rules\n          items:\n            type:\n            - object\n            allOf:\n            - $ref: '#/components/schemas/SortOrderAgencyPaginationV1'\n        page_size:\n          type: integer\n          minimum: 1\n          maximum: 5000\n          description: The size of the page to fetch\n          example: 25\n        page_offset:\n          type: integer\n          minimum: 1\n          description: The page number to fetch, starts counting from 1\n          example: 1\n      required:\n      - page_offset\n      - page_size\n    AgencyListRequest:\n      type: object\n      properties:\n        filters:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/AgencyFilterV1'\n        pagination:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/AgencyPaginationV1'\n      required:\n      - pagination\n    AgencyResponse:\n      type: object\n      properties:\n        agency_id:\n          type: integer\n        agency_name:\n          type: string\n        agency_code:\n          type: string\n        top_level_agency:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/AgencyResponse'\n        created_at:\n          type: string\n          format: date-time\n        updated_at:\n          type: string\n          format: date-time\n    AgencyListResponse:\n      type: object\n      properties:\n        pagination_info:\n          description: The pagination information for paginated endpoints\n          type: *id001\n          allOf:\n          - $ref: '#/components/schemas/PaginationInfo'\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          type: array\n          description: A list of agency records\n          items:\n            type:\n            - object\n            allOf:\n            - $ref: '#/components/schemas/AgencyResponse'\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    UserTokenLogoutResponse:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          example: null\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    UserTokenRefreshResponse:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          example: null\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    FundingInstrumentFilterV1:\n      type: object\n      properties:\n        one_of:\n          type: array\n          minItems: 1\n          items:\n            enum:\n            - cooperative_agreement\n            - grant\n            - procurement_contract\n            - other\n            type:\n            - string\n    FundingCategoryFilterV1:\n      type: object\n      properties:\n        one_of:\n          type: array\n          minItems: 1\n          items:\n            enum:\n            - recovery_act\n            - agriculture\n            - arts\n            - business_and_commerce\n            - community_development\n            - consumer_protection\n            - disaster_prevention_and_relief\n            - education\n            - employment_labor_and_training\n            - energy\n            - environment\n            - food_and_nutrition\n            - health\n            - housing\n            - humanities\n            - infrastructure_investment_and_jobs_act\n            - information_and_statistics\n            - income_security_and_social_services\n            - law_justice_and_legal_services\n            - natural_resources\n            - opportunity_zone_benefits\n            - regional_development\n            - science_technology_and_other_research_and_development\n            - transportation\n            - affordable_care_act\n            - other\n            type:\n            - string\n    ApplicantTypeFilterV1:\n      type: object\n      properties:\n        one_of:\n          type: array\n          minItems: 1\n          items:\n            enum:\n            - state_governments\n            - county_governments\n            - city_or_township_governments\n            - special_district_governments\n            - independent_school_districts\n            - public_and_state_institutions_of_higher_education\n            - private_institutions_of_higher_education\n            - federally_recognized_native_american_tribal_governments\n            - other_native_american_tribal_organizations\n            - public_and_indian_housing_authorities\n            - nonprofits_non_higher_education_with_501c3\n            - nonprofits_non_higher_education_without_501c3\n            - individuals\n            - for_profit_organizations_other_than_small_businesses\n            - small_businesses\n            - other\n            - unrestricted\n            type:\n            - string\n    OpportunityStatusFilterV1:\n      type: object\n      properties:\n        one_of:\n          type: array\n          minItems: 1\n          items:\n            enum:\n            - forecasted\n            - posted\n            - closed\n            - archived\n            type:\n            - string\n    AgencySearchFilterV1:\n      type: object\n      properties:\n        one_of:\n          type: array\n          minItems: 1\n          items:\n            type: string\n            minLength: 2\n            example: USAID\n    AssistanceListingNumberFilterV1:\n      type: object\n      properties:\n        one_of:\n          type: array\n          minItems: 1\n          items:\n            type: string\n            pattern: ^\\d{2}\\.\\d{2,3}$\n            example: '45.149'\n    IsCostSharingFilterV1:\n      type: object\n      properties:\n        one_of:\n          type:\n          - array\n          - 'null'\n          items:\n            type: boolean\n            example: true\n    ExpectedNumberAwardsFilterV1:\n      type: object\n      properties:\n        min:\n          type:\n          - integer\n          - 'null'\n          minimum: 0\n          example: 0\n        max:\n          type:\n          - integer\n          - 'null'\n          minimum: 0\n          example: 25\n    AwardFloorFilterV1:\n      type: object\n      properties:\n        min:\n          type:\n          - integer\n          - 'null'\n          minimum: 0\n          example: 0\n        max:\n          type:\n          - integer\n          - 'null'\n          minimum: 0\n          example: 10000\n    AwardCeilingFilterV1:\n      type: object\n      properties:\n        min:\n          type:\n          - integer\n          - 'null'\n          minimum: 0\n          example: 0\n        max:\n          type:\n          - integer\n          - 'null'\n          minimum: 0\n          example: 10000000\n    EstimatedTotalProgramFundingFilterV1:\n      type: object\n      properties:\n        min:\n          type:\n          - integer\n          - 'null'\n          minimum: 0\n          example: 0\n        max:\n          type:\n          - integer\n          - 'null'\n          minimum: 0\n          example: 10000000\n    PostDateFilterV1:\n      type: object\n      properties:\n        start_date:\n          type:\n          - string\n          - 'null'\n          format: date\n        end_date:\n          type:\n          - string\n          - 'null'\n          format: date\n        start_date_relative:\n          type:\n          - integer\n          - 'null'\n          minimum: -1000000\n          maximum: 1000000\n        end_date_relative:\n          type:\n          - integer\n          - 'null'\n          minimum: -1000000\n          maximum: 1000000\n    CloseDateFilterV1:\n      type: object\n      properties:\n        start_date:\n          type:\n          - string\n          - 'null'\n          format: date\n        end_date:\n          type:\n          - string\n          - 'null'\n          format: date\n        start_date_relative:\n          type:\n          - integer\n          - 'null'\n          minimum: -1000000\n          maximum: 1000000\n        end_date_relative:\n          type:\n          - integer\n          - 'null'\n          minimum: -1000000\n          maximum: 1000000\n    OpportunitySearchFilterV1:\n      type: object\n      properties:\n        funding_instrument:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/FundingInstrumentFilterV1'\n        funding_category:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/FundingCategoryFilterV1'\n        applicant_type:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/ApplicantTypeFilterV1'\n        opportunity_status:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/OpportunityStatusFilterV1'\n        agency:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/AgencySearchFilterV1'\n        assistance_listing_number:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/AssistanceListingNumberFilterV1'\n        is_cost_sharing:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/IsCostSharingFilterV1'\n        expected_number_of_awards:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/ExpectedNumberAwardsFilterV1'\n        award_floor:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/AwardFloorFilterV1'\n        award_ceiling:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/AwardCeilingFilterV1'\n        estimated_total_program_funding:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/EstimatedTotalProgramFundingFilterV1'\n        post_date:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/PostDateFilterV1'\n        close_date:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/CloseDateFilterV1'\n    ExperimentalV1:\n      type: object\n      properties:\n        scoring_rule:\n          default: !!python/object/apply:src.services.opportunities_v1.experimental_constant.ScoringRule\n          - default\n          description: Scoring rule to query against OpenSearch\n          enum:\n          - default\n          - expanded\n          - agency\n          type:\n          - string\n    SortOrderOpportunityPaginationV1:\n      type: object\n      properties:\n        order_by:\n          type: string\n          enum:\n          - relevancy\n          - opportunity_id\n          - opportunity_number\n          - opportunity_title\n          - post_date\n          - close_date\n          - agency_code\n          - agency_name\n          - top_level_agency_name\n          description: The field to sort the response by\n        sort_direction:\n          description: Whether to sort the response ascending or descending\n          enum:\n          - ascending\n          - descending\n          type:\n          - string\n      required:\n      - order_by\n      - sort_direction\n    OpportunityPaginationV1:\n      type: object\n      properties:\n        sort_order:\n          type: array\n          default:\n          - order_by: opportunity_id\n            sort_direction: descending\n          minItems: 1\n          maxItems: 5\n          description: The list of sorting rules\n          items:\n            type:\n            - object\n            allOf:\n            - $ref: '#/components/schemas/SortOrderOpportunityPaginationV1'\n        page_size:\n          type: integer\n          minimum: 1\n          maximum: 5000\n          description: The size of the page to fetch\n          example: 25\n        page_offset:\n          type: integer\n          minimum: 1\n          description: The page number to fetch, starts counting from 1\n          example: 1\n      required:\n      - page_offset\n      - page_size\n    OpportunitySearchRequestV1:\n      type: object\n      properties:\n        query:\n          type: string\n          minLength: 1\n          maxLength: 100\n          description: Query string which searches against several text fields\n          example: research\n        query_operator:\n          default: !!python/object/apply:src.api.opportunities_v1.opportunity_schemas.SearchQueryOperator\n          - AND\n          description: Query operator for combining search conditions\n          example: OR\n          enum:\n          - AND\n          - OR\n          type:\n          - string\n        filters:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/OpportunitySearchFilterV1'\n        experimental:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/ExperimentalV1'\n        pagination:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/OpportunityPaginationV1'\n        format:\n          default: !!python/object/apply:src.api.opportunities_v1.opportunity_schemas.SearchResponseFormat\n          - json\n          description: The format of the response\n          enum:\n          - json\n          - csv\n          type:\n          - string\n      required:\n      - pagination\n    OpportunityAssistanceListingV1:\n      type: object\n      properties:\n        program_title:\n          type:\n          - string\n          - 'null'\n          description: The name of the program, see https://sam.gov/content/assistance-listings\n            for more detail\n          example: Space Technology\n        assistance_listing_number:\n          type:\n          - string\n          - 'null'\n          description: The assistance listing number, see https://sam.gov/content/assistance-listings\n            for more detail\n          example: '43.012'\n    OpportunitySummaryV1:\n      type: object\n      properties:\n        summary_description:\n          type:\n          - string\n          - 'null'\n          description: The summary of the opportunity\n          example: This opportunity aims to unravel the mysteries of the universe.\n        is_cost_sharing:\n          type:\n          - boolean\n          - 'null'\n          description: Whether or not the opportunity has a cost sharing/matching\n            requirement\n        is_forecast:\n          type: boolean\n          description: Whether the opportunity is forecasted, that is, the information\n            is only an estimate and not yet official\n          example: false\n        close_date:\n          type:\n          - string\n          - 'null'\n          format: date\n          description: The date that the opportunity will close - only set if is_forecast=False\n        close_date_description:\n          type:\n          - string\n          - 'null'\n          description: Optional details regarding the close date\n          example: Proposals are due earlier than usual.\n        post_date:\n          type:\n          - string\n          - 'null'\n          format: date\n          description: The date the opportunity was posted\n        archive_date:\n          type:\n          - string\n          - 'null'\n          format: date\n          description: When the opportunity will be archived\n        expected_number_of_awards:\n          type:\n          - integer\n          - 'null'\n          description: The number of awards the opportunity is expected to award\n          example: 10\n        estimated_total_program_funding:\n          type:\n          - integer\n          - 'null'\n          description: The total program funding of the opportunity in US Dollars\n          example: 10000000\n        award_floor:\n          type:\n          - integer\n          - 'null'\n          description: The minimum amount an opportunity would award\n          example: 10000\n        award_ceiling:\n          type:\n          - integer\n          - 'null'\n          description: The maximum amount an opportunity would award\n          example: 100000\n        additional_info_url:\n          type:\n          - string\n          - 'null'\n          description: A URL to a website that can provide additional information\n            about the opportunity\n          example: grants.gov\n        additional_info_url_description:\n          type:\n          - string\n          - 'null'\n          description: The text to display for the additional_info_url link\n          example: Click me for more info\n        forecasted_post_date:\n          type:\n          - string\n          - 'null'\n          format: date\n          description: Forecasted opportunity only. The date the opportunity is expected\n            to be posted, and transition out of being a forecast\n        forecasted_close_date:\n          type:\n          - string\n          - 'null'\n          format: date\n          description: Forecasted opportunity only. The date the opportunity is expected\n            to be close once posted.\n        forecasted_close_date_description:\n          type:\n          - string\n          - 'null'\n          description: Forecasted opportunity only. Optional details regarding the\n            forecasted closed date.\n          example: Proposals will probably be due on this date\n        forecasted_award_date:\n          type:\n          - string\n          - 'null'\n          format: date\n          description: Forecasted opportunity only. The date the grantor plans to\n            award the opportunity.\n        forecasted_project_start_date:\n          type:\n          - string\n          - 'null'\n          format: date\n          description: Forecasted opportunity only. The date the grantor expects the\n            award recipient should start their project\n        fiscal_year:\n          type:\n          - integer\n          - 'null'\n          description: Forecasted opportunity only. The fiscal year the project is\n            expected to be funded and launched\n        funding_category_description:\n          type:\n          - string\n          - 'null'\n          description: Additional information about the funding category\n          example: Economic Support\n        applicant_eligibility_description:\n          type:\n          - string\n          - 'null'\n          description: Additional information about the types of applicants that are\n            eligible\n          example: All types of domestic applicants are eligible to apply\n        agency_contact_description:\n          type:\n          - string\n          - 'null'\n          description: Information regarding contacting the agency who owns the opportunity\n          example: For more information, reach out to Jane Smith at agency US-ABC\n        agency_email_address:\n          type:\n          - string\n          - 'null'\n          description: The contact email of the agency who owns the opportunity\n          example: fake_email@grants.gov\n        agency_email_address_description:\n          type:\n          - string\n          - 'null'\n          description: The text for the link to the agency email address\n          example: Click me to email the agency\n        version_number:\n          type: integer\n          description: The version number of the opportunity summary\n          example: 1\n        funding_instruments:\n          type: array\n          items:\n            enum:\n            - cooperative_agreement\n            - grant\n            - procurement_contract\n            - other\n            type:\n            - string\n        funding_categories:\n          type: array\n          items:\n            enum:\n            - recovery_act\n            - agriculture\n            - arts\n            - business_and_commerce\n            - community_development\n            - consumer_protection\n            - disaster_prevention_and_relief\n            - education\n            - employment_labor_and_training\n            - energy\n            - environment\n            - food_and_nutrition\n            - health\n            - housing\n            - humanities\n            - infrastructure_investment_and_jobs_act\n            - information_and_statistics\n            - income_security_and_social_services\n            - law_justice_and_legal_services\n            - natural_resources\n            - opportunity_zone_benefits\n            - regional_development\n            - science_technology_and_other_research_and_development\n            - transportation\n            - affordable_care_act\n            - other\n            type:\n            - string\n        applicant_types:\n          type: array\n          items:\n            enum:\n            - state_governments\n            - county_governments\n            - city_or_township_governments\n            - special_district_governments\n            - independent_school_districts\n            - public_and_state_institutions_of_higher_education\n            - private_institutions_of_higher_education\n            - federally_recognized_native_american_tribal_governments\n            - other_native_american_tribal_organizations\n            - public_and_indian_housing_authorities\n            - nonprofits_non_higher_education_with_501c3\n            - nonprofits_non_higher_education_without_501c3\n            - individuals\n            - for_profit_organizations_other_than_small_businesses\n            - small_businesses\n            - other\n            - unrestricted\n            type:\n            - string\n        created_at:\n          type: string\n          format: date-time\n          description: When the opportunity summary was created\n        updated_at:\n          type: string\n          format: date-time\n          description: When the opportunity summary was last updated\n    OpportunityV1:\n      type: object\n      properties:\n        opportunity_id:\n          type: integer\n          description: The internal ID of the opportunity\n          example: 12345\n        opportunity_number:\n          type:\n          - string\n          - 'null'\n          description: The funding opportunity number\n          example: ABC-123-XYZ-001\n        opportunity_title:\n          type:\n          - string\n          - 'null'\n          description: The title of the opportunity\n          example: Research into conservation techniques\n        agency:\n          type:\n          - string\n          - 'null'\n          description: 'DEPRECATED - use: agency_code'\n          example: US-ABC\n        agency_code:\n          type:\n          - string\n          - 'null'\n          description: The agency who created the opportunity\n          example: US-ABC\n        agency_name:\n          type:\n          - string\n          - 'null'\n          description: The name of the agency who created the oppportunity\n          example: Department of Examples\n        top_level_agency_name:\n          type:\n          - string\n          - 'null'\n          description: The name of the top level agency who created the oppportunity\n          example: Department of Examples\n        category:\n          description: The opportunity category\n          example: !!python/object/apply:src.constants.lookup_constants.OpportunityCategory\n          - discretionary\n          enum: &id002\n          - discretionary\n          - mandatory\n          - continuation\n          - earmark\n          - other\n          type:\n          - string\n          - 'null'\n          - 'null'\n        category_explanation:\n          type:\n          - string\n          - 'null'\n          description: Explanation of the category when the category is 'O' (other)\n          example: null\n        opportunity_assistance_listings:\n          type: array\n          items:\n            type: &id003\n            - object\n            allOf:\n            - $ref: '#/components/schemas/OpportunityAssistanceListingV1'\n        summary:\n          type: &id004\n          - object\n          allOf:\n          - $ref: '#/components/schemas/OpportunitySummaryV1'\n        opportunity_status:\n          description: The current status of the opportunity\n          example: !!python/object/apply:src.constants.lookup_constants.OpportunityStatus\n          - posted\n          enum: &id005\n          - forecasted\n          - posted\n          - closed\n          - archived\n          type: &id006\n          - string\n        created_at:\n          type: string\n          format: date-time\n          readOnly: true\n        updated_at:\n          type: string\n          format: date-time\n          readOnly: true\n    OpportunityFacetV1:\n      type: object\n      properties:\n        opportunity_status:\n          type: object\n          description: The counts of opportunity_status values in the full response\n          example:\n            posted: 1\n            forecasted: 2\n          additionalProperties:\n            type: integer\n        applicant_type:\n          type: object\n          description: The counts of applicant_type values in the full response\n          example:\n            state_governments: 3\n            county_governments: 2\n            city_or_township_governments: 1\n          additionalProperties:\n            type: integer\n        funding_instrument:\n          type: object\n          description: The counts of funding_instrument values in the full response\n          example:\n            cooperative_agreement: 4\n            grant: 3\n          additionalProperties:\n            type: integer\n        funding_category:\n          type: object\n          description: The counts of funding_category values in the full response\n          example:\n            recovery_act: 2\n            arts: 3\n            agriculture: 5\n          additionalProperties:\n            type: integer\n        agency:\n          type: object\n          description: The counts of agency values in the full response\n          example:\n            USAID: 4\n            DOC: 3\n          additionalProperties:\n            type: integer\n    OpportunitySearchResponseV1:\n      type: object\n      properties:\n        pagination_info:\n          description: The pagination information for paginated endpoints\n          type: *id001\n          allOf:\n          - $ref: '#/components/schemas/PaginationInfo'\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          type: array\n          items:\n            $ref: '#/components/schemas/OpportunityV1'\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n        facet_counts:\n          description: Counts of filter/facet values in the full response\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/OpportunityFacetV1'\n    User:\n      type: object\n      properties:\n        user_id:\n          type: string\n          description: The internal ID of a user\n          example: 861a0148-cf2c-432b-b0b3-690016299ab1\n        email:\n          type: string\n          description: The email address returned from Oauth2 provider\n          example: user@example.com\n        external_user_type:\n          description: The Oauth2 provider through which a user was authenticated\n          example: !!python/object/apply:src.constants.lookup_constants.ExternalUserType\n          - login_gov\n          enum:\n          - login_gov\n          type:\n          - string\n    UserGetResponse:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/User'\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    OpportunityAttachmentV1:\n      type: object\n      properties:\n        download_path:\n          type: string\n          description: The file's download path\n        file_size_bytes:\n          type: integer\n          description: The size of the file in bytes\n          example: 1024\n        created_at:\n          type: string\n          format: date-time\n          readOnly: true\n        updated_at:\n          type: string\n          format: date-time\n          readOnly: true\n        mime_type:\n          type: string\n          description: The MIME type of the attachment\n          example: application/pdf\n        file_name:\n          type: string\n          description: The name of the attachment file\n          example: my_NOFO.pdf\n        file_description:\n          type: string\n          description: A description of the attachment\n          example: The full announcement NOFO\n    OpportunityWithAttachmentsV1:\n      type: object\n      properties:\n        opportunity_id:\n          type: integer\n          description: The internal ID of the opportunity\n          example: 12345\n        opportunity_number:\n          type:\n          - string\n          - 'null'\n          description: The funding opportunity number\n          example: ABC-123-XYZ-001\n        opportunity_title:\n          type:\n          - string\n          - 'null'\n          description: The title of the opportunity\n          example: Research into conservation techniques\n        agency:\n          type:\n          - string\n          - 'null'\n          description: 'DEPRECATED - use: agency_code'\n          example: US-ABC\n        agency_code:\n          type:\n          - string\n          - 'null'\n          description: The agency who created the opportunity\n          example: US-ABC\n        agency_name:\n          type:\n          - string\n          - 'null'\n          description: The name of the agency who created the oppportunity\n          example: Department of Examples\n        top_level_agency_name:\n          type:\n          - string\n          - 'null'\n          description: The name of the top level agency who created the oppportunity\n          example: Department of Examples\n        category:\n          description: The opportunity category\n          example: !!python/object/apply:src.constants.lookup_constants.OpportunityCategory\n          - discretionary\n          enum: *id002\n          type:\n          - string\n          - 'null'\n          - 'null'\n        category_explanation:\n          type:\n          - string\n          - 'null'\n          description: Explanation of the category when the category is 'O' (other)\n          example: null\n        opportunity_assistance_listings:\n          type: array\n          items:\n            type: *id003\n            allOf:\n            - $ref: '#/components/schemas/OpportunityAssistanceListingV1'\n        summary:\n          type: *id004\n          allOf:\n          - $ref: '#/components/schemas/OpportunitySummaryV1'\n        opportunity_status:\n          description: The current status of the opportunity\n          example: !!python/object/apply:src.constants.lookup_constants.OpportunityStatus\n          - posted\n          enum: *id005\n          type: *id006\n        created_at:\n          type: string\n          format: date-time\n          readOnly: true\n        updated_at:\n          type: string\n          format: date-time\n          readOnly: true\n        attachments:\n          type: array\n          description: List of attachments associated with the opportunity\n          items:\n            type:\n            - object\n            allOf:\n            - $ref: '#/components/schemas/OpportunityAttachmentV1'\n    OpportunityGetResponseV1:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/OpportunityWithAttachmentsV1'\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    UserSaveSearchRequest:\n      type: object\n      properties:\n        name:\n          type: string\n          description: Name of the saved search\n          example: Example search\n        search_query:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/OpportunitySearchRequestV1'\n      required:\n      - name\n    UserSaveSearchResponse:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          example: null\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    SavedOpportunitySummaryV1:\n      type: object\n      properties:\n        post_date:\n          type: string\n          format: date\n          description: The date the opportunity was posted\n          example: '2024-01-01'\n        close_date:\n          type: string\n          format: date\n          description: The date the opportunity will close\n          example: '2024-01-01'\n        is_forecast:\n          type: boolean\n          description: Whether the opportunity is forecasted\n          example: false\n    SavedOpportunityResponseV1:\n      type: object\n      properties:\n        opportunity_id:\n          type: integer\n          description: The ID of the saved opportunity\n          example: 1234\n        opportunity_title:\n          type:\n          - string\n          - 'null'\n          description: The title of the opportunity\n          example: my title\n        opportunity_status:\n          description: The current status of the opportunity\n          example: !!python/object/apply:src.constants.lookup_constants.OpportunityStatus\n          - posted\n          enum:\n          - forecasted\n          - posted\n          - closed\n          - archived\n          type:\n          - string\n        summary:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/SavedOpportunitySummaryV1'\n    UserSavedOpportunitiesResponse:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          type: array\n          description: List of saved opportunities\n          items:\n            type:\n            - object\n            allOf:\n            - $ref: '#/components/schemas/SavedOpportunityResponseV1'\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    UserSaveOpportunityRequest:\n      type: object\n      properties:\n        opportunity_id:\n          type: integer\n      required:\n      - opportunity_id\n    UserSaveOpportunityResponse:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          example: null\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    SortOrderUserGetSavedSearchPaginationV1:\n      type: object\n      properties:\n        order_by:\n          type: string\n          enum:\n          - created_at\n          - updated_at\n          - name\n          description: The field to sort the response by\n        sort_direction:\n          description: Whether to sort the response ascending or descending\n          enum:\n          - ascending\n          - descending\n          type:\n          - string\n      required:\n      - order_by\n      - sort_direction\n    UserGetSavedSearchPaginationV1:\n      type: object\n      properties:\n        sort_order:\n          type: array\n          default:\n          - order_by: created_at\n            sort_direction: descending\n          minItems: 1\n          maxItems: 5\n          description: The list of sorting rules\n          items:\n            type:\n            - object\n            allOf:\n            - $ref: '#/components/schemas/SortOrderUserGetSavedSearchPaginationV1'\n        page_size:\n          type: integer\n          minimum: 1\n          maximum: 5000\n          description: The size of the page to fetch\n          example: 25\n        page_offset:\n          type: integer\n          minimum: 1\n          description: The page number to fetch, starts counting from 1\n          example: 1\n      required:\n      - page_offset\n      - page_size\n    UserSavedSearchesRequest:\n      type: object\n      properties:\n        pagination:\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/UserGetSavedSearchPaginationV1'\n      required:\n      - pagination\n    SavedSearchResponse:\n      type: object\n      properties:\n        saved_search_id:\n          type: string\n          format: uuid\n          description: The ID of the saved search\n          example: 123e4567-e89b-12d3-a456-426614174000\n        name:\n          type: string\n          description: Name of the saved search\n          example: Grant opportunities in California\n        search_query:\n          description: The saved search query parameters\n          type:\n          - object\n          allOf:\n          - $ref: '#/components/schemas/OpportunitySearchRequestV1'\n        created_at:\n          type: string\n          format: date-time\n          description: When the search was saved\n          example: '2024-01-01T00:00:00Z'\n    UserSavedSearchesResponse:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          type: array\n          description: List of saved searches\n          items:\n            type:\n            - object\n            allOf:\n            - $ref: '#/components/schemas/SavedSearchResponse'\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    UserUpdateSavedSearchRequest:\n      type: object\n      properties:\n        name:\n          type: string\n          description: Name of the saved search\n          example: Example search\n      required:\n      - name\n    UserUpdateSavedSearchResponse:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          example: null\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    UserDeleteSavedSearchResponse:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          example: null\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n    UserDeleteSavedOpportunityResponse:\n      type: object\n      properties:\n        message:\n          type: string\n          description: The message to return\n          example: Success\n        data:\n          example: null\n        status_code:\n          type: integer\n          description: The HTTP status code\n          example: 200\n  securitySchemes:\n    ApiKeyAuth:\n      type: apiKey\n      in: header\n      name: X-Auth\n    ApiJwtAuth:\n      type: apiKey\n      in: header\n      name: X-SGG-Token"}
{"path":"api/src/logging/audit.py","language":"python","type":"code","directory":"api/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/audit.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/opensearch/Dockerfile\nLanguage: unknown\nType: code\nDirectory: api/opensearch\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/opensearch/Dockerfile\nSize: 0.31 KB\nLast Modified: 2025-02-14T17:08:26.432Z"}
{"path":"api/src/logging/config.py","language":"python","type":"code","directory":"api/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/config.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"# Use OpenSearch base image\nFROM opensearchproject/opensearch:latest\n\n# Install the Ingest Attachment plugin\n# This image is only used for local development\nRUN /usr/share/opensearch/bin/opensearch-plugin install --batch ingest-attachment"}
{"path":"api/src/logging/decodelog.py","language":"python","type":"code","directory":"api/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/decodelog.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/pyproject.toml\nLanguage: toml\nType: code\nDirectory: api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/pyproject.toml\nSize: 4.72 KB\nLast Modified: 2025-02-14T17:08:26.433Z"}
{"path":"api/src/logging/flask_logger.py","language":"python","type":"code","directory":"api/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/flask_logger.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"[tool.poetry.dependencies]\n# See /documentation/api/package-depedency-management.md#Upgrading Python\n# for details on upgrading your Python version\npython = \"~3.13\"\nSQLAlchemy = { version = \"^2.0.21\", extras = [\"mypy\"] }\nalembic = \"^1.12.0\"\npython-dotenv = \"^1.0.0\"\npydantic = \"^2.4.2\"\nbotocore = \"^1.31.62\"\nboto3 = \"^1.28.62\"\nsmart-open = \"^7.0.0\"\npytz = \"^2023.3.post1\"\nAPIFlask = \"^2.1.0\"\nmarshmallow-dataclass = { extras = [\"enum\", \"union\"], version = \"^8.5.8\" }\nmarshmallow = \"^3.20.1\"\ngunicorn = \"^23.0.0\"\npsycopg = { extras = [\"binary\"], version = \"^3.1.19\" }\npydantic-settings = \"^2.0.3\"\nflask-cors = \"^5.0.0\"\nopensearch-py = \"^2.5.0\"\npyjwt = { extras = [\"crypto\"], version = \"^2.9.0\" }\nnewrelic = \"10.4.0\"\njinja2 = \">=3.1.5\"\ntenacity = \"^8.0\"\n\n\n[tool.poetry.group.dev.dependencies]\nblack = \"^24.0.0\"\nisort = \"^5.12.0\"\nmypy = \"^1.8.0\"\nmoto = { extras = [\"s3\"], version = \"^5.0.0\" }\ntypes-pytz = \"^2023.3.1.1\"\ncoverage = \"^7.3.2\"\nFaker = \"^28.0.0\"\nfactory-boy = \"^3.3.0\"\nbandit = \"^1.7.5\"\npytest = \"^8.0.0\"\npytest-watch = \"^4.2.0\"\ntypes-pyyaml = \"^6.0.12.11\"\nsetuptools = \"^75.0.0\"\npydot = \"3.0.3\"\nsadisplay = \"0.4.9\"\nruff = \"^0.8.0\"\ndebugpy = \"^1.8.1\"\nfreezegun = \"^1.5.0\"\ntypes-requests = \"^2.31\"\ngraphviz = \"^0.20.3\"\neralchemy = \"^1.5.0\"\n\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.poetry.scripts]\ndb-migrate = \"src.db.migrations.run:up\"\ndb-migrate-down = \"src.db.migrations.run:down\"\ndb-migrate-down-all = \"src.db.migrations.run:downall\"\ndb-seed-local = \"tests.lib.seed_local_db:seed_local_db\"\ncreate-erds = \"bin.create_erds:main\"\nsetup-postgres-db = \"src.db.migrations.setup_local_postgres_db:setup_local_postgres_db\"\nsetup-localstack = \"bin.setup_localstack:main\"\n\n[tool.black]\nline-length = 100\n\n[tool.isort]\nmulti_line_output = 3\ninclude_trailing_comma = true\nforce_grid_wrap = 0\nuse_parentheses = true\nline_length = 100\n\n[tool.ruff]\nline-length = 100\n# Some rules are considered preview-only, this allows them\n# assuming we enabled them below\npreview = true\n\n[tool.ruff.lint]\n# See: https://docs.astral.sh/ruff/rules/ for all possible rules\nselect = [\"B\", \"C\", \"E\", \"F\", \"W\", \"B9\"]\nignore = [\n  # too many leading '#' for block comment, we can format our comments however we want\n  \"E266\",\n  # Ignore line-too-long errors, assume the formatter handles that appropriately\n  \"E501\",\n  # Ignore rules regarding unecessary list / generator usage which complains about [e for e in MyEnum] #\n  \"C4\",\n  # Ignore rule that flags functions with many branches - sometimes we just have a lot of\n  # business rules that make sense to aggregate in one place.\n  \"C901\",\n]\n\n\n[tool.mypy]\n# https://mypy.readthedocs.io/en/stable/config_file.html\ncolor_output = true\nerror_summary = true\npretty = true\nshow_error_codes = true\nshow_column_numbers = true\nshow_error_context = true\n\nnamespace_packages = true\nignore_missing_imports = true\nwarn_unused_configs = true\n\ncheck_untyped_defs = true\ndisallow_incomplete_defs = true\ndisallow_untyped_defs = true\nno_implicit_optional = true\nstrict_equality = true\nwarn_no_return = true\nwarn_redundant_casts = true\nwarn_unreachable = true\nwarn_unused_ignores = true\n\nplugins = [\"pydantic.mypy\"]\n\n[tool.bandit]\n# Ignore audit logging test file since test audit logging requires a lot of operations that trigger bandit warnings\nexclude_dirs = [\"./tests/src/logging/test_audit.py\"]\n\n[[tool.mypy.overrides]]\n# Migrations are generated without \"-> None\"\n# for the returns. Rather than require manually\n# fixing this for every migration generated,\n# disable the check for that folder.\nmodule = \"src.db.migrations.versions.*\"\ndisallow_untyped_defs = false\n\n[tool.pytest.ini_options]\n# Ignore deprecation warnings in library code.\n# When a library has addressed its deprecation issues and we've updated the\n# library, we can remove the ignore filter for that library.\nfilterwarnings = [\n  \"ignore::DeprecationWarning:botocore.*\",\n] # pytest-watch errors if the closing bracket is on it's own line\n\nmarkers = [\n  \"audit: mark a test as a security audit log test, to be run isolated from other tests\",\n]\n\n[tool.coverage.run]\nomit = [\n  # Decodelog is only used for formatting logs locally\n  \"src/logging/decodelog.py\",\n  # app_config only runs via the gunicorn script which doens't happen locally\n  \"src/app_config.py\",\n  # Migrations aren't run in tests\n  \"src/db/migrations/**\",\n]\n\n[tool.coverage.report]\nfail_under = 80\n\nexclude_lines = [\n  # Exclude abstract & overloaad methods from\n  # code coverage reports as they won't ever directly run\n  \"@abc.abstractmethod\",\n  \"@abstractmethod\",\n  \"@typing.overload\",\n]"}
{"path":"api/src/logging/formatters.py","language":"python","type":"code","directory":"api/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/formatters.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/__init__.py\nLanguage: py\nType: code\nDirectory: api/src\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.433Z"}
{"path":"api/src/logging/pii.py","language":"python","type":"code","directory":"api/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/pii.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":""}
{"path":"api/src/pagination/__init__.py","language":"python","type":"code","directory":"api/src/pagination","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/pagination/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/adapters\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.433Z"}
{"path":"api/src/pagination/pagination_models.py","language":"python","type":"code","directory":"api/src/pagination","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/pagination/pagination_models.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":""}
{"path":"api/src/pagination/pagination_schema.py","language":"python","type":"code","directory":"api/src/pagination","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/pagination/pagination_schema.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/aws/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/aws\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/aws/__init__.py\nSize: 0.15 KB\nLast Modified: 2025-02-14T17:08:26.433Z"}
{"path":"api/src/pagination/paginator.py","language":"python","type":"code","directory":"api/src/pagination","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/pagination/paginator.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"__all__ = [\"get_s3_client\", \"S3Config\", \"get_boto_session\"]"}
{"path":"api/src/search/__init__.py","language":"python","type":"code","directory":"api/src/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/aws/aws_session.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/aws\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/aws/aws_session.py\nSize: 0.88 KB\nLast Modified: 2025-02-14T17:08:26.433Z"}
{"path":"api/src/search/backend/__init__.py","language":"python","type":"code","directory":"api/src/search/backend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/backend/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"from src.util.env_config import PydanticBaseEnvConfig\n\n\nclass BaseAwsConfig(PydanticBaseEnvConfig):\n    is_local_aws: bool = False\n\n\n_base_aws_config: BaseAwsConfig | None = None\n\n\ndef get_base_aws_config() -> BaseAwsConfig:\n    global _base_aws_config\n    if _base_aws_config is None:\n        _base_aws_config = BaseAwsConfig()\n\n    return _base_aws_config\n\n\ndef is_local_aws() -> bool:\n    \"\"\"Whether we are running against local AWS which affects the credentials we use (forces them to be not real)\"\"\"\n    return get_base_aws_config().is_local_aws\n\n\ndef get_boto_session() -> boto3.Session:\n    if is_local_aws():\n        # Locally, set fake creds in a region we don't actually use so we can't hit actual AWS resources\n        return boto3.Session(\n            aws_access_key_id=\"NO_CREDS\", aws_secret_access_key=\"NO_CREDS\", region_name=\"us-west-2\"\n        )\n\n    return boto3.Session()"}
{"path":"api/src/search/backend/load_opportunities_to_index.py","language":"python","type":"code","directory":"api/src/search/backend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/backend/load_opportunities_to_index.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/aws/pinpoint_adapter.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/aws\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/aws/pinpoint_adapter.py\nSize: 3.67 KB\nLast Modified: 2025-02-14T17:08:26.433Z"}
{"path":"api/src/search/backend/load_search_data.py","language":"python","type":"code","directory":"api/src/search/backend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/backend/load_search_data.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"import boto3\nimport botocore.client\nfrom botocore.exceptions import ClientError\nfrom pydantic import BaseModel, Field\n\nfrom src.adapters.aws import get_boto_session\nfrom src.adapters.aws.aws_session import is_local_aws\n\nlogger = logging.getLogger(__name__)\n\n# An example of what the Pinpoint response looks like:\n\"\"\"\n{\n    \"ResponseMetadata\": {\n        \"RequestId\": \"abcdef11-1111-2222-3333-4444abcabc\",\n        \"HTTPStatusCode\": 200,\n        \"HTTPHeaders\": {\n            # A bunch of generic HTTP/AWS headers\n        },\n        \"RetryAttempts\": 0\n    },\n    \"MessageResponse\": {\n        \"ApplicationId\": \"abc123\",\n        \"RequestId\": \"ABCD-ASDASDASDAS\",\n        \"Result\": {\n            \"person@fake.com\": {\n                \"DeliveryStatus\": \"SUCCESSFUL\",\n                \"StatusCode\": 200,\n                \"StatusMessage\": \"abcdef\"\n            }\n        }\n    }\n}\n\"\"\"\n\n\nclass PinpointResult(BaseModel):\n    delivery_status: str = Field(alias=\"DeliveryStatus\")\n    status_code: int = Field(alias=\"StatusCode\")\n    status_message: str = Field(alias=\"StatusMessage\")\n\n\nclass PinpointResponse(BaseModel):\n    results: dict[str, PinpointResult] = Field(alias=\"Result\", default_factory=dict)\n\n\ndef get_pinpoint_client(session: boto3.Session | None = None) -> botocore.client.BaseClient:\n    if session is None:\n        session = get_boto_session()\n\n    return session.client(\"pinpoint\")\n\n\ndef send_pinpoint_email_raw(\n    to_address: str,\n    subject: str,\n    message: str,\n    app_id: str,\n    pinpoint_client: botocore.client.BaseClient | None = None,\n) -> PinpointResponse:\n\n    if pinpoint_client is None:\n        pinpoint_client = get_pinpoint_client()\n\n    # Based on: https://docs.aws.amazon.com/code-library/latest/ug/python_3_pinpoint_code_examples.html\n    request = {\n        \"ApplicationId\": app_id,\n        \"MessageRequest\": {\n            \"Addresses\": {to_address: {\"ChannelType\": \"EMAIL\"}},\n            \"MessageConfiguration\": {\n                \"EmailMessage\": {\n                    # TODO - we'll switch this to use templates in the future\n                    #        so keeping this simple with html/text the same\n                    \"SimpleEmail\": {\n                        \"Subject\": {\"Charset\": \"UTF-8\", \"Data\": subject},\n                        \"HtmlPart\": {\"Charset\": \"UTF-8\", \"Data\": message},\n                        \"TextPart\": {\"Charset\": \"UTF-8\", \"Data\": message},\n                    }\n                }\n            },\n        },\n    }\n    # If we are running locally (or in unit tests), don't actually query\n    # AWS - unlike our other AWS integrations, there is no mocking support yet\n    # for Pinpoint, so we built something ourselves that also works when run locally\n    if is_local_aws():\n        return _handle_mock_response(request, to_address)\n\n    try:\n        raw_response = pinpoint_client.send_messages(**request)\n    except ClientError:\n        logger.exception(\"Failed to send email\")\n        raise\n\n    return PinpointResponse.model_validate(raw_response[\"MessageResponse\"])\n\n\n_mock_responses: list[tuple[dict, PinpointResponse]] = []\n\n\ndef _handle_mock_response(request: dict, to_address: str) -> PinpointResponse:\n    # By default, return a response that roughly looks like a real success\n    response = PinpointResponse(\n        Result={\n            to_address: PinpointResult(\n                DeliveryStatus=\"SUCCESSFUL\", StatusCode=200, StatusMessage=str(uuid.uuid4())\n            )\n        }\n    )\n    global _mock_responses\n    _mock_responses.append((request, response))\n\n    return response\n\n\ndef _clear_mock_responses() -> None:\n    global _mock_responses\n    _mock_responses = []\n\n\ndef _get_mock_responses() -> list[tuple[dict, PinpointResponse]]:\n    return _mock_responses"}
{"path":"api/src/search/backend/load_search_data_blueprint.py","language":"python","type":"code","directory":"api/src/search/backend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/backend/load_search_data_blueprint.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/aws/s3_adapter.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/aws\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/aws/s3_adapter.py\nSize: 1.33 KB\nLast Modified: 2025-02-14T17:08:26.433Z"}
{"path":"api/src/search/search_config.py","language":"python","type":"code","directory":"api/src/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/search_config.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"from src.adapters.aws import get_boto_session\nfrom src.util.env_config import PydanticBaseEnvConfig\n\n\nclass S3Config(PydanticBaseEnvConfig):\n    # We should generally not need to set this except\n    # locally to use localstack\n    s3_endpoint_url: str | None = None\n    presigned_s3_duration: int = 7200  # 2 hours in seconds\n\n    ### S3 Buckets\n    # note that we default these to None\n    # so that we don't need to set all of these for every\n    # process that uses S3\n\n    # Note these env vars get set as \"s3://...\"\n    public_files_bucket_path: str = Field(alias=\"PUBLIC_FILES_BUCKET\")\n    draft_files_bucket_path: str = Field(alias=\"DRAFT_FILES_BUCKET\")\n\n\ndef get_s3_client(\n    s3_config: S3Config | None = None,\n    session: boto3.Session | None = None,\n    boto_config: botocore.config.Config | None = None,\n) -> botocore.client.BaseClient:\n    if s3_config is None:\n        s3_config = S3Config()\n\n    params = {}\n    if s3_config.s3_endpoint_url is not None:\n        params[\"endpoint_url\"] = s3_config.s3_endpoint_url\n\n    if boto_config is None:\n        boto_config = botocore.config.Config(signature_version=\"s3v4\")\n\n    params[\"config\"] = boto_config\n\n    if session is None:\n        session = get_boto_session()\n\n    return session.client(\"s3\", **params)"}
{"path":"api/src/search/search_models.py","language":"python","type":"code","directory":"api/src/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/search_models.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/db/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/db\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/__init__.py\nSize: 1.07 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/services/agencies_v1/get_agencies.py","language":"python","type":"code","directory":"api/src/services/agencies_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/agencies_v1/get_agencies.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"This module contains the DBClient class, which is used to manage database connections.\nThis module can be used on it's own or with an application framework such as Flask.\n\nTo use this module with Flask, use the flask_db module.\n\nUsage:\n    import src.adapters.db as db\n\n    db_client = db.PostgresDBClient()\n\n    # non-ORM style usage\n    with db_client.get_connection() as conn:\n        conn.execute(...)\n\n    # ORM style usage\n    with db_client.get_session() as session:\n        session.query(...)\n        with session.begin():\n            session.add(...)\n\"\"\"\n\n# Re-export for convenience\nfrom src.adapters.db.client import Connection, DBClient, Session\nfrom src.adapters.db.clients.postgres_client import PostgresDBClient\nfrom src.adapters.db.clients.postgres_config import PostgresDBConfig\n\n# Do not import flask_db here, because this module is not dependent on any specific framework.\n# Code can choose to use this module on its own or with the flask_db module depending on needs.\n\n__all__ = [\"Connection\", \"DBClient\", \"Session\", \"PostgresDBClient\", \"PostgresDBConfig\"]"}
{"path":"api/src/services/extracts_v1/get_extracts.py","language":"python","type":"code","directory":"api/src/services/extracts_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/extracts_v1/get_extracts.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/db/client.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/db\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/client.py\nSize: 2.05 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/services/opportunities_v1/__init__.py","language":"python","type":"code","directory":"api/src/services/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunities_v1/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"For usage information look at the package docstring in __init__.py\n\"\"\"\n\nimport abc\nimport logging\n\nimport sqlalchemy\nfrom sqlalchemy.orm import session\n\n# Re-export the Connection type that is returned by the get_connection() method\n# to be used for type hints.\nConnection = sqlalchemy.engine.Connection\n\n# Re-export the Session type that is returned by the get_session() method\n# to be used for type hints.\nSession = session.Session\n\nlogger = logging.getLogger(__name__)\n\n\nclass DBClient(abc.ABC, metaclass=abc.ABCMeta):\n    \"\"\"Database connection manager.\n\n    This class is used to manage database connections for the Flask app.\n    It has methods for getting a new connection or session object.\n\n    A derived class must initialize _engine in the __init__ function\n    \"\"\"\n\n    _engine: sqlalchemy.engine.Engine\n\n    @abc.abstractmethod\n    def check_db_connection(self) -> None:\n        raise NotImplementedError()\n\n    def get_connection(self) -> Connection:\n        \"\"\"Return a new database connection object.\n\n        Use the connection to execute SQL queries without using the ORM.\n\n        Usage:\n            with db.get_connection() as conn:\n                conn.execute(...)\n        \"\"\"\n        return self._engine.connect()\n\n    def get_session(self) -> Session:\n        \"\"\"Return a new session object.\n\n        In general, only one session object should be created per request.\n\n        If you want to automatically commit or rollback the session, use\n        the session.begin() context manager.\n        See https://docs.sqlalchemy.org/en/13/orm/session_basics.html#when-do-i-construct-a-session-when-do-i-commit-it-and-when-do-i-close-it\n\n        Example:\n            with db.get_session() as session:\n                with session.begin():\n                    session.add(...)\n                # session is automatically committed here\n                # or rolled back if an exception is raised\n        \"\"\"\n        return Session(bind=self._engine, expire_on_commit=False, autocommit=False)"}
{"path":"api/src/services/opportunities_v1/experimental_constant.py","language":"python","type":"code","directory":"api/src/services/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunities_v1/experimental_constant.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/db/clients/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/db/clients\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/clients/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/services/opportunities_v1/get_opportunity.py","language":"python","type":"code","directory":"api/src/services/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunities_v1/get_opportunity.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":""}
{"path":"api/src/services/opportunities_v1/opportunity_to_csv.py","language":"python","type":"code","directory":"api/src/services/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunities_v1/opportunity_to_csv.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/db/clients/postgres_client.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/db/clients\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/clients/postgres_client.py\nSize: 4.47 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/services/opportunities_v1/search_opportunities.py","language":"python","type":"code","directory":"api/src/services/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunities_v1/search_opportunities.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"import boto3\nimport psycopg\nimport sqlalchemy\nimport sqlalchemy.pool as pool\n\nfrom src.adapters.db.client import DBClient\nfrom src.adapters.db.clients.postgres_config import PostgresDBConfig, get_db_config\n\nlogger = logging.getLogger(__name__)\n\n\nclass PostgresDBClient(DBClient):\n    \"\"\"\n    An implementation of a DBClient for connecting to a Postgres DB\n    as configured by parameters passed in from the db_config\n    \"\"\"\n\n    def __init__(self, db_config: PostgresDBConfig | None = None) -> None:\n        if not db_config:\n            db_config = get_db_config()\n        self._engine = self._configure_engine(db_config)\n\n        if db_config.check_connection_on_init:\n            self.check_db_connection()\n\n    def _configure_engine(self, db_config: PostgresDBConfig) -> sqlalchemy.engine.Engine:\n        # We want to be able to control the connection parameters for each\n        # connection because for IAM authentication with RDS, short-lived tokens are\n        # used as the password, and so we potentially need to generate a fresh token\n        # for each connection.\n        #\n        # For more details on building connection pools, see the docs:\n        # https://docs.sqlalchemy.org/en/13/core/pooling.html#constructing-a-pool\n        def get_conn() -> Any:\n            return psycopg.connect(**get_connection_parameters(db_config))\n\n        conn_pool = pool.QueuePool(get_conn, max_overflow=10, pool_size=20)\n\n        # The URL only needs to specify the dialect, since the connection pool\n        # handles the actual connections.\n        #\n        # (a SQLAlchemy Engine represents a Dialect+Pool)\n        return sqlalchemy.create_engine(\n            \"postgresql+psycopg://\",\n            pool=conn_pool,\n            hide_parameters=db_config.hide_sql_parameter_logs,\n            execution_options={\"schema_translate_map\": db_config.get_schema_translate_map()},\n            # TODO: Don't think we need this as we aren't using JSON columns, but keeping for reference\n            # json_serializer=lambda o: json.dumps(o, default=pydantic.json.pydantic_encoder),\n        )\n\n    def check_db_connection(self) -> None:\n        with self.get_connection() as conn:\n            conn_info = conn.connection.dbapi_connection.info  # type: ignore\n\n            logger.info(\n                \"connected to postgres db\",\n                extra={\n                    \"dbname\": conn_info.dbname,\n                    \"user\": conn_info.user,\n                    \"host\": conn_info.host,\n                    \"port\": conn_info.port,\n                    \"options\": conn_info.options,\n                    \"dsn_parameters\": conn_info.dsn,\n                    \"protocol_version\": conn_info.pgconn.protocol_version,\n                    \"server_version\": conn_info.server_version,\n                },\n            )\n            verify_ssl(conn_info)\n\n            # TODO add check_migrations_current to config\n            # if check_migrations_current:\n            #     have_all_migrations_run(engine)\n\n\ndef get_connection_parameters(db_config: PostgresDBConfig) -> dict[str, Any]:\n    connect_args: dict[str, Any] = {}\n\n    if db_config.password is None:\n        assert (\n            db_config.aws_region is not None\n        ), \"AWS region needs to be configured for DB IAM auth if DB password is not configured\"\n        password = generate_iam_auth_token(\n            db_config.aws_region, db_config.host, db_config.port, db_config.username\n        )\n    else:\n        password = db_config.password\n\n    return dict(\n        host=db_config.host,\n        dbname=db_config.name,\n        user=db_config.username,\n        password=password,\n        port=db_config.port,\n        connect_timeout=10,\n        sslmode=db_config.ssl_mode,\n        **connect_args,\n    )\n\n\ndef generate_iam_auth_token(aws_region: str, host: str, port: int, user: str) -> str:\n    logger.info(\n        \"generating db iam auth token\",\n        extra={\n            \"aws_region\": aws_region,\n            \"user\": user,\n            \"host\": host,\n            \"port\": port,\n        },\n    )\n    client = boto3.client(\"rds\", region_name=aws_region)\n    token = client.generate_db_auth_token(\n        DBHostname=host, Port=port, DBUsername=user, Region=aws_region\n    )\n    return token\n\n\ndef verify_ssl(connection_info: Any) -> None:\n    \"\"\"Verify that the database connection is encrypted and log a warning if not.\"\"\"\n    if connection_info.pgconn.ssl_in_use:\n        logger.info(\"database connection is using SSL\")\n    else:\n        logger.warning(\"database connection is not using SSL\")"}
{"path":"api/src/services/opportunity_attachments/__init__.py","language":"python","type":"code","directory":"api/src/services/opportunity_attachments","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunity_attachments/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/db/clients/postgres_config.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/db/clients\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/clients/postgres_config.py\nSize: 1.54 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/services/opportunity_attachments/attachment_util.py","language":"python","type":"code","directory":"api/src/services/opportunity_attachments","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunity_attachments/attachment_util.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"from pydantic import Field\n\nfrom src.constants.schema import Schemas\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass PostgresDBConfig(PydanticBaseEnvConfig):\n    check_connection_on_init: bool = Field(True, alias=\"DB_CHECK_CONNECTION_ON_INIT\")\n    aws_region: Optional[str] = Field(None, alias=\"AWS_REGION\")\n    host: str = Field(alias=\"DB_HOST\")\n    name: str = Field(alias=\"DB_NAME\")\n    username: str = Field(alias=\"DB_USER\")\n    password: Optional[str] = Field(None, alias=\"DB_PASSWORD\")\n    port: int = Field(5432, alias=\"DB_PORT\")\n    hide_sql_parameter_logs: bool = Field(True, alias=\"HIDE_SQL_PARAMETER_LOGS\")\n    ssl_mode: str = Field(\"require\", alias=\"DB_SSL_MODE\")\n\n    schema_prefix_override: str | None = Field(None)\n\n    def get_schema_translate_map(self) -> dict[str, str]:\n        prefix = \"\"\n        if self.schema_prefix_override is not None:\n            prefix = self.schema_prefix_override\n\n        return {schema: f\"{prefix}{schema}\" for schema in Schemas}\n\n\ndef get_db_config() -> PostgresDBConfig:\n    db_config = PostgresDBConfig()\n\n    logger.info(\n        \"Constructed database configuration\",\n        extra={\n            \"host\": db_config.host,\n            \"dbname\": db_config.name,\n            \"username\": db_config.username,\n            \"password\": \"***\" if db_config.password is not None else None,\n            \"port\": db_config.port,\n            \"hide_sql_parameter_logs\": db_config.hide_sql_parameter_logs,\n        },\n    )\n\n    return db_config"}
{"path":"api/src/services/service_utils.py","language":"python","type":"code","directory":"api/src/services","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/service_utils.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/db/flask_db.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/db\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/flask_db.py\nSize: 3.60 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/services/users/README.md","language":"markdown","type":"code","directory":"api/src/services/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.126Z","content":"To initialize this flask extension, call register_db_client() with an instance\nof a Flask app and an instance of a DBClient.\n\nExample:\n    import src.adapters.db as db\n    import src.adapters.db.flask_db as flask_db\n\n    db_client = db.PostgresDBClient()\n    app = APIFlask(__name__)\n    flask_db.register_db_client(db_client, app)\n\nThen, in a request handler, use the with_db_session decorator to get a\nnew database session that lasts for the duration of the request.\n\nExample:\n    import src.adapters.db as db\n    import src.adapters.db.flask_db as flask_db\n\n    @app.route(\"/health\")\n    @flask_db.with_db_session\n    def health(db_session: db.Session):\n        with db_session.begin():\n            ...\n\n\nAlternatively, if you want to get the database client directly, use the get_db\nfunction.\n\nExample:\n    from flask import current_app\n    import src.adapters.db.flask_db as flask_db\n\n    @app.route(\"/health\")\n    def health():\n        db_client = flask_db.get_db(current_app)\n        # db_client.get_connection() or db_client.get_session()\n\"\"\"\n\nfrom functools import wraps\nfrom typing import Callable, Concatenate, ParamSpec, TypeVar\n\nfrom flask import Flask, current_app\n\nimport src.adapters.db as db\nfrom src.adapters.db.client import DBClient\n\n_FLASK_EXTENSION_KEY_PREFIX = \"db\"\n_DEFAULT_CLIENT_NAME = \"default\"\n\n\ndef register_db_client(\n    db_client: DBClient, app: Flask, client_name: str = _DEFAULT_CLIENT_NAME\n) -> None:\n    \"\"\"Initialize the Flask app.\n\n    Add the database to the Flask app's extensions so that it can be\n    accessed by request handlers using the current app context.\n\n    If you use multiple DB clients, you can differentiate them by\n    specifying a client_name.\n\n    see get_db\n    \"\"\"\n    flask_extension_key = f\"{_FLASK_EXTENSION_KEY_PREFIX}{client_name}\"\n    app.extensions[flask_extension_key] = db_client\n\n\ndef get_db(app: Flask, client_name: str = _DEFAULT_CLIENT_NAME) -> DBClient:\n    \"\"\"Get the database connection for the given Flask app.\n\n    Use this in request handlers to access the database from the active Flask app.\n\n    Specify the same client_name as used in register_db_client to get the correct client\n\n    Example:\n        from flask import current_app\n        import src.adapters.db.flask_db as flask_db\n\n        @app.route(\"/health\")\n        def health():\n            db_client = flask_db.get_db(current_app)\n    \"\"\"\n    flask_extension_key = f\"{_FLASK_EXTENSION_KEY_PREFIX}{client_name}\"\n    return app.extensions[flask_extension_key]\n\n\nP = ParamSpec(\"P\")\nT = TypeVar(\"T\")\n\n\ndef with_db_session(\n    *, client_name: str = _DEFAULT_CLIENT_NAME\n) -> Callable[[Callable[Concatenate[db.Session, P], T]], Callable[P, T]]:\n    \"\"\"Decorator for functions that need a database session.\n\n    This decorator will create a new session object and pass it to the function\n    as the first positional argument. A transaction is not started automatically.\n    To start a transaction use db_session.begin()\n\n    Usage:\n        @with_db_session()\n        def foo(db_session: db.Session):\n            ...\n\n        @with_db_session()\n        def bar(db_session: db.Session, x, y):\n            ...\n\n        @with_db_session(client_name=\"legacy_db\")\n        def fiz(db_session: db.Session, x, y, z):\n            ...\n    \"\"\"\n\n    def decorator(f: Callable[Concatenate[db.Session, P], T]) -> Callable[P, T]:\n        @wraps(f)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\n            with get_db(current_app, client_name=client_name).get_session() as session:\n                return f(session, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"path":"api/src/services/users/__init__.py","language":"python","type":"code","directory":"api/src/services/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/db/type_decorators/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/db/type_decorators\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/type_decorators/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/services/users/create_saved_search.py","language":"python","type":"code","directory":"api/src/services/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/create_saved_search.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":""}
{"path":"api/src/services/users/delete_saved_opportunity.py","language":"python","type":"code","directory":"api/src/services/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/delete_saved_opportunity.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/db/type_decorators/postgres_type_decorators.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/db/type_decorators\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/db/type_decorators/postgres_type_decorators.py\nSize: 1.55 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/services/users/delete_saved_search.py","language":"python","type":"code","directory":"api/src/services/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/delete_saved_search.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"from sqlalchemy import Integer\nfrom sqlalchemy.types import TypeDecorator\n\nfrom src.db.models.lookup import LookupRegistry, LookupTable\n\n\nclass LookupColumn(TypeDecorator):\n    \"\"\"\n    A Postgres column decorator that wraps\n    an integer column representing a lookup int.\n\n    This takes in the LookupTable that the lookup value\n    is stored in, and handles converting the Lookup object\n    in-code into the integer in the DB automatically (and the reverse).\n    \"\"\"\n\n    impl = Integer\n\n    cache_ok = True\n\n    def __init__(self, lookup_table: Type[LookupTable], *args: Any, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n        self.lookup_table = lookup_table\n\n    def process_bind_param(self, value: Any | None, dialect: Any) -> int | None:\n        if value is None:\n            return None\n\n        if not LookupRegistry.is_valid_type_for_table(self.lookup_table, value):\n            raise Exception(\n                f\"Cannot convert value of type {type(value)} for binding column in table {self.lookup_table.get_table_name()}\"\n            )\n\n        return LookupRegistry.get_lookup_int_for_enum(self.lookup_table, value)\n\n    def process_result_value(self, value: Any | None, dialect: Any) -> Any | None:\n        if value is None:\n            return None\n\n        if not isinstance(value, int):\n            raise Exception(\n                f\"Cannot process value from DB of type {type(value)} in table {self.lookup_table.get_table_name()}\"\n            )\n\n        return LookupRegistry.get_enum_for_lookup_int(self.lookup_table, value)"}
{"path":"api/src/services/users/get_saved_opportunities.py","language":"python","type":"code","directory":"api/src/services/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/get_saved_opportunities.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/newrelic/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/newrelic\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/newrelic/__init__.py\nSize: 0.34 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/services/users/get_saved_searches.py","language":"python","type":"code","directory":"api/src/services/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/get_saved_searches.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"import newrelic.agent\n\nlogger = logging.getLogger(__name__)\n\n\ndef init_newrelic() -> None:\n    logger.info(\"Initializing New Relic\")\n    newrelic.agent.initialize(\n        config_file=os.path.join(os.path.dirname(__file__), \"../../..\", \"newrelic.ini\"),\n        environment=os.environ.get(\"ENVIRONMENT\", \"local\"),\n    )"}
{"path":"api/src/services/users/get_user.py","language":"python","type":"code","directory":"api/src/services/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/get_user.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/newrelic/events.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/newrelic\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/newrelic/events.py\nSize: 2.62 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/services/users/login_gov_callback_handler.py","language":"python","type":"code","directory":"api/src/services/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/login_gov_callback_handler.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"import flask\nimport newrelic.agent\n\nlogger = logging.getLogger(__name__)\n\nAGENT_ATTRIBUTE_LIMIT = 128\nEVENT_API_ATTRIBUTE_LIMIT = 255\n\n\ndef record_custom_event(event_type: str, params: dict[Any, Any]) -> None:\n    params[\"api.request.path\"] = params.get(\"request.path\")\n    params[\"api.request.method\"] = params.get(\"request.method\")\n    params[\"api.request.url_rule\"] = params.get(\"request.url_rule\")\n    params[\"api.request.id\"] = params.get(\"request.id\")\n\n    if flask.has_request_context():\n        params[\"api.request.internal_id\"] = getattr(flask.g, \"internal_request_id\", None)\n\n    # If there are more custom attributes than the limit, the agent will upload\n    # a partial payload, dropping keys after hitting its limit\n    attribute_count = len(params)\n    if attribute_count > AGENT_ATTRIBUTE_LIMIT:\n        logger.warning(\n            \"Payload exceeds New Relic Agent event attribute limit. Partial data likely present. Check CloudWatch for complete event data.\",\n            extra={\n                \"event_type\": event_type,\n                \"total_attribute_count\": attribute_count,\n            },\n        )\n        logger.info(event_type, extra=params)\n\n    # https://docs.newrelic.com/docs/apm/agents/python-agent/python-agent-api/recordcustomevent-python-agent-api/\n    newrelic.agent.record_custom_event(\n        event_type,\n        params,\n    )\n\n\ndef log_and_capture_exception(msg: str, extra: dict | None = None, only_warn: bool = False) -> None:\n    \"\"\"\n    Sometimes we want to record custom messages that do not match an exception message. For example, when a ValidationError contains\n    multiple issues, we want to log and record each one individually in New Relic. Injecting a new exception with a\n    human-readable error message is the only way for errors to receive custom messages.\n    This does not affect the traceback or any other visible attribute in New Relic. Everything else, including\n    the original exception class name, is retained and displayed.\n    \"\"\"\n\n    info = sys.exc_info()\n    info_with_readable_msg: BaseException | tuple[type, BaseException, TracebackType | None]\n\n    if info[0] is None:\n        exc = Exception(msg)\n        info_with_readable_msg = (type(exc), exc, exc.__traceback__)\n    else:\n        info_with_readable_msg = (info[0], Exception(msg), info[2])\n\n    if only_warn:\n        logger.warning(msg, extra=extra, exc_info=info_with_readable_msg)\n    else:\n        logger.error(msg, extra=extra, exc_info=info_with_readable_msg)\n\n    newrelic.agent.notice_error(\n        error=info_with_readable_msg,\n        attributes=extra,\n    )"}
{"path":"api/src/services/users/update_saved_searches.py","language":"python","type":"code","directory":"api/src/services/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/update_saved_searches.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/oauth/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/oauth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/oauth/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/static/swagger-ui-bundle.js","language":"javascript","type":"code","directory":"api/src/static","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/static/swagger-ui-bundle.js","size":1644826,"lastModified":"2025-02-14T17:08:31.126Z","content":""}
{"path":"api/src/static/swagger-ui-standalone-preset.js","language":"javascript","type":"code","directory":"api/src/static","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/static/swagger-ui-standalone-preset.js","size":1644826,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/oauth/login_gov/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/oauth/login_gov\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/oauth/login_gov/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/task/__init__.py","language":"python","type":"code","directory":"api/src/task","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":""}
{"path":"api/src/task/analytics/__init__.py","language":"python","type":"code","directory":"api/src/task/analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/analytics/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/oauth/login_gov/login_gov_oauth_client.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/oauth/login_gov\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/oauth/login_gov/login_gov_oauth_client.py\nSize: 1.67 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/task/analytics/create_analytics_db_csvs.py","language":"python","type":"code","directory":"api/src/task/analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/analytics/create_analytics_db_csvs.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"import requests\n\nfrom src.adapters.oauth.oauth_client import BaseOauthClient\nfrom src.adapters.oauth.oauth_client_models import OauthTokenRequest, OauthTokenResponse\nfrom src.auth.login_gov_jwt_auth import LoginGovConfig, get_config\n\n\nclass LoginGovOauthClient(BaseOauthClient):\n\n    def __init__(self, config: LoginGovConfig | None = None):\n        if config is None:\n            config = get_config()\n\n        self.config = config\n        self.session = self._build_session()\n\n    def _build_session(self, session: requests.Session | None = None) -> requests.Session:\n        \"\"\"Set things on the session that should be shared between all requests\"\"\"\n        if not session:\n            session = requests.Session()\n\n        session.headers.update({\"Content-Type\": \"application/x-www-form-urlencoded\"})\n\n        return session\n\n    def _request(self, method: str, full_url: str, **kwargs: Any) -> requests.Response:\n        \"\"\"Utility method for making a request with our session\"\"\"\n\n        # By default timeout after 5 seconds\n        if \"timeout\" not in kwargs:\n            kwargs[\"timeout\"] = 5\n\n        return self.session.request(method, full_url, **kwargs)\n\n    def get_token(self, request: OauthTokenRequest) -> OauthTokenResponse:\n        \"\"\"Query the login.gov token endpoint\"\"\"\n\n        body = {\n            \"code\": request.code,\n            \"grant_type\": request.grant_type,\n            \"client_assertion\": request.client_assertion,\n            \"client_assertion_type\": request.client_assertion_type,\n        }\n\n        response = self._request(\"POST\", self.config.login_gov_token_endpoint, data=body)\n\n        return OauthTokenResponse.model_validate_json(response.text)"}
{"path":"api/src/task/ecs_background_task.py","language":"python","type":"code","directory":"api/src/task","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/ecs_background_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/oauth/login_gov/mock_login_gov_oauth_client.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/oauth/login_gov\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/oauth/login_gov/mock_login_gov_oauth_client.py\nSize: 0.73 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/task/notifications/generate_notifications.py","language":"python","type":"code","directory":"api/src/task/notifications","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/notifications/generate_notifications.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"class MockLoginGovOauthClient(BaseOauthClient):\n\n    def __init__(self) -> None:\n        self.responses: dict[str, OauthTokenResponse] = {}\n\n    def add_token_response(self, code: str, response: OauthTokenResponse) -> None:\n        self.responses[code] = response\n\n    def get_token(self, request: OauthTokenRequest) -> OauthTokenResponse:\n        response = self.responses.get(request.code, None)\n\n        if response is None:\n            response = OauthTokenResponse(\n                error=\"error\", error_description=\"default mock error description\"\n            )\n\n        return response"}
{"path":"api/src/task/opportunities/__init__.py","language":"python","type":"code","directory":"api/src/task/opportunities","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/opportunities/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/oauth/oauth_client.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/oauth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/oauth/oauth_client.py\nSize: 0.37 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/task/opportunities/export_opportunity_data_task.py","language":"python","type":"code","directory":"api/src/task/opportunities","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/opportunities/export_opportunity_data_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"from src.adapters.oauth.oauth_client_models import OauthTokenRequest, OauthTokenResponse\n\n\nclass BaseOauthClient(abc.ABC, metaclass=abc.ABCMeta):\n\n    @abc.abstractmethod\n    def get_token(self, request: OauthTokenRequest) -> OauthTokenResponse:\n        \"\"\"Call the POST token endpoint\n\n        See: https://developers.login.gov/oidc/token/\n        \"\"\"\n        pass"}
{"path":"api/src/task/opportunities/set_current_opportunities_task.py","language":"python","type":"code","directory":"api/src/task/opportunities","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/opportunities/set_current_opportunities_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/oauth/oauth_client_models.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/oauth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/oauth/oauth_client_models.py\nSize: 0.89 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/task/subtask.py","language":"python","type":"code","directory":"api/src/task","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/subtask.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"from pydantic import BaseModel\n\n\n@dataclass\nclass OauthTokenRequest:\n    \"\"\"https://developers.login.gov/oidc/token/#request-parameters\"\"\"\n\n    code: str\n    client_assertion: str\n\n    grant_type: str = \"authorization_code\"\n    client_assertion_type: str = \"urn:ietf:params:oauth:client-assertion-type:jwt-bearer\"\n\n\nclass OauthTokenResponse(BaseModel):\n    \"\"\"https://developers.login.gov/oidc/token/#token-response\"\"\"\n\n    # These fields are given defaults so we don't need None-checks\n    # for them elsewhere, if the response didn't error, they have valid values\n    id_token: str = \"\"\n    access_token: str = \"\"\n    token_type: str = \"\"\n    expires_in: int = 0\n\n    # These fields are only set if the response errored\n    error: str | None = None\n    error_description: str | None = None\n\n    def is_error_response(self) -> bool:\n        return self.error is not None"}
{"path":"api/src/task/task.py","language":"python","type":"code","directory":"api/src/task","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/search/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/search/__init__.py\nSize: 0.28 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/task/task_blueprint.py","language":"python","type":"code","directory":"api/src/task","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/task_blueprint.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"__all__ = [\"SearchClient\", \"get_opensearch_config\", \"SearchQueryBuilder\"]"}
{"path":"api/src/util/__init__.py","language":"python","type":"code","directory":"api/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/search/flask_opensearch.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/search/flask_opensearch.py\nSize: 1.25 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/util/datetime_util.py","language":"python","type":"code","directory":"api/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/datetime_util.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"from flask import Flask, current_app\n\nfrom src.adapters.search import SearchClient\n\n_SEARCH_CLIENT_KEY = \"search-client\"\n\n\ndef register_search_client(search_client: SearchClient, app: Flask) -> None:\n    app.extensions[_SEARCH_CLIENT_KEY] = search_client\n\n\ndef get_search_client(app: Flask) -> SearchClient:\n    return app.extensions[_SEARCH_CLIENT_KEY]\n\n\nP = ParamSpec(\"P\")\nT = TypeVar(\"T\")\n\n\ndef with_search_client() -> Callable[[Callable[Concatenate[SearchClient, P], T]], Callable[P, T]]:\n    \"\"\"\n    Decorator for functions that need a search client.\n\n    This decorator will return the shared search client object which\n    has an internal connection pool that is shared.\n\n    Usage:\n        @with_search_client()\n        def foo(search_client: search.SearchClient):\n            ...\n\n        @with_search_client()\n        def bar(search_client: search.SearchClient, x: int, y: int):\n            ...\n    \"\"\"\n\n    def decorator(f: Callable[Concatenate[SearchClient, P], T]) -> Callable[P, T]:\n        @wraps(f)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\n            return f(get_search_client(current_app), *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"path":"api/src/util/deploy_metadata.py","language":"python","type":"code","directory":"api/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/deploy_metadata.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"File: api/src/adapters/search/opensearch_client.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/search/opensearch_client.py\nSize: 11.37 KB\nLast Modified: 2025-02-14T17:08:26.434Z"}
{"path":"api/src/util/dict_util.py","language":"python","type":"code","directory":"api/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/dict_util.py","size":1631931,"lastModified":"2025-02-14T17:08:31.126Z","content":"import opensearchpy\n\nfrom src.adapters.search.opensearch_config import OpensearchConfig, get_opensearch_config\nfrom src.adapters.search.opensearch_response import SearchResponse\n\nlogger = logging.getLogger(__name__)\n\n# By default, we'll override the default analyzer+tokenization\n# for a search index. You can provide your own when calling create_index\nDEFAULT_INDEX_ANALYSIS = {\n    \"analyzer\": {\n        \"default\": {\n            \"type\": \"custom\",\n            \"filter\": [\"lowercase\", \"custom_stemmer\"],\n            \"tokenizer\": \"standard\",\n        }\n    },\n    # Change the default stemming to use snowball which handles plural\n    # queries better than the default\n    # TODO - there are a lot of stemmers, we should take some time to figure out\n    #        which one works best with our particular dataset. Snowball is really\n    #        basic and naive (literally just adjusting suffixes on words in common patterns)\n    #        which might be fine generally, but we work with a lot of acronyms\n    #        and should verify that doesn't cause any issues.\n    # see: https://opensearch.org/docs/latest/analyzers/token-filters/index/\n    \"filter\": {\"custom_stemmer\": {\"type\": \"snowball\", \"name\": \"english\"}},\n}\n\n\nclass SearchClient:\n    def __init__(self, opensearch_config: OpensearchConfig | None = None) -> None:\n        if opensearch_config is None:\n            opensearch_config = get_opensearch_config()\n\n        # See: https://opensearch.org/docs/latest/clients/python-low-level/ for more details\n        self._client = opensearchpy.OpenSearch(**_get_connection_parameters(opensearch_config))\n\n    def create_index(\n        self,\n        index_name: str,\n        *,\n        shard_count: int = 1,\n        replica_count: int = 1,\n        analysis: dict | None = None,\n    ) -> None:\n        \"\"\"\n        Create an empty search index\n        \"\"\"\n\n        # Allow the user to adjust how the index analyzer + tokenization works\n        # but also have a general default.\n        if analysis is None:\n            analysis = DEFAULT_INDEX_ANALYSIS\n\n        body = {\n            \"settings\": {\n                \"index\": {\"number_of_shards\": shard_count, \"number_of_replicas\": replica_count},\n                \"analysis\": analysis,\n            },\n        }\n\n        logger.info(\"Creating search index %s\", index_name, extra={\"index_name\": index_name})\n        self._client.indices.create(index_name, body=body)\n\n    def delete_index(self, index_name: str) -> None:\n        \"\"\"\n        Delete an index. Can also delete all indexes via a prefix.\n        \"\"\"\n        logger.info(\"Deleting search index %s\", index_name, extra={\"index_name\": index_name})\n        self._client.indices.delete(index=index_name)\n\n    def put_pipeline(self, pipeline: dict, pipeline_name: str) -> None:\n        \"\"\"\n        Create a pipeline\n        \"\"\"\n        resp = self._client.ingest.put_pipeline(id=pipeline_name, body=pipeline)\n        if resp[\"acknowledged\"]:\n            logger.info(f\"Pipeline '{pipeline_name}' created successfully!\")\n        else:\n            status_code = resp[\"status\"] or 500\n            error_message = resp[\"error\"][\"reason\"] or \"Internal Server Error\"\n\n            raise Exception(\n                f\"Failed to create pipeline {pipeline_name}: {error_message}. Status code: {status_code}\"\n            )\n\n    def bulk_upsert(\n        self,\n        index_name: str,\n        records: Iterable[dict[str, Any]],\n        primary_key_field: str,\n        *,\n        refresh: bool = True,\n        pipeline: str | None = None,\n    ) -> None:\n        \"\"\"\n        Bulk upsert records to an index\n\n        See: https://opensearch.org/docs/latest/api-reference/document-apis/bulk/ for details\n        In this method we only use the \"index\" operation which creates or updates a record\n        based on the id value.\n        \"\"\"\n\n        bulk_operations = []\n\n        for record in records:\n            # For each record, we create two entries in the bulk operation list\n            # which include the unique ID + the actual record on separate lines\n            # When this is sent to the search index, this will send two lines like:\n            #\n            # {\"index\": {\"_id\": 123}}\n            # {\"opportunity_id\": 123, \"opportunity_title\": \"example title\", ...}\n            bulk_operations.append({\"index\": {\"_id\": record[primary_key_field]}})\n            bulk_operations.append(record)\n\n        logger.info(\n            \"Upserting records to %s\",\n            index_name,\n            extra={\n                \"index_name\": index_name,\n                \"record_count\": int(len(bulk_operations) / 2),\n                \"operation\": \"update\",\n            },\n        )\n        bulk_args = {\"index\": index_name, \"body\": bulk_operations, \"refresh\": refresh}\n        if pipeline:\n            bulk_args[\"pipeline\"] = pipeline\n\n        self._client.bulk(**bulk_args)\n\n    def bulk_delete(self, index_name: str, ids: Iterable[Any], *, refresh: bool = True) -> None:\n        \"\"\"\n        Bulk delete records from an index\n\n        See: https://opensearch.org/docs/latest/api-reference/document-apis/bulk/ for details.\n        In this method, we delete records based on the IDs passed in.\n        \"\"\"\n        bulk_operations = []\n\n        for _id in ids:\n            # { \"delete\": { \"_id\": \"tt2229499\" } }\n            bulk_operations.append({\"delete\": {\"_id\": _id}})\n\n        logger.info(\n            \"Deleting records from %s\",\n            index_name,\n            extra={\n                \"index_name\": index_name,\n                \"record_count\": len(bulk_operations),\n                \"operation\": \"delete\",\n            },\n        )\n        self._client.bulk(index=index_name, body=bulk_operations, refresh=refresh)\n\n    def index_exists(self, index_name: str) -> bool:\n        \"\"\"\n        Check if an index OR alias exists by a given name\n        \"\"\"\n        return self._client.indices.exists(index_name)\n\n    def alias_exists(self, alias_name: str) -> bool:\n        \"\"\"\n        Check if an alias exists\n        \"\"\"\n        existing_index_mapping = self._client.cat.aliases(alias_name, format=\"json\")\n        return len(existing_index_mapping) > 0\n\n    def cleanup_old_indices(self, index_prefix: str, indexes_to_keep: list[str]) -> None:\n        \"\"\"\n        Cleanup old indexes now that they aren't connected to the alias\n        \"\"\"\n        resp = self._client.cat.indices(f\"{index_prefix}-*\", format=\"json\", h=[\"index\"])\n\n        old_indexes = [\n            index[\"index\"] for index in resp if index[\"index\"] not in indexes_to_keep\n        ]  # omit the newly created one\n\n        for index in old_indexes:\n            self.delete_index(index)\n\n    def swap_alias_index(self, index_name: str, alias_name: str) -> None:\n        \"\"\"\n        For a given index, set it to the given alias. If any existing index(es) are\n        attached to the alias, remove them from the alias.\n\n        This operation is done atomically.\n        \"\"\"\n        extra = {\"index_name\": index_name, \"index_alias\": alias_name}\n        logger.info(\"Swapping index that backs alias %s\", alias_name, extra=extra)\n\n        existing_index_mapping = self._client.cat.aliases(alias_name, format=\"json\")\n        existing_indexes = [i[\"index\"] for i in existing_index_mapping]\n\n        logger.info(\n            \"Found existing indexes\", extra=extra | {\"existing_indexes\": \",\".join(existing_indexes)}\n        )\n\n        actions = [{\"add\": {\"index\": index_name, \"alias\": alias_name}}]\n\n        for index in existing_indexes:\n            actions.append({\"remove\": {\"index\": index, \"alias\": alias_name}})\n\n        self._client.indices.update_aliases({\"actions\": actions})\n\n    def search_raw(self, index_name: str, search_query: dict) -> dict:\n        # Simple wrapper around search if you don't want the request or response\n        # object handled in any special way.\n        return self._client.search(index=index_name, body=search_query)\n\n    def search(\n        self,\n        index_name: str,\n        search_query: dict,\n        include_scores: bool = True,\n        params: dict | None = None,\n        includes: list[str] | None = None,\n        excludes: list[str] | None = None,\n    ) -> SearchResponse:\n        if params is None:\n            params = {}\n\n        response = self._client.search(\n            index=index_name,\n            body=search_query,\n            params=params,\n            _source_includes=includes,\n            _source_excludes=excludes,\n        )\n        return SearchResponse.from_opensearch_response(response, include_scores)\n\n    def scroll(\n        self,\n        index_name: str,\n        search_query: dict,\n        include_scores: bool = True,\n        duration: str = \"10m\",\n    ) -> Generator[SearchResponse, None, None]:\n        \"\"\"\n        Scroll (iterate) over a large result set a given search query.\n\n        This query uses additional resources to keep the response open, but\n        keeps a consistent set of results and is useful for backend processes\n        that need to fetch a large amount of search data. After processing the results,\n        the scroll lock is closed for you.\n\n        This method is setup as a generator method and the results can be iterated over::\n\n            for response in search_client.scroll(\"my_index\", {\"size\": 10000}):\n                for record in response.records:\n                    process_record(record)\n\n\n        See: https://opensearch.org/docs/latest/api-reference/scroll/\n        \"\"\"\n\n        # start scroll\n        response = self.search(\n            index_name=index_name,\n            search_query=search_query,\n            include_scores=include_scores,\n            params={\"scroll\": duration},\n        )\n        scroll_id = response.scroll_id\n\n        yield response\n\n        # iterate\n        while True:\n            raw_response = self._client.scroll({\"scroll_id\": scroll_id, \"scroll\": duration})\n            response = SearchResponse.from_opensearch_response(raw_response, include_scores)\n\n            # The scroll ID can change between queries according to the docs, so we\n            # keep updating the value while iterating in case they change.\n            scroll_id = response.scroll_id\n\n            if len(response.records) == 0:\n                break\n\n            yield response\n\n        # close scroll\n        self._client.clear_scroll(scroll_id=scroll_id)\n\n\ndef _get_connection_parameters(opensearch_config: OpensearchConfig) -> dict[str, Any]:\n    # See: https://opensearch.org/docs/latest/clients/python-low-level/#connecting-to-opensearch\n    # for further details on configuring the connection to OpenSearch\n    params = dict(\n        hosts=[{\"host\": opensearch_config.search_endpoint, \"port\": opensearch_config.search_port}],\n        http_compress=True,\n        use_ssl=opensearch_config.search_use_ssl,\n        verify_certs=opensearch_config.search_verify_certs,\n        connection_class=opensearchpy.RequestsHttpConnection,\n        pool_maxsize=opensearch_config.search_connection_pool_size,\n    )\n\n    # We'll assume if the aws_region is set, we're running in AWS\n    # and should connect using the session credentials\n    if opensearch_config.aws_region:\n        # Get credentials and authorize with AWS Opensearch Serverless (es)\n        # TODO - once we have the user setup in Opensearch, we want to change to this approach\n        # credentials = boto3.Session().get_credentials()\n        # auth = opensearchpy.AWSV4SignerAuth(credentials, opensearch_config.aws_region, \"es\")\n        auth = (opensearch_config.search_username, opensearch_config.search_password)\n        params[\"http_auth\"] = auth\n\n    return params"}
{"path":"api/src/util/env_config.py","language":"python","type":"code","directory":"api/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/env_config.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/adapters/search/opensearch_config.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/search/opensearch_config.py\nSize: 1.29 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/src/util/file_util.py","language":"python","type":"code","directory":"api/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/file_util.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"from pydantic import Field\n\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass OpensearchConfig(PydanticBaseEnvConfig):\n\n    search_endpoint: str = Field(default=\"NOT_DEFINED\")  # SEARCH_ENDPOINT\n    search_port: int = Field(default=443)  # SEARCH_PORT\n\n    search_username: str | None = Field(default=None)  # SEARCH_USERNAME\n    search_password: str | None = Field(default=None)  # SEARCH_PASSWORD\n\n    search_use_ssl: bool = Field(default=True)  # SEARCH_USE_SSL\n    search_verify_certs: bool = Field(default=True)  # SEARCH_VERIFY_CERTS\n    search_connection_pool_size: int = Field(default=10)  # SEARCH_CONNECTION_POOL_SIZE\n\n    aws_region: str | None = Field(default=None)\n\n\ndef get_opensearch_config() -> OpensearchConfig:\n    opensearch_config = OpensearchConfig()\n\n    logger.info(\n        \"Constructed opensearch configuration\",\n        extra={\n            \"search_endpoint\": opensearch_config.search_endpoint,\n            \"search_port\": opensearch_config.search_port,\n            \"search_use_ssl\": opensearch_config.search_use_ssl,\n            \"search_verify_certs\": opensearch_config.search_verify_certs,\n            \"search_connection_pool_size\": opensearch_config.search_connection_pool_size,\n        },\n    )\n\n    return opensearch_config"}
{"path":"api/src/util/local.py","language":"python","type":"code","directory":"api/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/local.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/adapters/search/opensearch_query_builder.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/search/opensearch_query_builder.py\nSize: 10.35 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/src/util/string_utils.py","language":"python","type":"code","directory":"api/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/string_utils.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"from src.pagination.pagination_models import SortDirection\n\n\nclass SearchQueryBuilder:\n    \"\"\"\n    Utility to help build queries to OpenSearch\n\n    This helps with making sure everything we want in a search query goes\n    to the right spot in the large JSON object we're building. Note that\n    it still requires some understanding of OpenSearch (eg. when to add \".keyword\" to a field name)\n\n    For example, if you wanted to build a query against a search index containing\n    books with the following:\n        * Page size of 5, page number 1\n        * Sorted by relevancy score descending\n        * Scored on titles containing \"king\"\n        * Where the author is one of Brandon Sanderson or J R.R. Tolkien\n        * With a page count between 300 and 1000\n        * Returning aggregate counts of books by those authors in the full results\n\n    This query could either be built manually and look like:\n\n    {\n      \"size\": 5,\n      \"from\": 0,\n      \"track_scores\": true,\n      \"sort\": [\n        {\n          \"_score\": {\n            \"order\": \"desc\"\n          }\n        }\n      ],\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"simple_query_string\": {\n                \"query\": \"king\",\n                \"fields\": [\n                  \"title.keyword\"\n                ],\n                \"default_operator\": \"AND\"\n              }\n            }\n          ],\n          \"filter\": [\n            {\n              \"terms\": {\n                \"author.keyword\": [\n                  \"Brandon Sanderson\",\n                  \"J R.R. Tolkien\"\n                ]\n              },\n              \"range\": {\n                \"publication_date\": {\n                    \"gte\": 300,\n                    \"lte\": 1000\n                }\n              }\n            }\n          ]\n        }\n      },\n      \"aggs\": {\n        \"author\": {\n          \"terms\": {\n            \"field\": \"author.keyword\",\n            \"size\": 25,\n            \"min_doc_count\": 0\n          }\n        }\n      }\n    }\n\n    Or you could use the builder and produce the same result:\n\n    search_query = SearchQueryBuilder()\n                .pagination(page_size=5, page_number=1)\n                .sort_by([(\"relevancy\", SortDirection.DESCENDING)])\n                .simple_query(\"king\", fields=[\"title.keyword\"])\n                .filter_terms(\"author.keyword\", terms=[\"Brandon Sanderson\", \"J R.R. Tolkien\"])\n                .filter_int_range(\"page_count\", 300, 1000)\n                .aggregation_terms(aggregation_name=\"author\", field_name=\"author.keyword\", minimum_count=0)\n                .build()\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.page_size = 25\n        self.page_number = 1\n\n        self.sort_values: list[dict[str, dict[str, str]]] = []\n\n        self._track_total_hits: bool = True\n\n        self.must: list[dict] = []\n        self.filters: list[dict] = []\n\n        self.aggregations: dict[str, dict] = {}\n\n    def pagination(self, page_size: int, page_number: int) -> typing.Self:\n        \"\"\"\n        Set the pagination for the search request.\n\n        Note that page number should be the human-readable page number\n        and start counting from 1.\n        \"\"\"\n        self.page_size = page_size\n        self.page_number = page_number\n        return self\n\n    def sort_by(self, sort_values: list[typing.Tuple[str, SortDirection]]) -> typing.Self:\n        \"\"\"\n        List of tuples of field name + sort direction to sort by. If you wish to sort by the relevancy\n        score provide a field name of \"relevancy\".\n\n        The order of the tuples matters, and the earlier values will take precedence - or put another way\n        the first tuple is the \"primary sort\", the second is the \"secondary sort\", and so on. If\n        all of the primary sort values are unique, then the secondary sorts won't be relevant.\n\n        If this method is not called, no sort info will be added to the request, and OpenSearch\n        will internally default to sorting by relevancy score. If there is no scores calculated,\n        then the order is likely the IDs of the documents in the index.\n\n        Note that multiple calls to this method will erase any info provided in a prior call.\n        \"\"\"\n        for field, sort_direction in sort_values:\n            if field == \"relevancy\":\n                field = \"_score\"\n\n            self.sort_values.append({field: {\"order\": sort_direction.short_form()}})\n\n        return self\n\n    def track_total_hits(self, track_total_hits: bool) -> typing.Self:\n        \"\"\"\n        Whether or not to track the total number of hits in the response accurately.\n\n        By default OpenSearch will stop counting after 10k records are counted.\n        \"\"\"\n        self._track_total_hits = track_total_hits\n        return self\n\n    def simple_query(self, query: str, fields: list[str], query_operator: str) -> typing.Self:\n        \"\"\"\n        Adds a simple_query_string which queries against the provided fields.\n\n        The fields must include the full path to the object, and can include optional suffixes\n        to adjust the weighting. For example \"opportunity_title^4\" would increase any scores\n        derived from that field by 4x.\n\n        See: https://opensearch.org/docs/latest/query-dsl/full-text/simple-query-string/\n        \"\"\"\n        self.must.append(\n            {\n                \"simple_query_string\": {\n                    \"query\": query,\n                    \"fields\": fields,\n                    \"default_operator\": query_operator,\n                }\n            }\n        )\n\n        return self\n\n    def filter_terms(self, field: str, terms: list) -> typing.Self:\n        \"\"\"\n        For a given field, filter to a set of values.\n\n        These filters do not affect the relevancy score, they are purely\n        a binary filter on the overall results.\n        \"\"\"\n        self.filters.append({\"terms\": {field: terms}})\n        return self\n\n    def filter_int_range(\n        self, field: str, min_value: int | None, max_value: int | None\n    ) -> typing.Self:\n        \"\"\"\n        For a given field, filter results to a range of integer values.\n\n        If min or max is not provided, the range is unbounded and only\n        affects the minimum or maximum possible value. At least one min or max value must be specified.\n\n        These filters do not affect the relevancy score, they are purely\n        a binary filter on the overall results.\n        \"\"\"\n        if min_value is None and max_value is None:\n            raise ValueError(\"Cannot use int range filter if both min and max are None\")\n\n        range_filter = {}\n        if min_value is not None:\n            range_filter[\"gte\"] = min_value\n        if max_value is not None:\n            range_filter[\"lte\"] = max_value\n\n        self.filters.append({\"range\": {field: range_filter}})\n        return self\n\n    def adjust_date_format(self, in_date: datetime.date | int | None) -> str | None:\n        if in_date is None:\n            return None\n        if isinstance(in_date, int):\n            return f\"now{in_date:+}d\"\n\n        return in_date.isoformat()\n\n    def filter_date_range(\n        self,\n        field: str,\n        start_date: datetime.date | int | None,\n        end_date: datetime.date | int | None,\n    ) -> typing.Self:\n        \"\"\"\n        For a given field, filter results to a range of dates.\n\n        If start or end is not provided, the range is unbounded and only\n        affects the start or end date. At least one start or end date must be specified.\n\n        These filters do not affect the relevancy score, they are purely\n        a binary filter on the overall results.\n        \"\"\"\n        if start_date is None and end_date is None:\n            raise ValueError(\"Cannot use date range filter if both start and end dates are None\")\n\n        start_date_str = self.adjust_date_format(start_date)\n        end_date_str = self.adjust_date_format(end_date)\n\n        range_filter = {}\n        if start_date_str is not None:\n            range_filter[\"gte\"] = start_date_str\n        if end_date_str is not None:\n            range_filter[\"lte\"] = end_date_str\n\n        self.filters.append({\"range\": {field: range_filter}})\n        return self\n\n    def aggregation_terms(\n        self, aggregation_name: str, field_name: str, size: int = 25, minimum_count: int = 1\n    ) -> typing.Self:\n        \"\"\"\n        Add a term aggregation to the request. Aggregations are the counts of particular fields in the\n        full response and are often displayed next to filters in a search UI.\n\n        Size determines how many different values can be returned.\n        Minimum count determines how many occurrences need to occur to include in the response.\n            If you pass in 0 for this, then values that don't occur at all in the full result set will be returned.\n\n        see: https://opensearch.org/docs/latest/aggregations/bucket/terms/\n        \"\"\"\n        self.aggregations[aggregation_name] = {\n            \"terms\": {\"field\": field_name, \"size\": size, \"min_doc_count\": minimum_count}\n        }\n        return self\n\n    def build(self) -> dict:\n        \"\"\"\n        Build the search request\n        \"\"\"\n\n        # Base request\n        page_offset = self.page_size * (self.page_number - 1)\n        request: dict[str, typing.Any] = {\n            \"size\": self.page_size,\n            \"from\": page_offset,\n            # Always include the scores in the response objects\n            # even if we're sorting by non-relevancy\n            \"track_scores\": True,\n            \"track_total_hits\": self._track_total_hits,\n        }\n\n        # Add sorting if any was provided\n        if len(self.sort_values) > 0:\n            request[\"sort\"] = self.sort_values\n\n        # Add a bool query\n        #\n        # The \"must\" block contains anything relevant to scoring\n        # The \"filter\" block contains filters that don't affect scoring and act\n        #       as just binary filters\n        #\n        # See: https://opensearch.org/docs/latest/query-dsl/compound/bool/\n        bool_query = {}\n        if len(self.must) > 0:\n            bool_query[\"must\"] = self.must\n\n        if len(self.filters) > 0:\n            bool_query[\"filter\"] = self.filters\n\n        # Add the query object which wraps the bool query\n        query_obj = {}\n        if len(bool_query) > 0:\n            query_obj[\"bool\"] = bool_query\n\n        if len(query_obj) > 0:\n            request[\"query\"] = query_obj\n\n        # Add any aggregations\n        # see: https://opensearch.org/docs/latest/aggregations/\n        if len(self.aggregations) > 0:\n            request[\"aggs\"] = self.aggregations\n\n        return request"}
{"path":"api/src/validation/__init__.py","language":"python","type":"code","directory":"api/src/validation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/validation/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/adapters/search/opensearch_response.py\nLanguage: py\nType: code\nDirectory: api/src/adapters/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/adapters/search/opensearch_response.py\nSize: 3.47 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/src/validation/validation_constants.py","language":"python","type":"code","directory":"api/src/validation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/src/validation/validation_constants.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"@dataclasses.dataclass\nclass SearchResponse:\n    total_records: int\n\n    records: list[dict[str, typing.Any]]\n\n    aggregations: dict[str, dict[str, int]]\n\n    scroll_id: str | None\n\n    @classmethod\n    def from_opensearch_response(\n        cls, raw_json: dict[str, typing.Any], include_scores: bool = True\n    ) -> typing.Self:\n        \"\"\"\n        Convert a raw search response into something a bit more manageable\n        by un-nesting and restructuring a few of they key fields.\n        \"\"\"\n\n        \"\"\"\n        The hits object looks like:\n        {\n            \"total\": {\n              \"value\": 3,\n              \"relation\": \"eq\"\n            },\n            \"max_score\": 22.180708,\n            \"hits\": [\n                {\n                    \"_index\": \"opportunity-index-2024-05-21_15-49-24\",\n                    \"_id\": \"4\",\n                    \"_score\": 22.180708,\n                    \"_source\": {\n                        \"opportunity_id\": 4,\n                        \"opportunity_number\": \"ABC123-XYZ\",\n                    }\n                }\n            ]\n        }\n        \"\"\"\n        scroll_id = raw_json.get(\"_scroll_id\", None)\n\n        hits = raw_json.get(\"hits\", {})\n        hits_total = hits.get(\"total\", {})\n        total_records = hits_total.get(\"value\", 0)\n\n        raw_records: list[dict[str, typing.Any]] = hits.get(\"hits\", [])\n\n        records = []\n        for raw_record in raw_records:\n            record = raw_record.get(\"_source\", {})\n\n            if include_scores:\n                score: int | None = raw_record.get(\"_score\", None)\n                record[\"relevancy_score\"] = score\n\n            records.append(record)\n\n        raw_aggs: dict[str, dict[str, typing.Any]] = raw_json.get(\"aggregations\", {})\n        aggregations = _parse_aggregations(raw_aggs)\n\n        return cls(total_records, records, aggregations, scroll_id)\n\n\ndef _parse_aggregations(\n    raw_aggs: dict[str, dict[str, typing.Any]] | None\n) -> dict[str, dict[str, int]]:\n    # Note that this is assuming the response from a terms aggregation\n    # https://opensearch.org/docs/latest/aggregations/bucket/terms/\n\n    if raw_aggs is None:\n        return {}\n\n    \"\"\"\n    Terms aggregations look like:\n\n    \"aggregations\": {\n        \"applicant_types\": {\n          \"doc_count_error_upper_bound\": 0,\n          \"sum_other_doc_count\": 0,\n          \"buckets\": [\n            {\n              \"key\": \"for_profit_organizations_other_than_small_businesses\",\n              \"doc_count\": 1\n            },\n            {\n              \"key\": \"other\",\n              \"doc_count\": 1\n            },\n            {\n              \"key\": \"state_governments\",\n              \"doc_count\": 1\n            }\n          ]\n        },\n        \"agencies\": {\n          \"doc_count_error_upper_bound\": 0,\n          \"sum_other_doc_count\": 0,\n          \"buckets\": [\n            {\n              \"key\": \"USAID\",\n              \"doc_count\": 3\n            }\n          ]\n        }\n      }\n    \"\"\"\n\n    aggregations: dict[str, dict[str, int]] = {}\n    for field, raw_agg_value in raw_aggs.items():\n        buckets: list[dict[str, typing.Any]] = raw_agg_value.get(\"buckets\", [])\n\n        field_aggregation: dict[str, int] = {}\n        for bucket in buckets:\n            key = bucket.get(\"key\")\n            count = bucket.get(\"doc_count\", 0)\n\n            if key is None:\n                raise ValueError(\"Unable to parse aggregation, null key for %s\" % field)\n\n            field_aggregation[key] = count\n\n        aggregations[field] = field_aggregation\n\n    return aggregations"}
{"path":"api/tests/__init__.py","language":"python","type":"code","directory":"api/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/tests/conftest.py","language":"python","type":"code","directory":"api/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/conftest.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":""}
{"path":"api/tests/lib/__init__.py","language":"python","type":"code","directory":"api/tests/lib","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/agencies_v1/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/api/agencies_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/agencies_v1/__init__.py\nSize: 0.24 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/tests/lib/assertions.py","language":"python","type":"code","directory":"api/tests/lib","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/assertions.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"# import agency_routes module to register the API routes on the blueprint\nimport src.api.agencies_v1.agency_routes  # noqa: F401 E402 isort:skip\n\n__all__ = [\"agency_blueprint\"]"}
{"path":"api/tests/lib/auth_test_utils.py","language":"python","type":"code","directory":"api/tests/lib","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/auth_test_utils.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/agencies_v1/agency_blueprint.py\nLanguage: py\nType: code\nDirectory: api/src/api/agencies_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/agencies_v1/agency_blueprint.py\nSize: 0.17 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/tests/lib/db_testing.py","language":"python","type":"code","directory":"api/tests/lib","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/db_testing.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"agency_blueprint = APIBlueprint(\n    \"agency_v1\",\n    __name__,\n    tag=\"Agency v1\",\n    cli_group=\"agency_v1\",\n    url_prefix=\"/v1\",\n)"}
{"path":"api/tests/lib/seed_agencies.py","language":"python","type":"code","directory":"api/tests/lib","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/seed_agencies.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/agencies_v1/agency_routes.py\nLanguage: py\nType: code\nDirectory: api/src/api/agencies_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/agencies_v1/agency_routes.py\nSize: 1.88 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/tests/lib/seed_local_db.py","language":"python","type":"code","directory":"api/tests/lib","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/seed_local_db.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"import src.adapters.db as db\nimport src.adapters.db.flask_db as flask_db\nimport src.api.agencies_v1.agency_schema as agency_schema\nimport src.api.response as response\nfrom src.api.agencies_v1.agency_blueprint import agency_blueprint\nfrom src.auth.api_key_auth import api_key_auth\nfrom src.logging.flask_logger import add_extra_data_to_current_request_logs\nfrom src.services.agencies_v1.get_agencies import AgencyListParams, get_agencies\n\nlogger = logging.getLogger(__name__)\n\nexamples = {\n    \"example1\": {\n        \"summary\": \"No filters\",\n        \"value\": {\n            \"pagination\": {\n                \"sort_order\": [\n                    {\n                        \"order_by\": \"created_at\",\n                        \"sort_direction\": \"descending\",\n                    }\n                ],\n                \"page_offset\": 1,\n                \"page_size\": 25,\n            },\n        },\n    },\n}\n\n\n@agency_blueprint.post(\"/agencies\")\n@agency_blueprint.input(\n    agency_schema.AgencyListRequestSchema,\n    arg_name=\"raw_list_params\",\n    examples=examples,\n)\n@agency_blueprint.output(agency_schema.AgencyListResponseSchema)\n@agency_blueprint.auth_required(api_key_auth)\n@flask_db.with_db_session()\ndef agencies_get(db_session: db.Session, raw_list_params: dict) -> response.ApiResponse:\n    list_params: AgencyListParams = AgencyListParams.model_validate(raw_list_params)\n\n    # Call service with params to get results\n    with db_session.begin():\n        results, pagination_info = get_agencies(db_session, list_params)\n\n    add_extra_data_to_current_request_logs(\n        {\n            \"response.pagination.total_pages\": pagination_info.total_pages,\n            \"response.pagination.total_records\": pagination_info.total_records,\n        }\n    )\n    logger.info(\"Successfully fetched agencies\")\n\n    # Serialize results\n    return response.ApiResponse(message=\"Success\", data=results, pagination_info=pagination_info)"}
{"path":"api/tests/lib/seed_local_legacy_tables.py","language":"python","type":"code","directory":"api/tests/lib","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/seed_local_legacy_tables.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/agencies_v1/agency_schema.py\nLanguage: py\nType: code\nDirectory: api/src/api/agencies_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/agencies_v1/agency_schema.py\nSize: 1.25 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/tests/src/__init__.py","language":"python","type":"code","directory":"api/tests/src","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"class AgencyFilterV1Schema(Schema):\n    agency_id = fields.Integer()\n\n\nclass AgencyListRequestSchema(Schema):\n    filters = fields.Nested(AgencyFilterV1Schema())\n    pagination = fields.Nested(\n        generate_pagination_schema(\n            \"AgencyPaginationV1Schema\",\n            [\"agency_code\", \"agency_name\", \"created_at\"],\n            default_sort_order=[{\"order_by\": \"agency_code\", \"sort_direction\": \"ascending\"}],\n        ),\n        required=True,\n    )\n\n\nclass AgencyResponseSchema(Schema):\n    \"\"\"Schema for agency response\"\"\"\n\n    agency_id = fields.Integer()\n    agency_name = fields.String()\n    agency_code = fields.String()\n\n    top_level_agency = fields.Nested(lambda: AgencyResponseSchema(exclude=(\"top_level_agency\",)))\n\n    # Add timestamps from TimestampMixin\n    created_at = fields.DateTime()\n    updated_at = fields.DateTime()\n\n\nclass AgencyListResponseSchema(AbstractResponseSchema, PaginationMixinSchema):\n    data = fields.List(\n        fields.Nested(AgencyResponseSchema),\n        metadata={\"description\": \"A list of agency records\"},\n    )"}
{"path":"api/tests/src/adapters/__init__.py","language":"python","type":"code","directory":"api/tests/src/adapters","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/extracts_v1/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/api/extracts_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/extracts_v1/__init__.py\nSize: 0.24 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/tests/src/adapters/aws/__init__.py","language":"python","type":"code","directory":"api/tests/src/adapters/aws","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/aws/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"# import extract_routes module to register the API routes on the blueprint\nimport src.api.extracts_v1.extract_routes  # noqa: F401 E402 isort:skip\n\n__all__ = [\"extract_blueprint\"]"}
{"path":"api/tests/src/adapters/aws/test_pinpoint_adapter.py","language":"python","type":"code","directory":"api/tests/src/adapters/aws","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/aws/test_pinpoint_adapter.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/extracts_v1/extract_blueprint.py\nLanguage: py\nType: code\nDirectory: api/src/api/extracts_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/extracts_v1/extract_blueprint.py\nSize: 0.17 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/tests/src/adapters/db/clients/__init__.py","language":"python","type":"code","directory":"api/tests/src/adapters/db/clients","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/db/clients/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"extract_blueprint = APIBlueprint(\n    \"extract_v1\",\n    __name__,\n    tag=\"Extract v1\",\n    cli_group=\"extract_v1\",\n    url_prefix=\"/v1\",\n)"}
{"path":"api/tests/src/adapters/db/clients/test_postgres_client.py","language":"python","type":"code","directory":"api/tests/src/adapters/db/clients","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/db/clients/test_postgres_client.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/extracts_v1/extract_routes.py\nLanguage: py\nType: code\nDirectory: api/src/api/extracts_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/extracts_v1/extract_routes.py\nSize: 1.92 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/tests/src/adapters/db/test_db.py","language":"python","type":"code","directory":"api/tests/src/adapters/db","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/db/test_db.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"import src.adapters.db as db\nimport src.adapters.db.flask_db as flask_db\nimport src.api.extracts_v1.extract_schema as extract_schema\nimport src.api.response as response\nfrom src.api.extracts_v1.extract_blueprint import extract_blueprint\nfrom src.auth.api_key_auth import api_key_auth\nfrom src.logging.flask_logger import add_extra_data_to_current_request_logs\nfrom src.services.extracts_v1.get_extracts import ExtractListParams, get_extracts\n\nlogger = logging.getLogger(__name__)\n\nexamples = {\n    \"example1\": {\n        \"summary\": \"No filters\",\n        \"value\": {\n            \"pagination\": {\n                \"sort_order\": [\n                    {\n                        \"order_by\": \"created_at\",\n                        \"sort_direction\": \"descending\",\n                    }\n                ],\n                \"page_offset\": 1,\n                \"page_size\": 25,\n            },\n        },\n    },\n}\n\n\n@extract_blueprint.post(\"/extracts\")\n@extract_blueprint.input(\n    extract_schema.ExtractMetadataRequestSchema,\n    arg_name=\"raw_list_params\",\n    examples=examples,\n)\n@extract_blueprint.output(extract_schema.ExtractMetadataListResponseSchema)\n@extract_blueprint.auth_required(api_key_auth)\n@flask_db.with_db_session()\ndef extract_metadata_get(db_session: db.Session, raw_list_params: dict) -> response.ApiResponse:\n    list_params: ExtractListParams = ExtractListParams.model_validate(raw_list_params)\n\n    # Call service with params to get results\n    with db_session.begin():\n        results, pagination_info = get_extracts(db_session, list_params)\n\n    add_extra_data_to_current_request_logs(\n        {\n            \"response.pagination.total_pages\": pagination_info.total_pages,\n            \"response.pagination.total_records\": pagination_info.total_records,\n        }\n    )\n    logger.info(\"Successfully fetched extracts\")\n\n    # Serialize results\n    return response.ApiResponse(message=\"Success\", data=results, pagination_info=pagination_info)"}
{"path":"api/tests/src/adapters/db/test_flask_db.py","language":"python","type":"code","directory":"api/tests/src/adapters/db","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/db/test_flask_db.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/extracts_v1/extract_schema.py\nLanguage: py\nType: code\nDirectory: api/src/api/extracts_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/extracts_v1/extract_schema.py\nSize: 1.68 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/tests/src/adapters/db/type_decorators/__init__.py","language":"python","type":"code","directory":"api/tests/src/adapters/db/type_decorators","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/db/type_decorators/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"class DateRangeSchema(Schema):\n    start_date = fields.Date(\n        required=True,\n        allow_none=True,\n    )\n    end_date = fields.Date(\n        required=True,\n        allow_none=True,\n    )\n\n\nclass ExtractMetadataFilterV1Schema(Schema):\n    extract_type = fields.Enum(\n        ExtractType,\n        allow_none=True,\n        metadata={\n            \"description\": \"The type of extract to filter by\",\n            \"example\": \"opportunities_json\",\n        },\n    )\n    created_at = fields.Nested(DateRangeSchema, required=False)\n\n\nclass ExtractMetadataRequestSchema(AbstractResponseSchema):\n    filters = fields.Nested(ExtractMetadataFilterV1Schema())\n    pagination = fields.Nested(\n        generate_pagination_schema(\n            \"ExtractMetadataPaginationV1Schema\",\n            [\"created_at\"],\n        ),\n        required=True,\n    )\n\n\nclass ExtractMetadataResponseSchema(FileResponseSchema):\n    extract_metadata_id = fields.Integer(\n        metadata={\"description\": \"The ID of the extract metadata\", \"example\": 1}\n    )\n    extract_type = fields.String(\n        metadata={\"description\": \"The type of extract\", \"example\": \"opportunity_data_extract\"}\n    )\n\n\nclass ExtractMetadataListResponseSchema(AbstractResponseSchema, PaginationMixinSchema):\n    data = fields.List(\n        fields.Nested(ExtractMetadataResponseSchema),\n        metadata={\"description\": \"A list of extract metadata records\"},\n    )"}
{"path":"api/tests/src/adapters/db/type_decorators/test_postgres_type_decorators.py","language":"python","type":"code","directory":"api/tests/src/adapters/db/type_decorators","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/db/type_decorators/test_postgres_type_decorators.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/feature_flags/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/api/feature_flags\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/feature_flags/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/tests/src/adapters/oauth/__init__.py","language":"python","type":"code","directory":"api/tests/src/adapters/oauth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/oauth/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":""}
{"path":"api/tests/src/adapters/oauth/login_gov/__init__.py","language":"python","type":"code","directory":"api/tests/src/adapters/oauth/login_gov","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/oauth/login_gov/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/feature_flags/feature_flag.py\nLanguage: py\nType: code\nDirectory: api/src/api/feature_flags\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/feature_flags/feature_flag.py\nSize: 0.86 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/tests/src/adapters/oauth/login_gov/test_login_gov_oauth_client.py","language":"python","type":"code","directory":"api/tests/src/adapters/oauth/login_gov","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/oauth/login_gov/test_login_gov_oauth_client.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"class FeatureFlag(StrEnum):\n    \"\"\"\n    This enum class serves as a list of constant values for feature flags.\n\n    A value like:\n        EXAMPLE_FLAG_A = \"example_flag_a\"\n\n    Would be able to have its default value set by setting the \"EXAMPLE_FLAG_A\" environment\n    variable, and be overrideable in an endpoint by supplying it as \"FF-Example-Flag-A\" in a header.\n\n    \"\"\"\n\n    ### NOTE: This is a placeholder for future work, and only serves as a proof of concept of the idea.\n    #\n    # Header: FF-Enable-Opportunity-Log-Msg\n    # EnvVar: ENABLE_OPPORTUNITY_LOG_MSG\n    ENABLE_OPPORTUNITY_LOG_MSG = \"enable_opportunity_log_msg\"\n\n    def get_header_name(self) -> str:\n        value = \"-\".join([v.capitalize() for v in self.value.lower().split(\"_\")])\n\n        return f\"FF-{value}\"\n\n    def get_env_var_name(self) -> str:\n        return self.value.upper()"}
{"path":"api/tests/src/adapters/search/__init__.py","language":"python","type":"code","directory":"api/tests/src/adapters/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/search/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/feature_flags/feature_flag_config.py\nLanguage: py\nType: code\nDirectory: api/src/api/feature_flags\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/feature_flags/feature_flag_config.py\nSize: 1.72 KB\nLast Modified: 2025-02-14T17:08:26.435Z"}
{"path":"api/tests/src/adapters/search/test_opensearch_client.py","language":"python","type":"code","directory":"api/tests/src/adapters/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/search/test_opensearch_client.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"from pydantic import Field\n\nfrom src.api.feature_flags.feature_flag import FeatureFlag\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass FeatureFlagConfig(PydanticBaseEnvConfig):\n    \"\"\"\n    Configuration for feature flags.\n\n    The values here are defaults loaded from environment variables based\n    on the alias field.\n    \"\"\"\n\n    # ENABLE_OPPORTUNITY_LOG_MSG\n    enable_opportunity_log_msg: bool = Field(\n        False, alias=FeatureFlag.ENABLE_OPPORTUNITY_LOG_MSG.get_env_var_name()\n    )\n\n\n# Global, loaded once at startup by calling initialize\n\"\"\"\nNOTE: This structure of requiring you to initialize the config\nis to allow us to expand the feature flag implementation in the future\nto allow for updating feature flags while the application is still running.\n\nWhat we would need to add is a background thread that periodically\nchecks some external system (the database, S3, AWS app config, etc.)\nfor the current feature flag value, and load that into this configuration.\n\nBy having this structure of initialize() + get_feature_flag_config() - we don't\nneed to later modify the usage of the feature flag config, just this underlying implementation.\n\"\"\"\n_config: FeatureFlagConfig | None = None\n\n\ndef initialize() -> None:\n    global _config\n\n    if not _config:\n        _config = FeatureFlagConfig()\n        logger.info(\"Constructed feature flag configuration\", extra=_config.model_dump())\n\n\ndef get_feature_flag_config() -> FeatureFlagConfig:\n    if not _config:\n        raise Exception(\"Must call initialize() before fetching configuration\")\n\n    # Return a copy so the calling code can modify it if desired\n    # without altering the global value.\n    return _config.model_copy(deep=True)"}
{"path":"api/tests/src/adapters/search/test_opensearch_query_builder.py","language":"python","type":"code","directory":"api/tests/src/adapters/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/search/test_opensearch_query_builder.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/healthcheck.py\nLanguage: py\nType: code\nDirectory: api/src/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/healthcheck.py\nSize: 2.74 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/api/__init__.py","language":"python","type":"code","directory":"api/tests/src/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"from apiflask import APIBlueprint\nfrom sqlalchemy import text\nfrom werkzeug.exceptions import ServiceUnavailable\n\nimport src.adapters.db as db\nimport src.adapters.db.flask_db as flask_db\nfrom src.api import response\nfrom src.api.route_utils import raise_flask_error\nfrom src.api.schemas.extension import Schema, fields\nfrom src.api.schemas.response_schema import AbstractResponseSchema\nfrom src.util.deploy_metadata import get_deploy_metadata_config\n\nlogger = logging.getLogger(__name__)\n\n\nclass HealthcheckMetadataSchema(Schema):\n\n    commit_sha = fields.String(\n        metadata={\n            \"description\": \"The github commit sha for the latest deployed commit\",\n            \"example\": \"ffaca647223e0b6e54344122eefa73401f5ec131\",\n        }\n    )\n    commit_link = fields.String(\n        metadata={\n            \"description\": \"A github link to the latest deployed commit\",\n            \"example\": \"https://github.com/HHS/simpler-grants-gov/commit/main\",\n        }\n    )\n\n    release_notes_link = fields.String(\n        metadata={\n            \"description\": \"A github link to the release notes - direct if the latest deploy was a release\",\n            \"example\": \"https://github.com/HHS/simpler-grants-gov/releases\",\n        }\n    )\n\n    last_deploy_time = fields.DateTime(\n        metadata={\"description\": \"Latest deploy time in US/Eastern timezone\"}\n    )\n\n    deploy_whoami = fields.String(\n        metadata={\"description\": \"The latest user to deploy the application\", \"example\": \"runner\"}\n    )\n\n\nclass HealthcheckResponseSchema(AbstractResponseSchema):\n    # We don't have any data to return with the healthcheck endpoint\n    data = fields.Nested(HealthcheckMetadataSchema())\n\n\nhealthcheck_blueprint = APIBlueprint(\"healthcheck\", __name__, tag=\"Health\")\n\n\n@healthcheck_blueprint.get(\"/health\")\n@healthcheck_blueprint.output(HealthcheckResponseSchema)\n@healthcheck_blueprint.doc(responses=[200, ServiceUnavailable.code])\n@flask_db.with_db_session()\ndef health(db_session: db.Session) -> response.ApiResponse:\n    try:\n        with db_session.begin():\n            if db_session.scalar(text(\"SELECT 1 AS healthy\")) != 1:\n                raise Exception(\"Connection to Postgres DB failure\")\n\n    except Exception:\n        logger.exception(\"Connection to DB failure\")\n        raise_flask_error(ServiceUnavailable.code, message=\"Service Unavailable\")\n\n    metadata_config = get_deploy_metadata_config()\n\n    data = {\n        \"commit_sha\": metadata_config.deploy_github_sha,\n        \"commit_link\": metadata_config.deploy_commit,\n        \"release_notes_link\": metadata_config.release_notes,\n        \"last_deploy_time\": metadata_config.deploy_datetime_est,\n        \"deploy_whoami\": metadata_config.deploy_whoami,\n    }\n\n    return response.ApiResponse(message=\"Service healthy\", data=data)"}
{"path":"api/tests/src/api/agencies_v1/test_agencies_routes.py","language":"python","type":"code","directory":"api/tests/src/api/agencies_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/agencies_v1/test_agencies_routes.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/opportunities_v1/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/api/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/opportunities_v1/__init__.py\nSize: 0.27 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/api/extracts_v1/test_extract_schema.py","language":"python","type":"code","directory":"api/tests/src/api/extracts_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/extracts_v1/test_extract_schema.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"# import opportunity_routes module to register the API routes on the blueprint\nimport src.api.opportunities_v1.opportunity_routes  # noqa: F401 E402 isort:skip\n\n__all__ = [\"opportunity_blueprint\"]"}
{"path":"api/tests/src/api/extracts_v1/test_extracts_routes.py","language":"python","type":"code","directory":"api/tests/src/api/extracts_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/extracts_v1/test_extracts_routes.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/opportunities_v1/opportunity_blueprint.py\nLanguage: py\nType: code\nDirectory: api/src/api/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/opportunities_v1/opportunity_blueprint.py\nSize: 0.19 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/api/opportunities_v1/__init__.py","language":"python","type":"code","directory":"api/tests/src/api/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/opportunities_v1/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"opportunity_blueprint = APIBlueprint(\n    \"opportunity_v1\",\n    __name__,\n    tag=\"Opportunity v1\",\n    cli_group=\"opportunity_v1\",\n    url_prefix=\"/v1\",\n)"}
{"path":"api/tests/src/api/opportunities_v1/conftest.py","language":"python","type":"code","directory":"api/tests/src/api/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/opportunities_v1/conftest.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/opportunities_v1/opportunity_routes.py\nLanguage: py\nType: code\nDirectory: api/src/api/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/opportunities_v1/opportunity_routes.py\nSize: 8.24 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/api/opportunities_v1/test_opportunity_auth.py","language":"python","type":"code","directory":"api/tests/src/api/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/opportunities_v1/test_opportunity_auth.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"from flask import Response\n\nimport src.adapters.db as db\nimport src.adapters.db.flask_db as flask_db\nimport src.adapters.search as search\nimport src.adapters.search.flask_opensearch as flask_opensearch\nimport src.api.opportunities_v1.opportunity_schemas as opportunity_schemas\nimport src.api.response as response\nimport src.util.datetime_util as datetime_util\nfrom src.api.opportunities_v1.opportunity_blueprint import opportunity_blueprint\nfrom src.auth.api_key_auth import api_key_auth\nfrom src.logging.flask_logger import add_extra_data_to_current_request_logs\nfrom src.services.opportunities_v1.get_opportunity import get_opportunity\nfrom src.services.opportunities_v1.opportunity_to_csv import opportunities_to_csv\nfrom src.services.opportunities_v1.search_opportunities import search_opportunities\nfrom src.util.dict_util import flatten_dict\n\nlogger = logging.getLogger(__name__)\n\n# Descriptions in OpenAPI support markdown https://swagger.io/specification/\nSHARED_ALPHA_DESCRIPTION = \"\"\"\n__ALPHA VERSION__\n\nThis endpoint in its current form is primarily for testing and feedback.\n\nFeatures in this endpoint are still under heavy development, and subject to change. Not for production use.\n\nSee [Release Phases](https://github.com/github/roadmap?tab=readme-ov-file#release-phases) for further details.\n\"\"\"\n\nexamples = {\n    \"example1\": {\n        \"summary\": \"No filters\",\n        \"value\": {\n            \"pagination\": {\n                \"sort_order\": [\n                    {\n                        \"order_by\": \"opportunity_id\",\n                        \"sort_direction\": \"ascending\",\n                    }\n                ],\n                \"page_offset\": 1,\n                \"page_size\": 25,\n            },\n        },\n    },\n    \"example2\": {\n        \"summary\": \"All filters\",\n        \"value\": {\n            \"query\": \"research\",\n            \"filters\": {\n                \"agency\": {\"one_of\": [\"USAID\", \"DOC\"]},\n                \"applicant_type\": {\n                    \"one_of\": [\"state_governments\", \"county_governments\", \"individuals\"]\n                },\n                \"funding_category\": {\"one_of\": [\"recovery_act\", \"arts\", \"natural_resources\"]},\n                \"funding_instrument\": {\"one_of\": [\"cooperative_agreement\", \"grant\"]},\n                \"opportunity_status\": {\"one_of\": [\"forecasted\", \"posted\"]},\n                \"post_date\": {\"start_date\": \"2024-01-01\", \"end_date\": \"2024-02-01\"},\n                \"close_date\": {\n                    \"start_date\": \"2024-01-01\",\n                },\n            },\n            \"pagination\": {\n                \"sort_order\": [\n                    {\n                        \"order_by\": \"opportunity_id\",\n                        \"sort_direction\": \"ascending\",\n                    }\n                ],\n                \"page_offset\": 1,\n                \"page_size\": 25,\n            },\n        },\n    },\n    \"example3\": {\n        \"summary\": \"Query & opportunity_status filters\",\n        \"value\": {\n            \"query\": \"research\",\n            \"filters\": {\n                \"opportunity_status\": {\"one_of\": [\"forecasted\", \"posted\"]},\n            },\n            \"pagination\": {\n                \"sort_order\": [\n                    {\n                        \"order_by\": \"opportunity_id\",\n                        \"sort_direction\": \"ascending\",\n                    }\n                ],\n                \"page_offset\": 1,\n                \"page_size\": 25,\n            },\n        },\n    },\n    \"example4\": {\n        \"summary\": \"CSV file response\",\n        \"value\": {\n            \"format\": \"csv\",\n            \"filters\": {\n                \"opportunity_status\": {\"one_of\": [\"forecasted\", \"posted\"]},\n            },\n            \"pagination\": {\n                \"sort_order\": [\n                    {\n                        \"order_by\": \"opportunity_id\",\n                        \"sort_direction\": \"ascending\",\n                    }\n                ],\n                \"page_offset\": 1,\n                \"page_size\": 100,\n            },\n        },\n    },\n    \"example5\": {\n        \"summary\": \"Filter by award fields\",\n        \"value\": {\n            \"filters\": {\n                \"expected_number_of_awards\": {\"min\": 5},\n                \"award_floor\": {\"min\": 10000},\n                \"award_ceiling\": {\"max\": 1000000},\n                \"estimated_total_program_funding\": {\"min\": 100000, \"max\": 250000},\n            },\n            \"pagination\": {\n                \"sort_order\": [\n                    {\n                        \"order_by\": \"opportunity_id\",\n                        \"sort_direction\": \"ascending\",\n                    }\n                ],\n                \"page_offset\": 1,\n                \"page_size\": 25,\n            },\n        },\n    },\n    \"example6\": {\n        \"summary\": \"Filter by assistance listing numbers\",\n        \"value\": {\n            \"filters\": {\n                \"assistance_listing_number\": {\"one_of\": [\"43.001\", \"47.049\"]},\n            },\n            \"pagination\": {\n                \"sort_order\": [\n                    {\n                        \"order_by\": \"opportunity_id\",\n                        \"sort_direction\": \"ascending\",\n                    }\n                ],\n                \"page_offset\": 1,\n                \"page_size\": 25,\n            },\n        },\n    },\n    \"example7\": {\n        \"summary\": \"Primary sort agency_code desc, secondary sort opportunity_id asc\",\n        \"value\": {\n            \"pagination\": {\n                \"page_offset\": 1,\n                \"page_size\": 25,\n                \"sort_order\": [\n                    {\"order_by\": \"agency_code\", \"sort_direction\": \"descending\"},\n                    {\"order_by\": \"opportunity_id\", \"sort_direction\": \"ascending\"},\n                ],\n            }\n        },\n    },\n}\n\n\n@opportunity_blueprint.post(\"/opportunities/search\")\n@opportunity_blueprint.input(\n    opportunity_schemas.OpportunitySearchRequestV1Schema,\n    arg_name=\"search_params\",\n    examples=examples,\n)\n@opportunity_blueprint.output(opportunity_schemas.OpportunitySearchResponseV1Schema())\n@opportunity_blueprint.auth_required(api_key_auth)\n@opportunity_blueprint.doc(\n    description=SHARED_ALPHA_DESCRIPTION,\n    # This adds a file response schema\n    # in addition to the one added by the output decorator\n    responses={200: {\"content\": {\"application/octet-stream\": {}}}},  # type: ignore\n)\n@flask_opensearch.with_search_client()\ndef opportunity_search(\n    search_client: search.SearchClient, search_params: dict\n) -> response.ApiResponse | Response:\n    add_extra_data_to_current_request_logs(flatten_dict(search_params, prefix=\"request.body\"))\n    logger.info(\"POST /v1/opportunities/search\")\n\n    opportunities, aggregations, pagination_info = search_opportunities(\n        search_client, search_params\n    )\n\n    add_extra_data_to_current_request_logs(\n        {\n            \"response.pagination.total_pages\": pagination_info.total_pages,\n            \"response.pagination.total_records\": pagination_info.total_records,\n        }\n    )\n    logger.info(\"Successfully fetched opportunities\")\n\n    if search_params.get(\"format\") == opportunity_schemas.SearchResponseFormat.CSV:\n        # Convert the response into a CSV and return the contents\n        output = io.StringIO()\n        opportunities_to_csv(opportunities, output)\n        timestamp = datetime_util.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n        return Response(\n            output.getvalue().encode(\"utf-8\"),\n            content_type=\"text/csv\",\n            headers={\n                \"Content-Disposition\": f\"attachment; filename=opportunity_search_results_{timestamp}.csv\"\n            },\n        )\n\n    return response.ApiResponse(\n        message=\"Success\",\n        data=opportunities,\n        facet_counts=aggregations,\n        pagination_info=pagination_info,\n    )\n\n\n@opportunity_blueprint.get(\"/opportunities/<int:opportunity_id>\")\n@opportunity_blueprint.output(opportunity_schemas.OpportunityGetResponseV1Schema())\n@opportunity_blueprint.auth_required(api_key_auth)\n@opportunity_blueprint.doc(description=SHARED_ALPHA_DESCRIPTION)\n@flask_db.with_db_session()\ndef opportunity_get(db_session: db.Session, opportunity_id: int) -> response.ApiResponse:\n    add_extra_data_to_current_request_logs({\"opportunity.opportunity_id\": opportunity_id})\n    logger.info(\"GET /v1/opportunities/:opportunity_id\")\n    with db_session.begin():\n        opportunity = get_opportunity(db_session, opportunity_id)\n\n    return response.ApiResponse(message=\"Success\", data=opportunity)"}
{"path":"api/tests/src/api/opportunities_v1/test_opportunity_route_get.py","language":"python","type":"code","directory":"api/tests/src/api/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/opportunities_v1/test_opportunity_route_get.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/opportunities_v1/opportunity_schemas.py\nLanguage: py\nType: code\nDirectory: api/src/api/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/opportunities_v1/opportunity_schemas.py\nSize: 17.94 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/api/opportunities_v1/test_opportunity_route_search.py","language":"python","type":"code","directory":"api/tests/src/api/opportunities_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/opportunities_v1/test_opportunity_route_search.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"from src.api.schemas.extension import Schema, fields, validators\nfrom src.api.schemas.response_schema import (\n    AbstractResponseSchema,\n    FileResponseSchema,\n    PaginationMixinSchema,\n)\nfrom src.api.schemas.search_schema import (\n    BoolSearchSchemaBuilder,\n    DateSearchSchemaBuilder,\n    IntegerSearchSchemaBuilder,\n    StrSearchSchemaBuilder,\n)\nfrom src.constants.lookup_constants import (\n    ApplicantType,\n    FundingCategory,\n    FundingInstrument,\n    OpportunityCategory,\n    OpportunityStatus,\n)\nfrom src.pagination.pagination_schema import generate_pagination_schema\nfrom src.services.opportunities_v1.experimental_constant import ScoringRule\n\n\nclass SearchResponseFormat(StrEnum):\n    JSON = \"json\"\n    CSV = \"csv\"\n\n\nclass SearchQueryOperator(StrEnum):\n    AND = \"AND\"\n    OR = \"OR\"\n\n\nclass OpportunitySummaryV1Schema(Schema):\n    summary_description = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"The summary of the opportunity\",\n            \"example\": \"This opportunity aims to unravel the mysteries of the universe.\",\n        },\n    )\n    is_cost_sharing = fields.Boolean(\n        allow_none=True,\n        metadata={\n            \"description\": \"Whether or not the opportunity has a cost sharing/matching requirement\",\n        },\n    )\n    is_forecast = fields.Boolean(\n        metadata={\n            \"description\": \"Whether the opportunity is forecasted, that is, the information is only an estimate and not yet official\",\n            \"example\": False,\n        }\n    )\n\n    close_date = fields.Date(\n        allow_none=True,\n        metadata={\n            \"description\": \"The date that the opportunity will close - only set if is_forecast=False\",\n        },\n    )\n    close_date_description = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"Optional details regarding the close date\",\n            \"example\": \"Proposals are due earlier than usual.\",\n        },\n    )\n\n    post_date = fields.Date(\n        allow_none=True,\n        metadata={\n            \"description\": \"The date the opportunity was posted\",\n        },\n    )\n    archive_date = fields.Date(\n        allow_none=True,\n        metadata={\n            \"description\": \"When the opportunity will be archived\",\n        },\n    )\n    # not including unarchive date at the moment\n\n    expected_number_of_awards = fields.Integer(\n        allow_none=True,\n        metadata={\n            \"description\": \"The number of awards the opportunity is expected to award\",\n            \"example\": 10,\n        },\n    )\n    estimated_total_program_funding = fields.Integer(\n        allow_none=True,\n        metadata={\n            \"description\": \"The total program funding of the opportunity in US Dollars\",\n            \"example\": 10_000_000,\n        },\n    )\n    award_floor = fields.Integer(\n        allow_none=True,\n        metadata={\n            \"description\": \"The minimum amount an opportunity would award\",\n            \"example\": 10_000,\n        },\n    )\n    award_ceiling = fields.Integer(\n        allow_none=True,\n        metadata={\n            \"description\": \"The maximum amount an opportunity would award\",\n            \"example\": 100_000,\n        },\n    )\n\n    additional_info_url = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"A URL to a website that can provide additional information about the opportunity\",\n            \"example\": \"grants.gov\",\n        },\n    )\n    additional_info_url_description = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"The text to display for the additional_info_url link\",\n            \"example\": \"Click me for more info\",\n        },\n    )\n\n    forecasted_post_date = fields.Date(\n        allow_none=True,\n        metadata={\n            \"description\": \"Forecasted opportunity only. The date the opportunity is expected to be posted, and transition out of being a forecast\"\n        },\n    )\n    forecasted_close_date = fields.Date(\n        allow_none=True,\n        metadata={\n            \"description\": \"Forecasted opportunity only. The date the opportunity is expected to be close once posted.\"\n        },\n    )\n    forecasted_close_date_description = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"Forecasted opportunity only. Optional details regarding the forecasted closed date.\",\n            \"example\": \"Proposals will probably be due on this date\",\n        },\n    )\n    forecasted_award_date = fields.Date(\n        allow_none=True,\n        metadata={\n            \"description\": \"Forecasted opportunity only. The date the grantor plans to award the opportunity.\"\n        },\n    )\n    forecasted_project_start_date = fields.Date(\n        allow_none=True,\n        metadata={\n            \"description\": \"Forecasted opportunity only. The date the grantor expects the award recipient should start their project\"\n        },\n    )\n    fiscal_year = fields.Integer(\n        allow_none=True,\n        metadata={\n            \"description\": \"Forecasted opportunity only. The fiscal year the project is expected to be funded and launched\"\n        },\n    )\n\n    funding_category_description = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"Additional information about the funding category\",\n            \"example\": \"Economic Support\",\n        },\n    )\n    applicant_eligibility_description = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"Additional information about the types of applicants that are eligible\",\n            \"example\": \"All types of domestic applicants are eligible to apply\",\n        },\n    )\n    agency_contact_description = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"Information regarding contacting the agency who owns the opportunity\",\n            \"example\": \"For more information, reach out to Jane Smith at agency US-ABC\",\n        },\n    )\n    agency_email_address = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"The contact email of the agency who owns the opportunity\",\n            \"example\": \"fake_email@grants.gov\",\n        },\n    )\n    agency_email_address_description = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"The text for the link to the agency email address\",\n            \"example\": \"Click me to email the agency\",\n        },\n    )\n\n    version_number = fields.Integer(\n        metadata={\"description\": \"The version number of the opportunity summary\", \"example\": 1}\n    )\n\n    funding_instruments = fields.List(fields.Enum(FundingInstrument))\n    funding_categories = fields.List(fields.Enum(FundingCategory))\n    applicant_types = fields.List(fields.Enum(ApplicantType))\n\n    created_at = fields.DateTime(\n        metadata={\"description\": \"When the opportunity summary was created\"}\n    )\n    updated_at = fields.DateTime(\n        metadata={\"description\": \"When the opportunity summary was last updated\"}\n    )\n\n\nclass OpportunityAssistanceListingV1Schema(Schema):\n    program_title = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"The name of the program, see https://sam.gov/content/assistance-listings for more detail\",\n            \"example\": \"Space Technology\",\n        },\n    )\n    assistance_listing_number = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"The assistance listing number, see https://sam.gov/content/assistance-listings for more detail\",\n            \"example\": \"43.012\",\n        },\n    )\n\n\nclass OpportunityV1Schema(Schema):\n    opportunity_id = fields.Integer(\n        metadata={\"description\": \"The internal ID of the opportunity\", \"example\": 12345},\n    )\n\n    opportunity_number = fields.String(\n        allow_none=True,\n        metadata={\"description\": \"The funding opportunity number\", \"example\": \"ABC-123-XYZ-001\"},\n    )\n    opportunity_title = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"The title of the opportunity\",\n            \"example\": \"Research into conservation techniques\",\n        },\n    )\n    # TODO - we'll want to remove this field in the future\n    # but need to make sure the frontend is not using it\n    agency = fields.String(\n        allow_none=True,\n        metadata={\"description\": \"DEPRECATED - use: agency_code\", \"example\": \"US-ABC\"},\n    )\n    agency_code = fields.String(\n        allow_none=True,\n        metadata={\"description\": \"The agency who created the opportunity\", \"example\": \"US-ABC\"},\n    )\n\n    agency_name = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"The name of the agency who created the oppportunity\",\n            \"example\": \"Department of Examples\",\n        },\n    )\n\n    top_level_agency_name = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"The name of the top level agency who created the oppportunity\",\n            \"example\": \"Department of Examples\",\n        },\n    )\n\n    category = fields.Enum(\n        OpportunityCategory,\n        allow_none=True,\n        metadata={\n            \"description\": \"The opportunity category\",\n            \"example\": OpportunityCategory.DISCRETIONARY,\n        },\n    )\n    category_explanation = fields.String(\n        allow_none=True,\n        metadata={\n            \"description\": \"Explanation of the category when the category is 'O' (other)\",\n            \"example\": None,\n        },\n    )\n\n    opportunity_assistance_listings = fields.List(\n        fields.Nested(OpportunityAssistanceListingV1Schema())\n    )\n    summary = fields.Nested(OpportunitySummaryV1Schema())\n\n    opportunity_status = fields.Enum(\n        OpportunityStatus,\n        metadata={\n            \"description\": \"The current status of the opportunity\",\n            \"example\": OpportunityStatus.POSTED,\n        },\n    )\n\n    created_at = fields.DateTime(dump_only=True)\n    updated_at = fields.DateTime(dump_only=True)\n\n\nclass OpportunityAttachmentV1Schema(FileResponseSchema):\n    mime_type = fields.String(\n        metadata={\"description\": \"The MIME type of the attachment\", \"example\": \"application/pdf\"}\n    )\n    file_name = fields.String(\n        metadata={\"description\": \"The name of the attachment file\", \"example\": \"my_NOFO.pdf\"}\n    )\n    file_description = fields.String(\n        metadata={\n            \"description\": \"A description of the attachment\",\n            \"example\": \"The full announcement NOFO\",\n        }\n    )\n\n\nclass OpportunityWithAttachmentsV1Schema(OpportunityV1Schema):\n    attachments = fields.List(\n        fields.Nested(OpportunityAttachmentV1Schema),\n        attribute=\"opportunity_attachments\",  # This maps to the model's field name\n        metadata={\"description\": \"List of attachments associated with the opportunity\"},\n    )\n\n\nclass OpportunitySearchFilterV1Schema(Schema):\n    funding_instrument = fields.Nested(\n        StrSearchSchemaBuilder(\"FundingInstrumentFilterV1Schema\")\n        .with_one_of(allowed_values=FundingInstrument)\n        .build()\n    )\n    funding_category = fields.Nested(\n        StrSearchSchemaBuilder(\"FundingCategoryFilterV1Schema\")\n        .with_one_of(allowed_values=FundingCategory)\n        .build()\n    )\n    applicant_type = fields.Nested(\n        StrSearchSchemaBuilder(\"ApplicantTypeFilterV1Schema\")\n        .with_one_of(allowed_values=ApplicantType)\n        .build()\n    )\n    opportunity_status = fields.Nested(\n        StrSearchSchemaBuilder(\"OpportunityStatusFilterV1Schema\")\n        .with_one_of(allowed_values=OpportunityStatus)\n        .build()\n    )\n    agency = fields.Nested(\n        StrSearchSchemaBuilder(\"AgencySearchFilterV1Schema\")\n        .with_one_of(example=\"USAID\", minimum_length=2)\n        .build()\n    )\n    assistance_listing_number = fields.Nested(\n        StrSearchSchemaBuilder(\"AssistanceListingNumberFilterV1Schema\")\n        .with_one_of(\n            example=\"45.149\", pattern=r\"^\\d{2}\\.\\d{2,3}$\"\n        )  # Always of the format ##.## or ##.###\n        .build()\n    )\n    is_cost_sharing = fields.Nested(\n        BoolSearchSchemaBuilder(\"IsCostSharingFilterV1Schema\").with_one_of(example=True).build()\n    )\n    expected_number_of_awards = fields.Nested(\n        IntegerSearchSchemaBuilder(\"ExpectedNumberAwardsFilterV1Schema\")\n        .with_integer_range(min_example=0, max_example=25)\n        .build()\n    )\n\n    award_floor = fields.Nested(\n        IntegerSearchSchemaBuilder(\"AwardFloorFilterV1Schema\")\n        .with_integer_range(min_example=0, max_example=10_000)\n        .build()\n    )\n\n    award_ceiling = fields.Nested(\n        IntegerSearchSchemaBuilder(\"AwardCeilingFilterV1Schema\")\n        .with_integer_range(min_example=0, max_example=10_000_000)\n        .build()\n    )\n\n    estimated_total_program_funding = fields.Nested(\n        IntegerSearchSchemaBuilder(\"EstimatedTotalProgramFundingFilterV1Schema\")\n        .with_integer_range(min_example=0, max_example=10_000_000)\n        .build()\n    )\n\n    post_date = fields.Nested(\n        DateSearchSchemaBuilder(\"PostDateFilterV1Schema\").with_date_range().build()\n    )\n\n    close_date = fields.Nested(\n        DateSearchSchemaBuilder(\"CloseDateFilterV1Schema\").with_date_range().build()\n    )\n\n\nclass OpportunityFacetV1Schema(Schema):\n    opportunity_status = fields.Dict(\n        keys=fields.String(),\n        values=fields.Integer(),\n        metadata={\n            \"description\": \"The counts of opportunity_status values in the full response\",\n            \"example\": {\"posted\": 1, \"forecasted\": 2},\n        },\n    )\n    applicant_type = fields.Dict(\n        keys=fields.String(),\n        values=fields.Integer(),\n        metadata={\n            \"description\": \"The counts of applicant_type values in the full response\",\n            \"example\": {\n                \"state_governments\": 3,\n                \"county_governments\": 2,\n                \"city_or_township_governments\": 1,\n            },\n        },\n    )\n    funding_instrument = fields.Dict(\n        keys=fields.String(),\n        values=fields.Integer(),\n        metadata={\n            \"description\": \"The counts of funding_instrument values in the full response\",\n            \"example\": {\"cooperative_agreement\": 4, \"grant\": 3},\n        },\n    )\n    funding_category = fields.Dict(\n        keys=fields.String(),\n        values=fields.Integer(),\n        metadata={\n            \"description\": \"The counts of funding_category values in the full response\",\n            \"example\": {\"recovery_act\": 2, \"arts\": 3, \"agriculture\": 5},\n        },\n    )\n    agency = fields.Dict(\n        keys=fields.String(),\n        values=fields.Integer(),\n        metadata={\n            \"description\": \"The counts of agency values in the full response\",\n            \"example\": {\"USAID\": 4, \"DOC\": 3},\n        },\n    )\n\n\nclass ExperimentalV1Schema(Schema):\n    scoring_rule = fields.Enum(\n        ScoringRule,\n        load_default=ScoringRule.DEFAULT,\n        metadata={\n            \"description\": \"Scoring rule to query against OpenSearch\",\n            \"default\": ScoringRule.DEFAULT,\n        },\n    )\n\n\nclass OpportunitySearchRequestV1Schema(Schema):\n    query = fields.String(\n        metadata={\n            \"description\": \"Query string which searches against several text fields\",\n            \"example\": \"research\",\n        },\n        validate=[validators.Length(min=1, max=100)],\n    )\n    query_operator = fields.Enum(\n        SearchQueryOperator,\n        load_default=SearchQueryOperator.AND,\n        metadata={\n            \"description\": \"Query operator for combining search conditions\",\n            \"example\": \"OR\",\n        },\n    )\n\n    filters = fields.Nested(OpportunitySearchFilterV1Schema())\n    experimental = fields.Nested(ExperimentalV1Schema())\n    pagination = fields.Nested(\n        generate_pagination_schema(\n            \"OpportunityPaginationV1Schema\",\n            [\n                \"relevancy\",\n                \"opportunity_id\",\n                \"opportunity_number\",\n                \"opportunity_title\",\n                \"post_date\",\n                \"close_date\",\n                \"agency_code\",\n                \"agency_name\",\n                \"top_level_agency_name\",\n            ],\n            default_sort_order=[{\"order_by\": \"opportunity_id\", \"sort_direction\": \"descending\"}],\n        ),\n        required=True,\n    )\n\n    format = fields.Enum(\n        SearchResponseFormat,\n        load_default=SearchResponseFormat.JSON,\n        metadata={\n            \"description\": \"The format of the response\",\n            \"default\": SearchResponseFormat.JSON,\n        },\n    )\n\n\nclass OpportunityGetResponseV1Schema(AbstractResponseSchema):\n    data = fields.Nested(OpportunityWithAttachmentsV1Schema())\n\n\nclass OpportunityVersionV1Schema(Schema):\n    opportunity = fields.Nested(OpportunityV1Schema())\n    forecasts = fields.Nested(OpportunitySummaryV1Schema(many=True))\n    non_forecasts = fields.Nested(OpportunitySummaryV1Schema(many=True))\n\n\nclass OpportunitySearchResponseV1Schema(AbstractResponseSchema, PaginationMixinSchema):\n    data = fields.Nested(OpportunityV1Schema(many=True))\n\n    facet_counts = fields.Nested(\n        OpportunityFacetV1Schema(),\n        metadata={\"description\": \"Counts of filter/facet values in the full response\"},\n    )\n\n\nclass SavedOpportunitySummaryV1Schema(Schema):\n    post_date = fields.Date(\n        metadata={\"description\": \"The date the opportunity was posted\", \"example\": \"2024-01-01\"}\n    )\n    close_date = fields.Date(\n        metadata={\"description\": \"The date the opportunity will close\", \"example\": \"2024-01-01\"}\n    )\n    is_forecast = fields.Boolean(\n        metadata={\"description\": \"Whether the opportunity is forecasted\", \"example\": False}\n    )\n\n\nclass SavedOpportunityResponseV1Schema(Schema):\n    opportunity_id = fields.Integer(\n        metadata={\"description\": \"The ID of the saved opportunity\", \"example\": 1234}\n    )\n    opportunity_title = fields.String(\n        allow_none=True,\n        metadata={\"description\": \"The title of the opportunity\", \"example\": \"my title\"},\n    )\n    opportunity_status = fields.Enum(\n        OpportunityStatus,\n        metadata={\n            \"description\": \"The current status of the opportunity\",\n            \"example\": OpportunityStatus.POSTED,\n        },\n    )\n\n    summary = fields.Nested(SavedOpportunitySummaryV1Schema())"}
{"path":"api/tests/src/api/test_healthcheck.py","language":"python","type":"code","directory":"api/tests/src/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/test_healthcheck.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/response.py\nLanguage: py\nType: code\nDirectory: api/src/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/response.py\nSize: 4.68 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/api/test_index_route.py","language":"python","type":"code","directory":"api/tests/src/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/test_index_route.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"import apiflask\nimport flask\n\nfrom src.api.schemas.extension import MarshmallowErrorContainer\nfrom src.pagination.pagination_models import PaginationInfo\nfrom src.util.dict_util import flatten_dict\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclasses.dataclass\nclass ValidationErrorDetail:\n    type: str\n    message: str = \"\"\n    field: Optional[str] = None\n\n\nclass ValidationException(apiflask.exceptions.HTTPError):\n    def __init__(\n        self,\n        errors: list[ValidationErrorDetail],\n        message: str = \"Invalid request\",\n        detail: Any = None,\n    ):\n        super().__init__(\n            status_code=422,\n            message=message,\n            detail=detail,\n            extra_data={\"validation_issues\": errors},\n        )\n        self.errors = errors\n\n\n@dataclasses.dataclass\nclass ApiResponse:\n    \"\"\"Base response model for all API responses.\"\"\"\n\n    message: str\n    data: Optional[Any] = None\n    warnings: list[ValidationErrorDetail] = dataclasses.field(default_factory=list)\n    errors: list[ValidationErrorDetail] = dataclasses.field(default_factory=list)\n    status_code: int = 200\n\n    pagination_info: PaginationInfo | None = None\n    facet_counts: dict | None = None\n\n\ndef redirect_response(location: str, code: int = 302) -> flask.Response:\n    \"\"\"Wrapper around Flask redirects to handle typing issues\"\"\"\n    return cast(flask.Response, flask.redirect(location, code))\n\n\ndef process_marshmallow_issues(marshmallow_issues: dict) -> list[ValidationErrorDetail]:\n    validation_errors: list[ValidationErrorDetail] = []\n\n    # Marshmallow structures its issues as\n    # {\"path\": {\"to\": {\"value\": [\"issue1\", \"issue2\"]}}}\n    # this flattens that to {\"path.to.value\": [\"issue1\", \"issue2\"]}\n    flattened_issues = flatten_dict(marshmallow_issues)\n\n    # Take the flattened issues and create properly formatted\n    # error messages by translating the Marshmallow codes\n    for field, value in flattened_issues.items():\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, MarshmallowErrorContainer):\n                    msg = f\"Unconfigured error in Marshmallow validation errors, expected MarshmallowErrorContainer, but got {item.__class__.__name__}\"\n                    logger.error(msg)\n                    raise AssertionError(msg)\n\n                # If marshmallow expects a field to be an object\n                # then it adds \"._schema\", we don't want that so trim it here\n                validation_errors.append(\n                    ValidationErrorDetail(\n                        field=field.removesuffix(\"._schema\"),\n                        message=item.message,\n                        type=item.key,\n                    )\n                )\n        else:\n            logger.error(\n                \"Error format in json section was not formatted as expected, expected a list, got a %s\",\n                type(value),\n            )\n\n    return validation_errors\n\n\ndef restructure_error_response(error: apiflask.exceptions.HTTPError) -> Tuple[dict, int, Any]:\n    # Note that body needs to have the same schema as the ErrorResponseSchema we defined\n    # in app.api.route.schemas.response_schema.py\n    body = {\n        \"message\": error.message,\n        # we rename detail to data so success and error responses are consistent\n        \"data\": error.detail,\n        \"status_code\": error.status_code,\n        \"internal_request_id\": getattr(flask.g, \"internal_request_id\", None),\n    }\n    validation_errors: list[ValidationErrorDetail] = []\n\n    # Process Marshmallow issues and convert them to the proper format\n    # Marshmallow issues are put in the json error detail - the body of the request\n    if isinstance(error.detail, dict):\n        marshmallow_issues = error.detail.get(\"json\")\n        if marshmallow_issues:\n            validation_errors.extend(process_marshmallow_issues(marshmallow_issues))\n\n            # We don't want to make the response confusing\n            # so we remove the now-duplicate error detail\n            del body[\"data\"][\"json\"]\n\n        marshmallow_issues = error.detail.get(\"headers\")\n        if marshmallow_issues:\n            validation_errors.extend(process_marshmallow_issues(marshmallow_issues))\n\n            del body[\"data\"][\"headers\"]\n\n    # If we called raise_flask_error with a list of validation_issues\n    # then they get appended to the error response here\n    additional_validation_issues = error.extra_data.get(\"validation_issues\")\n    if additional_validation_issues:\n        validation_errors.extend(additional_validation_issues)\n\n    # Attach formatted errors to the error response\n    body[\"errors\"] = validation_errors\n\n    return body, error.status_code, error.headers"}
{"path":"api/tests/src/api/test_route_error_format.py","language":"python","type":"code","directory":"api/tests/src/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/test_route_error_format.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/route_utils.py\nLanguage: py\nType: code\nDirectory: api/src/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/route_utils.py\nSize: 0.74 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/api/users/__init__.py","language":"python","type":"code","directory":"api/tests/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"from apiflask import abort\nfrom apiflask.types import ResponseHeaderType\n\nfrom src.api.response import ValidationErrorDetail\n\n\ndef raise_flask_error(\n    status_code: int,\n    message: str | None = None,\n    detail: Any = None,\n    headers: ResponseHeaderType | None = None,\n    validation_issues: list[ValidationErrorDetail] | None = None,\n) -> Never:\n    # Wrapper around the abort method which makes an error during API processing\n    # work properly when APIFlask generates a response.\n    # mypy doesn't realize this method never returns, so we define the same method\n    # with a return type of Never.\n    abort(\n        status_code, message, detail, headers, extra_data={\"validation_issues\": validation_issues}\n    )"}
{"path":"api/tests/src/api/users/test_user_delete_saved_opportunity.py","language":"python","type":"code","directory":"api/tests/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_delete_saved_opportunity.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/schemas/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/api/schemas\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/api/users/test_user_delete_saved_search.py","language":"python","type":"code","directory":"api/tests/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_delete_saved_search.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":""}
{"path":"api/tests/src/api/users/test_user_get_saved_searches.py","language":"python","type":"code","directory":"api/tests/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_get_saved_searches.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/schemas/extension/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/api/schemas/extension\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/extension/__init__.py\nSize: 0.23 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/api/users/test_user_route_get.py","language":"python","type":"code","directory":"api/tests/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_route_get.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"__all__ = [\"fields\", \"validators\", \"Schema\", \"MarshmallowErrorContainer\"]"}
{"path":"api/tests/src/api/users/test_user_route_login.py","language":"python","type":"code","directory":"api/tests/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_route_login.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/schemas/extension/field_validators.py\nLanguage: py\nType: code\nDirectory: api/src/api/schemas/extension\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/extension/field_validators.py\nSize: 5.43 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/api/users/test_user_route_token.py","language":"python","type":"code","directory":"api/tests/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_route_token.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"from apiflask import validators\nfrom marshmallow import ValidationError\n\nfrom src.api.schemas.extension.schema_common import MarshmallowErrorContainer\nfrom src.validation.validation_constants import ValidationErrorType\n\nValidator = validators.Validator  # re-export\n\n\nclass Regexp(validators.Regexp):\n    REGEX_ERROR = MarshmallowErrorContainer(\n        ValidationErrorType.FORMAT, \"String does not match expected pattern.\"\n    )\n\n    @typing.overload\n    def __call__(self, value: str) -> str: ...\n\n    @typing.overload\n    def __call__(self, value: bytes) -> bytes: ...\n\n    def __call__(self, value: str | bytes) -> str | bytes:\n        if self.regex.match(value) is None:  # type: ignore\n            raise ValidationError([self.REGEX_ERROR])\n\n        return value\n\n\nclass Length(validators.Length):\n    \"\"\"Validator which succeeds if the value passed to it has a\n    length between a minimum and maximum. Uses len(), so it\n    can work for strings, lists, or anything with length.\n\n    :param min: The minimum length. If not provided, minimum length\n        will not be checked.\n    :param max: The maximum length. If not provided, maximum length\n        will not be checked.\n    :param equal: The exact length. If provided, maximum and minimum\n        length will not be checked.\n    :param error: Error message to raise in case of a validation error.\n        Can be interpolated with `{input}`, `{min}` and `{max}`.\n    \"\"\"\n\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"message_min\": MarshmallowErrorContainer(\n            ValidationErrorType.MIN_LENGTH, \"Shorter than minimum length {min}.\"\n        ),\n        \"message_max\": MarshmallowErrorContainer(\n            ValidationErrorType.MAX_LENGTH, \"Longer than maximum length {max}.\"\n        ),\n        \"message_all\": MarshmallowErrorContainer(\n            ValidationErrorType.MIN_OR_MAX_LENGTH, \"Length must be between {min} and {max}.\"\n        ),\n        \"message_equal\": MarshmallowErrorContainer(\n            ValidationErrorType.EQUALS, \"Length must be {equal}.\"\n        ),\n    }\n\n    def _make_error(self, key: str) -> ValidationError:\n        try:\n            # Make a copy of the error mapping so we aren't modifying\n            # the class-level configurations above when we do formatting\n            error_container = copy.copy(self.error_mapping[key])\n        except KeyError as error:\n            class_name = self.__class__.__name__\n            message = (\n                \"ValidationError raised by `{class_name}`, but error key `{key}` does \"\n                \"not exist in the `error_messages` dictionary.\"\n            ).format(class_name=class_name, key=key)\n            raise AssertionError(message) from error\n\n        error_container.message = error_container.message.format(\n            min=self.min, max=self.max, equal=self.equal\n        )\n\n        return ValidationError([error_container])\n\n    def __call__(self, value: typing.Sized) -> typing.Sized:\n        length = len(value)\n\n        if self.equal is not None:\n            if length != self.equal:\n                raise self._make_error(\"message_equal\")\n            return value\n\n        if self.min is not None and length < self.min:\n            key = \"message_min\" if self.max is None else \"message_all\"\n            raise self._make_error(key)\n\n        if self.max is not None and length > self.max:\n            key = \"message_max\" if self.min is None else \"message_all\"\n            raise self._make_error(key)\n\n        return value\n\n\nclass Email(validators.Email):\n    EMAIL_ERROR = MarshmallowErrorContainer(\n        ValidationErrorType.FORMAT, \"Not a valid email address.\"\n    )\n\n    def __call__(self, value: str) -> str:\n        try:\n            return super().__call__(value)\n        except ValidationError:\n            # Fix the validation error to have our format\n            raise ValidationError([self.EMAIL_ERROR]) from None\n\n\nclass OneOf(validators.OneOf):\n    \"\"\"\n    Validator which succeeds if ``value`` is a member of ``choices``.\n\n    Use this when you want to limit the choices, but don't need the value to be an enum\n    \"\"\"\n\n    CONTAINS_ONLY_ERROR = MarshmallowErrorContainer(\n        ValidationErrorType.INVALID_CHOICE, \"Value must be one of: {choices_text}\"\n    )\n\n    def __call__(self, value: typing.Any) -> typing.Any:\n        if value not in self.choices:\n            error_container = copy.copy(self.CONTAINS_ONLY_ERROR)\n            error_container.message = error_container.message.format(choices_text=self.choices_text)\n            raise ValidationError([error_container])\n\n        return value\n\n\n_T = typing.TypeVar(\"_T\")\n\n\nclass Range(validators.Range):\n    def _format_error(self, value: _T, message: str) -> list[MarshmallowErrorContainer]:  # type: ignore\n        # The method this overrides returns a string, but we'll modify it to return one of\n        # our error containers instead which works, but MyPy doesn't like.\n\n        is_min = False\n        is_max = False\n        if self.min is not None or self.max_inclusive is not None:\n            is_min = True\n        if self.max is not None or self.max_inclusive is not None:\n            is_max = True\n\n        if is_min and is_max:\n            error_type = ValidationErrorType.MIN_OR_MAX_VALUE\n        elif is_min:\n            error_type = ValidationErrorType.MIN_VALUE\n        else:  # must be max, init requires you set something\n            error_type = ValidationErrorType.MAX_VALUE\n\n        return [MarshmallowErrorContainer(error_type, super()._format_error(value, message))]"}
{"path":"api/tests/src/api/users/test_user_save_opportunity_post.py","language":"python","type":"code","directory":"api/tests/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_save_opportunity_post.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/schemas/extension/schema.py\nLanguage: py\nType: code\nDirectory: api/src/api/schemas/extension\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/extension/schema.py\nSize: 1.86 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/api/users/test_user_save_search_post.py","language":"python","type":"code","directory":"api/tests/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_save_search_post.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"import apiflask\nfrom marshmallow import EXCLUDE\n\nfrom src.api.schemas.extension.schema_common import MarshmallowErrorContainer\nfrom src.validation.validation_constants import ValidationErrorType\n\n\nclass Schema(apiflask.Schema):\n    # There's no clean way to override the error messages at the schema-level\n    # as they get stored directly into the internal error store of the Schema object\n    #\n    # This approach is a little hacky, but we just change the default error messages to\n    # return the error container objects directly to work around that\n    _default_error_messages = cast(\n        dict[str, str],\n        {\n            \"type\": MarshmallowErrorContainer(\n                key=ValidationErrorType.INVALID, message=\"Invalid input type.\"\n            ),\n            \"unknown\": MarshmallowErrorContainer(\n                key=ValidationErrorType.UNKNOWN, message=\"Unknown field.\"\n            ),\n        },\n    )\n\n    def __init__(self, **kwargs: Any) -> None:\n        super().__init__(**kwargs)\n\n        # In order for the OpenAPI docs to display correctly\n        # we need to set sub-schemas as partial=True, as the\n        # apispec library doesn't handle recursively passing that down\n        # like it should through nested/list objects.\n        if self.partial is True:\n            for field in self.declared_fields.values():\n                # If the field has nested, then it's a\n                # Nested field object\n                if hasattr(field, \"nested\"):\n                    field.nested.partial = True\n\n                # If the field has inner, then it's a list\n                # which has a nested schema within it\n                if hasattr(field, \"inner\"):\n                    if hasattr(field.inner, \"nested\"):\n                        field.inner.nested.partial = True\n\n    class Meta:\n        # Ignore any extra fields\n        unknown = EXCLUDE"}
{"path":"api/tests/src/api/users/test_user_saved_opportunities_get.py","language":"python","type":"code","directory":"api/tests/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_saved_opportunities_get.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/schemas/extension/schema_common.py\nLanguage: py\nType: code\nDirectory: api/src/api/schemas/extension\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/extension/schema_common.py\nSize: 0.19 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/api/users/test_user_update_saved_search.py","language":"python","type":"code","directory":"api/tests/src/api/users","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_update_saved_search.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"from src.validation.validation_constants import ValidationErrorType\n\n\n@dataclasses.dataclass\nclass MarshmallowErrorContainer:\n    key: ValidationErrorType\n    message: str"}
{"path":"api/tests/src/auth/test_api_jwt_auth.py","language":"python","type":"code","directory":"api/tests/src/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/auth/test_api_jwt_auth.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/schemas/extension/schema_fields.py\nLanguage: py\nType: code\nDirectory: api/src/api/schemas/extension\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/extension/schema_fields.py\nSize: 9.75 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/auth/test_api_key_auth.py","language":"python","type":"code","directory":"api/tests/src/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/auth/test_api_key_auth.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"from apiflask import fields as original_fields\nfrom marshmallow import ValidationError\n\nfrom src.api.schemas.extension.field_validators import Range\nfrom src.api.schemas.extension.schema_common import MarshmallowErrorContainer\nfrom src.validation.validation_constants import ValidationErrorType\n\n\nclass MixinField(original_fields.Field):\n    \"\"\"\n    Field mixin class to override the make_error method on each of\n    our field classes defined below.\n\n    Note that in Python when a class inherits from multiple classes,\n    the left-most one takes precedence, so if any subclass of Field\n    were to modify the make_error method, that should take precedence\n    over this one.\n\n    As make_error is only defined once in the Field class, this is fine\n    \"\"\"\n\n    # Any derived class can specify an error_mapping object\n    # and it will be used / override the defaults here\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"required\": MarshmallowErrorContainer(\n            ValidationErrorType.REQUIRED, \"Missing data for required field.\"\n        ),\n        \"invalid\": MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Invalid value.\"),\n        \"null\": MarshmallowErrorContainer(ValidationErrorType.NOT_NULL, \"Field may not be null.\"),\n        # not sure when this one gets hit, a failed validator uses the validator message\n        \"validator_failed\": MarshmallowErrorContainer(\n            ValidationErrorType.INVALID, \"Invalid value.\"\n        ),\n    }\n\n    def __init__(self, **kwargs: typing.Any) -> None:\n        super().__init__(**kwargs)\n\n        # The actual error mapping used for a specific instance\n        self._error_mapping: dict[str, MarshmallowErrorContainer] = {}\n\n        # This iterates over all classes and updates the error\n        # mapping with the most-specific class values overriding\n        # the most generic.\n        for cls in reversed(self.__class__.__mro__):\n            # Copy the error mapping values so any alterations don't\n            # affect other class objects\n            configured_error_mapping = getattr(cls, \"error_mapping\", {})\n            for k, v in configured_error_mapping.items():\n                self._error_mapping[k] = copy.copy(v)\n\n    def make_error(self, key: str, **kwargs: typing.Any) -> ValidationError:\n        \"\"\"Helper method to make a `ValidationError` with an error message\n        from ``self.error_mapping``.\n        \"\"\"\n        try:\n            error_container = self._error_mapping[key]\n        except KeyError as error:\n            class_name = self.__class__.__name__\n            message = (\n                \"ValidationError raised by `{class_name}`, but error key `{key}` does \"\n                \"not exist in the `error_mapping` dictionary.\"\n            ).format(class_name=class_name, key=key)\n            raise AssertionError(message) from error\n\n        if kwargs:\n            error_container.message = error_container.message.format(**kwargs)\n\n        return ValidationError([error_container])\n\n\nclass String(original_fields.String, MixinField):\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"invalid\": MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid string.\"),\n        \"invalid_utf8\": MarshmallowErrorContainer(\n            ValidationErrorType.INVALID, \"Not a valid utf-8 string.\"\n        ),\n    }\n\n\nclass Integer(original_fields.Integer, MixinField):\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"invalid\": MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid integer.\"),\n    }\n\n    def __init__(self, restrict_to_32bit_int: bool = False, **kwargs: typing.Any):\n        # By default, we'll restrict all integer values to 32-bits so that they can be stored in\n        # Postgres' integer column. If you wish to process a larger value, simply set this to false or specify\n        # your own min/max Range.\n        if restrict_to_32bit_int:\n            validators = kwargs.get(\"validate\", [])\n\n            # If a different range is specified, skip adding this one to avoid duplicate error messages\n            has_range_validator = False\n            for validator in validators:\n                if isinstance(validator, Range):\n                    has_range_validator = True\n                    break\n\n            if not has_range_validator:\n                validators.append(Range(-2147483648, 2147483647))\n                kwargs[\"validate\"] = validators\n\n        super().__init__(**kwargs)\n\n\nclass Boolean(original_fields.Boolean, MixinField):\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"invalid\": MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid boolean.\"),\n    }\n\n\nclass Decimal(original_fields.Decimal, MixinField):\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"invalid\": MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid decimal.\"),\n        \"special\": MarshmallowErrorContainer(\n            ValidationErrorType.SPECIAL_NUMERIC,\n            \"Special numeric values (nan or infinity) are not permitted.\",\n        ),\n    }\n\n\nclass UUID(original_fields.UUID, MixinField):\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"invalid\": MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid UUID.\"),\n        \"invalid_uuid\": MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid UUID.\"),\n    }\n\n    def __init__(self, **kwargs: typing.Any):\n        super().__init__(**kwargs)\n\n        # Set a default value for the UUID if none supplied\n        example_value = kwargs.get(\"metadata\", {}).get(\n            \"example\", \"123e4567-e89b-12d3-a456-426614174000\"\n        )\n        self.metadata[\"example\"] = example_value\n\n\nclass Date(original_fields.Date, MixinField):\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"invalid\": MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid date.\"),\n        \"format\": MarshmallowErrorContainer(\n            ValidationErrorType.FORMAT, \"'{input}' cannot be formatted as a date.\"\n        ),\n    }\n\n\nclass DateTime(original_fields.DateTime, MixinField):\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"invalid\": MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid datetime.\"),\n        \"invalid_awareness\": MarshmallowErrorContainer(\n            ValidationErrorType.INVALID, \"Not a valid datetime.\"\n        ),\n        \"format\": MarshmallowErrorContainer(\n            ValidationErrorType.FORMAT, \"'{input}' cannot be formatted as a datetime.\"\n        ),\n    }\n\n\nclass List(original_fields.List, MixinField):\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"invalid\": MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid list.\"),\n    }\n\n\nclass Nested(original_fields.Nested, MixinField):\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"type\": MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Invalid type.\"),\n    }\n\n    def __init__(self, nested: typing.Any, **kwargs: typing.Any):\n        super().__init__(nested=nested, **kwargs)\n        # We set this to object so that if it's nullable, it'll\n        # get generated in the OpenAPI to allow nullable\n        type_values = [\"object\"]\n        if self.allow_none:\n            type_values.append(\"null\")\n        self.metadata[\"type\"] = type_values\n\n\nclass Raw(original_fields.Raw, MixinField):\n    # No error mapping changed from the default\n    pass\n\n\nclass Dict(original_fields.Dict, MixinField):\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"invalid\": MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid dict.\"),\n    }\n\n\nclass Enum(MixinField):\n    \"\"\"\n    Custom field class for handling unioning together multiple Python enums into\n    a single enum field in the generated openapi schema.\n\n    For example, if you have an enum with values x, y, z, and another enum with values a, b, c\n    using this class all 6 of these values would be possible, and when the value\n    is deserialized, we would properly convert it to the proper enum object\n    \"\"\"\n\n    error_mapping: dict[str, MarshmallowErrorContainer] = {\n        \"unknown\": MarshmallowErrorContainer(\n            ValidationErrorType.INVALID_CHOICE, \"Must be one of: {choices}.\"\n        ),\n    }\n\n    def __init__(self, *enums: typing.Type[enum.Enum], **kwargs: typing.Any) -> None:\n        super().__init__(**kwargs)\n\n        self.enums = enums\n        self.field = original_fields.Field()\n\n        self.enum_mapping = {}\n\n        possible_choices = []\n        for e in self.enums:\n            for raw_enum_value in e:\n                enum_value = str(self.field._serialize(raw_enum_value.value, None, None))\n                possible_choices.append(enum_value)\n                self.enum_mapping[enum_value] = e\n\n        self.choices_text = \", \".join(possible_choices)\n        # Set the enum metadata\n        self.metadata[\"enum\"] = possible_choices\n        # Set the type so Swagger will know it's an enum-string\n        if self.metadata.get(\"type\") is None:\n            type_values = [\"string\"]\n            if self.allow_none:\n                type_values.append(\"null\")\n            self.metadata[\"type\"] = type_values\n\n    def _serialize(\n        self, value: typing.Any, attr: str | None, obj: typing.Any, **kwargs: typing.Any\n    ) -> typing.Any:\n        if value is None:\n            return None\n\n        val = value\n        return self.field._serialize(val, attr, obj, **kwargs)\n\n    def _deserialize(\n        self,\n        value: typing.Any,\n        attr: str | None,\n        data: typing.Mapping[str, typing.Any] | None,\n        **kwargs: typing.Any,\n    ) -> typing.Any:\n        val = self.field._deserialize(value, attr, data, **kwargs)\n\n        enum_type = self.enum_mapping.get(val)\n        if not enum_type:\n            raise self.make_error(\"unknown\", choices=self.choices_text)\n\n        return enum_type(val)"}
{"path":"api/tests/src/auth/test_login_gov_jwt_auth.py","language":"python","type":"code","directory":"api/tests/src/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/auth/test_login_gov_jwt_auth.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/schemas/response_schema.py\nLanguage: py\nType: code\nDirectory: api/src/api/schemas\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/response_schema.py\nSize: 2.57 KB\nLast Modified: 2025-02-14T17:08:26.436Z"}
{"path":"api/tests/src/data_migration/__init__.py","language":"python","type":"code","directory":"api/tests/src/data_migration","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"class ValidationIssueSchema(Schema):\n    type = fields.String(metadata={\"description\": \"The type of error\", \"example\": \"invalid\"})\n    message = fields.String(\n        metadata={\"description\": \"The message to return\", \"example\": \"Not a valid string.\"}\n    )\n    field = fields.String(\n        metadata={\"description\": \"The field that failed\", \"example\": \"summary.summary_description\"}\n    )\n\n\nclass AbstractResponseSchema(Schema):\n    message = fields.String(metadata={\"description\": \"The message to return\", \"example\": \"Success\"})\n    data = fields.MixinField(metadata={\"description\": \"The REST resource object\"}, dump_default={})\n    status_code = fields.Integer(\n        metadata={\"description\": \"The HTTP status code\", \"example\": 200}, dump_default=200\n    )\n\n\nclass WarningMixinSchema(Schema):\n    warnings = fields.List(\n        fields.Nested(ValidationIssueSchema()),\n        metadata={\n            \"description\": \"A list of warnings - indicating something you may want to be aware of, but did not prevent handling of the request\"\n        },\n        dump_default=[],\n    )\n\n\nclass PaginationMixinSchema(Schema):\n    pagination_info = fields.Nested(\n        PaginationInfoSchema(),\n        metadata={\"description\": \"The pagination information for paginated endpoints\"},\n    )\n\n\nclass ErrorResponseSchema(Schema):\n    data = fields.MixinField(\n        metadata={\n            \"description\": \"Additional data that might be useful in resolving an error (see specific endpoints for details, this is used infrequently)\",\n            \"example\": {},\n        },\n        dump_default={},\n    )\n    message = fields.String(\n        metadata={\"description\": \"General description of the error\", \"example\": \"Error\"}\n    )\n    status_code = fields.Integer(metadata={\"description\": \"The HTTP status code of the error\"})\n    errors = fields.List(\n        fields.Nested(ValidationIssueSchema()), metadata={\"example\": []}, dump_default=[]\n    )\n    internal_request_id = fields.String(\n        metadata={\n            \"description\": \"An internal tracking ID\",\n            \"example\": \"550e8400-e29b-41d4-a716-446655440000\",\n        }\n    )\n\n\nclass FileResponseSchema(Schema):\n    download_path = fields.String(\n        metadata={\n            \"description\": \"The file's download path\",\n        },\n    )\n    file_size_bytes = fields.Integer(\n        metadata={\"description\": \"The size of the file in bytes\", \"example\": 1024}\n    )\n    created_at = fields.DateTime(dump_only=True)\n    updated_at = fields.DateTime(dump_only=True)"}
{"path":"api/tests/src/data_migration/load/__init__.py","language":"python","type":"code","directory":"api/tests/src/data_migration/load","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/load/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/schemas/search_schema.py\nLanguage: py\nType: code\nDirectory: api/src/api/schemas\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/schemas/search_schema.py\nSize: 11.85 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/data_migration/load/test_load_oracle_data_task.py","language":"python","type":"code","directory":"api/tests/src/data_migration/load","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/load/test_load_oracle_data_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"from marshmallow import ValidationError, validates_schema\n\nfrom src.api.schemas.extension import MarshmallowErrorContainer, Schema, fields, validators\nfrom src.validation.validation_constants import ValidationErrorType\n\n\nclass BaseSearchSchema(Schema):\n    @validates_schema\n    def validates_non_empty(self, data: dict, **kwargs: Any) -> None:\n        \"\"\"\n        For any search schema, validates that the value provided actually has a filter set.\n\n        For example, a request like:\n\n            {\n                \"filters\": {\n                    \"my_field\": {}\n                }\n            }\n\n        would be invalid as \"my_field\" needs at least something within it. Note that providing\n        no filters / excluding \"my_field\" entirely is perfectly fine, we're just trying to avoid\n        having something partially filled out to keep the logic downstream a bit simpler.\n        \"\"\"\n        if data == {}:\n            raise ValidationError(\n                [\n                    MarshmallowErrorContainer(\n                        ValidationErrorType.INVALID, \"At least one filter rule must be provided.\"\n                    )\n                ]\n            )\n\n\nclass BaseSearchSchemaBuilder:\n    def __init__(self, schema_class_name: str):\n        # schema fields are the fields and functions of the class\n        self.schema_fields: dict[str, fields.MixinField | Callable[..., Any]] = {}\n        # The schema class name is used on the endpoint\n        self.schema_class_name = schema_class_name\n\n    def build(self) -> Schema:\n        return BaseSearchSchema.from_dict(self.schema_fields, name=self.schema_class_name)  # type: ignore\n\n\nclass StrSearchSchemaBuilder(BaseSearchSchemaBuilder):\n    \"\"\"\n    Builder for setting up a filter in a search endpoint schema.\n\n    Our schemas are setup to look like:\n\n        {\n            \"filters\": {\n                \"field\": {\n                    \"one_of\": [\"x\", \"y\", \"z\"]\n                }\n            }\n        }\n\n    This helps generate the filters for a given field. At the moment,\n    only a one_of filter is implemented.\n\n    Usage::\n\n        # In a search request schema, you would use it like so\n\n        class OpportunitySearchFilterSchema(Schema):\n            example_enum_field = fields.Nested(\n                StrSearchSchemaBuilder(\"ExampleEnumFieldSchema\")\n                    .with_one_of(allowed_values=ExampleEnum)\n                    .build()\n            )\n\n            example_str_field = fields.Nested(\n                StrSearchSchemaBuilder(\"ExampleStrFieldSchema\")\n                    .with_one_of(example=\"example_value\", minimum_length=5)\n                    .build()\n            )\n    \"\"\"\n\n    def with_one_of(\n        self,\n        *,\n        allowed_values: Type[StrEnum] | None = None,\n        pattern: str | Pattern | None = None,\n        example: str | None = None,\n        minimum_length: int | None = None\n    ) -> \"StrSearchSchemaBuilder\":\n        if pattern is not None and allowed_values is not None:\n            raise Exception(\"Cannot specify both a pattern and allowed_values\")\n\n        metadata = {}\n        if example:\n            metadata[\"example\"] = example\n\n        # We assume it's just a list of strings\n        if allowed_values is None:\n            params: dict = {\"metadata\": metadata}\n\n            field_validators: list[validators.Validator] = []\n            if minimum_length is not None:\n                field_validators.append(validators.Length(min=minimum_length))\n\n            if pattern is not None:\n                field_validators.append(validators.Regexp(regex=pattern))\n\n            if len(field_validators) > 0:\n                params[\"validate\"] = field_validators\n\n            list_type: fields.MixinField = fields.String(**params)\n\n        # Otherwise it is an enum type which handles allowed values\n        else:\n            list_type = fields.Enum(allowed_values, metadata=metadata)\n\n        # Note that the list requires at least one value (sending us just [] will raise a validation error)\n        self.schema_fields[\"one_of\"] = fields.List(list_type, validate=[validators.Length(min=1)])\n\n        return self\n\n\nclass IntegerSearchSchemaBuilder(BaseSearchSchemaBuilder):\n    \"\"\"\n    Builder for setting up a filter in a search endpoint schema for an integer.\n\n    Our schemas are setup to look like:\n\n        {\n            \"filters\": {\n                \"field\": {\n                    \"min\": 1,\n                    \"max\": 5\n                }\n            }\n        }\n\n    This helps generate the filters for a given field. At the moment,\n    only a min and max filter are implemented, and can be used to filter\n    on a range of values.\n\n    Usage::\n\n        # In a search request schema, you would use it like so\n\n        class OpportunitySearchFilterSchema(Schema):\n            example_int_field = fields.Nested(\n                IntegerSearchSchemaBuilder(\"ExampleIntFieldSchema\")\n                    .with_integer_range(min_example=1, max_example=25)\n                    .build()\n            )\n    \"\"\"\n\n    def with_integer_range(\n        self,\n        min_example: int | None = None,\n        max_example: int | None = None,\n        positive_only: bool = True,\n    ) -> \"IntegerSearchSchemaBuilder\":\n        self._with_minimum_value(min_example, positive_only)\n        self._with_maximum_value(max_example, positive_only)\n        self._with_int_range_validator()\n        return self\n\n    def _with_minimum_value(\n        self, example: int | None = None, positive_only: bool = True\n    ) -> \"IntegerSearchSchemaBuilder\":\n        metadata = {}\n        if example is not None:\n            metadata[\"example\"] = example\n\n        field_validators = []\n        if positive_only:\n            field_validators.append(validators.Range(min=0))\n\n        self.schema_fields[\"min\"] = fields.Integer(\n            allow_none=True, metadata=metadata, validate=field_validators\n        )\n        return self\n\n    def _with_maximum_value(\n        self, example: int | None = None, positive_only: bool = True\n    ) -> \"IntegerSearchSchemaBuilder\":\n        metadata = {}\n        if example is not None:\n            metadata[\"example\"] = example\n\n        field_validators = []\n        if positive_only:\n            field_validators.append(validators.Range(min=0))\n\n        self.schema_fields[\"max\"] = fields.Integer(\n            allow_none=True, metadata=metadata, validate=field_validators\n        )\n        return self\n\n    def _with_int_range_validator(self) -> \"IntegerSearchSchemaBuilder\":\n        # Define a schema validator function that we'll use to define any\n        # rules that go across fields in the validation\n        @validates_schema\n        def validate_int_range(_: Any, data: dict, **kwargs: Any) -> None:\n            min_value = data.get(\"min\", None)\n            max_value = data.get(\"max\", None)\n\n            # Error if min and max value are None (either explicitly set, or because they are missing)\n            if min_value is None and max_value is None:\n                raise ValidationError(\n                    [\n                        MarshmallowErrorContainer(\n                            ValidationErrorType.REQUIRED,\n                            \"At least one of min or max must be provided.\",\n                        )\n                    ]\n                )\n\n        self.schema_fields[\"validate_int_range\"] = validate_int_range\n        return self\n\n\nclass BoolSearchSchemaBuilder(BaseSearchSchemaBuilder):\n    \"\"\"\n    Builder for setting up a filter in a search endpoint schema.\n\n    Our schemas are setup to look like:\n\n        {\n            \"filters\": {\n                \"field\": {\n                    \"one_of\": [\"True\", \"False\"]\n                }\n            }\n        }\n\n    This helps generate the filters for a given field. At the moment,\n    only a one_of filter is implemented - note that any truthy value\n    as determined by Marshmallow is accepted (including \"yes\", \"y\", 1 - for true)\n\n    While it doesn't quite make sense to filter by multiple boolean values in most cases,\n    we err on the side of consistency with the structure of the query to match other types.\n\n    Usage::\n\n        # In a search request schema, you would use it like so\n\n        class OpportunitySearchFilterSchema(Schema):\n            example_bool_field = fields.Nested(\n                BoolSearchSchemaBuilder(\"ExampleBoolFieldSchema\")\n                    .with_one_of(example=True)\n                    .build()\n            )\n    \"\"\"\n\n    def with_one_of(self, example: bool | None = None) -> \"BoolSearchSchemaBuilder\":\n        metadata = {}\n        if example is not None:\n            metadata[\"example\"] = example\n\n        self.schema_fields[\"one_of\"] = fields.List(\n            fields.Boolean(metadata=metadata), allow_none=True\n        )\n        return self\n\n\nclass DateSearchSchemaBuilder(BaseSearchSchemaBuilder):\n    \"\"\"\n    Builder for setting up a filter for a range of dates in the search endpoint schema.\n\n    Example of what this might look like:\n        {\n            \"filters\": {\n                \"post_date\": {\n                    \"start_date\": \"YYYY-MM-DD\",\n                    \"end_date\": \"YYYY-MM-DD\"\n                }\n            }\n        }\n\n    Support for start_date and\n    end_date filters have been partially implemented.\n\n    Usage::\n    # In a search request schema, you would use it like so:\n\n        example_startend_date_field = fields.Nested(\n            DateSearchSchemaBuilder(\"ExampleStartEndDateFieldSchema\")\n                .with_date_range()\n                .build()\n        )\n    \"\"\"\n\n    def with_date_range(self) -> \"DateSearchSchemaBuilder\":\n        self.schema_fields[\"start_date\"] = fields.Date(allow_none=True)\n        self.schema_fields[\"end_date\"] = fields.Date(allow_none=True)\n\n        self.schema_fields[\"start_date_relative\"] = fields.Integer(\n            allow_none=True, validate=[validators.Range(min=-1000000, max=1000000)]\n        )\n        self.schema_fields[\"end_date_relative\"] = fields.Integer(\n            allow_none=True, validate=[validators.Range(min=-1000000, max=1000000)]\n        )\n\n        self._with_date_range_validator()\n\n        return self\n\n    def _with_date_range_validator(self) -> \"DateSearchSchemaBuilder\":\n        # Define a schema validator function that we'll use to define any\n        # rules that go across fields in the validation\n        @validates_schema\n        def validate_date_range(_: Any, data: dict, **kwargs: Any) -> None:\n\n            start_date = data.get(\"start_date\", None)\n            end_date = data.get(\"end_date\", None)\n\n            start_date_relative = data.get(\"start_date_relative\", None)\n            end_date_relative = data.get(\"end_date_relative\", None)\n\n            # Error if both relative date and absolute date provided for either start or end date\n            if (\"start_date\" in data and \"start_date_relative\" in data) or (\n                \"end_date\" in data and \"end_date_relative\" in data\n            ):\n                raise ValidationError(\n                    [\n                        MarshmallowErrorContainer(\n                            ValidationErrorType.INVALID,\n                            \"Cannot have both absolute and relative start/end date.\",\n                        )\n                    ]\n                )\n\n            # Error if both start and end date for either relative or absolute date are None (either explicitly set, or because they are missing)\n            if (\n                start_date is None\n                and end_date is None\n                and start_date_relative is None\n                and end_date_relative is None\n            ):\n                raise ValidationError(\n                    [\n                        MarshmallowErrorContainer(\n                            ValidationErrorType.REQUIRED,\n                            \"At least one of start_date/start_date_relative or end_date/end_date_relative must be provided.\",\n                        )\n                    ]\n                )\n\n        self.schema_fields[\"validate_date_range\"] = validate_date_range\n        return self"}
{"path":"api/tests/src/data_migration/load/test_sql.py","language":"python","type":"code","directory":"api/tests/src/data_migration/load","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/load/test_sql.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/users/README.md\nLanguage: md\nType: code\nDirectory: api/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/users/README.md\nSize: 0.13 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/data_migration/test_setup_foreign_tables.py","language":"python","type":"code","directory":"api/tests/src/data_migration","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/test_setup_foreign_tables.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":""}
{"path":"api/tests/src/data_migration/transformation/__init__.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/users/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/users/__init__.py\nSize: 0.22 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/data_migration/transformation/conftest.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/conftest.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"# import user_routes module to register the API routes on the blueprint\nimport src.api.users.user_routes  # noqa: F401 E402 isort:skip\n\n__all__ = [\"user_blueprint\"]"}
{"path":"api/tests/src/data_migration/transformation/subtask/__init__.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/users/user_blueprint.py\nLanguage: py\nType: code\nDirectory: api/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/users/user_blueprint.py\nSize: 0.17 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/data_migration/transformation/subtask/test_transform_agency.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_agency.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"user_blueprint = APIBlueprint(\n    \"user_v1\",\n    __name__,\n    tag=\"User v1\",\n    cli_group=\"user_v1\",\n    url_prefix=\"/v1/users\",\n)"}
{"path":"api/tests/src/data_migration/transformation/subtask/test_transform_applicant_type.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_applicant_type.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/users/user_routes.py\nLanguage: py\nType: code\nDirectory: api/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/users/user_routes.py\nSize: 13.18 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/data_migration/transformation/subtask/test_transform_assistance_listing.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_assistance_listing.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"import flask\n\nimport src.adapters.search.flask_opensearch as flask_opensearch\nfrom src.adapters import db, search\nfrom src.adapters.db import flask_db\nfrom src.api import response\nfrom src.api.route_utils import raise_flask_error\nfrom src.api.users import user_schemas\nfrom src.api.users.user_blueprint import user_blueprint\nfrom src.api.users.user_schemas import (\n    UserDeleteSavedOpportunityResponseSchema,\n    UserDeleteSavedSearchResponseSchema,\n    UserGetResponseSchema,\n    UserSavedOpportunitiesResponseSchema,\n    UserSavedSearchesRequestSchema,\n    UserSavedSearchesResponseSchema,\n    UserSaveOpportunityRequestSchema,\n    UserSaveOpportunityResponseSchema,\n    UserSaveSearchRequestSchema,\n    UserSaveSearchResponseSchema,\n    UserTokenLogoutResponseSchema,\n    UserTokenRefreshResponseSchema,\n    UserUpdateSavedSearchRequestSchema,\n    UserUpdateSavedSearchResponseSchema,\n)\nfrom src.auth.api_jwt_auth import api_jwt_auth, refresh_token_expiration\nfrom src.auth.auth_utils import with_login_redirect_error_handler\nfrom src.auth.login_gov_jwt_auth import get_final_redirect_uri, get_login_gov_redirect_uri\nfrom src.db.models.user_models import UserSavedOpportunity, UserTokenSession\nfrom src.services.users.create_saved_search import create_saved_search\nfrom src.services.users.delete_saved_opportunity import delete_saved_opportunity\nfrom src.services.users.delete_saved_search import delete_saved_search\nfrom src.services.users.get_saved_opportunities import get_saved_opportunities\nfrom src.services.users.get_saved_searches import get_saved_searches\nfrom src.services.users.get_user import get_user\nfrom src.services.users.login_gov_callback_handler import (\n    handle_login_gov_callback_request,\n    handle_login_gov_token,\n)\nfrom src.services.users.update_saved_searches import update_saved_search\n\nlogger = logging.getLogger(__name__)\n\nLOGIN_DESCRIPTION = \"\"\"\nTo use this endpoint, click [this link](/v1/users/login) which will redirect\nyou to an OAuth provider where you can sign into an account.\n\nDo not try to use the execute option below as OpenAPI will not redirect your browser for you.\n\nThe token you receive can then be set to the X-SGG-Token header for authenticating with endpoints.\n\"\"\"\n\n\n@user_blueprint.get(\"/login\")\n@user_blueprint.doc(responses=[302], description=LOGIN_DESCRIPTION)\n@with_login_redirect_error_handler()\n@flask_db.with_db_session()\ndef user_login(db_session: db.Session) -> flask.Response:\n    logger.info(\"GET /v1/users/login\")\n    with db_session.begin():\n        redirect_uri = get_login_gov_redirect_uri(db_session)\n\n    return response.redirect_response(redirect_uri)\n\n\n@user_blueprint.get(\"/login/callback\")\n@user_blueprint.input(user_schemas.UserLoginGovCallbackSchema, location=\"query\")\n@user_blueprint.doc(responses=[302], hide=True)\n@with_login_redirect_error_handler()\n@flask_db.with_db_session()\ndef user_login_callback(db_session: db.Session, query_data: dict) -> flask.Response:\n    logger.info(\"GET /v1/users/login/callback\")\n\n    # We process this in two separate DB transactions\n    # as we delete state at the end of the first handler\n    # even if it were to later error to avoid replay attacks\n    with db_session.begin():\n        data = handle_login_gov_callback_request(query_data, db_session)\n    with db_session.begin():\n        result = handle_login_gov_token(db_session, data)\n\n    # Redirect to the final location for the user\n    return response.redirect_response(\n        get_final_redirect_uri(\"success\", result.token, result.is_user_new)\n    )\n\n\n@user_blueprint.get(\"/login/result\")\n@user_blueprint.doc(hide=True)\ndef login_result() -> flask.Response:\n    logger.info(\"GET /v1/users/login/result\")\n    \"\"\"Dummy endpoint for easily displaying the results of the login flow without the frontend\"\"\"\n\n    # Echo back the query args as JSON for some readability\n    return flask.jsonify(flask.request.args)\n\n\n@user_blueprint.post(\"/token/logout\")\n@user_blueprint.output(UserTokenLogoutResponseSchema)\n@user_blueprint.doc(responses=[200, 401])\n@user_blueprint.auth_required(api_jwt_auth)\n@flask_db.with_db_session()\ndef user_token_logout(db_session: db.Session) -> response.ApiResponse:\n    logger.info(\"POST /v1/users/token/logout\")\n\n    user_token_session: UserTokenSession = api_jwt_auth.get_user_token_session()\n    with db_session.begin():\n        user_token_session.is_valid = False\n        db_session.add(user_token_session)\n\n    logger.info(\n        \"Logged out a user\",\n        extra={\n            \"user_token_session.token_id\": str(user_token_session.token_id),\n            \"user_token_session.user_id\": str(user_token_session.user_id),\n        },\n    )\n\n    return response.ApiResponse(message=\"Success\")\n\n\n@user_blueprint.post(\"/token/refresh\")\n@user_blueprint.output(UserTokenRefreshResponseSchema)\n@user_blueprint.doc(responses=[200, 401])\n@user_blueprint.auth_required(api_jwt_auth)\n@flask_db.with_db_session()\ndef user_token_refresh(db_session: db.Session) -> response.ApiResponse:\n    logger.info(\"POST /v1/users/token/refresh\")\n\n    user_token_session: UserTokenSession = api_jwt_auth.get_user_token_session()\n\n    with db_session.begin():\n        refresh_token_expiration(user_token_session)\n        db_session.add(user_token_session)\n\n    logger.info(\n        \"Refreshed a user token\",\n        extra={\n            \"user_token_session.token_id\": str(user_token_session.token_id),\n            \"user_token_session.user_id\": str(user_token_session.user_id),\n        },\n    )\n\n    return response.ApiResponse(message=\"Success\")\n\n\n@user_blueprint.get(\"/<uuid:user_id>\")\n@user_blueprint.output(UserGetResponseSchema)\n@user_blueprint.doc(responses=[200, 401])\n@user_blueprint.auth_required(api_jwt_auth)\n@flask_db.with_db_session()\ndef user_get(db_session: db.Session, user_id: UUID) -> response.ApiResponse:\n    logger.info(\"GET /v1/users/:user_id\")\n\n    user_token_session: UserTokenSession = api_jwt_auth.get_user_token_session()\n\n    if user_token_session.user_id == user_id:\n        with db_session.begin():\n            user = get_user(db_session, user_id)\n\n        return response.ApiResponse(message=\"Success\", data=user)\n\n    raise_flask_error(401, \"Unauthorized user\")\n\n\n@user_blueprint.post(\"/<uuid:user_id>/saved-opportunities\")\n@user_blueprint.input(UserSaveOpportunityRequestSchema, location=\"json\")\n@user_blueprint.output(UserSaveOpportunityResponseSchema)\n@user_blueprint.doc(responses=[200, 401])\n@user_blueprint.auth_required(api_jwt_auth)\n@flask_db.with_db_session()\ndef user_save_opportunity(\n    db_session: db.Session, user_id: UUID, json_data: dict\n) -> response.ApiResponse:\n    logger.info(\"POST /v1/users/:user_id/saved-opportunities\")\n\n    user_token_session: UserTokenSession = api_jwt_auth.get_user_token_session()\n\n    # Verify the authenticated user matches the requested user_id\n    if user_token_session.user_id != user_id:\n        raise_flask_error(401, \"Unauthorized user\")\n\n    # Create the saved opportunity record\n    saved_opportunity = UserSavedOpportunity(\n        user_id=user_id, opportunity_id=json_data[\"opportunity_id\"]\n    )\n\n    with db_session.begin():\n        db_session.add(saved_opportunity)\n\n    logger.info(\n        \"Saved opportunity for user\",\n        extra={\n            \"user.id\": str(user_id),\n            \"opportunity.id\": json_data[\"opportunity_id\"],\n        },\n    )\n\n    return response.ApiResponse(message=\"Success\")\n\n\n@user_blueprint.delete(\"/<uuid:user_id>/saved-opportunities/<int:opportunity_id>\")\n@user_blueprint.output(UserDeleteSavedOpportunityResponseSchema)\n@user_blueprint.doc(responses=[200, 401, 404])\n@user_blueprint.auth_required(api_jwt_auth)\n@flask_db.with_db_session()\ndef user_delete_saved_opportunity(\n    db_session: db.Session, user_id: UUID, opportunity_id: int\n) -> response.ApiResponse:\n    logger.info(\"DELETE /v1/users/:user_id/saved-opportunities/:opportunity_id\")\n\n    user_token_session: UserTokenSession = api_jwt_auth.get_user_token_session()\n\n    # Verify the authenticated user matches the requested user_id\n    if user_token_session.user_id != user_id:\n        raise_flask_error(401, \"Unauthorized user\")\n\n    with db_session.begin():\n        # Delete the saved opportunity\n        delete_saved_opportunity(db_session, user_id, opportunity_id)\n\n    return response.ApiResponse(message=\"Success\")\n\n\n@user_blueprint.get(\"/<uuid:user_id>/saved-opportunities\")\n@user_blueprint.output(UserSavedOpportunitiesResponseSchema)\n@user_blueprint.doc(responses=[200, 401])\n@user_blueprint.auth_required(api_jwt_auth)\n@flask_db.with_db_session()\ndef user_get_saved_opportunities(db_session: db.Session, user_id: UUID) -> response.ApiResponse:\n    logger.info(\"GET /v1/users/:user_id/saved-opportunities\")\n\n    user_token_session: UserTokenSession = api_jwt_auth.get_user_token_session()\n\n    # Verify the authenticated user matches the requested user_id\n    if user_token_session.user_id != user_id:\n        raise_flask_error(401, \"Unauthorized user\")\n\n    # Get all saved opportunities for the user with their related opportunity data\n    saved_opportunities = get_saved_opportunities(db_session, user_id)\n\n    return response.ApiResponse(message=\"Success\", data=saved_opportunities)\n\n\n@user_blueprint.post(\"/<uuid:user_id>/saved-searches\")\n@user_blueprint.input(UserSaveSearchRequestSchema, location=\"json\")\n@user_blueprint.output(UserSaveSearchResponseSchema)\n@user_blueprint.doc(responses=[200, 401])\n@user_blueprint.auth_required(api_jwt_auth)\n@flask_db.with_db_session()\n@flask_opensearch.with_search_client()\ndef user_save_search(\n    search_client: search.SearchClient, db_session: db.Session, user_id: UUID, json_data: dict\n) -> response.ApiResponse:\n    logger.info(\"POST /v1/users/:user_id/saved-searches\")\n\n    user_token_session: UserTokenSession = api_jwt_auth.get_user_token_session()\n\n    # Verify the authenticated user matches the requested user_id\n    if user_token_session.user_id != user_id:\n        raise_flask_error(401, \"Unauthorized user\")\n\n    with db_session.begin():\n        saved_search = create_saved_search(search_client, db_session, user_id, json_data)\n\n    logger.info(\n        \"Saved search for user\",\n        extra={\n            \"user.id\": str(user_id),\n            \"saved_search.id\": str(saved_search.saved_search_id),\n        },\n    )\n\n    return response.ApiResponse(message=\"Success\")\n\n\n@user_blueprint.delete(\"/<uuid:user_id>/saved-searches/<uuid:saved_search_id>\")\n@user_blueprint.output(UserDeleteSavedSearchResponseSchema)\n@user_blueprint.doc(responses=[200, 401, 404])\n@user_blueprint.auth_required(api_jwt_auth)\n@flask_db.with_db_session()\ndef user_delete_saved_search(\n    db_session: db.Session, user_id: UUID, saved_search_id: UUID\n) -> response.ApiResponse:\n    logger.info(\"DELETE /v1/users/:user_id/saved-searches/:saved_search_id\")\n\n    user_token_session: UserTokenSession = api_jwt_auth.get_user_token_session()\n\n    # Verify the authenticated user matches the requested user_id\n    if user_token_session.user_id != user_id:\n        raise_flask_error(401, \"Unauthorized user\")\n\n    with db_session.begin():\n        delete_saved_search(db_session, user_id, saved_search_id)\n\n    logger.info(\n        \"Deleted saved search\",\n        extra={\n            \"user.id\": str(user_id),\n            \"saved_search.id\": saved_search_id,\n        },\n    )\n\n    return response.ApiResponse(message=\"Success\")\n\n\n@user_blueprint.post(\"/<uuid:user_id>/saved-searches/list\")\n@user_blueprint.input(UserSavedSearchesRequestSchema, location=\"json\")\n@user_blueprint.output(UserSavedSearchesResponseSchema)\n@user_blueprint.doc(responses=[200, 401, 404])\n@user_blueprint.auth_required(api_jwt_auth)\n@flask_db.with_db_session()\ndef user_get_saved_searches(\n    db_session: db.Session, user_id: UUID, json_data: dict\n) -> response.ApiResponse:\n    logger.info(\"POST /v1/users/:user_id/saved-searches/list\")\n    user_token_session: UserTokenSession = api_jwt_auth.get_user_token_session()\n\n    # Verify the authenticated user matches the requested user_id\n    if user_token_session.user_id != user_id:\n        raise_flask_error(401, \"Unauthorized user\")\n\n    saved_searches, pagination_info = get_saved_searches(db_session, user_id, json_data)\n\n    return response.ApiResponse(\n        message=\"Success\",\n        data=saved_searches,\n        pagination_info=pagination_info,\n    )\n\n\n@user_blueprint.put(\"/<uuid:user_id>/saved-searches/<uuid:saved_search_id>\")\n@user_blueprint.input(UserUpdateSavedSearchRequestSchema, location=\"json\")\n@user_blueprint.output(UserUpdateSavedSearchResponseSchema)\n@user_blueprint.doc(responses=[200, 401, 404])\n@user_blueprint.auth_required(api_jwt_auth)\n@flask_db.with_db_session()\ndef user_update_saved_search(\n    db_session: db.Session, user_id: UUID, saved_search_id: UUID, json_data: dict\n) -> response.ApiResponse:\n    logger.info(\"PUT /v1/users/:user_id/saved-searches/:saved_search_id\")\n\n    user_token_session: UserTokenSession = api_jwt_auth.get_user_token_session()\n\n    # Verify the authenticated user matches the requested user_id\n    if user_token_session.user_id != user_id:\n        raise_flask_error(401, \"Unauthorized user\")\n\n    with db_session.begin():\n        updated_saved_search = update_saved_search(db_session, user_id, saved_search_id, json_data)\n\n    logger.info(\n        \"Updated saved search for user\",\n        extra={\n            \"user.id\": str(user_id),\n            \"saved_search.id\": str(updated_saved_search.saved_search_id),\n        },\n    )\n\n    return response.ApiResponse(message=\"Success\")"}
{"path":"api/tests/src/data_migration/transformation/subtask/test_transform_funding_category.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_funding_category.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"File: api/src/api/users/user_schemas.py\nLanguage: py\nType: code\nDirectory: api/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/api/users/user_schemas.py\nSize: 4.97 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/data_migration/transformation/subtask/test_transform_funding_instrument.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_funding_instrument.py","size":1631931,"lastModified":"2025-02-14T17:08:31.127Z","content":"class UserTokenHeaderSchema(Schema):\n    x_oauth_login_gov = fields.String(\n        data_key=\"X-OAuth-login-gov\",\n        metadata={\n            \"description\": \"The login_gov header token\",\n        },\n    )\n\n\nclass UserSchema(Schema):\n    user_id = fields.String(\n        metadata={\n            \"description\": \"The internal ID of a user\",\n            \"example\": \"861a0148-cf2c-432b-b0b3-690016299ab1\",\n        }\n    )\n    email = fields.String(\n        metadata={\n            \"description\": \"The email address returned from Oauth2 provider\",\n            \"example\": \"user@example.com\",\n        }\n    )\n    external_user_type = fields.Enum(\n        ExternalUserType,\n        metadata={\n            \"description\": \"The Oauth2 provider through which a user was authenticated\",\n            \"example\": ExternalUserType.LOGIN_GOV,\n        },\n    )\n\n\nclass UserLoginGovCallbackSchema(Schema):\n    # This is defining the inputs we receive on the callback from login.gov's\n    # authorization endpoint and must match:\n    # https://developers.login.gov/oidc/authorization/#authorization-response\n    code = fields.String(\n        metadata={\n            \"description\": \"A unique authorization code that can be passed to the token endpoint\"\n        }\n    )\n    state = fields.String(\n        metadata={\"description\": \"The state value originally provided by us when calling login.gov\"}\n    )\n    error = fields.String(\n        allow_none=True,\n        metadata={\"description\": \"The error type, either access_denied or invalid_request\"},\n    )\n    error_description = fields.String(\n        allow_none=True, metadata={\"description\": \"A description of the error\"}\n    )\n\n\nclass UserTokenRefreshResponseSchema(AbstractResponseSchema):\n    # No data returned\n    data = fields.MixinField(metadata={\"example\": None})\n\n\nclass UserTokenLogoutResponseSchema(AbstractResponseSchema):\n    # No data returned\n    data = fields.MixinField(metadata={\"example\": None})\n\n\nclass UserGetResponseSchema(AbstractResponseSchema):\n    data = fields.Nested(UserSchema)\n\n\nclass UserSaveOpportunityRequestSchema(Schema):\n    opportunity_id = fields.Integer(required=True)\n\n\nclass UserSaveOpportunityResponseSchema(AbstractResponseSchema):\n    data = fields.MixinField(metadata={\"example\": None})\n\n\nclass UserDeleteSavedOpportunityResponseSchema(AbstractResponseSchema):\n    data = fields.MixinField(metadata={\"example\": None})\n\n\nclass UserSavedOpportunitiesResponseSchema(AbstractResponseSchema):\n    data = fields.List(\n        fields.Nested(SavedOpportunityResponseV1Schema),\n        metadata={\"description\": \"List of saved opportunities\"},\n    )\n\n\nclass UserSaveSearchRequestSchema(Schema):\n    name = fields.String(\n        required=True,\n        metadata={\"description\": \"Name of the saved search\", \"example\": \"Example search\"},\n    )\n    search_query = search_query = fields.Nested(OpportunitySearchRequestV1Schema)\n\n\nclass UserSaveSearchResponseSchema(AbstractResponseSchema):\n    data = fields.MixinField(metadata={\"example\": None})\n\n\nclass SavedSearchResponseSchema(Schema):\n    saved_search_id = fields.UUID(\n        metadata={\n            \"description\": \"The ID of the saved search\",\n            \"example\": \"123e4567-e89b-12d3-a456-426614174000\",\n        }\n    )\n    name = fields.String(\n        metadata={\n            \"description\": \"Name of the saved search\",\n            \"example\": \"Grant opportunities in California\",\n        }\n    )\n    search_query = fields.Nested(\n        OpportunitySearchRequestV1Schema,\n        metadata={\"description\": \"The saved search query parameters\"},\n    )\n    created_at = fields.DateTime(\n        metadata={\"description\": \"When the search was saved\", \"example\": \"2024-01-01T00:00:00Z\"}\n    )\n\n\nclass UserSavedSearchesRequestSchema(Schema):\n    pagination = fields.Nested(\n        generate_pagination_schema(\n            \"UserGetSavedSearchPaginationV1Schema\",\n            [\"created_at\", \"updated_at\", \"name\"],\n            default_sort_order=[{\"order_by\": \"created_at\", \"sort_direction\": \"descending\"}],\n        ),\n        required=True,\n    )\n\n\nclass UserSavedSearchesResponseSchema(AbstractResponseSchema):\n    data = fields.List(\n        fields.Nested(SavedSearchResponseSchema), metadata={\"description\": \"List of saved searches\"}\n    )\n\n\nclass UserDeleteSavedSearchResponseSchema(AbstractResponseSchema):\n    data = fields.MixinField(metadata={\"example\": None})\n\n\nclass UserUpdateSavedSearchRequestSchema(Schema):\n    name = fields.String(\n        required=True,\n        metadata={\"description\": \"Name of the saved search\", \"example\": \"Example search\"},\n    )\n\n\nclass UserUpdateSavedSearchResponseSchema(AbstractResponseSchema):\n    data = fields.MixinField(metadata={\"example\": None})"}
{"path":"api/tests/src/data_migration/transformation/subtask/test_transform_opportunity.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_opportunity.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/app.py\nLanguage: py\nType: code\nDirectory: api/src\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/app.py\nSize: 6.00 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/data_migration/transformation/subtask/test_transform_opportunity_attachment.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_opportunity_attachment.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"from apiflask import APIFlask, exceptions\nfrom flask_cors import CORS\nfrom pydantic import Field\n\nimport src.adapters.db as db\nimport src.adapters.db.flask_db as flask_db\nimport src.adapters.search as search\nimport src.adapters.search.flask_opensearch as flask_opensearch\nimport src.api.feature_flags.feature_flag_config as feature_flag_config\nimport src.logging\nimport src.logging.flask_logger as flask_logger\nfrom src.adapters.newrelic import init_newrelic\nfrom src.api.agencies_v1 import agency_blueprint as agencies_v1_blueprint\nfrom src.api.extracts_v1 import extract_blueprint as extracts_v1_blueprint\nfrom src.api.healthcheck import healthcheck_blueprint\nfrom src.api.opportunities_v1 import opportunity_blueprint as opportunities_v1_blueprint\nfrom src.api.response import restructure_error_response\nfrom src.api.schemas import response_schema\nfrom src.api.users.user_blueprint import user_blueprint\nfrom src.app_config import AppConfig\nfrom src.auth.api_jwt_auth import initialize_jwt_auth\nfrom src.auth.auth_utils import get_app_security_scheme\nfrom src.auth.login_gov_jwt_auth import initialize_login_gov_config\nfrom src.data_migration.data_migration_blueprint import data_migration_blueprint\nfrom src.search.backend.load_search_data_blueprint import load_search_data_blueprint\nfrom src.task import task_blueprint\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\nTITLE = \"Simpler Grants API\"\nAPI_OVERALL_VERSION = \"v0\"\nAPI_DESCRIPTION = \"\"\"\nBack end API for simpler.grants.gov.\n\nThis API is an ALPHA VERSION! Its current form is primarily for testing and feedback. Features are still under heavy development, and subject to change. Not for production use.\n\nSee [Release Phases](https://github.com/github/roadmap?tab=readme-ov-file#release-phases) for further details.\n\"\"\"\n\n\nclass AuthEndpointConfig(PydanticBaseEnvConfig):\n    auth_endpoint: bool = Field(False, alias=\"ENABLE_AUTH_ENDPOINT\")\n\n\ndef create_app() -> APIFlask:\n    app = APIFlask(__name__, title=TITLE, version=API_OVERALL_VERSION)\n\n    setup_logging(app)\n    init_newrelic()\n    register_db_client(app)\n\n    feature_flag_config.initialize()\n\n    CORS(app)\n    configure_app(app)\n    register_blueprints(app)\n    register_index(app)\n    register_robots_txt(app)\n    register_search_client(app)\n\n    auth_endpoint_config = AuthEndpointConfig()\n    if auth_endpoint_config.auth_endpoint:\n        initialize_login_gov_config()\n        initialize_jwt_auth()\n\n    return app\n\n\ndef setup_logging(app: APIFlask) -> None:\n    src.logging.init(__package__)\n    flask_logger.init_app(logging.root, app)\n\n\ndef register_db_client(app: APIFlask) -> None:\n    db_client = db.PostgresDBClient()\n    flask_db.register_db_client(db_client, app)\n\n\ndef register_search_client(app: APIFlask) -> None:\n    search_client = search.SearchClient()\n    flask_opensearch.register_search_client(search_client, app)\n\n\ndef configure_app(app: APIFlask) -> None:\n    app_config = AppConfig()\n\n    # Modify the response schema to instead use the format of our ApiResponse class\n    # which adds additional details to the object.\n    # https://apiflask.com/schema/#base-response-schema-customization\n    # app.config[\"BASE_RESPONSE_SCHEMA\"] = response_schema.ResponseSchema\n    app.config[\"HTTP_ERROR_SCHEMA\"] = response_schema.ErrorResponseSchema\n    app.config[\"VALIDATION_ERROR_SCHEMA\"] = response_schema.ErrorResponseSchema\n    app.config[\"SWAGGER_UI_CSS\"] = \"/static/swagger-ui.min.css\"\n    app.config[\"SWAGGER_UI_BUNDLE_JS\"] = \"/static/swagger-ui-bundle.js\"\n    app.config[\"SWAGGER_UI_STANDALONE_PRESET_JS\"] = \"/static/swagger-ui-standalone-preset.js\"\n    app.config[\"SWAGGER_UI_CONFIG\"] = {\n        \"persistAuthorization\": app_config.persist_authorization_openapi\n    }\n    # Removing because the server dropdown has accessibility issues.\n    app.config[\"SERVERS\"] = \".\"\n    app.config[\"DOCS_FAVICON\"] = \"https://simpler.grants.gov/img/favicon.ico\"\n\n    # Set a few values for the Swagger endpoint\n    app.config[\"OPENAPI_VERSION\"] = \"3.1.0\"\n\n    app.json.compact = False  # type: ignore\n\n    # Set various general OpenAPI config values\n    app.info = {\n        \"description\": API_DESCRIPTION,\n        \"contact\": {\n            \"name\": \"Simpler Grants.gov\",\n            \"url\": \"https://simpler.grants.gov/\",\n            \"email\": \"simpler@grants.gov\",\n        },\n    }\n\n    # Set the security schema and define the header param\n    # where we expect the API token to reside.\n    # See: https://apiflask.com/authentication/#use-external-authentication-library\n    app.security_schemes = get_app_security_scheme()\n\n    @app.error_processor\n    def error_processor(error: exceptions.HTTPError) -> Tuple[dict, int, Any]:\n        return restructure_error_response(error)\n\n\ndef register_blueprints(app: APIFlask) -> None:\n    app.register_blueprint(healthcheck_blueprint)\n    app.register_blueprint(opportunities_v1_blueprint)\n    app.register_blueprint(extracts_v1_blueprint)\n    app.register_blueprint(agencies_v1_blueprint)\n\n    auth_endpoint_config = AuthEndpointConfig()\n    if auth_endpoint_config.auth_endpoint:\n        app.register_blueprint(user_blueprint)\n\n    # Non-api blueprints\n    app.register_blueprint(data_migration_blueprint)\n    app.register_blueprint(task_blueprint)\n    app.register_blueprint(load_search_data_blueprint)\n\n\ndef get_project_root_dir() -> str:\n    return os.path.join(os.path.dirname(__file__), \"..\")\n\n\ndef register_index(app: APIFlask) -> None:\n    @app.route(\"/\")\n    @app.doc(hide=True)\n    def index() -> str:\n        return \"\"\"\n            <!Doctype html>\n            <html>\n                <head><title>Home</title></head>\n                <body>\n                    <h1>Home</h1>\n                    <p>Visit <a href=\"/docs\">/docs</a> to view the api documentation for this project.</p>\n                </body>\n            </html>\n        \"\"\"\n\n\ndef register_robots_txt(app: APIFlask) -> None:\n    @app.route(\"/robots.txt\")\n    @app.doc(hide=True)\n    def robots() -> str:\n        return \"\"\"\n        User-Agent: *\n        Allow: /docs\n        Disallow: /\n        \"\"\""}
{"path":"api/tests/src/data_migration/transformation/subtask/test_transform_opportunity_summary.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation/subtask","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_opportunity_summary.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/app_config.py\nLanguage: py\nType: code\nDirectory: api/src\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/app_config.py\nSize: 0.68 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/data_migration/transformation/test_transform_oracle_data_task.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/test_transform_oracle_data_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"class AppConfig(PydanticBaseEnvConfig):\n    # Set HOST to 127.0.0.1 by default to avoid other machines on the network\n    # from accessing the application. This is especially important if you are\n    # running the application locally on a public network. This needs to be\n    # overriden to 0.0.0.0 when running in a container\n    # See https://flask.palletsprojects.com/en/2.2.x/api/#flask.Flask.run\n    host: str = \"127.0.0.1\"\n    port: int = 8080\n\n    # For the OpenAPI docs, set whether the auth tokens are stored\n    # across refreshes of the page. Currently we only set this to true locally\n    persist_authorization_openapi: bool = False"}
{"path":"api/tests/src/data_migration/transformation/test_transform_util.py","language":"python","type":"code","directory":"api/tests/src/data_migration/transformation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/test_transform_util.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/auth/README.md\nLanguage: md\nType: code\nDirectory: api/src/auth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/README.md\nSize: 0.13 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/db/__init__.py","language":"python","type":"code","directory":"api/tests/src/db","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":""}
{"path":"api/tests/src/db/models/__init__.py","language":"python","type":"code","directory":"api/tests/src/db/models","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/auth/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/auth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/db/models/factories.py","language":"python","type":"code","directory":"api/tests/src/db/models","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/factories.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":""}
{"path":"api/tests/src/db/models/lookup/__init__.py","language":"python","type":"code","directory":"api/tests/src/db/models/lookup","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/lookup/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/auth/api_jwt_auth.py\nLanguage: py\nType: code\nDirectory: api/src/auth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/api_jwt_auth.py\nSize: 7.50 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/db/models/lookup/test_lookup.py","language":"python","type":"code","directory":"api/tests/src/db/models/lookup","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/lookup/test_lookup.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"import jwt\nfrom pydantic import Field\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import selectinload\n\nimport src.util.datetime_util as datetime_util\nfrom src.adapters import db\nfrom src.adapters.db import flask_db\nfrom src.api.route_utils import raise_flask_error\nfrom src.auth.auth_errors import JwtValidationError\nfrom src.auth.jwt_user_http_token_auth import JwtUserHttpTokenAuth\nfrom src.db.models.user_models import User, UserTokenSession\nfrom src.logging.flask_logger import add_extra_data_to_current_request_logs\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\napi_jwt_auth = JwtUserHttpTokenAuth(\n    \"ApiKey\", header=\"X-SGG-Token\", security_scheme_name=\"ApiJwtAuth\"\n)\n\n\nclass ApiJwtConfig(PydanticBaseEnvConfig):\n\n    private_key: str = Field(alias=\"API_JWT_PRIVATE_KEY\")\n    public_key: str = Field(alias=\"API_JWT_PUBLIC_KEY\")\n\n    issuer: str = Field(\"simpler-grants-api\", alias=\"API_JWT_ISSUER\")\n    audience: str = Field(\"simpler-grants-api\", alias=\"API_JWT_AUDIENCE\")\n\n    algorithm: str = Field(\"RS256\", alias=\"API_JWT_ALGORITHM\")\n\n    token_expiration_minutes: int = Field(30, alias=\"API_JWT_TOKEN_EXPIRATION_MINUTES\")\n\n\n# Initialize a config at startup that we'll use below\n_config: ApiJwtConfig | None = None\n\n\ndef initialize_jwt_auth() -> None:\n    global _config\n    if not _config:\n        _config = ApiJwtConfig()\n        logger.info(\n            \"Constructed JWT configuration\",\n            extra={\n                # NOTE: We don't just log the entire config\n                # because that would include the encryption keys\n                \"issuer\": _config.issuer,\n                \"audience\": _config.audience,\n                \"algorithm\": _config.algorithm,\n                \"token_expiration_minutes\": _config.token_expiration_minutes,\n            },\n        )\n\n\ndef get_config() -> ApiJwtConfig:\n    global _config\n\n    if _config is None:\n        raise Exception(\"No JWT configuration - initialize_jwt_auth() must be run first\")\n\n    return _config\n\n\ndef create_jwt_for_user(\n    user: User, db_session: db.Session, config: ApiJwtConfig | None = None, email: str | None = None\n) -> Tuple[str, UserTokenSession]:\n    if config is None:\n        config = get_config()\n\n    # Generate a random ID\n    token_id = uuid.uuid4()\n\n    # Always do all time checks in UTC for consistency\n    current_time = datetime_util.utcnow()\n    expiration_time = current_time + timedelta(minutes=config.token_expiration_minutes)\n\n    # Create the session in the DB\n    user_token_session = UserTokenSession(user=user, token_id=token_id, expires_at=expiration_time)\n    db_session.add(user_token_session)\n\n    # Create the JWT with information we'll want to receive back\n    payload = {\n        \"sub\": str(token_id),\n        # iat -> issued at\n        \"iat\": current_time,\n        \"aud\": config.audience,\n        \"iss\": config.issuer,\n        \"email\": email,\n        \"user_id\": str(user.user_id),\n    }\n\n    logger.info(\n        \"Created JWT token\",\n        extra={\n            \"auth.user_id\": str(user_token_session.user_id),\n            \"auth.token_id\": str(user_token_session.token_id),\n        },\n    )\n\n    return jwt.encode(payload, config.private_key, algorithm=\"RS256\"), user_token_session\n\n\ndef parse_jwt_for_user(\n    token: str, db_session: db.Session, config: ApiJwtConfig | None = None\n) -> UserTokenSession:\n    \"\"\"Handle processing a jwt token, and connecting it to a user token session in our DB\"\"\"\n    if config is None:\n        config = get_config()\n\n    current_timestamp = datetime_util.utcnow()\n\n    try:\n        parsed_jwt: dict = jwt.decode(\n            token,\n            config.public_key,\n            algorithms=[config.algorithm],\n            issuer=config.issuer,\n            audience=config.audience,\n            options={\n                \"verify_signature\": True,\n                \"verify_iat\": True,\n                \"verify_aud\": True,\n                \"verify_iss\": True,\n                # We do not set the following fields\n                # so do not want to validate.\n                \"verify_exp\": False,  # expiration is managed in the DB\n                \"verify_nbf\": False,  # Tokens are always fine to use immediately\n            },\n        )\n\n    except jwt.ImmatureSignatureError as e:  # IAT errors hit this\n        raise JwtValidationError(\"Token not yet valid\") from e\n    except jwt.InvalidIssuerError as e:\n        raise JwtValidationError(\"Unknown Issuer\") from e\n    except jwt.InvalidAudienceError as e:\n        raise JwtValidationError(\"Unknown Audience\") from e\n    except jwt.PyJWTError as e:\n        # Every other error case wrap in the same generic error message.\n        raise JwtValidationError(\"Unable to process token\") from e\n\n    sub_id = parsed_jwt.get(\"sub\", None)\n    if sub_id is None:\n        raise JwtValidationError(\"Token missing sub field\")\n\n    token_session: UserTokenSession | None = db_session.execute(\n        select(UserTokenSession)\n        .where(UserTokenSession.token_id == sub_id)\n        .options(selectinload(\"*\"))\n    ).scalar()\n\n    # We check both the token expires_at timestamp as well as an\n    # is_valid flag to make sure the token is still valid.\n    if token_session is None:\n        raise JwtValidationError(\"Token session does not exist\")\n    if token_session.expires_at < current_timestamp:\n        raise JwtValidationError(\"Token expired\")\n    if token_session.is_valid is False:\n        raise JwtValidationError(\"Token is no longer valid\")\n\n    return token_session\n\n\n@api_jwt_auth.verify_token\n@flask_db.with_db_session()\ndef decode_token(db_session: db.Session, token: str) -> UserTokenSession:\n    \"\"\"\n    Process an internal jwt token as created by the above create_jwt_for_user method.\n\n    To add this auth to an endpoint, simply put::\n\n        from src.auth.api_jwt_auth import api_jwt_auth\n\n        @example_blueprint.get(\"/example\")\n        @example_blueprint.auth_required(api_jwt_auth)\n        @flask_db.with_db_session()\n        def example_method(db_session: db.Session) -> response.ApiResponse:\n            # The token session object can be fetched from the auth object\n            token_session: UserTokenSession = api_jwt_auth.current_user\n\n            # If you want to modify the token_session or user, you will\n            # need to add it to the DB session otherwise it won't do anything\n            db_session.add(token_session)\n            token_session.expires_at = ...\n            ...\n    \"\"\"\n\n    try:\n        user_token_session = parse_jwt_for_user(token, db_session)\n\n        add_extra_data_to_current_request_logs(\n            {\n                \"auth.user_id\": str(user_token_session.user_id),\n                \"auth.token_id\": str(user_token_session.token_id),\n            }\n        )\n        logger.info(\"JWT Authentication Successful\")\n\n        # Return the user token session object\n        return user_token_session\n    except JwtValidationError as e:\n        # If validation of the jwt fails, pass the error message back to the user\n        # The message is just the value we set when constructing the JwtValidationError\n        logger.info(\"JWT Authentication Failed for provided token\", extra={\"auth.issue\": e.message})\n        raise_flask_error(401, e.message)\n\n\ndef refresh_token_expiration(\n    token_session: UserTokenSession, config: ApiJwtConfig | None = None\n) -> UserTokenSession:\n    if config is None:\n        config = get_config()\n\n    expiration_time = datetime_util.utcnow() + timedelta(minutes=config.token_expiration_minutes)\n    token_session.expires_at = expiration_time\n\n    return token_session"}
{"path":"api/tests/src/db/models/lookup/test_lookup_registry.py","language":"python","type":"code","directory":"api/tests/src/db/models/lookup","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/lookup/test_lookup_registry.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/auth/api_key_auth.py\nLanguage: py\nType: code\nDirectory: api/src/auth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/api_key_auth.py\nSize: 3.38 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/db/models/lookup/test_sync_lookup_values.py","language":"python","type":"code","directory":"api/tests/src/db/models/lookup","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/lookup/test_sync_lookup_values.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"import flask\nfrom apiflask import HTTPTokenAuth\n\nfrom src.api.route_utils import raise_flask_error\nfrom src.logging.flask_logger import add_extra_data_to_current_request_logs\n\nlogger = logging.getLogger(__name__)\n\n# Initialize the authorization context\n# this needs to be attached to your\n# routes as `your_blueprint.auth_required(api_key_auth)`\n# in order to enable authorization\napi_key_auth = HTTPTokenAuth(\"ApiKey\", header=\"X-Auth\", security_scheme_name=\"ApiKeyAuth\")\n\n\n@dataclass\nclass User:\n    # A very basic \"user\" for the purposes of logging\n    # which API key was used to call us.\n    username: str\n\n\nAPI_AUTH_USERS: dict[str, User] | None = None\n\n\n@api_key_auth.verify_token\ndef verify_token(token: str) -> User:\n    logger.info(\"Authenticating provided token\")\n\n    user = process_token(token)\n\n    # Note that the current user can also be found\n    # by doing api_key_auth.current_user once in\n    # the request context. This is here in case\n    # multiple authentication approaches exist\n    # in your API, you don't need to check each\n    # one in order to figure out which was actually used\n    flask.g.current_user = user\n\n    # Add the \"username\" to all logs for the rest of the request lifecycle\n    add_extra_data_to_current_request_logs({\"auth.username\": user.username})\n\n    logger.info(\"Authentication successful\")\n\n    return user\n\n\ndef _get_auth_users() -> dict[str, User] | None:\n    # To avoid every request re-parsing the auth token\n    # string, load it once and store in a global variable\n    # This doesn't really account for threading, but worst case\n    # multiple threads write the same value\n    global API_AUTH_USERS\n\n    if API_AUTH_USERS is not None:\n        return API_AUTH_USERS\n\n    raw_auth_tokens = os.getenv(\"API_AUTH_TOKEN\", None)\n\n    if raw_auth_tokens is None:\n        return None\n\n    API_AUTH_USERS = {}\n\n    # Auth tokens will look like some_value,another_value,a_third_value\n    # and get usernames like:\n    # some_value    -> auth_token_0\n    # another_value -> auth_token_1\n    # a_third_value -> auth_token_2\n    #\n    # This username is just used for logging, and is deliberately very\n    # simple until we build out authentication more. This just gives us the most\n    # barebones ability to do distinguish our users.\n    auth_tokens = raw_auth_tokens.split(\",\")\n    for i, token in enumerate(auth_tokens):\n        API_AUTH_USERS[token] = User(username=f\"auth_token_{i}\")\n\n    return API_AUTH_USERS\n\n\ndef process_token(token: str) -> User:\n    # WARNING: this isn't really a production ready\n    # auth approach. In reality the user object returned\n    # here should be pulled from a DB or auth service, but\n    # as there are several types of authentication, we are\n    # keeping this pretty basic for the out-of-the-box approach\n    api_auth_users = _get_auth_users()\n\n    if not api_auth_users:\n        logger.error(\n            \"Authentication is not setup, please add an API_AUTH_TOKEN environment variable.\"\n        )\n        raise_flask_error(\n            401, \"Authentication is not setup properly and the user cannot be authenticated\"\n        )\n\n    user = api_auth_users.get(token, None)\n\n    if user is None:\n        logger.info(\"Authentication failed for provided auth token.\")\n        raise_flask_error(\n            401, \"The server could not verify that you are authorized to access the URL requested\"\n        )\n\n    return user"}
{"path":"api/tests/src/db/models/test_factories.py","language":"python","type":"code","directory":"api/tests/src/db/models","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/test_factories.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/auth/auth_errors.py\nLanguage: py\nType: code\nDirectory: api/src/auth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/auth_errors.py\nSize: 0.28 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/db/models/test_relationships.py","language":"python","type":"code","directory":"api/tests/src/db/models","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/test_relationships.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"def __init__(self, message: str):\n        super().__init__(message)\n        self.message = message"}
{"path":"api/tests/src/db/test_migrations.py","language":"python","type":"code","directory":"api/tests/src/db","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/test_migrations.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/auth/auth_utils.py\nLanguage: py\nType: code\nDirectory: api/src/auth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/auth_utils.py\nSize: 2.64 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/logging/__init__.py","language":"python","type":"code","directory":"api/tests/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/logging/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"import flask\nfrom apiflask.exceptions import HTTPError\n\nfrom src.api import response\nfrom src.auth.login_gov_jwt_auth import get_final_redirect_uri\n\nlogger = logging.getLogger(__name__)\n\nP = ParamSpec(\"P\")\nINTERNAL_ERROR = \"internal error\"\n\n\ndef get_app_security_scheme() -> dict[str, Any]:\n    return {\n        \"ApiKeyAuth\": {\"type\": \"apiKey\", \"in\": \"header\", \"name\": \"X-Auth\"},\n        \"ApiJwtAuth\": {\"type\": \"apiKey\", \"in\": \"header\", \"name\": \"X-SGG-Token\"},\n    }\n\n\ndef with_login_redirect_error_handler() -> Callable[..., Callable[P, flask.Response]]:\n    \"\"\"Wrapper function to handle catching errors and redirecting\n\n    Because several of our login functions don't have standard 2xx returns\n    and instead redirect the user, we also redirect in the case of errors\n    so that they stay on the frontend, but we pass errors along.\n\n    Usage::\n\n        @with_login_redirect_error_handler()\n        def foo(...):\n            logger.info(\"hello\")\n\n            if condition:\n                raise_flask_error(...) # this will get caught and a redirect will occur\n\n            return ...\n\n    \"\"\"\n\n    def decorator(f: Callable[P, flask.Response]) -> Callable[P, flask.Response]:\n        @functools.wraps(f)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> flask.Response:\n            try:\n                return f(*args, **kwargs)\n            except HTTPError as e:\n                # HTTPError is what raise_flask_error raises\n                # and should encompass our \"expected\" errors\n                # that aren't a concern, as long as it isn't a 5xx\n                message = e.message\n                logger.info(\"Login flow failed: %s\", message)\n\n                # But we still don't expect 5xx errors\n                if e.status_code >= 500:\n                    message = INTERNAL_ERROR\n                    logger.exception(\n                        \"Unexpected error occurred in login flow via raise_flask_error: %s\",\n                        e.message,\n                    )\n\n                return response.redirect_response(\n                    get_final_redirect_uri(\"error\", error_description=message)\n                )\n            except Exception:\n                # Any other exception, we'll just use a generic error message to be safe\n                # but this means an unexpected error occurred and we should log an error\n                logger.exception(\"Unexpected error occurred in login flow\")\n                return response.redirect_response(\n                    get_final_redirect_uri(\"error\", error_description=INTERNAL_ERROR)\n                )\n\n        return wrapper\n\n    return decorator"}
{"path":"api/tests/src/logging/test_audit.py","language":"python","type":"code","directory":"api/tests/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/logging/test_audit.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/auth/jwt_user_http_token_auth.py\nLanguage: py\nType: code\nDirectory: api/src/auth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/jwt_user_http_token_auth.py\nSize: 0.51 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/logging/test_flask_logger.py","language":"python","type":"code","directory":"api/tests/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/logging/test_flask_logger.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"from apiflask import HTTPTokenAuth\n\nfrom src.db.models.user_models import UserTokenSession\n\n\nclass JwtUserHttpTokenAuth(HTTPTokenAuth):\n\n    def get_user_token_session(self) -> UserTokenSession:\n        \"\"\"Wrapper method around the current_user value to handle type issues\n\n        Note that this value gets set based on whatever is returned from the method\n        you configure for @<your JwtUserHttpTokenAuth obj>.verify_token\n        \"\"\"\n        return cast(UserTokenSession, self.current_user)"}
{"path":"api/tests/src/logging/test_formatters.py","language":"python","type":"code","directory":"api/tests/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/logging/test_formatters.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/auth/login_gov_jwt_auth.py\nLanguage: py\nType: code\nDirectory: api/src/auth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/auth/login_gov_jwt_auth.py\nSize: 10.25 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/logging/test_logging.py","language":"python","type":"code","directory":"api/tests/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/logging/test_logging.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"import flask\nimport jwt\nfrom pydantic import Field\n\nfrom src.adapters import db\nfrom src.auth.auth_errors import JwtValidationError\nfrom src.db.models.user_models import LoginGovState\nfrom src.util import datetime_util\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass LoginGovConfig(PydanticBaseEnvConfig):\n    \"\"\"\n    Configuration for login.gov JWT auth\n\n    Do not create this directly, instead call get_login_gov_config\n    which will handle setting it up for you.\n    \"\"\"\n\n    # Public keys likely won't ever be set by an env var, so it defaults\n    # to an empty dict and gets overriden by any call to _refresh_keys\n    public_key_map: dict[str, jwt.PyJWK | str] = Field(\n        alias=\"LOGIN_GOV_PUBLIC_KEY_MAP\", default_factory=dict\n    )\n\n    encryption_algorithm: str = Field(alias=\"LOGIN_GOV_ENCRYPTION_ALGORITHM\", default=\"RS256\")\n\n    client_id: str = Field(alias=\"LOGIN_GOV_CLIENT_ID\")\n    acr_value: str = Field(alias=\"LOGIN_GOV_ACR_VALUE\", default=\"urn:acr.login.gov:auth-only\")\n    scope: str = Field(alias=\"LOGIN_GOV_SCOPE\", default=\"openid email\")\n\n    # While all of these endpoints are under the same root, we define the full\n    # path each time because the local mock uses a different naming convention\n    login_gov_endpoint: str = Field(alias=\"LOGIN_GOV_ENDPOINT\")\n    login_gov_jwk_endpoint: str = Field(alias=\"LOGIN_GOV_JWK_ENDPOINT\")\n    login_gov_auth_endpoint: str = Field(alias=\"LOGIN_GOV_AUTH_ENDPOINT\")\n    login_gov_token_endpoint: str = Field(alias=\"LOGIN_GOV_TOKEN_ENDPOINT\")\n\n    # Where we send a user after they have successfully logged in\n    # for now we'll always send them to the same place (a frontend page)\n    login_final_destination: str = Field(alias=\"LOGIN_FINAL_DESTINATION\")\n\n    # The private key we gave login.gov for private_key_jwt validation in the token endpoint\n    # See: https://developers.login.gov/oidc/token/#client_assertion\n    login_gov_client_assertion_private_key: str = Field(\n        alias=\"LOGIN_GOV_CLIENT_ASSERTION_PRIVATE_KEY\"\n    )\n\n\n# Initialize a config at startup\n_config: LoginGovConfig | None = None\n\n\ndef initialize_login_gov_config() -> None:\n    global _config\n    if not _config:\n        _config = LoginGovConfig()\n\n        logger.info(\n            \"Constructed login.gov configuration\",\n            extra={\n                \"login_gov_endpoint\": _config.login_gov_endpoint,\n                \"login_gov_jwk_endpoint\": _config.login_gov_jwk_endpoint,\n                \"login_gov_auth_endpoint\": _config.login_gov_auth_endpoint,\n            },\n        )\n\n\ndef get_config() -> LoginGovConfig:\n    global _config\n\n    if _config is None:\n        raise Exception(\n            \"No Login.gov configuration - initialize_login_gov_config() must be run first\"\n        )\n\n    return _config\n\n\n@dataclasses.dataclass\nclass LoginGovUser:\n    user_id: str\n    email: str\n\n\ndef _refresh_keys(config: LoginGovConfig) -> None:\n    \"\"\"\n    WARNING:\n        This implementation is technically incorrect as it does\n        not account for thread safety. If multiple threads attempt to\n        refresh the token at the same time, they will all set it separately.\n\n        Assignment in python should be atomic, and the Python global-interpreter-lock\n        likely make this less risky, but there is no guarantee that this won't\n        cause issues as we use it.\n\n        We will evaluate this behavior over time and see if it causes us any issues.\n        For now, we are fine accepting this risk as the complexity of caching this\n        in a thread-safe way (eg. database, redis, or using Python locks)\n        isn't seen as worthwhile at the moment.\n    \"\"\"\n    logger.info(\"Refreshing login.gov JWKs\")\n    jwk_client = jwt.PyJWKClient(config.login_gov_jwk_endpoint)\n    public_keys = jwk_client.get_jwk_set()\n\n    public_key_map: dict[str, jwt.PyJWK | str] = {\n        key.key_id: key for key in public_keys.keys if key.key_id is not None\n    }\n\n    if public_key_map.keys() != config.public_key_map.keys():\n        logger.info(\"Found login.gov JWKs %s\", public_key_map.keys())\n\n    # This line is possibly an issue for the reasons described above.\n    config.public_key_map = public_key_map\n\n\ndef get_login_gov_redirect_uri(db_session: db.Session, config: LoginGovConfig | None = None) -> str:\n    if config is None:\n        config = get_config()\n\n    nonce = uuid.uuid4()\n    state = uuid.uuid4()\n\n    # Ask Flask for its own URI - specifying we want the callback route\n    # .user_login_callback points to the function itself defined in user_routes.py\n    redirect_uri = flask.url_for(\".user_login_callback\", _external=True)\n\n    # We want to redirect to the authorization endpoint of login.gov\n    # See: https://developers.login.gov/oidc/authorization/\n    encoded_params = urllib.parse.urlencode(\n        {\n            \"client_id\": config.client_id,\n            \"nonce\": nonce,\n            \"state\": state,\n            \"redirect_uri\": redirect_uri,\n            \"acr_values\": config.acr_value,\n            \"scope\": config.scope,\n            # These are statically defined by the spec\n            \"prompt\": \"select_account\",\n            \"response_type\": \"code\",\n        }\n    )\n\n    # Add the state to the DB\n    db_session.add(LoginGovState(login_gov_state_id=state, nonce=nonce))\n\n    return f\"{config.login_gov_auth_endpoint}?{encoded_params}\"\n\n\ndef get_login_gov_client_assertion(config: LoginGovConfig | None = None) -> str:\n    \"\"\"Generate a client assertion token for login.gov auth\"\"\"\n    if config is None:\n        config = get_config()\n\n    # Docs recommend a 5 minute expiration time\n    current_time = datetime_util.utcnow()\n    expiration_time = current_time + timedelta(minutes=5)\n\n    # See: https://developers.login.gov/oidc/token/#client_assertion\n    client_assertion_payload = {\n        \"iss\": config.client_id,\n        \"sub\": config.client_id,\n        \"aud\": config.login_gov_token_endpoint,\n        \"jti\": str(uuid.uuid4()),\n        \"exp\": expiration_time,\n    }\n\n    return jwt.encode(\n        client_assertion_payload, config.login_gov_client_assertion_private_key, algorithm=\"RS256\"\n    )\n\n\ndef get_final_redirect_uri(\n    message: str,\n    token: str | None = None,\n    is_user_new: bool | None = None,\n    error_description: str | None = None,\n    config: LoginGovConfig | None = None,\n) -> str:\n    if config is None:\n        config = get_config()\n\n    params: dict = {\"message\": message}\n\n    if token is not None:\n        params[\"token\"] = token\n\n    if is_user_new is not None:\n        params[\"is_user_new\"] = int(is_user_new)  # put booleans in the URL as 0/1\n\n    if error_description is not None:\n        params[\"error_description\"] = error_description\n\n    encoded_params = urllib.parse.urlencode(params)\n\n    return f\"{config.login_final_destination}?{encoded_params}\"\n\n\ndef validate_token(token: str, nonce: str, config: LoginGovConfig | None = None) -> LoginGovUser:\n    if not config:\n        config = get_config()\n\n    try:\n        # To get the KID, we need parse the jwt\n        unverified_token = jwt.api_jwt.decode_complete(token, options={\"verify_signature\": False})\n    except jwt.DecodeError as e:\n        # This would mean the token was malformed - likely not a jwt at all\n        raise JwtValidationError(\"Unable to parse token - invalid format\") from e\n\n    # Get the KID (key ID)\n    kid: str | None = unverified_token.get(\"header\", {}).get(\"kid\", None)\n    if kid is None:\n        raise JwtValidationError(\"Auth token missing KID\")\n\n    public_key = _get_key_for_kid(kid, config)\n\n    return _validate_token_with_key(token, nonce, public_key, config)\n\n\ndef _get_key_for_kid(kid: str, config: LoginGovConfig, refresh: bool = True) -> jwt.PyJWK | str:\n    \"\"\"Get the public key for the given KID (Key ID)\"\"\"\n    key = config.public_key_map.get(kid, None)\n    if key is not None:\n        return key\n\n    # Fetch the latest keys from login.gov and try again\n    if refresh:\n        _refresh_keys(config)\n        return _get_key_for_kid(kid, config, refresh=False)\n\n    raise JwtValidationError(\"No public key could be found for token\")\n\n\ndef _validate_token_with_key(\n    token: str, nonce: str, public_key: jwt.PyJWK | str, config: LoginGovConfig\n) -> LoginGovUser:\n    # We are processing the id_token as described on:\n    # https://developers.login.gov/oidc/token/#token-response\n    try:\n        data = jwt.api_jwt.decode_complete(\n            token,\n            public_key,\n            algorithms=[config.encryption_algorithm],\n            issuer=config.login_gov_endpoint,\n            audience=config.client_id,\n            # By default these options are already set to validate\n            # but making it very clear / explicit the validations we are doing\n            options={\n                \"verify_signature\": True,\n                \"verify_exp\": True,\n                \"verify_iat\": True,\n                \"verify_nbf\": True,\n                \"verify_aud\": True,\n                \"verify_iss\": True,\n            },\n        )\n        payload = data.get(\"payload\", {})\n\n        payload_nonce = payload.get(\"nonce\", None)\n        if payload_nonce != nonce:\n            raise JwtValidationError(\"Nonce does not match expected\")\n\n        user_id = payload[\"sub\"]\n        email = payload[\"email\"]\n\n        return LoginGovUser(user_id=user_id, email=email)\n\n    # Most exceptions will result in an outright error\n    # as the only change to calls to this function are the public keys\n    # we use to validate. Unless it is a public-key-validation related error\n    # just reraise as a JwtValidationError here\n    except KeyError as e:\n        raise JwtValidationError(\"Token Missing Required Field(s)\") from e\n    except jwt.ExpiredSignatureError as e:\n        raise JwtValidationError(\"Expired Token\") from e\n    except jwt.ImmatureSignatureError as e:  # IAT and NBF errors hit this\n        raise JwtValidationError(\"Token not yet valid\") from e\n    except jwt.InvalidIssuerError as e:\n        raise JwtValidationError(\"Unknown Issuer\") from e\n    except jwt.InvalidAudienceError as e:\n        raise JwtValidationError(\"Unknown Audience\") from e\n    except jwt.InvalidSignatureError as e:  # Token signature does not match\n        raise JwtValidationError(\"Invalid Signature\") from e\n    except jwt.PyJWTError as e:  # Every other type of JWT error not caught above\n        raise JwtValidationError(\"Unable to process token\") from e"}
{"path":"api/tests/src/logging/test_pii.py","language":"python","type":"code","directory":"api/tests/src/logging","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/logging/test_pii.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/constants/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/constants\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/constants/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.437Z"}
{"path":"api/tests/src/pagination/__init__.py","language":"python","type":"code","directory":"api/tests/src/pagination","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/pagination/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":""}
{"path":"api/tests/src/pagination/test_paginator.py","language":"python","type":"code","directory":"api/tests/src/pagination","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/pagination/test_paginator.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/constants/lookup_constants.py\nLanguage: py\nType: code\nDirectory: api/src/constants\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/constants/lookup_constants.py\nSize: 4.71 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/schemas/__init__.py","language":"python","type":"code","directory":"api/tests/src/schemas","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/schemas/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"class OpportunityStatus(StrEnum):\n    FORECASTED = \"forecasted\"\n    POSTED = \"posted\"\n    CLOSED = \"closed\"\n    ARCHIVED = \"archived\"\n\n\nclass OpportunityCategoryLegacy(StrEnum):\n    # These are only used where the legacy system\n    # needs to specify the values and can be removed\n    # when our v0 endpoint is removed\n    DISCRETIONARY = \"D\"\n    MANDATORY = \"M\"\n    CONTINUATION = \"C\"\n    EARMARK = \"E\"\n    OTHER = \"O\"\n\n\nclass OpportunityCategory(StrEnum):\n    DISCRETIONARY = \"discretionary\"\n    MANDATORY = \"mandatory\"\n    CONTINUATION = \"continuation\"\n    EARMARK = \"earmark\"\n    OTHER = \"other\"\n\n\nclass ApplicantType(StrEnum):\n    # https://grants.gov/system-to-system/grantor-system-to-system/schemas/grants-funding-synopsis#EligibleApplicantTypes\n    # Comment is the legacy systems code\n\n    STATE_GOVERNMENTS = \"state_governments\"  # 00\n    COUNTY_GOVERNMENTS = \"county_governments\"  # 01\n    CITY_OR_TOWNSHIP_GOVERNMENTS = \"city_or_township_governments\"  # 02\n    SPECIAL_DISTRICT_GOVERNMENTS = \"special_district_governments\"  # 04\n\n    INDEPENDENT_SCHOOL_DISTRICTS = \"independent_school_districts\"  # 05\n    PUBLIC_AND_STATE_INSTITUTIONS_OF_HIGHER_EDUCATION = (\n        \"public_and_state_institutions_of_higher_education\"  # 06\n    )\n    PRIVATE_INSTITUTIONS_OF_HIGHER_EDUCATION = \"private_institutions_of_higher_education\"  # 20\n\n    FEDERALLY_RECOGNIZED_NATIVE_AMERICAN_TRIBAL_GOVERNMENTS = (\n        \"federally_recognized_native_american_tribal_governments\"  # 07\n    )\n    # This was previously \"Native American tribal organizations other than Federally recognized tribal governments\"\n    OTHER_NATIVE_AMERICAN_TRIBAL_ORGANIZATIONS = \"other_native_american_tribal_organizations\"  # 11\n    PUBLIC_AND_INDIAN_HOUSING_AUTHORITIES = \"public_and_indian_housing_authorities\"  # 08\n\n    NONPROFITS_NON_HIGHER_EDUCATION_WITH_501C3 = \"nonprofits_non_higher_education_with_501c3\"  # 12\n    NONPROFITS_NON_HIGHER_EDUCATION_WITHOUT_501C3 = (\n        \"nonprofits_non_higher_education_without_501c3\"  # 13\n    )\n\n    INDIVIDUALS = \"individuals\"  # 21\n    FOR_PROFIT_ORGANIZATIONS_OTHER_THAN_SMALL_BUSINESSES = (\n        \"for_profit_organizations_other_than_small_businesses\"  # 22\n    )\n    SMALL_BUSINESSES = \"small_businesses\"  # 23\n    OTHER = \"other\"  # 25\n    UNRESTRICTED = \"unrestricted\"  # 99\n\n\nclass FundingCategory(StrEnum):\n    # https://grants.gov/system-to-system/grantor-system-to-system/schemas/grants-funding-synopsis#FundingActivityCategory\n    # Comment is the legacy systems code\n\n    RECOVERY_ACT = \"recovery_act\"  # RA\n    AGRICULTURE = \"agriculture\"  # AG\n    ARTS = \"arts\"  # AR\n    BUSINESS_AND_COMMERCE = \"business_and_commerce\"  # BC\n    COMMUNITY_DEVELOPMENT = \"community_development\"  # CD\n    CONSUMER_PROTECTION = \"consumer_protection\"  # CP\n    DISASTER_PREVENTION_AND_RELIEF = \"disaster_prevention_and_relief\"  # DPR\n    EDUCATION = \"education\"  # ED\n    EMPLOYMENT_LABOR_AND_TRAINING = \"employment_labor_and_training\"  # ELT\n    ENERGY = \"energy\"  # EN\n    ENVIRONMENT = \"environment\"  # ENV\n    FOOD_AND_NUTRITION = \"food_and_nutrition\"  # FN\n    HEALTH = \"health\"  # HL\n    HOUSING = \"housing\"  # HO\n    HUMANITIES = \"humanities\"  # HU\n    INFRASTRUCTURE_INVESTMENT_AND_JOBS_ACT = \"infrastructure_investment_and_jobs_act\"  # IIJ\n    INFORMATION_AND_STATISTICS = \"information_and_statistics\"  # IS\n    INCOME_SECURITY_AND_SOCIAL_SERVICES = \"income_security_and_social_services\"  # ISS\n    LAW_JUSTICE_AND_LEGAL_SERVICES = \"law_justice_and_legal_services\"  # LJL\n    NATURAL_RESOURCES = \"natural_resources\"  # NR\n    OPPORTUNITY_ZONE_BENEFITS = \"opportunity_zone_benefits\"  # OZ\n    REGIONAL_DEVELOPMENT = \"regional_development\"  # RD\n    SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT = (\n        \"science_technology_and_other_research_and_development\"  # ST\n    )\n    TRANSPORTATION = \"transportation\"  # T\n    AFFORDABLE_CARE_ACT = \"affordable_care_act\"  # ACA\n    OTHER = \"other\"  # O\n\n\nclass FundingInstrument(StrEnum):\n    # https://grants.gov/system-to-system/grantor-system-to-system/schemas/grants-funding-synopsis#FundingInstrument\n    # Comment is the legacy systems code\n\n    COOPERATIVE_AGREEMENT = \"cooperative_agreement\"  # CA\n    GRANT = \"grant\"  # G\n    PROCUREMENT_CONTRACT = \"procurement_contract\"  # PC\n    OTHER = \"other\"  # O\n\n\nclass AgencyDownloadFileType(StrEnum):\n    XML = \"xml\"\n    PDF = \"pdf\"\n\n\nclass AgencySubmissionNotificationSetting(StrEnum):\n    NEVER = \"never\"\n    FIRST_APPLICATION_ONLY = \"first_application_only\"\n    ALWAYS = \"always\"\n\n\nclass ExtractType(StrEnum):\n    OPPORTUNITIES_JSON = \"opportunities_json\"\n    OPPORTUNITIES_CSV = \"opportunities_csv\"\n\n\nclass ExternalUserType(StrEnum):\n    LOGIN_GOV = \"login_gov\"\n\n\nclass JobStatus(StrEnum):\n    STARTED = \"started\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\""}
{"path":"api/tests/src/schemas/extension/__init__.py","language":"python","type":"code","directory":"api/tests/src/schemas/extension","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/schemas/extension/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/constants/schema.py\nLanguage: py\nType: code\nDirectory: api/src/constants\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/constants/schema.py\nSize: 0.11 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/schemas/extension/test_schema_fields.py","language":"python","type":"code","directory":"api/tests/src/schemas/extension","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/schemas/extension/test_schema_fields.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"class Schemas(StrEnum):\n    API = \"api\"\n    LEGACY = \"legacy\"\n    STAGING = \"staging\""}
{"path":"api/tests/src/schemas/schema_validation_utils.py","language":"python","type":"code","directory":"api/tests/src/schemas","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/schemas/schema_validation_utils.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/__init__.py\nSize: 0.31 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/search/__init__.py","language":"python","type":"code","directory":"api/tests/src/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/search/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"from . import command  # noqa: F401\n\n# import any of the other files so they get initialized and attached to the blueprint\nimport src.data_migration.setup_foreign_tables  # noqa: F401 E402 isort:skip\n\n__all__ = [\"data_migration_blueprint\"]"}
{"path":"api/tests/src/search/backend/__init__.py","language":"python","type":"code","directory":"api/tests/src/search/backend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/search/backend/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/command/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/command\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/command/__init__.py\nSize: 0.11 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/search/backend/test_load_opportunities_to_index.py","language":"python","type":"code","directory":"api/tests/src/search/backend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/search/backend/test_load_opportunities_to_index.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":""}
{"path":"api/tests/src/services/extracts_v1/test_get_extracts.py","language":"python","type":"code","directory":"api/tests/src/services/extracts_v1","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/services/extracts_v1/test_get_extracts.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/command/load_transform.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/command\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/command/load_transform.py\nSize: 2.02 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/services/opportunity_attachments/__init__.py","language":"python","type":"code","directory":"api/tests/src/services/opportunity_attachments","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/services/opportunity_attachments/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"import logging\n\nimport click\n\nimport src.adapters.db as db\nimport src.adapters.db.flask_db as flask_db\nimport src.db.models.foreign\nimport src.db.models.staging\nfrom src.task.ecs_background_task import ecs_background_task\nfrom src.task.opportunities.set_current_opportunities_task import SetCurrentOpportunitiesTask\n\nfrom ..data_migration_blueprint import data_migration_blueprint\nfrom ..load.load_oracle_data_task import LoadOracleDataTask\nfrom ..transformation.transform_oracle_data_task import TransformOracleDataTask\n\nlogger = logging.getLogger(__name__)\n\n\n@data_migration_blueprint.cli.command(\n    \"load-transform\", help=\"Load and transform data from the legacy database into our database\"\n)\n@click.option(\"--load/--no-load\", default=True, help=\"run LoadOracleDataTask\")\n@click.option(\"--transform/--no-transform\", default=True, help=\"run TransformOracleDataTask\")\n@click.option(\n    \"--set-current/--no-set-current\", default=True, help=\"run SetCurrentOpportunitiesTask\"\n)\n@click.option(\n    \"--insert-chunk-size\", default=800, help=\"chunk size for load inserts\", show_default=True\n)\n@click.option(\"--tables-to-load\", \"-t\", help=\"table to load\", multiple=True)\n@flask_db.with_db_session()\n@ecs_background_task(task_name=\"load-transform\")\ndef load_transform(\n    db_session: db.Session,\n    load: bool,\n    transform: bool,\n    set_current: bool,\n    insert_chunk_size: int,\n    tables_to_load: list[str],\n) -> None:\n    logger.info(\"load and transform start\")\n\n    foreign_tables = {t.name: t for t in src.db.models.foreign.metadata.tables.values()}\n    staging_tables = {t.name: t for t in src.db.models.staging.metadata.tables.values()}\n\n    if load:\n        LoadOracleDataTask(\n            db_session, foreign_tables, staging_tables, tables_to_load, insert_chunk_size\n        ).run()\n    if transform:\n        TransformOracleDataTask(db_session).run()\n    if set_current:\n        SetCurrentOpportunitiesTask(db_session).run()\n\n    logger.info(\"load and transform complete\")"}
{"path":"api/tests/src/services/opportunity_attachments/test_attachment_util.py","language":"python","type":"code","directory":"api/tests/src/services/opportunity_attachments","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/services/opportunity_attachments/test_attachment_util.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/data_migration_blueprint.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/data_migration_blueprint.py\nSize: 0.16 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/task/__init__.py","language":"python","type":"code","directory":"api/tests/src/task","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"data_migration_blueprint = APIBlueprint(\n    \"data-migration\", __name__, enable_openapi=False, cli_group=\"data-migration\"\n)"}
{"path":"api/tests/src/task/analytics/__init__.py","language":"python","type":"code","directory":"api/tests/src/task/analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/analytics/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/load/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/load\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/load/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/task/analytics/test_create_analytics_db_csvs.py","language":"python","type":"code","directory":"api/tests/src/task/analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/analytics/test_create_analytics_db_csvs.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":""}
{"path":"api/tests/src/task/notifications/test_generate_notifications.py","language":"python","type":"code","directory":"api/tests/src/task/notifications","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/notifications/test_generate_notifications.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/load/load_oracle_data_task.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/load\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/load/load_oracle_data_task.py\nSize: 9.51 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/task/opportunities/__init__.py","language":"python","type":"code","directory":"api/tests/src/task/opportunities","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/opportunities/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"import sqlalchemy\n\nimport src.db.models.foreign\nimport src.db.models.staging\nimport src.logging\nimport src.task.task\nfrom src.adapters import db\nfrom src.util import datetime_util\n\nfrom . import sql\n\nlogger = logging.getLogger(__name__)\n\nTABLES_TO_LOAD = [\n    \"topportunity\",\n    \"topportunity_cfda\",\n    \"tsynopsis\",\n    \"tsynopsis_hist\",\n    \"tforecast\",\n    \"tforecast_hist\",\n    \"tapplicanttypes_forecast\",\n    \"tapplicanttypes_forecast_hist\",\n    \"tapplicanttypes_synopsis\",\n    \"tapplicanttypes_synopsis_hist\",\n    \"tfundactcat_forecast\",\n    \"tfundactcat_forecast_hist\",\n    \"tfundactcat_synopsis\",\n    \"tfundactcat_synopsis_hist\",\n    \"tfundinstr_forecast\",\n    \"tfundinstr_forecast_hist\",\n    \"tfundinstr_synopsis\",\n    \"tfundinstr_synopsis_hist\",\n    \"tgroups\",\n]\n\n\nclass LoadOracleDataTask(src.task.task.Task):\n    \"\"\"Task to load data from legacy tables to staging tables.\"\"\"\n\n    def __init__(\n        self,\n        db_session: db.Session,\n        foreign_tables: dict[str, sqlalchemy.Table],\n        staging_tables: dict[str, sqlalchemy.Table],\n        tables_to_load: list[str] | None = None,\n        insert_chunk_size: int = 800,\n    ) -> None:\n\n        if tables_to_load is None or len(tables_to_load) == 0:\n            tables_to_load = TABLES_TO_LOAD\n\n        foreign_tables = {k: v for (k, v) in foreign_tables.items() if k in tables_to_load}\n        staging_tables = {k: v for (k, v) in staging_tables.items() if k in tables_to_load}\n\n        if foreign_tables.keys() != staging_tables.keys():\n            raise ValueError(\"keys of foreign_tables and staging_tables must be equal\")\n\n        super().__init__(db_session)\n        self.foreign_tables = foreign_tables\n        self.staging_tables = staging_tables\n        self.insert_chunk_size = insert_chunk_size\n\n    def run_task(self) -> None:\n        \"\"\"Main task process, called by run().\"\"\"\n        with self.db_session.begin():\n            self.log_database_settings()\n        self.load_data()\n\n    def log_database_settings(self) -> None:\n        \"\"\"Log database settings related to foreign tables for easier troubleshooting.\"\"\"\n        metadata = sqlalchemy.MetaData()\n        engine = self.db_session.bind\n\n        # Use reflection to define built-in views as Table objects.\n        foreign_servers = sqlalchemy.Table(\n            \"foreign_servers\", metadata, autoload_with=engine, schema=\"information_schema\"\n        )\n        foreign_server_options = sqlalchemy.Table(\n            \"foreign_server_options\", metadata, autoload_with=engine, schema=\"information_schema\"\n        )\n\n        logger.info(\n            \"foreign server settings\",\n            extra={\n                \"foreign_servers\": self.db_session.execute(\n                    sqlalchemy.select(foreign_servers)\n                ).all(),\n                \"foreign_server_options\": self.db_session.execute(\n                    sqlalchemy.select(foreign_server_options)\n                ).all(),\n            },\n        )\n\n    def load_data(self) -> None:\n        \"\"\"Load the data for all tables defined in the mapping.\"\"\"\n        for table_name in self.foreign_tables:\n            try:\n                self.load_data_for_table(table_name)\n            except Exception:\n                logger.exception(\"table load error\", extra={\"table\": table_name})\n\n    def load_data_for_table(self, table_name: str) -> None:\n        \"\"\"Load new and updated rows for a single table from the foreign table to the staging table.\"\"\"\n        logger.info(\"process table\", extra={\"table\": table_name})\n        foreign_table = self.foreign_tables[table_name]\n        staging_table = self.staging_tables[table_name]\n\n        self.log_row_count(\"before\", foreign_table, staging_table)\n\n        self.do_update(foreign_table, staging_table)\n        self.do_insert(foreign_table, staging_table)\n        self.do_mark_deleted(foreign_table, staging_table)\n\n        self.log_row_count(\"after\", staging_table)\n\n    def do_insert(self, foreign_table: sqlalchemy.Table, staging_table: sqlalchemy.Table) -> int:\n        \"\"\"Determine new rows by primary key, and copy them into the staging table.\"\"\"\n        log_extra = {\"table\": foreign_table.name}\n\n        logger.info(\"Fetching records to be inserted\", extra=log_extra)\n        select_sql = sql.build_select_new_rows_sql(foreign_table, staging_table)\n        with self.db_session.begin():\n            new_ids = self.db_session.execute(select_sql).all()\n\n        t0 = time.monotonic()\n        insert_chunk_count = []\n        logger.info(\"Fetched records to be inserted, beginning batches\", extra=log_extra)\n        for batch_of_new_ids in itertools.batched(new_ids, self.insert_chunk_size):\n            insert_from_select_sql = sql.build_insert_select_sql(\n                foreign_table, staging_table, batch_of_new_ids\n            )\n\n            # Execute the INSERT.\n            with self.db_session.begin():\n                self.db_session.execute(insert_from_select_sql)\n\n            insert_chunk_count.append(len(batch_of_new_ids))\n            logger.info(\n                \"insert chunk done\",\n                extra=log_extra\n                | {\n                    \"count\": sum(insert_chunk_count),\n                    \"total\": len(new_ids),\n                },\n            )\n\n        t1 = time.monotonic()\n        total_insert_count = sum(insert_chunk_count)\n        self.increment(\"count.insert.total\", total_insert_count)\n        self.increment(f\"count.insert.{staging_table.name}\", total_insert_count)\n        self.set_metrics(\n            {\n                f\"count.insert.chunk.{staging_table.name}\": \",\".join(map(str, insert_chunk_count)),\n                f\"time.insert.{staging_table.name}\": round(t1 - t0, 3),\n            }\n        )\n\n        return total_insert_count\n\n    def do_update(self, foreign_table: sqlalchemy.Table, staging_table: sqlalchemy.Table) -> int:\n        \"\"\"Find updated rows using last_upd_date, copy them, and reset transformed_at to NULL.\"\"\"\n        log_extra = {\"table\": foreign_table.name}\n\n        logger.info(\"Fetching records to be updated\", extra=log_extra)\n        select_sql = sql.build_select_updated_rows_sql(foreign_table, staging_table)\n        with self.db_session.begin():\n            update_ids = self.db_session.execute(select_sql).all()\n\n        t0 = time.monotonic()\n        update_chunk_count = []\n        logger.info(\"Fetched records to be updated, beginning batches\", extra=log_extra)\n        for batch_of_update_ids in itertools.batched(update_ids, self.insert_chunk_size):\n            update_sql = sql.build_update_sql(\n                foreign_table, staging_table, batch_of_update_ids\n            ).values(transformed_at=None)\n\n            with self.db_session.begin():\n                self.db_session.execute(update_sql)\n\n            update_chunk_count.append(len(batch_of_update_ids))\n            logger.info(\n                \"update chunk done\",\n                extra=log_extra | {\"count\": sum(update_chunk_count), \"total\": len(update_ids)},\n            )\n\n        t1 = time.monotonic()\n        total_update_count = sum(update_chunk_count)\n        self.increment(\"count.update.total\", total_update_count)\n        self.increment(f\"count.update.{staging_table.name}\", total_update_count)\n        self.set_metrics(\n            {\n                f\"count.update.chunk.{staging_table.name}\": \",\".join(map(str, update_chunk_count)),\n                f\"time.update.{staging_table.name}\": round(t1 - t0, 3),\n            }\n        )\n\n        return total_update_count\n\n    def do_mark_deleted(\n        self, foreign_table: sqlalchemy.Table, staging_table: sqlalchemy.Table\n    ) -> int:\n        \"\"\"Find deleted rows, set is_deleted=TRUE, and reset transformed_at to NULL.\"\"\"\n        log_extra = {\"table\": foreign_table.name}\n\n        logger.info(\"Fetching records to be deleted\", extra=log_extra)\n        update_sql = sql.build_mark_deleted_sql(foreign_table, staging_table).values(\n            transformed_at=None,\n            deleted_at=datetime_util.utcnow(),\n        )\n\n        t0 = time.monotonic()\n        logger.info(\"Fetched records to be deleted\", extra=log_extra)\n        with self.db_session.begin():\n            result = self.db_session.execute(update_sql)\n        t1 = time.monotonic()\n        delete_count = result.rowcount\n\n        self.increment(\"count.delete.total\", delete_count)\n        self.set_metrics({f\"count.delete.{staging_table.name}\": delete_count})\n        self.set_metrics({f\"time.delete.{staging_table.name}\": round(t1 - t0, 3)})\n        logger.info(\"Delete done\", extra=log_extra | {\"count\": delete_count})\n\n        return delete_count\n\n    def log_row_count(self, message: str, *tables: sqlalchemy.Table) -> None:\n        \"\"\"Log the number of rows in each of the tables using SQL COUNT().\"\"\"\n        extra: dict = {}\n        with self.db_session.begin():\n            for table in tables:\n                count = self.db_session.query(table).count()\n                extra[\"table\"] = table.name\n                extra[f\"count.{table.schema}.{table.name}\"] = count\n                self.set_metrics({f\"count.{message}.{table.schema}.{table.name}\": count})\n        logger.info(f\"row count {message}\", extra=extra, stacklevel=2)\n\n\ndef main() -> None:\n    with src.logging.init(__package__):\n        db_client = db.PostgresDBClient()\n\n        foreign_tables = {t.name: t for t in src.db.models.foreign.metadata.tables.values()}\n        staging_tables = {t.name: t for t in src.db.models.staging.metadata.tables.values()}\n\n        with db_client.get_session() as db_session:\n            LoadOracleDataTask(db_session, foreign_tables, staging_tables).run()\n\n\nif __name__ == \"__main__\":\n    main()"}
{"path":"api/tests/src/task/opportunities/test_export_opportunity_data_task.py","language":"python","type":"code","directory":"api/tests/src/task/opportunities","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/opportunities/test_export_opportunity_data_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/load/sql.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/load\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/load/sql.py\nSize: 4.50 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/task/opportunities/test_set_current_opportunities_task.py","language":"python","type":"code","directory":"api/tests/src/task/opportunities","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/opportunities/test_set_current_opportunities_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"from typing import Iterable\n\nimport sqlalchemy\n\n\ndef build_select_new_rows_sql(\n    source_table: sqlalchemy.Table, destination_table: sqlalchemy.Table\n) -> sqlalchemy.Select:\n    \"\"\"Build a `SELECT id1, id2, ... FROM <source_table>` query that finds new rows in source_table.\"\"\"\n\n    # `SELECT id1, id2, id3, ... FROM <source_table>`    (id1, id2, ... is the multipart primary key)\n    return (\n        sqlalchemy.select(*source_table.primary_key.columns)\n        .where(\n            # `WHERE (id1, id2, id3, ...) NOT IN`\n            sqlalchemy.tuple_(*source_table.primary_key.columns).not_in(\n                # `(SELECT (id1, id2, id3, ...) FROM <destination_table>)`    (subquery)\n                sqlalchemy.select(*destination_table.primary_key.columns)\n            )\n        )\n        .order_by(*source_table.primary_key.columns)\n    )\n\n\ndef build_insert_select_sql(\n    source_table: sqlalchemy.Table,\n    destination_table: sqlalchemy.Table,\n    ids: Iterable[tuple | sqlalchemy.Row],\n) -> sqlalchemy.Insert:\n    \"\"\"Build an `INSERT INTO ... SELECT ... FROM ...` query for new rows.\"\"\"\n\n    all_columns = tuple(c.name for c in source_table.columns)\n\n    # `SELECT col1, col2, ..., FALSE AS is_deleted FROM <source_table>`\n    select_sql = sqlalchemy.select(\n        source_table, sqlalchemy.literal_column(\"FALSE\").label(\"is_deleted\")\n    ).where(\n        # `WHERE (id1, id2, ...)\n        #  IN ((a1, a2), (b1, b2), ...)`\n        sqlalchemy.tuple_(*source_table.primary_key.columns).in_(ids),\n    )\n    # `INSERT INTO <destination_table> (col1, col2, ..., is_deleted) SELECT ...`\n    insert_from_select_sql = sqlalchemy.insert(destination_table).from_select(\n        all_columns + (destination_table.c.is_deleted,), select_sql\n    )\n\n    return insert_from_select_sql\n\n\ndef build_select_updated_rows_sql(\n    source_table: sqlalchemy.Table, destination_table: sqlalchemy.Table\n) -> sqlalchemy.Select:\n    \"\"\"Build a `SELECT id1, id2, ... FROM <source_table>` query that finds updated rows in source_table.\"\"\"\n\n    # `SELECT id1, id2, id3, ... FROM <destination_table>`\n    return (\n        sqlalchemy.select(*destination_table.primary_key.columns)\n        .join(\n            # `JOIN <source_table>\n            #  ON (id1, id2, ...) = (id1, id2, ...)`\n            source_table,\n            sqlalchemy.tuple_(*destination_table.primary_key.columns)\n            == sqlalchemy.tuple_(*source_table.primary_key.columns),\n        )\n        # `WHERE ...`\n        # NOTE: The legacy system doesn't populate the last_upd_date unless at least one update\n        #       has occurred, so we need to fallback to the created_date otherwise it will always be\n        #       null on the destination table side\n        .where(\n            sqlalchemy.func.coalesce(\n                destination_table.c.last_upd_date, destination_table.c.created_date\n            )\n            < source_table.c.last_upd_date\n        )\n        .order_by(*source_table.primary_key.columns)\n    )\n\n\ndef build_update_sql(\n    source_table: sqlalchemy.Table,\n    destination_table: sqlalchemy.Table,\n    ids: Iterable[tuple | sqlalchemy.Row],\n) -> sqlalchemy.Update:\n    \"\"\"Build an `UPDATE ... SET ... WHERE ...` statement for updated rows.\"\"\"\n\n    return (\n        # `UPDATE <destination_table>`\n        sqlalchemy.update(destination_table)\n        # `SET col1=source_table.col1, col2=source_table.col2, ...`\n        .values(dict(source_table.columns))\n        # `WHERE ...`\n        .where(\n            sqlalchemy.tuple_(*destination_table.primary_key.columns)\n            == sqlalchemy.tuple_(*source_table.primary_key.columns),\n            sqlalchemy.tuple_(*source_table.primary_key.columns).in_(ids),\n        )\n    )\n\n\ndef build_mark_deleted_sql(\n    source_table: sqlalchemy.Table, destination_table: sqlalchemy.Table\n) -> sqlalchemy.Update:\n    \"\"\"Build an `UPDATE ... SET is_deleted = TRUE WHERE ...` statement for deleted rows.\"\"\"\n    return (\n        # `UPDATE <destination_table>`\n        sqlalchemy.update(destination_table)\n        # `SET is_deleted = TRUE`\n        .values(is_deleted=True)\n        # `WHERE`\n        .where(\n            # `is_deleted == FALSE`\n            destination_table.c.is_deleted == False,  # noqa: E712\n            # `AND (id1, id2, id3, ...) NOT IN`    (id1, id2, ... is the multipart primary key)\n            sqlalchemy.tuple_(*destination_table.primary_key.columns).not_in(\n                # `(SELECT (id1, id2, id3, ...) FROM <source_table>)`    (subquery)\n                sqlalchemy.select(*source_table.primary_key.columns)\n            ),\n        )\n    )"}
{"path":"api/tests/src/task/test_ecs_background_task.py","language":"python","type":"code","directory":"api/tests/src/task","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/test_ecs_background_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/setup_foreign_tables.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/setup_foreign_tables.py\nSize: 2.48 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/task/test_task.py","language":"python","type":"code","directory":"api/tests/src/task","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/test_task.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"import sqlalchemy\nfrom pydantic import Field\n\nimport src.adapters.db as db\nimport src.adapters.db.flask_db as flask_db\nimport src.db.models.foreign\nimport src.db.models.foreign.dialect\nfrom src.constants.schema import Schemas\nfrom src.data_migration.data_migration_blueprint import data_migration_blueprint\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass ForeignTableConfig(PydanticBaseEnvConfig):\n    is_local_foreign_table: bool = Field(False)\n    schema_name: str = Field(Schemas.LEGACY)\n\n\n@data_migration_blueprint.cli.command(\n    \"setup-foreign-tables\", help=\"Setup the foreign tables for connecting to the Oracle database\"\n)\n@flask_db.with_db_session()\ndef setup_foreign_tables(db_session: db.Session) -> None:\n    logger.info(\"Beginning setup of foreign Oracle tables\")\n\n    config = ForeignTableConfig()\n\n    with db_session.begin():\n        _run_create_table_commands(db_session, config)\n\n    logger.info(\"Successfully ran setup-foreign-tables\")\n\n\ndef build_sql(table: sqlalchemy.schema.Table, is_local: bool, schema_name: str) -> str:\n    \"\"\"\n    Build the SQL for creating a possibly foreign data table. If running\n    with is_local, it instead creates a regular table.\n\n    is_local is True::\n\n        CREATE TABLE IF NOT EXISTS foreign_example_table\n        (ID integer CONSTRAINT EXAMPLE_TABLE_pkey PRIMARY KEY NOT NULL,DESCRIPTION text)\n\n    is_local is False::\n\n        CREATE FOREIGN TABLE IF NOT EXISTS foreign_example_table\n        (ID integer OPTIONS (key 'true') NOT NULL,DESCRIPTION text)\n        SERVER grants OPTIONS (schema 'EGRANTSADMIN', table 'EXAMPLE_TABLE')\n    \"\"\"\n\n    create_table = sqlalchemy.schema.CreateTable(table, if_not_exists=True)\n    if is_local:\n        compiler = create_table.compile(\n            dialect=sqlalchemy.dialects.postgresql.dialect(),\n            schema_translate_map={Schemas.LEGACY.name: schema_name},\n        )\n    else:\n        compiler = create_table.compile(\n            dialect=src.db.models.foreign.dialect.ForeignTableDialect(),\n            schema_translate_map={Schemas.LEGACY.name: schema_name},\n        )\n    return str(compiler).strip()\n\n\ndef _run_create_table_commands(db_session: db.Session, config: ForeignTableConfig) -> None:\n    for table in src.db.models.foreign.metadata.tables.values():\n        sql = build_sql(table, config.is_local_foreign_table, config.schema_name)\n        logger.info(\"create table\", extra={\"table\": table.name, \"sql\": sql})\n        db_session.execute(sqlalchemy.text(sql))"}
{"path":"api/tests/src/util/__init__.py","language":"python","type":"code","directory":"api/tests/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/transformation/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/util/parametrize_utils.py","language":"python","type":"code","directory":"api/tests/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/parametrize_utils.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":""}
{"path":"api/tests/src/util/test_datetime_util.py","language":"python","type":"code","directory":"api/tests/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/test_datetime_util.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/transformation/subtask/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/util/test_deploy_metadata.py","language":"python","type":"code","directory":"api/tests/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/test_deploy_metadata.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":""}
{"path":"api/tests/src/util/test_dict_util.py","language":"python","type":"code","directory":"api/tests/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/test_dict_util.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/transformation/subtask/abstract_transform_subtask.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/abstract_transform_subtask.py\nSize: 8.77 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/src/util/test_file_util.py","language":"python","type":"code","directory":"api/tests/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/test_file_util.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"from sqlalchemy import and_, select\nfrom sqlalchemy.orm import selectinload\n\nimport src.data_migration.transformation.transform_constants as transform_constants\nfrom src.db.models.opportunity_models import Opportunity, OpportunitySummary\nfrom src.task.subtask import SubTask\nfrom src.task.task import Task\n\nlogger = logging.getLogger(__name__)\n\n\nclass AbstractTransformSubTask(SubTask):\n    def __init__(self, task: Task):\n        super().__init__(task)\n\n        # This is a bit of a hacky way of making sure the task passed into this method\n        # is the TransformOracleDataTask class. We could make this init function take in that\n        # type specifically, but we'd run into circular type dependencies which are complex to resolve\n        transform_time = getattr(task, \"transform_time\", None)\n        if transform_time is None:\n            raise Exception(\"Task passed into AbstractTransformSubTask must have a transform_time\")\n\n        self.transform_time: datetime = transform_time\n\n    def has_more_to_process(self) -> bool:\n        \"\"\"Method for the derived classes to override if\n        they want to indicate they have more batches to process\n\n        If you do not override this, exactly one batch will get processed\n        \"\"\"\n        return False\n\n    def run_subtask(self) -> None:\n        batch_num = 0\n        while True:\n            batch_num += 1\n            with self.db_session.begin():\n                self.transform_records()\n                logger.info(\n                    \"Finished running set of transformations for %s - committing results\",\n                    self.cls_name(),\n                )\n\n                if not self.has_more_to_process():\n                    break\n\n                # As a sanity check, if more than 100 batches run, stop processing\n                # and we'll assume the job got stuck.\n                if batch_num > 100:\n                    logger.error(\n                        \"Job %s has run 100 batches, stopping further processing in case job is stuck\",\n                        self.cls_name,\n                    )\n\n            # As a safety net, expire all references in the session\n            # after running. This avoids any potential complexities in\n            # cached data between separate subtasks running.\n            # By default sessions actually do this when committing, but\n            # our db session creation logic disables it, so it's the ordinary behavior.\n            self.db_session.expire_all()\n\n    @abc.abstractmethod\n    def transform_records(self) -> None:\n        \"\"\"Abstract method implemented by derived, returns True when done processing\"\"\"\n        pass\n\n    def _handle_delete(\n        self,\n        source: transform_constants.S,\n        target: transform_constants.D | None,\n        record_type: str,\n        extra: dict,\n        error_on_missing_target: bool = False,\n    ) -> None:\n        # If the target we want to delete is None, we have nothing to delete\n        if target is None:\n            # In some scenarios we want to error when this happens\n            if error_on_missing_target:\n                raise ValueError(\"Cannot delete %s record as it does not exist\" % record_type)\n\n            # In a lot of scenarios, we actually just want to log a message as it is expected to happen\n            # For example, if we are deleting an opportunity_summary record, and already deleted the opportunity,\n            # then SQLAlchemy would have deleted the opportunity_summary for us already. When we later go to delete\n            # it, we'd hit this case, which isn't a problem.\n            logger.info(\"Cannot delete %s record as it does not exist\", record_type, extra=extra)\n            source.transformation_notes = transform_constants.ORPHANED_DELETE_RECORD\n            self.increment(\n                transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED, prefix=record_type\n            )\n            return\n\n        logger.info(\"Deleting %s record\", record_type, extra=extra)\n        self.increment(transform_constants.Metrics.TOTAL_RECORDS_DELETED, prefix=record_type)\n        self.db_session.delete(target)\n\n    def _is_orphaned_historical(\n        self,\n        parent_record: Opportunity | OpportunitySummary | None,\n        source_record: transform_constants.SourceAny,\n    ) -> bool:\n        return parent_record is None and source_record.is_historical_table\n\n    def _handle_orphaned_historical(\n        self, source_record: transform_constants.SourceAny, record_type: str, extra: dict\n    ) -> None:\n        logger.warning(\n            \"Historical %s does not have a corresponding parent record - cannot import, but will mark as processed\",\n            record_type,\n            extra=extra,\n        )\n        self.increment(\n            transform_constants.Metrics.TOTAL_HISTORICAL_ORPHANS_SKIPPED, prefix=record_type\n        )\n        source_record.transformation_notes = transform_constants.ORPHANED_HISTORICAL_RECORD\n\n    def fetch(\n        self,\n        source_model: Type[transform_constants.S],\n        destination_model: Type[transform_constants.D],\n        join_clause: Sequence,\n    ) -> list[Tuple[transform_constants.S, transform_constants.D | None]]:\n        # The real type is: Sequence[Row[Tuple[S, D | None]]]\n        # but MyPy is weird about this and the Row+Tuple causes some\n        # confusion in the parsing so it ends up assuming everything is Any\n        # So just cast it to a simpler type that doesn't confuse anything\n        return cast(\n            list[Tuple[transform_constants.S, transform_constants.D | None]],\n            self.db_session.execute(\n                select(source_model, destination_model)\n                .join(destination_model, and_(*join_clause), isouter=True)\n                .where(source_model.transformed_at.is_(None))\n                .execution_options(yield_per=5000)\n            ),\n        )\n\n    def fetch_with_opportunity(\n        self,\n        source_model: Type[transform_constants.S],\n        destination_model: Type[transform_constants.D],\n        join_clause: Sequence,\n        batch_size: int = 5000,\n        limit: int | None = None,\n    ) -> list[Tuple[transform_constants.S, transform_constants.D | None, Opportunity | None]]:\n        # Similar to the above fetch function, but also grabs an opportunity record\n        # Note that this requires your source_model to have an opportunity_id field defined.\n\n        select_query = (\n            select(source_model, destination_model, Opportunity)\n            .join(destination_model, and_(*join_clause), isouter=True)\n            .join(\n                Opportunity,\n                source_model.opportunity_id == Opportunity.opportunity_id,  # type: ignore[attr-defined]\n                isouter=True,\n            )\n            .where(source_model.transformed_at.is_(None))\n            .execution_options(yield_per=batch_size)\n        )\n\n        if limit is not None:\n            select_query = select_query.limit(limit)\n\n        return cast(\n            list[Tuple[transform_constants.S, transform_constants.D | None, Opportunity | None]],\n            self.db_session.execute(select_query),\n        )\n\n    def fetch_with_opportunity_summary(\n        self,\n        source_model: Type[transform_constants.S],\n        destination_model: Type[transform_constants.D],\n        join_clause: Sequence,\n        is_forecast: bool,\n        is_historical_table: bool,\n        relationship_load_value: Any,\n    ) -> list[\n        Tuple[transform_constants.S, transform_constants.D | None, OpportunitySummary | None]\n    ]:\n        # setup the join clause for getting the opportunity summary\n\n        opportunity_summary_join_clause = [\n            source_model.opportunity_id == OpportunitySummary.opportunity_id,  # type: ignore[attr-defined]\n            OpportunitySummary.is_forecast.is_(is_forecast),\n        ]\n\n        if is_historical_table:\n            opportunity_summary_join_clause.append(\n                source_model.revision_number == OpportunitySummary.revision_number  # type: ignore[attr-defined]\n            )\n        else:\n            opportunity_summary_join_clause.append(OpportunitySummary.revision_number.is_(None))\n\n        return cast(\n            list[\n                Tuple[\n                    transform_constants.S, transform_constants.D | None, OpportunitySummary | None\n                ]\n            ],\n            self.db_session.execute(\n                select(source_model, destination_model, OpportunitySummary)\n                .join(OpportunitySummary, and_(*opportunity_summary_join_clause), isouter=True)\n                .join(destination_model, and_(*join_clause), isouter=True)\n                .where(source_model.transformed_at.is_(None))\n                .options(selectinload(relationship_load_value))\n                .execution_options(yield_per=5000, populate_existing=True)\n            ),\n        )"}
{"path":"api/tests/src/util/test_string_utils.py","language":"python","type":"code","directory":"api/tests/src/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/test_string_utils.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/transformation/subtask/transform_agency.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_agency.py\nSize: 16.19 KB\nLast Modified: 2025-02-14T17:08:26.438Z"}
{"path":"api/tests/util/__init__.py","language":"python","type":"code","directory":"api/tests/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/util/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"from pydantic import Field\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import selectinload\n\nimport src.data_migration.transformation.transform_constants as transform_constants\nimport src.data_migration.transformation.transform_util as transform_util\nfrom src.constants.lookup_constants import (\n    AgencyDownloadFileType,\n    AgencySubmissionNotificationSetting,\n)\nfrom src.data_migration.transformation.subtask.abstract_transform_subtask import (\n    AbstractTransformSubTask,\n)\nfrom src.db.models.agency_models import Agency, AgencyContactInfo, LinkAgencyDownloadFileType\nfrom src.db.models.staging.tgroups import Tgroups\nfrom src.task.task import Task\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\nNULLABLE_FIELDS = {\n    \"AgencyCode\",  # Note this is the sub_agency_code in our system\n    \"AgencyContactEMail2\",\n    \"ldapGp\",\n    \"description\",\n    \"label\",\n}\n\nAGENCY_FIELD_MAP = {\n    \"AgencyName\": \"agency_name\",\n    \"AgencyCode\": \"sub_agency_code\",\n    \"AgencyCFDA\": \"assistance_listing_number\",\n    \"AgencyDownload\": \"agency_download_file_types\",\n    \"AgencyNotify\": \"agency_submission_notification_setting\",\n    \"ldapGp\": \"ldap_group\",\n    \"description\": \"description\",\n    \"label\": \"label\",\n    \"multilevel\": \"is_multilevel_agency\",\n    \"HasS2SCert\": \"has_system_to_system_certificate\",\n    \"ViewPkgsInGracePeriod\": \"can_view_packages_in_grace_period\",\n    \"multiproject\": \"is_multiproject\",\n    \"ImageWS\": \"is_image_workspace_enabled\",\n    \"ValidationWS\": \"is_validation_workspace_enabled\",\n}\n\nAGENCY_CONTACT_INFO_FIELD_MAP = {\n    \"AgencyContactName\": \"contact_name\",\n    \"AgencyContactAddress1\": \"address_line_1\",\n    \"AgencyContactAddress2\": \"address_line_2\",\n    \"AgencyContactCity\": \"city\",\n    \"AgencyContactState\": \"state\",\n    \"AgencyContactZipCode\": \"zip_code\",\n    \"AgencyContactTelephone\": \"phone_number\",\n    \"AgencyContactEMail\": \"primary_email\",\n    \"AgencyContactEMail2\": \"secondary_email\",\n}\n\nNOT_MAPPED_FIELDS = {\n    \"AgencyEnroll\",\n    \"ForecastPOC\",\n    \"ForecastPOCEmail\",\n    \"ForecastPOCEmailDesc\",\n    \"ForecastPOCPhone\",\n    \"SynopsisPOC\",\n    \"SynopsisPOCEmail\",\n    \"SynopsisPOCEmailDesc\",\n    \"PackagePOC\",\n    # These fields were only found in the test environment\n    \"ASSISTCompatible\",\n    \"SAMValidation\",\n    # This was added in Jan 2025 in Grants.gov, we aren't using it yet\n    \"AllowSubmitWithExpSAM\",\n}\n\nREQUIRED_FIELDS = {\n    \"AgencyName\",\n    \"AgencyCFDA\",\n    \"AgencyDownload\",\n    \"AgencyNotify\",\n    \"AgencyContactName\",\n    \"AgencyContactAddress1\",\n    \"AgencyContactCity\",\n    \"AgencyContactState\",\n    \"AgencyContactZipCode\",\n    \"AgencyContactTelephone\",\n    \"AgencyContactEMail\",\n}\n\n\nclass AgencyConfig(PydanticBaseEnvConfig):\n    # TODO - we might want to put this somewhere more central\n    #        as we might want to filter these out in other places\n    test_agency_config: set[str] = Field(\n        default={\"GDIT\", \"IVV\", \"IVPDF\", \"0001\", \"FGLT\", \"NGMS\", \"NGMS-Sub1\", \"SECSCAN\"}\n    )\n\n\n@dataclass\nclass TgroupAgency:\n    \"\"\"\n    Container class for holding all tgroup records for\n    a given agency.\n    \"\"\"\n\n    agency_code: str\n    tgroups: list[Tgroups] = field(default_factory=list)\n\n    has_update: bool = False\n\n    def add_tgroup(self, tgroup: Tgroups) -> None:\n        if tgroup.transformed_at is None:\n            self.has_update = True\n\n        self.tgroups.append(tgroup)\n\n    def get_updated_field_names(self) -> set[str]:\n        return {tgroup.get_field_name() for tgroup in self.tgroups if tgroup.transformed_at is None}\n\n\n@dataclass\nclass AgencyUpdates:\n    \"\"\"\n    Container class for holding all of the necessary updates\n    for an agency\n    \"\"\"\n\n    agency_updates: dict[str, Any] = field(default_factory=dict)\n    agency_contact_info_updates: dict[str, Any] = field(default_factory=dict)\n    agency_download_file_types: set[AgencyDownloadFileType] = field(default_factory=set)\n\n    agency_created_at: datetime | None = None\n    agency_updated_at: datetime | None = None\n\n\nclass TransformAgency(AbstractTransformSubTask):\n    def __init__(self, task: Task, agency_config: AgencyConfig | None = None) -> None:\n        super().__init__(task)\n\n        if agency_config is None:\n            agency_config = AgencyConfig()\n\n        self.agency_config = agency_config\n\n    def transform_records(self) -> None:\n        # fetch tgroup records\n        tgroup_map = self.fetch_tgroup_mapping()\n\n        # Fetch all existing agencies\n        agency_map = self.fetch_agency_mapping()\n\n        for agency_code, tgroup_agency in tgroup_map.items():\n            agency = agency_map.get(agency_code)\n\n            try:\n                self.process_tgroups(tgroup_agency, agency)\n            except ValueError:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_ERROR_COUNT,\n                    prefix=transform_constants.AGENCY,\n                )\n                logger.exception(\"Failed to process agency\", extra={\"agency_code\": agency_code})\n\n    def fetch_tgroup_mapping(self) -> dict[str, TgroupAgency]:\n        tgroups = self.db_session.scalars(select(Tgroups))\n\n        tgroup_mapping: dict[str, TgroupAgency] = {}\n\n        for tgroup in tgroups:\n            agency_code = tgroup.get_agency_code()\n\n            if agency_code not in tgroup_mapping:\n                tgroup_mapping[agency_code] = TgroupAgency(agency_code)\n\n            tgroup_mapping[agency_code].add_tgroup(tgroup)\n\n        return tgroup_mapping\n\n    def fetch_agency_mapping(self) -> dict[str, Agency]:\n        agencies = self.db_session.scalars(\n            select(Agency).options(selectinload(Agency.agency_contact_info))\n        )\n\n        return {agency.agency_code: agency for agency in agencies}\n\n    def process_tgroups(self, tgroup_agency: TgroupAgency, agency: Agency | None) -> None:\n        log_extra = {\"agency_code\": tgroup_agency.agency_code}\n        logger.info(\"Processing agency\", extra=log_extra)\n        if not tgroup_agency.has_update:\n            logger.info(\"No updates for agency\", extra=log_extra)\n            return\n\n        # Only increment counter for agencies with something to update\n        self.increment(\n            transform_constants.Metrics.TOTAL_RECORDS_PROCESSED, prefix=transform_constants.AGENCY\n        )\n\n        # New agency insert case\n        is_insert = False\n        if agency is None:\n            is_insert = True\n            # If any field that is required for creating an agency is missing, we want to error\n            missing_required_fields = REQUIRED_FIELDS - tgroup_agency.get_updated_field_names()\n            if missing_required_fields:\n                raise ValueError(\n                    \"Cannot create agency %s as required fields are missing: %s\"\n                    % (tgroup_agency.agency_code, \",\".join(missing_required_fields))\n                )\n\n            logger.info(\"Creating new agency\", extra=log_extra)\n            agency = Agency(agency_code=tgroup_agency.agency_code)\n            agency.agency_contact_info = AgencyContactInfo()\n        else:\n            logger.info(\"Updating agency\", extra=log_extra)\n\n        updates = get_agency_updates(tgroup_agency)\n        apply_updates(\n            agency, updates.agency_updates, updates.agency_created_at, updates.agency_updated_at\n        )\n        apply_updates(\n            agency.agency_contact_info,\n            updates.agency_contact_info_updates,\n            updates.agency_created_at,\n            updates.agency_updated_at,\n        )\n        self.update_agency_download_file_types(agency, updates.agency_download_file_types)\n\n        # Set whether the agency is a test agency based on the config\n        is_test_agency = tgroup_agency.agency_code in self.agency_config.test_agency_config\n        agency.is_test_agency = is_test_agency\n\n        # After we have fully updated the agency, set the transformed_at timestamp\n        # for all tgroup records that weren't already set.\n        for tgroup in tgroup_agency.tgroups:\n            if tgroup.transformed_at is None:\n                tgroup.transformed_at = self.transform_time\n\n        if is_insert:\n            self.increment(\n                transform_constants.Metrics.TOTAL_RECORDS_INSERTED,\n                prefix=transform_constants.AGENCY,\n            )\n        else:\n            self.increment(\n                transform_constants.Metrics.TOTAL_RECORDS_UPDATED, prefix=transform_constants.AGENCY\n            )\n\n        self.db_session.add(agency)\n        logger.info(\"Processed agency\", extra=log_extra)\n\n    def update_agency_download_file_types(\n        self, agency: Agency, agency_download_file_types: set[AgencyDownloadFileType]\n    ) -> None:\n        # If the download file types we have set is already the same, just return\n        if agency.agency_download_file_types == agency_download_file_types:\n            return\n\n        file_types_to_delete = set(agency.agency_download_file_types) - agency_download_file_types\n        file_types_to_add = agency_download_file_types - set(agency.agency_download_file_types)\n\n        for link_agency_download_file_type in agency.link_agency_download_file_types:\n            if link_agency_download_file_type.agency_download_file_type in file_types_to_delete:\n                self.db_session.delete(link_agency_download_file_type)\n\n        for file_type_to_add in file_types_to_add:\n            self.db_session.add(\n                LinkAgencyDownloadFileType(\n                    agency=agency, agency_download_file_type=file_type_to_add\n                )\n            )\n\n\nclass TransformAgencyHierarchy(AbstractTransformSubTask):\n    def __init__(self, task: Task):\n        super().__init__(task)\n\n    def transform_records(self) -> None:\n        agencies = self.db_session.scalars(select(Agency)).all()\n        agency_map = {agency.agency_code: agency for agency in agencies}\n\n        for agency in agencies:\n            top_level_agency_code = self.get_top_level_agency_code(agency.agency_code)\n            if top_level_agency_code and top_level_agency_code in agency_map:\n                agency.top_level_agency = agency_map[top_level_agency_code]\n\n    def get_top_level_agency_code(self, agency_code: str) -> str | None:\n        if \"-\" not in agency_code:\n            return None\n        return agency_code.split(\"-\")[0]\n\n\n############################\n# Transformation / utility functions\n############################\n\nAGENCY_DOWNLOAD_FILE_TYPE_MAP = {\n    \"0\": set(),\n    \"1\": {AgencyDownloadFileType.XML},\n    \"2\": {AgencyDownloadFileType.XML, AgencyDownloadFileType.PDF},\n    \"3\": {AgencyDownloadFileType.PDF},\n}\n\nAGENCY_SUBMISSION_NOTIFICATION_SETTING_MAP = {\n    \"1\": AgencySubmissionNotificationSetting.NEVER,\n    \"2\": AgencySubmissionNotificationSetting.FIRST_APPLICATION_ONLY,\n    \"3\": AgencySubmissionNotificationSetting.ALWAYS,\n}\n\n\ndef get_agency_updates(tgroup_agency: TgroupAgency) -> AgencyUpdates:\n    updates = AgencyUpdates()\n\n    for tgroup in tgroup_agency.tgroups:\n        if not tgroup.is_modified:\n            continue\n\n        tgroup_field_name = tgroup.get_field_name()\n\n        # TODO - how we want to actually handle deleted rows likely needs more investigation\n        #        and discussion - do we assume that if certain fields are deleted that the\n        #        entire agency should be deleted? Can they even be deleted once an opportunity refers to them?\n        #        Rather than focus too much on that detail right now, I'm deferring\n        #        a more thorough investigation to later\n        # For now - we'll error any agency that has deleted rows except for a few\n        # specific fields we know are safe to delete.\n        if tgroup.is_deleted:\n            if tgroup_field_name not in NULLABLE_FIELDS:\n                raise ValueError(\n                    \"Field %s in tgroups cannot be deleted as it is not nullable\"\n                    % tgroup_field_name\n                )\n            value = None\n        else:\n            value = convert_field_values(tgroup_field_name, tgroup.value)\n\n        if tgroup_field_name == \"AgencyDownload\":\n            updates.agency_download_file_types = value  # type: ignore[assignment]\n\n        elif tgroup_field_name in AGENCY_FIELD_MAP:\n            field_name = AGENCY_FIELD_MAP[tgroup_field_name]\n            updates.agency_updates[field_name] = value\n\n        elif tgroup_field_name in AGENCY_CONTACT_INFO_FIELD_MAP:\n            field_name = AGENCY_CONTACT_INFO_FIELD_MAP[tgroup_field_name]\n            updates.agency_contact_info_updates[field_name] = value\n\n        elif tgroup_field_name in NOT_MAPPED_FIELDS:\n            logger.info(\n                \"Skipping processing of field %s for %s\",\n                tgroup_field_name,\n                tgroup_agency.agency_code,\n            )\n            continue\n\n        else:\n            raise ValueError(\"Unknown tgroups agency field %s\" % tgroup_field_name)\n\n        # We effectively need to merge the created_at/updated_at timestamps to the earliest/latest respectively\n        created_at, updated_at = transform_util.get_create_update_timestamps(\n            tgroup.created_date, tgroup.last_upd_date\n        )\n\n        if updates.agency_created_at is None or created_at < updates.agency_created_at:\n            updates.agency_created_at = created_at\n\n        if updates.agency_updated_at is None or updated_at > updates.agency_updated_at:\n            updates.agency_updated_at = updated_at\n\n    return updates\n\n\ndef convert_field_values(field_name: str, value: str | None) -> Any:\n    if field_name == \"AgencyDownload\":\n        return transform_agency_download_file_types(value)\n    elif field_name == \"AgencyNotify\":\n        return transform_agency_notify(value)\n    elif field_name == \"multilevel\":\n        return transform_util.convert_true_false_bool(value)\n    elif field_name == \"HasS2SCert\":\n        return transform_util.convert_yn_bool(value)\n    elif field_name == \"multiproject\":\n        return transform_util.convert_yn_bool(value)\n    elif field_name == \"ViewPkgsInGracePeriod\":\n        return transform_util.convert_yn_bool(value)\n    elif field_name == \"ImageWS\":\n        return transform_util.convert_yn_bool(value)\n    elif field_name == \"ValidationWS\":\n        return transform_util.convert_yn_bool(value)\n    elif field_name == \"AgencyContactAddress2\":\n        return transform_util.convert_null_like_to_none(value)\n\n    return value\n\n\ndef transform_agency_download_file_types(value: str | None) -> set[AgencyDownloadFileType]:\n    if value not in AGENCY_DOWNLOAD_FILE_TYPE_MAP:\n        raise ValueError(\"Unrecognized agency download file type value %s\" % value)\n\n    return AGENCY_DOWNLOAD_FILE_TYPE_MAP[value]\n\n\ndef transform_agency_notify(value: str | None) -> AgencySubmissionNotificationSetting:\n    if value not in AGENCY_SUBMISSION_NOTIFICATION_SETTING_MAP:\n        raise ValueError(\"Unrecognized agency notify setting value: %s\" % value)\n\n    return AGENCY_SUBMISSION_NOTIFICATION_SETTING_MAP[value]\n\n\ndef apply_updates(\n    record: Agency | AgencyContactInfo | None,\n    updates: dict[str, Any],\n    created_at: datetime | None,\n    updated_at: datetime | None,\n) -> None:\n    # Note MyPy doesn't quite follow the typing in this function because it thinks\n    # created_at/updated_at aren't ever None. While they aren't ever null in the DB,\n    # before we insert a record they may not be set. Hence the type:ignores here\n\n    if record is None:\n        # This shouldn't happen but need to make mypy happy because agency contact info\n        # can technically be null\n        raise ValueError(\"Cannot pass none value into apply_updates\")\n\n    for field_name, value in updates.items():\n        setattr(record, field_name, value)\n\n    # We will only set created_at if the value doesn't already exist on the record\n    # It would be confusing to change the created_at timestamp after the initial insert\n    if record.created_at is None and created_at is not None:  # type: ignore[unreachable]\n        record.created_at = created_at  # type: ignore[unreachable]\n\n    # Updated at we'll either set if the value currently is null (ie. we're doing an insert)\n    # or if it is greater than whatever already exists.\n    if record.updated_at is None and updated_at is not None:  # type: ignore[unreachable]\n        record.updated_at = updated_at  # type: ignore[unreachable]\n    elif (\n        record.updated_at is not None and updated_at is not None and record.updated_at < updated_at\n    ):\n        record.updated_at = updated_at"}
{"path":"api/tests/util/convert_oracle_csvs_to_postgres.py","language":"python","type":"code","directory":"api/tests/util","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/util/convert_oracle_csvs_to_postgres.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/transformation/subtask/transform_applicant_type.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_applicant_type.py\nSize: 7.28 KB\nLast Modified: 2025-02-14T17:08:26.439Z"}
{"path":"api/tool/__init__.py","language":"python","type":"code","directory":"api/tool","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tool/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nimport src.data_migration.transformation.transform_util as transform_util\nfrom src.data_migration.transformation.subtask.abstract_transform_subtask import (\n    AbstractTransformSubTask,\n)\nfrom src.db.models.opportunity_models import LinkOpportunitySummaryApplicantType, OpportunitySummary\nfrom src.db.models.staging.forecast import TapplicanttypesForecast\nfrom src.db.models.staging.synopsis import TapplicanttypesSynopsis\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformApplicantType(AbstractTransformSubTask):\n    def transform_records(self) -> None:\n        link_table = LinkOpportunitySummaryApplicantType\n        relationship_load_value = OpportunitySummary.link_applicant_types\n\n        logger.info(\"Processing forecast applicant types\")\n        forecast_applicant_type_records = self.fetch_with_opportunity_summary(\n            TapplicanttypesForecast,\n            link_table,\n            [\n                TapplicanttypesForecast.at_frcst_id\n                == LinkOpportunitySummaryApplicantType.legacy_applicant_type_id,\n                OpportunitySummary.opportunity_summary_id\n                == LinkOpportunitySummaryApplicantType.opportunity_summary_id,\n            ],\n            is_forecast=True,\n            is_historical_table=False,\n            relationship_load_value=relationship_load_value,\n        )\n        self.process_link_applicant_types_group(forecast_applicant_type_records)\n\n        logger.info(\"Processing synopsis applicant types\")\n        synopsis_applicant_type_records = self.fetch_with_opportunity_summary(\n            TapplicanttypesSynopsis,\n            link_table,\n            [\n                TapplicanttypesSynopsis.at_syn_id\n                == LinkOpportunitySummaryApplicantType.legacy_applicant_type_id,\n                OpportunitySummary.opportunity_summary_id\n                == LinkOpportunitySummaryApplicantType.opportunity_summary_id,\n            ],\n            is_forecast=False,\n            is_historical_table=False,\n            relationship_load_value=relationship_load_value,\n        )\n        self.process_link_applicant_types_group(synopsis_applicant_type_records)\n\n    def process_link_applicant_types_group(\n        self,\n        records: Sequence[\n            Tuple[\n                transform_constants.SourceApplicantType,\n                LinkOpportunitySummaryApplicantType | None,\n                OpportunitySummary | None,\n            ]\n        ],\n    ) -> None:\n        for source_applicant_type, target_applicant_type, opportunity_summary in records:\n            try:\n                self.process_link_applicant_type(\n                    source_applicant_type, target_applicant_type, opportunity_summary\n                )\n            except ValueError:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_ERROR_COUNT,\n                    prefix=transform_constants.APPLICANT_TYPE,\n                )\n                logger.exception(\n                    \"Failed to process opportunity summary applicant type\",\n                    extra=transform_util.get_log_extra_applicant_type(source_applicant_type),\n                )\n\n    def process_link_applicant_type(\n        self,\n        source_applicant_type: transform_constants.SourceApplicantType,\n        target_applicant_type: LinkOpportunitySummaryApplicantType | None,\n        opportunity_summary: OpportunitySummary | None,\n    ) -> None:\n        self.increment(\n            transform_constants.Metrics.TOTAL_RECORDS_PROCESSED,\n            prefix=transform_constants.APPLICANT_TYPE,\n        )\n        extra = transform_util.get_log_extra_applicant_type(source_applicant_type)\n        logger.info(\"Processing applicant type\", extra=extra)\n\n        if source_applicant_type.is_deleted:\n            self._handle_delete(\n                source_applicant_type,\n                target_applicant_type,\n                transform_constants.APPLICANT_TYPE,\n                extra,\n            )\n\n        # Historical records are linked to other historical records, however\n        # we don't import historical opportunity records, so if the opportunity\n        # was deleted, we won't have created the opportunity summary. Whenever we do\n        # support historical opportunities, we'll have these all marked with a\n        # flag that we can use to reprocess these.\n        elif self._is_orphaned_historical(opportunity_summary, source_applicant_type):\n            self._handle_orphaned_historical(\n                source_applicant_type, transform_constants.APPLICANT_TYPE, extra\n            )\n\n        elif opportunity_summary is None:\n            # This shouldn't be possible as the incoming data has foreign keys, but as a safety net\n            # we'll make sure the opportunity actually exists\n            raise ValueError(\n                \"Applicant type record cannot be processed as the opportunity summary for it does not exist\"\n            )\n        else:\n            # To avoid incrementing metrics for records we fail to transform, record\n            # here whether it's an insert/update and we'll increment after transforming\n            is_insert = target_applicant_type is None\n\n            logger.info(\"Transforming and upserting applicant type\", extra=extra)\n            transformed_applicant_type = transform_util.convert_opportunity_summary_applicant_type(\n                source_applicant_type, target_applicant_type, opportunity_summary\n            )\n\n            # Before we insert, we have to still be certain we're not adding a duplicate record\n            # because the primary key of the legacy tables is the legacy ID + lookup value + opportunity ID\n            # its possible for the same lookup value to appear multiple times because the legacy ID is different\n            # This would hit a conflict in our DBs primary key, so we need to verify that won't happen\n            if (\n                is_insert\n                and transformed_applicant_type.applicant_type in opportunity_summary.applicant_types\n            ):\n                self.increment(\n                    transform_constants.Metrics.TOTAL_DUPLICATE_RECORDS_SKIPPED,\n                    prefix=transform_constants.APPLICANT_TYPE,\n                )\n                logger.warning(\n                    \"Skipping applicant type record\",\n                    extra=extra | {\"applicant_type\": transformed_applicant_type.applicant_type},\n                )\n            elif is_insert:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_INSERTED,\n                    prefix=transform_constants.APPLICANT_TYPE,\n                )\n                # We append to the relationship so SQLAlchemy immediately attaches it to its cached\n                # opportunity summary object so that the above check works when we receive dupes in the same batch\n                opportunity_summary.link_applicant_types.append(transformed_applicant_type)\n            else:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_UPDATED,\n                    prefix=transform_constants.APPLICANT_TYPE,\n                )\n                self.db_session.merge(transformed_applicant_type)\n\n        logger.info(\"Processed applicant type\", extra=extra)\n        source_applicant_type.transformed_at = self.transform_time"}
{"path":"api/tool/console/__init__.py","language":"python","type":"code","directory":"api/tool/console","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tool/console/__init__.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/transformation/subtask/transform_assistance_listing.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_assistance_listing.py\nSize: 5.46 KB\nLast Modified: 2025-02-14T17:08:26.439Z"}
{"path":"api/tool/console/interactive.py","language":"python","type":"code","directory":"api/tool/console","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/api/tool/console/interactive.py","size":1631931,"lastModified":"2025-02-14T17:08:31.128Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nimport src.data_migration.transformation.transform_util as transform_util\nfrom src.data_migration.transformation.subtask.abstract_transform_subtask import (\n    AbstractTransformSubTask,\n)\nfrom src.db.models.opportunity_models import Opportunity, OpportunityAssistanceListing\nfrom src.db.models.staging.opportunity import TopportunityCfda\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformAssistanceListing(AbstractTransformSubTask):\n    def transform_records(self) -> None:\n        assistance_listings: list[\n            Tuple[TopportunityCfda, OpportunityAssistanceListing | None, Opportunity | None]\n        ] = self.fetch_with_opportunity(\n            TopportunityCfda,\n            OpportunityAssistanceListing,\n            [\n                TopportunityCfda.opp_cfda_id\n                == OpportunityAssistanceListing.opportunity_assistance_listing_id\n            ],\n        )\n\n        for (\n            source_assistance_listing,\n            target_assistance_listing,\n            opportunity,\n        ) in assistance_listings:\n            try:\n                self.process_assistance_listing(\n                    source_assistance_listing, target_assistance_listing, opportunity\n                )\n            except ValueError:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_ERROR_COUNT,\n                    prefix=transform_constants.ASSISTANCE_LISTING,\n                )\n                logger.exception(\n                    \"Failed to process assistance listing\",\n                    extra={\n                        \"opportunity_assistance_listing_id\": source_assistance_listing.opp_cfda_id\n                    },\n                )\n\n    def process_assistance_listing(\n        self,\n        source_assistance_listing: TopportunityCfda,\n        target_assistance_listing: OpportunityAssistanceListing | None,\n        opportunity: Opportunity | None,\n    ) -> None:\n        self.increment(\n            transform_constants.Metrics.TOTAL_RECORDS_PROCESSED,\n            prefix=transform_constants.ASSISTANCE_LISTING,\n        )\n        extra = {\n            \"opportunity_assistance_listing_id\": source_assistance_listing.opp_cfda_id,\n            \"opportunity_id\": source_assistance_listing.opportunity_id,\n        }\n        logger.info(\"Processing assistance listing\", extra=extra)\n\n        if source_assistance_listing.is_deleted:\n            self._handle_delete(\n                source_assistance_listing,\n                target_assistance_listing,\n                transform_constants.ASSISTANCE_LISTING,\n                extra,\n            )\n\n        elif opportunity is None:\n            # The Oracle system we're importing these from does not have a foreign key between\n            # the opportunity ID in the TOPPORTUNITY_CFDA table and the TOPPORTUNITY table.\n            # There are many (2306 as of writing) orphaned CFDA records, created between 2007 and 2011\n            # We don't want to continuously process these, so won't error for these, and will just\n            # mark them as transformed below.\n            self.increment(\n                transform_constants.Metrics.TOTAL_RECORDS_ORPHANED,\n                prefix=transform_constants.ASSISTANCE_LISTING,\n            )\n            logger.info(\n                \"Assistance listing is orphaned and does not connect to any opportunity\",\n                extra=extra,\n            )\n            source_assistance_listing.transformation_notes = transform_constants.ORPHANED_CFDA\n\n        elif not source_assistance_listing.programtitle or not source_assistance_listing.cfdanumber:\n            self.increment(\n                transform_constants.Metrics.TOTAL_RECORDS_SKIPPED,\n                prefix=transform_constants.ASSISTANCE_LISTING,\n            )\n            logger.info(\n                \"Skipping assistance listing with empty required fields\",\n                extra={\n                    **extra,\n                    \"programtitle\": source_assistance_listing.programtitle,\n                    \"cfdanumber\": source_assistance_listing.cfdanumber,\n                },\n            )\n            source_assistance_listing.transformation_notes = \"empty_assistance_listing\"\n            source_assistance_listing.transformed_at = self.transform_time\n\n        else:\n            # To avoid incrementing metrics for records we fail to transform, record\n            # here whether it's an insert/update and we'll increment after transforming\n            is_insert = target_assistance_listing is None\n\n            logger.info(\"Transforming and upserting assistance listing\", extra=extra)\n            transformed_assistance_listing = transform_util.transform_assistance_listing(\n                source_assistance_listing, target_assistance_listing\n            )\n\n            if is_insert:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_INSERTED,\n                    prefix=transform_constants.ASSISTANCE_LISTING,\n                )\n                self.db_session.add(transformed_assistance_listing)\n            else:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_UPDATED,\n                    prefix=transform_constants.ASSISTANCE_LISTING,\n                )\n                self.db_session.merge(transformed_assistance_listing)\n\n        logger.info(\"Processed assistance listing\", extra=extra)\n        source_assistance_listing.transformed_at = self.transform_time"}
{"path":"docker-compose.yml","language":"yaml","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/docker-compose.yml","size":0,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/transformation/subtask/transform_funding_category.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_funding_category.py\nSize: 9.01 KB\nLast Modified: 2025-02-14T17:08:26.439Z"}
{"path":"documentation/README.md","language":"markdown","type":"code","directory":"documentation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.128Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nimport src.data_migration.transformation.transform_util as transform_util\nfrom src.data_migration.transformation.subtask.abstract_transform_subtask import (\n    AbstractTransformSubTask,\n)\nfrom src.db.models.opportunity_models import (\n    LinkOpportunitySummaryFundingCategory,\n    OpportunitySummary,\n)\nfrom src.db.models.staging.forecast import TfundactcatForecast, TfundactcatForecastHist\nfrom src.db.models.staging.synopsis import TfundactcatSynopsis, TfundactcatSynopsisHist\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformFundingCategory(AbstractTransformSubTask):\n    def transform_records(self) -> None:\n        link_table = LinkOpportunitySummaryFundingCategory\n        relationship_load_value = OpportunitySummary.link_funding_categories\n\n        logger.info(\"Processing forecast funding categories\")\n        forecast_funding_category_records = self.fetch_with_opportunity_summary(\n            TfundactcatForecast,\n            link_table,\n            [\n                TfundactcatForecast.fac_frcst_id\n                == LinkOpportunitySummaryFundingCategory.legacy_funding_category_id,\n                OpportunitySummary.opportunity_summary_id\n                == LinkOpportunitySummaryFundingCategory.opportunity_summary_id,\n            ],\n            is_forecast=True,\n            is_historical_table=False,\n            relationship_load_value=relationship_load_value,\n        )\n        self.process_link_funding_categories_group(forecast_funding_category_records)\n\n        logger.info(\"Processing historical forecast funding categories\")\n        forecast_funding_category_hist_records = self.fetch_with_opportunity_summary(\n            TfundactcatForecastHist,\n            link_table,\n            [\n                TfundactcatForecastHist.fac_frcst_id\n                == LinkOpportunitySummaryFundingCategory.legacy_funding_category_id,\n                OpportunitySummary.opportunity_summary_id\n                == LinkOpportunitySummaryFundingCategory.opportunity_summary_id,\n            ],\n            is_forecast=True,\n            is_historical_table=True,\n            relationship_load_value=relationship_load_value,\n        )\n        self.process_link_funding_categories_group(forecast_funding_category_hist_records)\n\n        logger.info(\"Processing synopsis funding categories\")\n        synopsis_funding_category_records = self.fetch_with_opportunity_summary(\n            TfundactcatSynopsis,\n            link_table,\n            [\n                TfundactcatSynopsis.fac_syn_id\n                == LinkOpportunitySummaryFundingCategory.legacy_funding_category_id,\n                OpportunitySummary.opportunity_summary_id\n                == LinkOpportunitySummaryFundingCategory.opportunity_summary_id,\n            ],\n            is_forecast=False,\n            is_historical_table=False,\n            relationship_load_value=relationship_load_value,\n        )\n        self.process_link_funding_categories_group(synopsis_funding_category_records)\n\n        logger.info(\"Processing historical synopsis funding categories\")\n        synopsis_funding_category_hist_records = self.fetch_with_opportunity_summary(\n            TfundactcatSynopsisHist,\n            link_table,\n            [\n                TfundactcatSynopsisHist.fac_syn_id\n                == LinkOpportunitySummaryFundingCategory.legacy_funding_category_id,\n                OpportunitySummary.opportunity_summary_id\n                == LinkOpportunitySummaryFundingCategory.opportunity_summary_id,\n            ],\n            is_forecast=False,\n            is_historical_table=True,\n            relationship_load_value=relationship_load_value,\n        )\n        self.process_link_funding_categories_group(synopsis_funding_category_hist_records)\n\n    def process_link_funding_categories_group(\n        self,\n        records: Sequence[\n            Tuple[\n                transform_constants.SourceFundingCategory,\n                LinkOpportunitySummaryFundingCategory | None,\n                OpportunitySummary | None,\n            ]\n        ],\n    ) -> None:\n        for source_funding_category, target_funding_category, opportunity_summary in records:\n            try:\n                self.process_link_funding_category(\n                    source_funding_category, target_funding_category, opportunity_summary\n                )\n            except ValueError:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_ERROR_COUNT,\n                    prefix=transform_constants.FUNDING_CATEGORY,\n                )\n                logger.exception(\n                    \"Failed to process opportunity summary funding category\",\n                    extra=transform_util.get_log_extra_funding_category(source_funding_category),\n                )\n\n    def process_link_funding_category(\n        self,\n        source_funding_category: transform_constants.SourceFundingCategory,\n        target_funding_category: LinkOpportunitySummaryFundingCategory | None,\n        opportunity_summary: OpportunitySummary | None,\n    ) -> None:\n        self.increment(\n            transform_constants.Metrics.TOTAL_RECORDS_PROCESSED,\n            prefix=transform_constants.FUNDING_CATEGORY,\n        )\n        extra = transform_util.get_log_extra_funding_category(source_funding_category)\n        logger.info(\"Processing funding category\", extra=extra)\n\n        if source_funding_category.is_deleted:\n            self._handle_delete(\n                source_funding_category,\n                target_funding_category,\n                transform_constants.FUNDING_CATEGORY,\n                extra,\n            )\n\n        # Historical records are linked to other historical records, however\n        # we don't import historical opportunity records, so if the opportunity\n        # was deleted, we won't have created the opportunity summary. Whenever we do\n        # support historical opportunities, we'll have these all marked with a\n        # flag that we can use to reprocess these.\n        elif self._is_orphaned_historical(opportunity_summary, source_funding_category):\n            self._handle_orphaned_historical(\n                source_funding_category, transform_constants.FUNDING_CATEGORY, extra\n            )\n\n        elif opportunity_summary is None:\n            # This shouldn't be possible as the incoming data has foreign keys, but as a safety net\n            # we'll make sure the opportunity actually exists\n            raise ValueError(\n                \"Funding category record cannot be processed as the opportunity summary for it does not exist\"\n            )\n        else:\n            # To avoid incrementing metrics for records we fail to transform, record\n            # here whether it's an insert/update and we'll increment after transforming\n            is_insert = target_funding_category is None\n\n            logger.info(\"Transforming and upserting funding category\", extra=extra)\n            transformed_funding_category = (\n                transform_util.convert_opportunity_summary_funding_category(\n                    source_funding_category, target_funding_category, opportunity_summary\n                )\n            )\n\n            # Before we insert, we have to still be certain we're not adding a duplicate record\n            # because the primary key of the legacy tables is the legacy ID + lookup value + opportunity ID\n            # its possible for the same lookup value to appear multiple times because the legacy ID is different\n            # This would hit a conflict in our DBs primary key, so we need to verify that won't happen\n            if (\n                is_insert\n                and transformed_funding_category.funding_category\n                in opportunity_summary.funding_categories\n            ):\n                self.increment(\n                    transform_constants.Metrics.TOTAL_DUPLICATE_RECORDS_SKIPPED,\n                    prefix=transform_constants.FUNDING_CATEGORY,\n                )\n                logger.warning(\n                    \"Skipping funding category record\",\n                    extra=extra\n                    | {\"funding_category\": transformed_funding_category.funding_category},\n                )\n            elif is_insert:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_INSERTED,\n                    prefix=transform_constants.FUNDING_CATEGORY,\n                )\n                # We append to the relationship so SQLAlchemy immediately attaches it to its cached\n                # opportunity summary object so that the above check works when we receive dupes in the same batch\n                opportunity_summary.link_funding_categories.append(transformed_funding_category)\n            else:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_UPDATED,\n                    prefix=transform_constants.FUNDING_CATEGORY,\n                )\n                self.db_session.merge(transformed_funding_category)\n\n        logger.info(\"Processed funding category\", extra=extra)\n        source_funding_category.transformed_at = self.transform_time"}
{"path":"documentation/analytics/README.md","language":"markdown","type":"code","directory":"documentation/analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.128Z","content":"File: api/src/data_migration/transformation/subtask/transform_funding_instrument.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_funding_instrument.py\nSize: 9.19 KB\nLast Modified: 2025-02-14T17:08:26.439Z"}
{"path":"documentation/analytics/development.md","language":"markdown","type":"code","directory":"documentation/analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/development.md","size":894508,"lastModified":"2025-02-14T17:08:31.128Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nimport src.data_migration.transformation.transform_util as transform_util\nfrom src.data_migration.transformation.subtask.abstract_transform_subtask import (\n    AbstractTransformSubTask,\n)\nfrom src.db.models.opportunity_models import (\n    LinkOpportunitySummaryFundingInstrument,\n    OpportunitySummary,\n)\nfrom src.db.models.staging.forecast import TfundinstrForecast, TfundinstrForecastHist\nfrom src.db.models.staging.synopsis import TfundinstrSynopsis, TfundinstrSynopsisHist\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformFundingInstrument(AbstractTransformSubTask):\n    def transform_records(self) -> None:\n        link_table = LinkOpportunitySummaryFundingInstrument\n        relationship_load_value = OpportunitySummary.link_funding_instruments\n\n        logger.info(\"Processing forecast funding instruments\")\n        forecast_funding_instrument_records = self.fetch_with_opportunity_summary(\n            TfundinstrForecast,\n            link_table,\n            [\n                TfundinstrForecast.fi_frcst_id\n                == LinkOpportunitySummaryFundingInstrument.legacy_funding_instrument_id,\n                OpportunitySummary.opportunity_summary_id\n                == LinkOpportunitySummaryFundingInstrument.opportunity_summary_id,\n            ],\n            is_forecast=True,\n            is_historical_table=False,\n            relationship_load_value=relationship_load_value,\n        )\n        self.process_link_funding_instruments_group(forecast_funding_instrument_records)\n\n        logger.info(\"Processing historical forecast funding instruments\")\n        forecast_funding_instrument_hist_records = self.fetch_with_opportunity_summary(\n            TfundinstrForecastHist,\n            link_table,\n            [\n                TfundinstrForecastHist.fi_frcst_id\n                == LinkOpportunitySummaryFundingInstrument.legacy_funding_instrument_id,\n                OpportunitySummary.opportunity_summary_id\n                == LinkOpportunitySummaryFundingInstrument.opportunity_summary_id,\n            ],\n            is_forecast=True,\n            is_historical_table=True,\n            relationship_load_value=relationship_load_value,\n        )\n        self.process_link_funding_instruments_group(forecast_funding_instrument_hist_records)\n\n        logger.info(\"Processing synopsis funding instruments\")\n        synopsis_funding_instrument_records = self.fetch_with_opportunity_summary(\n            TfundinstrSynopsis,\n            link_table,\n            [\n                TfundinstrSynopsis.fi_syn_id\n                == LinkOpportunitySummaryFundingInstrument.legacy_funding_instrument_id,\n                OpportunitySummary.opportunity_summary_id\n                == LinkOpportunitySummaryFundingInstrument.opportunity_summary_id,\n            ],\n            is_forecast=False,\n            is_historical_table=False,\n            relationship_load_value=relationship_load_value,\n        )\n        self.process_link_funding_instruments_group(synopsis_funding_instrument_records)\n\n        logger.info(\"Processing historical synopsis funding instruments\")\n        synopsis_funding_instrument_hist_records = self.fetch_with_opportunity_summary(\n            TfundinstrSynopsisHist,\n            link_table,\n            [\n                TfundinstrSynopsisHist.fi_syn_id\n                == LinkOpportunitySummaryFundingInstrument.legacy_funding_instrument_id,\n                OpportunitySummary.opportunity_summary_id\n                == LinkOpportunitySummaryFundingInstrument.opportunity_summary_id,\n            ],\n            is_forecast=False,\n            is_historical_table=True,\n            relationship_load_value=relationship_load_value,\n        )\n        self.process_link_funding_instruments_group(synopsis_funding_instrument_hist_records)\n\n    def process_link_funding_instruments_group(\n        self,\n        records: Sequence[\n            Tuple[\n                transform_constants.SourceFundingInstrument,\n                LinkOpportunitySummaryFundingInstrument | None,\n                OpportunitySummary | None,\n            ]\n        ],\n    ) -> None:\n        for source_funding_instrument, target_funding_instrument, opportunity_summary in records:\n            try:\n                self.process_link_funding_instrument(\n                    source_funding_instrument, target_funding_instrument, opportunity_summary\n                )\n            except ValueError:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_ERROR_COUNT,\n                    prefix=transform_constants.FUNDING_INSTRUMENT,\n                )\n                logger.exception(\n                    \"Failed to process opportunity summary funding instrument\",\n                    extra=transform_util.get_log_extra_funding_instrument(\n                        source_funding_instrument\n                    ),\n                )\n\n    def process_link_funding_instrument(\n        self,\n        source_funding_instrument: transform_constants.SourceFundingInstrument,\n        target_funding_instrument: LinkOpportunitySummaryFundingInstrument | None,\n        opportunity_summary: OpportunitySummary | None,\n    ) -> None:\n        self.increment(\n            transform_constants.Metrics.TOTAL_RECORDS_PROCESSED,\n            prefix=transform_constants.FUNDING_INSTRUMENT,\n        )\n        extra = transform_util.get_log_extra_funding_instrument(source_funding_instrument)\n        logger.info(\"Processing funding instrument\", extra=extra)\n\n        if source_funding_instrument.is_deleted:\n            self._handle_delete(\n                source_funding_instrument,\n                target_funding_instrument,\n                transform_constants.FUNDING_INSTRUMENT,\n                extra,\n            )\n\n        # Historical records are linked to other historical records, however\n        # we don't import historical opportunity records, so if the opportunity\n        # was deleted, we won't have created the opportunity summary. Whenever we do\n        # support historical opportunities, we'll have these all marked with a\n        # flag that we can use to reprocess these.\n        elif self._is_orphaned_historical(opportunity_summary, source_funding_instrument):\n            self._handle_orphaned_historical(\n                source_funding_instrument, transform_constants.FUNDING_INSTRUMENT, extra\n            )\n\n        elif opportunity_summary is None:\n            # This shouldn't be possible as the incoming data has foreign keys, but as a safety net\n            # we'll make sure the opportunity actually exists\n            raise ValueError(\n                \"Funding instrument record cannot be processed as the opportunity summary for it does not exist\"\n            )\n\n        else:\n            # To avoid incrementing metrics for records we fail to transform, record\n            # here whether it's an insert/update and we'll increment after transforming\n            is_insert = target_funding_instrument is None\n\n            logger.info(\"Transforming and upserting funding instrument\", extra=extra)\n            transformed_funding_instrument = (\n                transform_util.convert_opportunity_summary_funding_instrument(\n                    source_funding_instrument, target_funding_instrument, opportunity_summary\n                )\n            )\n\n            # Before we insert, we have to still be certain we're not adding a duplicate record\n            # because the primary key of the legacy tables is the legacy ID + lookup value + opportunity ID\n            # its possible for the same lookup value to appear multiple times because the legacy ID is different\n            # This would hit a conflict in our DBs primary key, so we need to verify that won't happen\n            if (\n                is_insert\n                and transformed_funding_instrument.funding_instrument\n                in opportunity_summary.funding_instruments\n            ):\n                self.increment(\n                    transform_constants.Metrics.TOTAL_DUPLICATE_RECORDS_SKIPPED,\n                    prefix=transform_constants.FUNDING_INSTRUMENT,\n                )\n                logger.warning(\n                    \"Skipping funding instrument record\",\n                    extra=extra\n                    | {\"funding_instrument\": transformed_funding_instrument.funding_instrument},\n                )\n            elif is_insert:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_INSERTED,\n                    prefix=transform_constants.FUNDING_INSTRUMENT,\n                )\n                # We append to the relationship so SQLAlchemy immediately attaches it to its cached\n                # opportunity summary object so that the above check works when we receive dupes in the same batch\n                opportunity_summary.link_funding_instruments.append(transformed_funding_instrument)\n            else:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_UPDATED,\n                    prefix=transform_constants.FUNDING_INSTRUMENT,\n                )\n                self.db_session.merge(transformed_funding_instrument)\n\n        logger.info(\"Processed funding instrument\", extra=extra)\n        source_funding_instrument.transformed_at = self.transform_time"}
{"path":"documentation/analytics/formatting-and-linting.md","language":"markdown","type":"code","directory":"documentation/analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/formatting-and-linting.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/data_migration/transformation/subtask/transform_opportunity.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_opportunity.py\nSize: 4.82 KB\nLast Modified: 2025-02-14T17:08:26.439Z"}
{"path":"documentation/analytics/metrics/README.md","language":"markdown","type":"code","directory":"documentation/analytics/metrics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/metrics/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nimport src.data_migration.transformation.transform_util as transform_util\nfrom src.adapters.aws import S3Config\nfrom src.data_migration.transformation.subtask.abstract_transform_subtask import (\n    AbstractTransformSubTask,\n)\nfrom src.db.models.opportunity_models import Opportunity\nfrom src.db.models.staging.opportunity import Topportunity\nfrom src.services.opportunity_attachments import attachment_util\nfrom src.task.task import Task\nfrom src.util import file_util\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformOpportunity(AbstractTransformSubTask):\n\n    def __init__(self, task: Task, s3_config: S3Config | None = None):\n        super().__init__(task)\n\n        if s3_config is None:\n            s3_config = S3Config()\n\n        self.s3_config = s3_config\n\n    def transform_records(self) -> None:\n        # Fetch all opportunities that were modified\n        # Alongside that, grab the existing opportunity record\n        opportunities: list[Tuple[Topportunity, Opportunity | None]] = self.fetch(\n            Topportunity,\n            Opportunity,\n            [Topportunity.opportunity_id == Opportunity.opportunity_id],\n        )\n\n        for source_opportunity, target_opportunity in opportunities:\n            try:\n                self.process_opportunity(source_opportunity, target_opportunity)\n            except ValueError:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_ERROR_COUNT,\n                    prefix=transform_constants.OPPORTUNITY,\n                )\n                logger.exception(\n                    \"Failed to process opportunity\",\n                    extra={\"opportunity_id\": source_opportunity.opportunity_id},\n                )\n\n    def process_opportunity(\n        self, source_opportunity: Topportunity, target_opportunity: Opportunity | None\n    ) -> None:\n        self.increment(\n            transform_constants.Metrics.TOTAL_RECORDS_PROCESSED,\n            prefix=transform_constants.OPPORTUNITY,\n        )\n        extra = {\"opportunity_id\": source_opportunity.opportunity_id}\n        logger.info(\"Processing opportunity\", extra=extra)\n\n        if source_opportunity.is_deleted:\n            self._handle_delete(\n                source_opportunity,\n                target_opportunity,\n                transform_constants.OPPORTUNITY,\n                extra,\n            )\n\n            # Cleanup the attachments from s3\n            if target_opportunity is not None:\n                for attachment in target_opportunity.opportunity_attachments:\n                    file_util.delete_file(attachment.file_location)\n\n        else:\n            # To avoid incrementing metrics for records we fail to transform, record\n            # here whether it's an insert/update and we'll increment after transforming\n            is_insert = target_opportunity is None\n\n            was_draft = target_opportunity.is_draft if target_opportunity else None\n\n            logger.info(\"Transforming and upserting opportunity\", extra=extra)\n            transformed_opportunity = transform_util.transform_opportunity(\n                source_opportunity, target_opportunity\n            )\n\n            if is_insert:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_INSERTED,\n                    prefix=transform_constants.OPPORTUNITY,\n                )\n                self.db_session.add(transformed_opportunity)\n            else:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_UPDATED,\n                    prefix=transform_constants.OPPORTUNITY,\n                )\n                self.db_session.merge(transformed_opportunity)\n\n                # If an opportunity went from being a draft to not a draft (published)\n                # then we need to move all of its attachments to the public bucket\n                # from the draft s3 bucket.\n                if was_draft and transformed_opportunity.is_draft is False:\n                    for attachment in cast(Opportunity, target_opportunity).opportunity_attachments:\n                        # Determine the new path\n                        file_name = attachment_util.adjust_legacy_file_name(attachment.file_name)\n                        s3_path = attachment_util.get_s3_attachment_path(\n                            file_name,\n                            attachment.attachment_id,\n                            transformed_opportunity,\n                            self.s3_config,\n                        )\n\n                        # Move the file\n                        file_util.move_file(attachment.file_location, s3_path)\n                        attachment.file_location = s3_path\n\n        logger.info(\"Processed opportunity\", extra=extra)\n        source_opportunity.transformed_at = self.transform_time"}
{"path":"documentation/analytics/metrics/burndown.md","language":"markdown","type":"code","directory":"documentation/analytics/metrics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/metrics/burndown.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/data_migration/transformation/subtask/transform_opportunity_attachment.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_opportunity_attachment.py\nSize: 9.24 KB\nLast Modified: 2025-02-14T17:08:26.439Z"}
{"path":"documentation/analytics/metrics/burnup.md","language":"markdown","type":"code","directory":"documentation/analytics/metrics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/metrics/burnup.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nimport src.data_migration.transformation.transform_util as transform_util\nfrom src.adapters.aws import S3Config\nfrom src.data_migration.transformation.subtask.abstract_transform_subtask import (\n    AbstractTransformSubTask,\n)\nfrom src.db.models.opportunity_models import Opportunity, OpportunityAttachment\nfrom src.db.models.staging.attachment import TsynopsisAttachment\nfrom src.services.opportunity_attachments import attachment_util\nfrom src.task.task import Task\nfrom src.util import file_util\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformOpportunityAttachmentConfig(PydanticBaseEnvConfig):\n    # This is just for testing, we want to be able to only\n    # import a few attachments when manually testing.\n    total_attachments_to_process: int | None = None\n\n    transform_opportunity_attachment_batch_size: int = 100\n\n\nclass TransformOpportunityAttachment(AbstractTransformSubTask):\n\n    def __init__(self, task: Task, s3_config: S3Config | None = None):\n        super().__init__(task)\n\n        if s3_config is None:\n            s3_config = S3Config()\n\n        self.s3_config = s3_config\n\n        self.attachment_config = TransformOpportunityAttachmentConfig()\n\n        self.total_attachments_processed = 0\n        self.has_unprocessed_records = True\n\n    def has_more_to_process(self) -> bool:\n        return self.has_unprocessed_records\n\n    def transform_records(self) -> None:\n\n        # Fetch staging attachment / our attachment / opportunity groups\n        records = self.fetch_with_opportunity(\n            TsynopsisAttachment,\n            OpportunityAttachment,\n            [TsynopsisAttachment.syn_att_id == OpportunityAttachment.attachment_id],\n            # We load opportunity attachments into memory, so need to process very small batches\n            # to avoid running out of memory.\n            batch_size=self.attachment_config.transform_opportunity_attachment_batch_size,\n            limit=self.attachment_config.transform_opportunity_attachment_batch_size,\n        )\n\n        records_processed = self.process_opportunity_attachment_group(records)\n\n        # If we have processed up to the test config value, stop processing entirely\n        if (\n            self.attachment_config.total_attachments_to_process is not None\n            and self.total_attachments_processed\n            >= self.attachment_config.total_attachments_to_process\n        ):\n            self.has_unprocessed_records = False\n\n        # Assume if we had fewer than the batch size\n        # we're probably done\n        if records_processed != self.attachment_config.transform_opportunity_attachment_batch_size:\n            self.has_unprocessed_records = False\n\n    def process_opportunity_attachment_group(\n        self,\n        records: Sequence[\n            tuple[TsynopsisAttachment, OpportunityAttachment | None, Opportunity | None]\n        ],\n    ) -> int:\n\n        records_processed = 0\n        for source_attachment, target_attachment, opportunity in records:\n            try:\n                # Note we increment first in case there are errors, want it to always increment\n                records_processed += 1\n                self.total_attachments_processed += 1\n\n                self.process_opportunity_attachment(\n                    source_attachment, target_attachment, opportunity\n                )\n            except ValueError:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_ERROR_COUNT,\n                    prefix=transform_constants.OPPORTUNITY_ATTACHMENT,\n                )\n                logger.exception(\n                    \"Failed to process opportunity attachment\",\n                    extra=transform_util.get_log_extra_opportunity_attachment(source_attachment),\n                )\n\n        return records_processed\n\n    def process_opportunity_attachment(\n        self,\n        source_attachment: TsynopsisAttachment,\n        target_attachment: OpportunityAttachment | None,\n        opportunity: Opportunity | None,\n    ) -> None:\n\n        self.increment(\n            transform_constants.Metrics.TOTAL_RECORDS_PROCESSED,\n            prefix=transform_constants.OPPORTUNITY_ATTACHMENT,\n        )\n\n        extra = transform_util.get_log_extra_opportunity_attachment(source_attachment)\n        logger.info(\"Processing opportunity attachment\", extra=extra)\n\n        if source_attachment.is_deleted:\n            self._handle_delete(\n                source=source_attachment,\n                target=target_attachment,\n                record_type=transform_constants.OPPORTUNITY_ATTACHMENT,\n                extra=extra,\n            )\n\n            # Delete the file from s3 as well\n            if target_attachment is not None:\n                file_util.delete_file(target_attachment.file_location)\n\n        elif opportunity is None:\n            # This shouldn't be possible as the incoming data has foreign keys, but as a safety net\n            # we'll make sure the opportunity actually exists\n            raise ValueError(\n                \"Opportunity attachment cannot be processed as the opportunity for it does not exist\"\n            )\n\n        else:\n            # To avoid incrementing metrics for records we fail to transform, record\n            # here whether it's an insert/update and we'll increment after transforming\n            is_insert = target_attachment is None\n\n            prior_attachment_location = (\n                target_attachment.file_location if target_attachment else None\n            )\n\n            logger.info(\"Transforming and upserting opportunity attachment\", extra=extra)\n\n            transformed_opportunity_attachment = transform_opportunity_attachment(\n                source_attachment, target_attachment, opportunity, self.s3_config\n            )\n\n            # Write the file to s3\n            write_file(source_attachment, transformed_opportunity_attachment)\n\n            # If this was an update, and the file name changed\n            # Cleanup the old file from s3.\n            if (\n                prior_attachment_location is not None\n                and prior_attachment_location != transformed_opportunity_attachment.file_location\n            ):\n                file_util.delete_file(prior_attachment_location)\n\n            logger.info(\"Transforming and upserting opportunity attachment\", extra=extra)\n\n            if is_insert:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_INSERTED,\n                    prefix=transform_constants.OPPORTUNITY_ATTACHMENT,\n                )\n                self.db_session.add(transformed_opportunity_attachment)\n            else:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_UPDATED,\n                    prefix=transform_constants.OPPORTUNITY_ATTACHMENT,\n                )\n                self.db_session.merge(transformed_opportunity_attachment)\n\n        logger.info(\"Processed opportunity attachment\", extra=extra)\n        source_attachment.transformed_at = self.transform_time\n\n\ndef transform_opportunity_attachment(\n    source_attachment: TsynopsisAttachment,\n    incoming_attachment: OpportunityAttachment | None,\n    opportunity: Opportunity,\n    s3_config: S3Config,\n) -> OpportunityAttachment:\n\n    log_extra = transform_util.get_log_extra_opportunity_attachment(source_attachment)\n\n    if incoming_attachment is None:\n        logger.info(\"Creating new opportunity attachment record\", extra=log_extra)\n\n    # Adjust the file_name to remove characters clunky in URLs\n    if source_attachment.file_name is None:\n        raise ValueError(\"Opportunity attachment does not have a file name, cannot process.\")\n    file_name = attachment_util.adjust_legacy_file_name(source_attachment.file_name)\n\n    file_location = attachment_util.get_s3_attachment_path(\n        file_name, source_attachment.syn_att_id, opportunity, s3_config\n    )\n\n    # We always create a new record here and merge it in the calling function\n    # this way if there is any error doing the transformation, we don't modify the existing one.\n    target_attachment = OpportunityAttachment(\n        attachment_id=source_attachment.syn_att_id,\n        opportunity_id=source_attachment.opportunity_id,\n        # Note we calculate the file location here, but haven't yet done anything\n        # with s3, the calling function, will handle writing the file to s3.\n        file_location=file_location,\n        mime_type=source_attachment.mime_type,\n        file_name=file_name,\n        file_description=source_attachment.file_desc,\n        file_size_bytes=source_attachment.file_lob_size,\n        created_by=source_attachment.creator_id,\n        updated_by=source_attachment.last_upd_id,\n        legacy_folder_id=source_attachment.syn_att_folder_id,\n    )\n\n    transform_util.transform_update_create_timestamp(\n        source_attachment, target_attachment, log_extra=log_extra\n    )\n\n    return target_attachment\n\n\ndef write_file(\n    source_attachment: TsynopsisAttachment, destination_attachment: OpportunityAttachment\n) -> None:\n\n    if source_attachment.file_lob is None:\n        raise ValueError(\"Attachment is null, cannot copy\")\n\n    with file_util.open_stream(destination_attachment.file_location, \"wb\") as outfile:\n        outfile.write(source_attachment.file_lob)"}
{"path":"documentation/analytics/metrics/percent-complete.md","language":"markdown","type":"code","directory":"documentation/analytics/metrics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/metrics/percent-complete.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/data_migration/transformation/subtask/transform_opportunity_summary.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/subtask/transform_opportunity_summary.py\nSize: 6.12 KB\nLast Modified: 2025-02-14T17:08:26.439Z"}
{"path":"documentation/analytics/technical-overview.md","language":"markdown","type":"code","directory":"documentation/analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/technical-overview.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nimport src.data_migration.transformation.transform_util as transform_util\nfrom src.data_migration.transformation.subtask.abstract_transform_subtask import (\n    AbstractTransformSubTask,\n)\nfrom src.db.models.opportunity_models import Opportunity, OpportunitySummary\nfrom src.db.models.staging.forecast import Tforecast, TforecastHist\nfrom src.db.models.staging.synopsis import Tsynopsis, TsynopsisHist\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformOpportunitySummary(AbstractTransformSubTask):\n    def transform_records(self) -> None:\n        logger.info(\"Processing opportunity summaries\")\n        logger.info(\"Processing synopsis records\")\n        synopsis_records = self.fetch_with_opportunity(\n            Tsynopsis,\n            OpportunitySummary,\n            [\n                Tsynopsis.opportunity_id == OpportunitySummary.opportunity_id,\n                OpportunitySummary.is_forecast.is_(False),\n                OpportunitySummary.revision_number.is_(None),\n            ],\n        )\n        self.process_opportunity_summary_group(synopsis_records)\n\n        logger.info(\"Processing synopsis hist records\")\n        synopsis_hist_records = self.fetch_with_opportunity(\n            TsynopsisHist,\n            OpportunitySummary,\n            [\n                TsynopsisHist.opportunity_id == OpportunitySummary.opportunity_id,\n                TsynopsisHist.revision_number == OpportunitySummary.revision_number,\n                OpportunitySummary.is_forecast.is_(False),\n            ],\n        )\n        self.process_opportunity_summary_group(synopsis_hist_records)\n\n        logger.info(\"Processing forecast records\")\n        forecast_records = self.fetch_with_opportunity(\n            Tforecast,\n            OpportunitySummary,\n            [\n                Tforecast.opportunity_id == OpportunitySummary.opportunity_id,\n                OpportunitySummary.is_forecast.is_(True),\n                OpportunitySummary.revision_number.is_(None),\n            ],\n        )\n        self.process_opportunity_summary_group(forecast_records)\n\n        logger.info(\"Processing forecast hist records\")\n        forecast_hist_records = self.fetch_with_opportunity(\n            TforecastHist,\n            OpportunitySummary,\n            [\n                TforecastHist.opportunity_id == OpportunitySummary.opportunity_id,\n                TforecastHist.revision_number == OpportunitySummary.revision_number,\n                OpportunitySummary.is_forecast.is_(True),\n            ],\n        )\n        self.process_opportunity_summary_group(forecast_hist_records)\n\n    def process_opportunity_summary_group(\n        self,\n        records: Sequence[\n            Tuple[transform_constants.SourceSummary, OpportunitySummary | None, Opportunity | None]\n        ],\n    ) -> None:\n        for source_summary, target_summary, opportunity in records:\n            try:\n                self.process_opportunity_summary(source_summary, target_summary, opportunity)\n            except ValueError:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_ERROR_COUNT,\n                    prefix=transform_constants.OPPORTUNITY_SUMMARY,\n                )\n                logger.exception(\n                    \"Failed to process opportunity summary\",\n                    extra=transform_util.get_log_extra_summary(source_summary),\n                )\n\n    def process_opportunity_summary(\n        self,\n        source_summary: transform_constants.SourceSummary,\n        target_summary: OpportunitySummary | None,\n        opportunity: Opportunity | None,\n    ) -> None:\n        self.increment(\n            transform_constants.Metrics.TOTAL_RECORDS_PROCESSED,\n            prefix=transform_constants.OPPORTUNITY_SUMMARY,\n        )\n        extra = transform_util.get_log_extra_summary(source_summary)\n        logger.info(\"Processing opportunity summary\", extra=extra)\n\n        if source_summary.is_deleted:\n            self._handle_delete(\n                source_summary, target_summary, transform_constants.OPPORTUNITY_SUMMARY, extra\n            )\n\n        # Historical records are linked to other historical records, however\n        # we don't import historical opportunity records, so if the opportunity\n        # was deleted, we don't have anything to link these to. Whenever we do\n        # support historical opportunities, we'll have these all marked with a\n        # flag that we can use to reprocess these.\n        elif self._is_orphaned_historical(opportunity, source_summary):\n            self._handle_orphaned_historical(\n                source_summary, transform_constants.OPPORTUNITY_SUMMARY, extra\n            )\n\n        elif opportunity is None:\n            # This shouldn't be possible as the incoming data has foreign keys, but as a safety net\n            # we'll make sure the opportunity actually exists\n            raise ValueError(\n                \"Opportunity summary cannot be processed as the opportunity for it does not exist\"\n            )\n\n        else:\n            # To avoid incrementing metrics for records we fail to transform, record\n            # here whether it's an insert/update and we'll increment after transforming\n            is_insert = target_summary is None\n\n            logger.info(\"Transforming and upserting opportunity summary\", extra=extra)\n            transformed_opportunity_summary = transform_util.transform_opportunity_summary(\n                source_summary, target_summary\n            )\n\n            if is_insert:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_INSERTED,\n                    prefix=transform_constants.OPPORTUNITY_SUMMARY,\n                )\n                self.db_session.add(transformed_opportunity_summary)\n            else:\n                self.increment(\n                    transform_constants.Metrics.TOTAL_RECORDS_UPDATED,\n                    prefix=transform_constants.OPPORTUNITY_SUMMARY,\n                )\n                self.db_session.merge(transformed_opportunity_summary)\n\n        logger.info(\"Processed opportunity summary\", extra=extra)\n        source_summary.transformed_at = self.transform_time"}
{"path":"documentation/analytics/testing.md","language":"markdown","type":"code","directory":"documentation/analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/testing.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/data_migration/transformation/transform_constants.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/transform_constants.py\nSize: 2.38 KB\nLast Modified: 2025-02-14T17:08:26.439Z"}
{"path":"documentation/analytics/usage.md","language":"markdown","type":"code","directory":"documentation/analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/usage.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"from src.db.models.base import ApiSchemaTable\nfrom src.db.models.staging.forecast import (\n    TapplicanttypesForecast,\n    TapplicanttypesForecastHist,\n    Tforecast,\n    TforecastHist,\n    TfundactcatForecast,\n    TfundactcatForecastHist,\n    TfundinstrForecast,\n    TfundinstrForecastHist,\n)\nfrom src.db.models.staging.staging_base import StagingParamMixin\nfrom src.db.models.staging.synopsis import (\n    TapplicanttypesSynopsis,\n    TapplicanttypesSynopsisHist,\n    TfundactcatSynopsis,\n    TfundactcatSynopsisHist,\n    TfundinstrSynopsis,\n    TfundinstrSynopsisHist,\n    Tsynopsis,\n    TsynopsisHist,\n)\n\nORPHANED_CFDA = \"orphaned_cfda\"\nORPHANED_HISTORICAL_RECORD = \"orphaned_historical_record\"\nORPHANED_DELETE_RECORD = \"orphaned_delete_record\"\n\nOPPORTUNITY = \"opportunity\"\nASSISTANCE_LISTING = \"assistance_listing\"\nOPPORTUNITY_SUMMARY = \"opportunity_summary\"\nAPPLICANT_TYPE = \"applicant_type\"\nFUNDING_CATEGORY = \"funding_category\"\nFUNDING_INSTRUMENT = \"funding_instrument\"\nAGENCY = \"agency\"\nOPPORTUNITY_ATTACHMENT = \"opportunity_attachment\"\n\n\nclass Metrics(StrEnum):\n    TOTAL_RECORDS_PROCESSED = \"total_records_processed\"\n    TOTAL_RECORDS_DELETED = \"total_records_deleted\"\n    TOTAL_RECORDS_INSERTED = \"total_records_inserted\"\n    TOTAL_RECORDS_UPDATED = \"total_records_updated\"\n    TOTAL_RECORDS_SKIPPED = \"total_records_skipped\"\n    TOTAL_RECORDS_ORPHANED = \"total_records_orphaned\"\n    TOTAL_DUPLICATE_RECORDS_SKIPPED = \"total_duplicate_records_skipped\"\n    TOTAL_HISTORICAL_ORPHANS_SKIPPED = \"total_historical_orphans_skipped\"\n    TOTAL_DELETE_ORPHANS_SKIPPED = \"total_delete_orphans_skipped\"\n\n    TOTAL_ERROR_COUNT = \"total_error_count\"\n\n\nS = TypeVar(\"S\", bound=StagingParamMixin)\nD = TypeVar(\"D\", bound=ApiSchemaTable)\n\n\nSourceSummary: TypeAlias = Tforecast | Tsynopsis | TforecastHist | TsynopsisHist\n\nSourceApplicantType: TypeAlias = (\n    TapplicanttypesForecast\n    | TapplicanttypesForecastHist\n    | TapplicanttypesSynopsis\n    | TapplicanttypesSynopsisHist\n)\n\nSourceFundingCategory: TypeAlias = (\n    TfundactcatForecast | TfundactcatForecastHist | TfundactcatSynopsis | TfundactcatSynopsisHist\n)\n\nSourceFundingInstrument: TypeAlias = (\n    TfundinstrForecastHist | TfundinstrForecast | TfundinstrSynopsisHist | TfundinstrSynopsis\n)\n\nSourceAny: TypeAlias = (\n    SourceSummary | SourceApplicantType | SourceFundingCategory | SourceFundingInstrument\n)"}
{"path":"documentation/api/README.md","language":"markdown","type":"code","directory":"documentation/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/data_migration/transformation/transform_oracle_data_task.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/transform_oracle_data_task.py\nSize: 3.62 KB\nLast Modified: 2025-02-14T17:08:26.439Z"}
{"path":"documentation/api/api-details.md","language":"markdown","type":"code","directory":"documentation/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/api-details.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"from pydantic_settings import SettingsConfigDict\n\nimport src.data_migration.transformation.transform_constants as transform_constants\nfrom src.adapters import db\nfrom src.data_migration.transformation.subtask.transform_agency import (\n    TransformAgency,\n    TransformAgencyHierarchy,\n)\nfrom src.data_migration.transformation.subtask.transform_applicant_type import (\n    TransformApplicantType,\n)\nfrom src.data_migration.transformation.subtask.transform_assistance_listing import (\n    TransformAssistanceListing,\n)\nfrom src.data_migration.transformation.subtask.transform_funding_category import (\n    TransformFundingCategory,\n)\nfrom src.data_migration.transformation.subtask.transform_funding_instrument import (\n    TransformFundingInstrument,\n)\nfrom src.data_migration.transformation.subtask.transform_opportunity import TransformOpportunity\nfrom src.data_migration.transformation.subtask.transform_opportunity_attachment import (\n    TransformOpportunityAttachment,\n)\nfrom src.data_migration.transformation.subtask.transform_opportunity_summary import (\n    TransformOpportunitySummary,\n)\nfrom src.task.task import Task\nfrom src.util import datetime_util\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransformOracleDataTaskConfig(PydanticBaseEnvConfig):\n    model_config = SettingsConfigDict(env_prefix=\"TRANSFORM_ORACLE_DATA_\")\n\n    enable_opportunity: bool = True  # TRANSFORM_ORACLE_DATA_ENABLE_OPPORTUNITY\n    enable_assistance_listing: bool = True  # TRANSFORM_ORACLE_DATA_ENABLE_ASSISTANCE_LISTING\n    enable_opportunity_summary: bool = True  # TRANSFORM_ORACLE_DATA_ENABLE_OPPORTUNITY_SUMMARY\n    enable_applicant_type: bool = True  # TRANSFORM_ORACLE_DATA_ENABLE_APPLICANT_TYPE\n    enable_funding_category: bool = True  # TRANSFORM_ORACLE_DATA_ENABLE_FUNDING_CATEGORY\n    enable_funding_instrument: bool = True  # TRANSFORM_ORACLE_DATA_ENABLE_FUNDING_INSTRUMENT\n    enable_agency: bool = True  # TRANSFORM_ORACLE_DATA_ENABLE_AGENCY\n    enable_opportunity_attachment: bool = (\n        False  # TRANSFORM_ORACLE_DATA_ENABLE_OPPORTUNITY_ATTACHMENT\n    )\n\n\nclass TransformOracleDataTask(Task):\n    Metrics = transform_constants.Metrics\n\n    def __init__(\n        self,\n        db_session: db.Session,\n        transform_time: datetime | None = None,\n        transform_config: TransformOracleDataTaskConfig | None = None,\n    ) -> None:\n        super().__init__(db_session)\n\n        if transform_time is None:\n            transform_time = datetime_util.utcnow()\n        self.transform_time = transform_time\n\n        if transform_config is None:\n            transform_config = TransformOracleDataTaskConfig()\n        self.transform_config = transform_config\n\n    def run_task(self) -> None:\n        if self.transform_config.enable_opportunity:\n            TransformOpportunity(self).run()\n\n        if self.transform_config.enable_assistance_listing:\n            TransformAssistanceListing(self).run()\n\n        if self.transform_config.enable_opportunity_summary:\n            TransformOpportunitySummary(self).run()\n\n        if self.transform_config.enable_applicant_type:\n            TransformApplicantType(self).run()\n\n        if self.transform_config.enable_funding_category:\n            TransformFundingCategory(self).run()\n\n        if self.transform_config.enable_funding_instrument:\n            TransformFundingInstrument(self).run()\n\n        if self.transform_config.enable_agency:\n            TransformAgency(self).run()\n            TransformAgencyHierarchy(self).run()\n\n        if self.transform_config.enable_opportunity_attachment:\n            TransformOpportunityAttachment(self).run()"}
{"path":"documentation/api/api-versioning.md","language":"markdown","type":"code","directory":"documentation/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/api-versioning.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/data_migration/transformation/transform_util.py\nLanguage: py\nType: code\nDirectory: api/src/data_migration/transformation\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/data_migration/transformation/transform_util.py\nSize: 22.55 KB\nLast Modified: 2025-02-14T17:08:26.439Z"}
{"path":"documentation/api/authentication.md","language":"markdown","type":"code","directory":"documentation/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/authentication.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"from src.constants.lookup_constants import (\n    ApplicantType,\n    FundingCategory,\n    FundingInstrument,\n    OpportunityCategory,\n)\nfrom src.data_migration.transformation.transform_constants import (\n    SourceApplicantType,\n    SourceFundingCategory,\n    SourceFundingInstrument,\n    SourceSummary,\n)\nfrom src.db.models.base import TimestampMixin\nfrom src.db.models.opportunity_models import (\n    LinkOpportunitySummaryApplicantType,\n    LinkOpportunitySummaryFundingCategory,\n    LinkOpportunitySummaryFundingInstrument,\n    Opportunity,\n    OpportunityAssistanceListing,\n    OpportunitySummary,\n)\nfrom src.db.models.staging.attachment import TsynopsisAttachment\nfrom src.db.models.staging.opportunity import Topportunity, TopportunityCfda\nfrom src.db.models.staging.staging_base import StagingBase\nfrom src.util import datetime_util\n\nlogger = logging.getLogger(__name__)\n\nOPPORTUNITY_CATEGORY_MAP = {\n    \"D\": OpportunityCategory.DISCRETIONARY,\n    \"M\": OpportunityCategory.MANDATORY,\n    \"C\": OpportunityCategory.CONTINUATION,\n    \"E\": OpportunityCategory.EARMARK,\n    \"O\": OpportunityCategory.OTHER,\n}\n\nAPPLICANT_TYPE_MAP = {\n    \"00\": ApplicantType.STATE_GOVERNMENTS,\n    \"01\": ApplicantType.COUNTY_GOVERNMENTS,\n    \"02\": ApplicantType.CITY_OR_TOWNSHIP_GOVERNMENTS,\n    \"04\": ApplicantType.SPECIAL_DISTRICT_GOVERNMENTS,\n    \"05\": ApplicantType.INDEPENDENT_SCHOOL_DISTRICTS,\n    \"06\": ApplicantType.PUBLIC_AND_STATE_INSTITUTIONS_OF_HIGHER_EDUCATION,\n    \"07\": ApplicantType.FEDERALLY_RECOGNIZED_NATIVE_AMERICAN_TRIBAL_GOVERNMENTS,\n    \"08\": ApplicantType.PUBLIC_AND_INDIAN_HOUSING_AUTHORITIES,\n    \"11\": ApplicantType.OTHER_NATIVE_AMERICAN_TRIBAL_ORGANIZATIONS,\n    \"12\": ApplicantType.NONPROFITS_NON_HIGHER_EDUCATION_WITH_501C3,\n    \"13\": ApplicantType.NONPROFITS_NON_HIGHER_EDUCATION_WITHOUT_501C3,\n    \"20\": ApplicantType.PRIVATE_INSTITUTIONS_OF_HIGHER_EDUCATION,\n    \"21\": ApplicantType.INDIVIDUALS,\n    \"22\": ApplicantType.FOR_PROFIT_ORGANIZATIONS_OTHER_THAN_SMALL_BUSINESSES,\n    \"23\": ApplicantType.SMALL_BUSINESSES,\n    \"25\": ApplicantType.OTHER,\n    \"99\": ApplicantType.UNRESTRICTED,\n}\n\nFUNDING_CATEGORY_MAP = {\n    \"RA\": FundingCategory.RECOVERY_ACT,\n    \"AG\": FundingCategory.AGRICULTURE,\n    \"AR\": FundingCategory.ARTS,\n    \"BC\": FundingCategory.BUSINESS_AND_COMMERCE,\n    \"CD\": FundingCategory.COMMUNITY_DEVELOPMENT,\n    \"CP\": FundingCategory.CONSUMER_PROTECTION,\n    \"DPR\": FundingCategory.DISASTER_PREVENTION_AND_RELIEF,\n    \"ED\": FundingCategory.EDUCATION,\n    \"ELT\": FundingCategory.EMPLOYMENT_LABOR_AND_TRAINING,\n    \"EN\": FundingCategory.ENERGY,\n    \"ENV\": FundingCategory.ENVIRONMENT,\n    \"FN\": FundingCategory.FOOD_AND_NUTRITION,\n    \"HL\": FundingCategory.HEALTH,\n    \"HO\": FundingCategory.HOUSING,\n    \"HU\": FundingCategory.HUMANITIES,\n    \"IIJ\": FundingCategory.INFRASTRUCTURE_INVESTMENT_AND_JOBS_ACT,\n    \"IS\": FundingCategory.INFORMATION_AND_STATISTICS,\n    \"ISS\": FundingCategory.INCOME_SECURITY_AND_SOCIAL_SERVICES,\n    \"LJL\": FundingCategory.LAW_JUSTICE_AND_LEGAL_SERVICES,\n    \"NR\": FundingCategory.NATURAL_RESOURCES,\n    \"OZ\": FundingCategory.OPPORTUNITY_ZONE_BENEFITS,\n    \"RD\": FundingCategory.REGIONAL_DEVELOPMENT,\n    \"ST\": FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT,\n    \"T\": FundingCategory.TRANSPORTATION,\n    \"ACA\": FundingCategory.AFFORDABLE_CARE_ACT,\n    \"O\": FundingCategory.OTHER,\n}\n\nFUNDING_INSTRUMENT_MAP = {\n    \"CA\": FundingInstrument.COOPERATIVE_AGREEMENT,\n    \"G\": FundingInstrument.GRANT,\n    \"PC\": FundingInstrument.PROCUREMENT_CONTRACT,\n    \"O\": FundingInstrument.OTHER,\n}\n\n\ndef is_empty_str(value: str | None) -> bool:\n    # null/None, \"\" and any amount of just whitespace\n    # are treated as an empty string\n    return value is None or value.strip() == \"\"\n\n\ndef transform_opportunity(\n    source_opportunity: Topportunity, existing_opportunity: Opportunity | None\n) -> Opportunity:\n    log_extra = {\"opportunity_id\": source_opportunity.opportunity_id}\n\n    if existing_opportunity is None:\n        logger.info(\"Creating new opportunity record\", extra=log_extra)\n\n    # We always create a new opportunity record here and merge it in the calling function\n    # this way if there is any error doing the transformation, we don't modify the existing one.\n    target_opportunity = Opportunity(opportunity_id=source_opportunity.opportunity_id)\n\n    target_opportunity.opportunity_number = source_opportunity.oppnumber\n    target_opportunity.opportunity_title = source_opportunity.opptitle\n    target_opportunity.agency_code = source_opportunity.owningagency\n    target_opportunity.category = transform_opportunity_category(source_opportunity.oppcategory)\n    target_opportunity.category_explanation = source_opportunity.category_explanation\n    target_opportunity.revision_number = source_opportunity.revision_number\n    target_opportunity.modified_comments = source_opportunity.modified_comments\n    target_opportunity.publisher_user_id = source_opportunity.publisheruid\n    target_opportunity.publisher_profile_id = source_opportunity.publisher_profile_id\n\n    # The legacy system doesn't actually have this value as a boolean. There are several\n    # different letter codes. However, their API implementation also does this for their draft flag.\n    target_opportunity.is_draft = source_opportunity.is_draft != \"N\"\n    transform_update_create_timestamp(source_opportunity, target_opportunity, log_extra=log_extra)\n\n    return target_opportunity\n\n\ndef transform_opportunity_category(value: str | None) -> OpportunityCategory | None:\n    if is_empty_str(value):\n        return None\n\n    if value not in OPPORTUNITY_CATEGORY_MAP:\n        raise ValueError(\"Unrecognized opportunity category: %s\" % value)\n\n    return OPPORTUNITY_CATEGORY_MAP[value]\n\n\ndef transform_applicant_type(value: str | None) -> ApplicantType | None:\n    if is_empty_str(value):\n        return None\n\n    if value not in APPLICANT_TYPE_MAP:\n        raise ValueError(\"Unrecognized applicant type: %s\" % value)\n\n    return APPLICANT_TYPE_MAP[value]\n\n\ndef transform_funding_category(value: str | None) -> FundingCategory | None:\n    if is_empty_str(value):\n        return None\n\n    if value not in FUNDING_CATEGORY_MAP:\n        raise ValueError(\"Unrecognized funding category: %s\" % value)\n\n    return FUNDING_CATEGORY_MAP[value]\n\n\ndef transform_funding_instrument(value: str | None) -> FundingInstrument | None:\n    if is_empty_str(value):\n        return None\n\n    if value not in FUNDING_INSTRUMENT_MAP:\n        raise ValueError(\"Unrecognized funding instrument: %s\" % value)\n\n    return FUNDING_INSTRUMENT_MAP[value]\n\n\ndef transform_assistance_listing(\n    source_assistance_listing: TopportunityCfda,\n    existing_assistance_listing: OpportunityAssistanceListing | None,\n) -> OpportunityAssistanceListing:\n    log_extra = {\"opportunity_assistance_listing_id\": source_assistance_listing.opp_cfda_id}\n\n    if existing_assistance_listing is None:\n        logger.info(\"Creating new assistance listing record\", extra=log_extra)\n\n    # We always create a new assistance listing record here and merge it in the calling function\n    # this way if there is any error doing the transformation, we don't modify the existing one.\n    target_assistance_listing = OpportunityAssistanceListing(\n        opportunity_assistance_listing_id=source_assistance_listing.opp_cfda_id,\n        opportunity_id=source_assistance_listing.opportunity_id,\n    )\n\n    target_assistance_listing.assistance_listing_number = source_assistance_listing.cfdanumber\n    target_assistance_listing.program_title = source_assistance_listing.programtitle\n\n    transform_update_create_timestamp(\n        source_assistance_listing, target_assistance_listing, log_extra=log_extra\n    )\n\n    return target_assistance_listing\n\n\ndef transform_opportunity_summary(\n    source_summary: SourceSummary, incoming_summary: OpportunitySummary | None\n) -> OpportunitySummary:\n    log_extra = get_log_extra_summary(source_summary)\n\n    if incoming_summary is None:\n        logger.info(\"Creating new opportunity summary record\", extra=log_extra)\n        # These values are a part of a unique key for identifying across tables, we don't\n        # ever want to modify them once created\n        target_summary = OpportunitySummary(\n            opportunity_id=source_summary.opportunity_id,\n            is_forecast=source_summary.is_forecast,\n            # Revision number is only found in the historical table, use getattr\n            # to avoid type checking\n            revision_number=getattr(source_summary, \"revision_number\", None),\n        )\n    else:\n        # We create a new summary object and merge it outside this function\n        # that way if any modifications occur on the object and then it errors\n        # they aren't actually applied\n        target_summary = OpportunitySummary(\n            opportunity_summary_id=incoming_summary.opportunity_summary_id\n        )\n\n    # Fields in all 4 source tables\n    target_summary.version_number = source_summary.version_nbr\n    target_summary.is_cost_sharing = convert_yn_bool(source_summary.cost_sharing)\n    target_summary.post_date = source_summary.posting_date\n    target_summary.archive_date = source_summary.archive_date\n    target_summary.expected_number_of_awards = convert_numeric_str_to_int(\n        source_summary.number_of_awards\n    )\n    target_summary.estimated_total_program_funding = convert_numeric_str_to_int(\n        source_summary.est_funding\n    )\n    target_summary.award_floor = convert_numeric_str_to_int(source_summary.award_floor)\n    target_summary.award_ceiling = convert_numeric_str_to_int(source_summary.award_ceiling)\n    target_summary.additional_info_url = source_summary.fd_link_url\n    target_summary.additional_info_url_description = source_summary.fd_link_desc\n    target_summary.modification_comments = source_summary.modification_comments\n    target_summary.funding_category_description = source_summary.oth_cat_fa_desc\n    target_summary.applicant_eligibility_description = source_summary.applicant_elig_desc\n    target_summary.agency_name = source_summary.ac_name\n    target_summary.agency_email_address = source_summary.ac_email_addr\n    target_summary.agency_email_address_description = source_summary.ac_email_desc\n    target_summary.can_send_mail = convert_yn_bool(source_summary.sendmail)\n    target_summary.publisher_profile_id = source_summary.publisher_profile_id\n    target_summary.publisher_user_id = source_summary.publisheruid\n    target_summary.updated_by = source_summary.last_upd_id\n    target_summary.created_by = source_summary.creator_id\n\n    target_summary.summary_description = source_summary.description\n    target_summary.agency_code = source_summary.agency_code\n    target_summary.agency_phone_number = source_summary.agency_phone_number\n\n    # These fields are only on synopsis records, use getattr to avoid isinstance\n    target_summary.agency_contact_description = getattr(source_summary, \"agency_contact_desc\", None)\n    target_summary.close_date = getattr(source_summary, \"response_date\", None)\n    target_summary.close_date_description = getattr(source_summary, \"response_date_desc\", None)\n    target_summary.unarchive_date = getattr(source_summary, \"unarchive_date\", None)\n\n    # These fields are only on forecast records, use getattr to avoid isinstance\n    target_summary.forecasted_post_date = getattr(source_summary, \"est_synopsis_posting_date\", None)\n    target_summary.forecasted_close_date = getattr(source_summary, \"est_appl_response_date\", None)\n    target_summary.forecasted_close_date_description = getattr(\n        source_summary, \"est_appl_response_date_desc\", None\n    )\n    target_summary.forecasted_award_date = getattr(source_summary, \"est_award_date\", None)\n    target_summary.forecasted_project_start_date = getattr(\n        source_summary, \"est_project_start_date\", None\n    )\n    target_summary.fiscal_year = getattr(source_summary, \"fiscal_year\", None)\n\n    # Set whether it is deleted based on action_type, which only appears on the historical records\n    target_summary.is_deleted = convert_action_type_to_is_deleted(\n        getattr(source_summary, \"action_type\", None)\n    )\n\n    transform_update_create_timestamp(source_summary, target_summary, log_extra=log_extra)\n\n    return target_summary\n\n\ndef convert_opportunity_summary_applicant_type(\n    source_applicant_type: SourceApplicantType,\n    existing_applicant_type: LinkOpportunitySummaryApplicantType | None,\n    opportunity_summary: OpportunitySummary,\n) -> LinkOpportunitySummaryApplicantType:\n    log_extra = get_log_extra_applicant_type(source_applicant_type)\n\n    # NOTE: The columns we're working with here are mostly the primary keys\n    #       While we do support updates, that's really only going to affect\n    #       the last update user + timestamps. From checking the prod data\n    #       there are basically zero updates to this data (~5 occurred 10+ years ago)\n    if existing_applicant_type is None:\n        logger.info(\"Creating new applicant type record\", extra=log_extra)\n\n    applicant_type = transform_applicant_type(source_applicant_type.at_id)\n\n    target_applicant_type = LinkOpportunitySummaryApplicantType(\n        opportunity_summary_id=opportunity_summary.opportunity_summary_id,\n        legacy_applicant_type_id=source_applicant_type.legacy_applicant_type_id,\n        applicant_type=applicant_type,\n        updated_by=source_applicant_type.last_upd_id,\n        created_by=source_applicant_type.creator_id,\n    )\n    transform_update_create_timestamp(\n        source_applicant_type, target_applicant_type, log_extra=log_extra\n    )\n\n    return target_applicant_type\n\n\ndef convert_opportunity_summary_funding_instrument(\n    source_funding_instrument: SourceFundingInstrument,\n    existing_funding_instrument: LinkOpportunitySummaryFundingInstrument | None,\n    opportunity_summary: OpportunitySummary,\n) -> LinkOpportunitySummaryFundingInstrument:\n    log_extra = get_log_extra_funding_instrument(source_funding_instrument)\n\n    # NOTE: The columns we're working with here are mostly the primary keys\n    #       While we do support updates, that's really only going to affect\n    #       the last update user + timestamps. From checking the prod data\n    #       there are basically zero updates to this data (~5 occurred 10+ years ago)\n    if existing_funding_instrument is None:\n        logger.info(\"Creating new funding instrument record\", extra=log_extra)\n\n    funding_instrument = transform_funding_instrument(source_funding_instrument.fi_id)\n\n    target_funding_instrument = LinkOpportunitySummaryFundingInstrument(\n        opportunity_summary_id=opportunity_summary.opportunity_summary_id,\n        legacy_funding_instrument_id=source_funding_instrument.legacy_funding_instrument_id,\n        funding_instrument=funding_instrument,\n        updated_by=source_funding_instrument.last_upd_id,\n        created_by=source_funding_instrument.creator_id,\n    )\n\n    transform_update_create_timestamp(\n        source_funding_instrument, target_funding_instrument, log_extra=log_extra\n    )\n\n    return target_funding_instrument\n\n\ndef convert_opportunity_summary_funding_category(\n    source_funding_category: SourceFundingCategory,\n    existing_funding_category: LinkOpportunitySummaryFundingCategory | None,\n    opportunity_summary: OpportunitySummary,\n) -> LinkOpportunitySummaryFundingCategory:\n    log_extra = get_log_extra_funding_category(source_funding_category)\n\n    # NOTE: The columns we're working with here are mostly the primary keys\n    #       While we do support updates, that's really only going to affect\n    #       the last update user + timestamps. From checking the prod data\n    #       there are basically zero updates to this data (~5 occurred 10+ years ago)\n    if existing_funding_category is None:\n        logger.info(\"Creating new funding category record\", extra=log_extra)\n\n    funding_category = transform_funding_category(source_funding_category.fac_id)\n\n    target_funding_category = LinkOpportunitySummaryFundingCategory(\n        opportunity_summary_id=opportunity_summary.opportunity_summary_id,\n        legacy_funding_category_id=source_funding_category.legacy_funding_category_id,\n        funding_category=funding_category,\n        updated_by=source_funding_category.last_upd_id,\n        created_by=source_funding_category.creator_id,\n    )\n\n    transform_update_create_timestamp(\n        source_funding_category, target_funding_category, log_extra=log_extra\n    )\n\n    return target_funding_category\n\n\ndef convert_est_timestamp_to_utc(timestamp: datetime | None) -> datetime | None:\n    if timestamp is None:\n        return None\n\n    # The timestamps we get from the legacy system have no timezone info\n    # but we know the database uses US Eastern timezone by default\n    #\n    # First add the America/New_York timezone without any other modification\n    aware_timestamp = datetime_util.make_timezone_aware(timestamp, \"US/Eastern\")\n    # Then adjust the timezone to UTC this will handle any DST or other conversion complexities\n    return datetime_util.adjust_timezone(aware_timestamp, \"UTC\")\n\n\ndef get_create_update_timestamps(\n    source_created_date: datetime | None,\n    source_last_upd_date: datetime | None,\n    log_extra: dict | None = None,\n) -> Tuple[datetime, datetime]:\n    created_timestamp = convert_est_timestamp_to_utc(source_created_date)\n    updated_timestamp = convert_est_timestamp_to_utc(source_last_upd_date)\n\n    # This is incredibly rare, but possible - because our system requires\n    # we set something, we'll default to the current time and log a warning.\n    if created_timestamp is None:\n        if log_extra is None:\n            log_extra = {}\n\n        logger.warning(\n            \"Record does not have a created_date timestamp set, assuming value to be now.\",\n            extra=log_extra,\n        )\n        created_timestamp = datetime_util.utcnow()\n\n    if updated_timestamp is None:\n        # In the legacy system, they don't set whether something was updated\n        # until it receives an update. We always set the value, and on initial insert\n        # want it to be the same as the created_at.\n        updated_timestamp = created_timestamp\n\n    return created_timestamp, updated_timestamp\n\n\ndef transform_update_create_timestamp(\n    source: StagingBase, target: TimestampMixin, log_extra: dict | None = None\n) -> None:\n    # Convert the source timestamps to UTC\n    # Note: the type ignores are because created_date/last_upd_date are added\n    #       on the individual class definitions, not the base class - due to how\n    #       we need to maintain the column order of the legacy system.\n    #       Every legacy table does have these columns.\n    created_timestamp, updated_timestamp = get_create_update_timestamps(source.created_date, source.last_upd_date, log_extra)  # type: ignore[attr-defined]\n\n    target.created_at = created_timestamp\n    target.updated_at = updated_timestamp\n\n\nTRUTHY = {\"Y\", \"Yes\", \"y\", \"yes\"}\nFALSEY = {\"N\", \"No\", \"n\", \"no\"}\n\n\ndef convert_yn_bool(value: str | None) -> bool | None:\n    # Booleans in the Oracle database are stored as varchar/char\n    # columns with the values as Y/N (very rarely Yes/No)\n    if is_empty_str(value):\n        return None\n\n    if value in TRUTHY:\n        return True\n\n    if value in FALSEY:\n        return False\n\n    # Just in case the column isn't actually a boolean\n    raise ValueError(\"Unexpected Y/N bool value: %s\" % value)\n\n\ndef convert_true_false_bool(value: str | None) -> bool | None:\n    if is_empty_str(value):\n        return None\n\n    return value == \"TRUE\"\n\n\ndef convert_null_like_to_none(value: str | None) -> str | None:\n    if value is None:\n        return None\n\n    if value.lower() == \"null\":\n        return None\n\n    return value\n\n\ndef convert_action_type_to_is_deleted(value: str | None) -> bool:\n    # Action type can be U (update) or D (delete)\n    # however many older records seem to not have this set at all\n    # The legacy system looks like it treats anything that isn't D\n    # the same, so we'll go with that assumption as well.\n    if is_empty_str(value):\n        return False\n\n    if value == \"D\":  # D = Delete\n        return True\n\n    if value == \"U\":  # U = Update\n        return False\n\n    raise ValueError(\"Unexpected action type value: %s\" % value)\n\n\ndef convert_numeric_str_to_int(value: str | None) -> int | None:\n    if is_empty_str(value):\n        return None\n\n    try:\n        # We know the value is a string and not None from the above\n        # function call but mypy can't see that, so ignore it here.\n        return int(value)  # type: ignore\n    except ValueError:\n        # From what we've found in the legacy data, some of these numeric strings\n        # are written out as \"none\", \"not available\", \"n/a\" or similar. All of these\n        # we're fine with collectively treating as null-equivalent\n        return None\n\n\ndef get_log_extra_summary(source_summary: SourceSummary) -> dict:\n    return {\n        \"opportunity_id\": source_summary.opportunity_id,\n        \"is_forecast\": source_summary.is_forecast,\n        # This value only exists on non-historical records\n        # use getattr instead of an isinstance if/else for simplicity\n        \"revision_number\": getattr(source_summary, \"revision_number\", None),\n        \"table_name\": source_summary.__tablename__,\n    }\n\n\ndef get_log_extra_applicant_type(source_applicant_type: SourceApplicantType) -> dict:\n    return {\n        \"opportunity_id\": source_applicant_type.opportunity_id,\n        \"at_frcst_id\": getattr(source_applicant_type, \"at_frcst_id\", None),\n        \"at_syn_id\": getattr(source_applicant_type, \"at_syn_id\", None),\n        \"revision_number\": getattr(source_applicant_type, \"revision_number\", None),\n        \"table_name\": source_applicant_type.__tablename__,\n    }\n\n\ndef get_log_extra_funding_category(source_funding_category: SourceFundingCategory) -> dict:\n    return {\n        \"opportunity_id\": source_funding_category.opportunity_id,\n        \"fac_frcst_id\": getattr(source_funding_category, \"fac_frcst_id\", None),\n        \"fac_syn_id\": getattr(source_funding_category, \"fac_syn_id\", None),\n        \"revision_number\": getattr(source_funding_category, \"revision_number\", None),\n        \"table_name\": source_funding_category.__tablename__,\n    }\n\n\ndef get_log_extra_funding_instrument(source_funding_instrument: SourceFundingInstrument) -> dict:\n    return {\n        \"opportunity_id\": source_funding_instrument.opportunity_id,\n        \"fi_frcst_id\": getattr(source_funding_instrument, \"fi_frcst_id\", None),\n        \"fi_syn_id\": getattr(source_funding_instrument, \"fi_syn_id\", None),\n        \"revision_number\": getattr(source_funding_instrument, \"revision_number\", None),\n        \"table_name\": source_funding_instrument.__tablename__,\n    }\n\n\ndef get_log_extra_opportunity_attachment(source_attachment: TsynopsisAttachment) -> dict:\n    return {\n        \"opportunity_id\": source_attachment.opportunity_id,\n        \"syn_att_id\": source_attachment.syn_att_id,\n        \"att_revision_number\": source_attachment.att_revision_number,\n    }"}
{"path":"documentation/api/database/database-access-management.md","language":"markdown","type":"code","directory":"documentation/api/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/database/database-access-management.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/db\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.439Z"}
{"path":"documentation/api/database/database-local-usage.md","language":"markdown","type":"code","directory":"documentation/api/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/database/database-local-usage.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":""}
{"path":"documentation/api/database/database-management.md","language":"markdown","type":"code","directory":"documentation/api/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/database/database-management.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.439Z"}
{"path":"documentation/api/database/database-testing.md","language":"markdown","type":"code","directory":"documentation/api/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/database/database-testing.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":""}
{"path":"documentation/api/database/erds/README.md","language":"markdown","type":"code","directory":"documentation/api/database/erds","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/database/erds/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/alembic.ini\nLanguage: ini\nType: code\nDirectory: api/src/db/migrations\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/alembic.ini\nSize: 1.76 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/api/development.md","language":"markdown","type":"code","directory":"documentation/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/development.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"[alembic]\n# path to migration scripts\nscript_location = src/db/migrations\n\nfile_template = %%(year)d_%%(month).2d_%%(day).2d_%%(slug)s\n\n# This fixes Alembic's importing to use the directory we run from (generally the root api directory)\nprepend_sys_path = .\n\n# template used to generate migration files\n# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d_%%(minute).2d_%%(second).2d_%%(rev)s_%%(slug)s\n\n# timezone to use when rendering the date\n# within the migration file as well as the filename.\n# string value is passed to dateutil.tz.gettz()\n# leave blank for localtime\n# timezone =\n\n# max length of characters to apply to the\n# \"slug\" field\n# truncate_slug_length = 40\n\n# set to 'true' to run the environment during\n# the 'revision' command, regardless of autogenerate\n# revision_environment = false\n\n# set to 'true' to allow .pyc and .pyo files without\n# a source .py file to be detected as revisions in the\n# versions/ directory\n# sourceless = false\n\n# version location specification; this defaults\n# to migrations/versions.  When using multiple version\n# directories, initial revisions must be specified with --version-path\n# version_locations = %(here)s/bar %(here)s/bat migrations/versions\n\n# the output encoding used when revision files\n# are written from script.py.mako\n# output_encoding = utf-8\n\n[post_write_hooks]\n# post_write_hooks defines scripts or Python functions that are run\n# on newly generated revision scripts.  See the documentation for further\n# detail and examples\n\n# format using \"black\" - use the console_scripts runner, against the \"black\" entrypoint\nhooks=isort, black\n\nisort.type=console_scripts\nisort.entrypoint=isort\nisort.options= --atomic\n\nblack.type=console_scripts\nblack.entrypoint=black\n# black.options=-l 79"}
{"path":"documentation/api/error-handling.md","language":"markdown","type":"code","directory":"documentation/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/error-handling.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/env.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/env.py\nSize: 3.16 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/api/feature-flags.md","language":"markdown","type":"code","directory":"documentation/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/feature-flags.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"import alembic.context as context\nimport sqlalchemy\n\nimport src.adapters.db as db\nimport src.logging\nfrom src.constants.schema import Schemas\nfrom src.db.models import metadata\nfrom src.db.models.staging import metadata as staging_metadata\n\nfrom src.adapters.db.type_decorators.postgres_type_decorators import LookupColumn  # isort:skip\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\nlogger = logging.getLogger(\"migrations\")\n\n# Initialize logging\nwith src.logging.init(\"migrations\"):\n    # add your model's MetaData object here\n    # for 'autogenerate' support\n    # from myapp import mymodel\n    # target_metadata = mymodel.Base.metadata\n    target_metadata = [metadata, staging_metadata]\n\n    # other values from the config, defined by the needs of env.py,\n    # can be acquired:\n    # my_important_option = config.get_main_option(\"my_important_option\")\n    # ... etc.\n\n    def include_object(\n        object: sqlalchemy.schema.SchemaItem,\n        name: str | None,\n        type_: str,\n        reflected: bool,\n        compare_to: Any,\n    ) -> bool:\n        # We don't want alembic to try and drop its own table\n        if name == \"alembic_version\":\n            return False\n\n        if type_ == \"schema\" and getattr(object, \"schema\", None) is not None:\n            return False\n\n        if type_ == \"table\" and name is not None and name.startswith(\"foreign_\"):\n            # We create foreign tables to an Oracle database, if we see those locally\n            # just ignore them as they aren't something we want included in Alembic\n            return False\n        if type_ == \"table\" and getattr(object, \"schema\", None) == Schemas.LEGACY:\n            return False\n        else:\n            return True\n\n    def render_item(type_: str, obj: Any, autogen_context: Any) -> Any:\n        # Alembic tries to set the type of the column as LookupColumn\n        # despite it being derived from the Integer column type,\n        # so force it to be Integer during it's generation process\n        if type_ == \"type\" and isinstance(obj, LookupColumn):\n            return \"sa.Integer()\"\n\n        # False means to use the default processing\n        return False\n\n    def run_migrations_online() -> None:\n        \"\"\"Run migrations in 'online' mode.\n\n        In this scenario we need to create an Engine\n        and associate a connection with the context.\n\n        \"\"\"\n\n        db_client = db.PostgresDBClient()\n\n        with db_client.get_connection() as connection:\n            context.configure(\n                connection=connection,\n                target_metadata=target_metadata,\n                include_schemas=True,\n                include_object=include_object,\n                compare_type=True,\n                render_item=render_item,\n                version_table_schema=Schemas.API,\n            )\n            with context.begin_transaction():\n                context.run_migrations()\n\n    # No need to support running migrations in offline mode.\n    # When running locally we have the local containerized database.\n    # When running in the cloud we'll have the actual cloud database.\n    run_migrations_online()"}
{"path":"documentation/api/formatting-and-linting.md","language":"markdown","type":"code","directory":"documentation/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/formatting-and-linting.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/run.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/run.py\nSize: 3.50 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/api/lookup-values.md","language":"markdown","type":"code","directory":"documentation/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/lookup-values.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"import alembic.command as command\nimport alembic.script as script\nimport sqlalchemy\nfrom alembic.config import Config\nfrom alembic.runtime import migration\n\nimport src.logging\nfrom src.db.models.lookup.sync_lookup_values import sync_lookup_values\n\nlogger = logging.getLogger(__name__)\nalembic_cfg = Config(os.path.join(os.path.dirname(__file__), \"./alembic.ini\"))\n\n# Override the script_location to be absolute based on this file's directory.\nalembic_cfg.set_main_option(\"script_location\", os.path.dirname(__file__))\n\n\ndef up(revision: str = \"head\") -> None:\n    enable_query_logging()\n    command.upgrade(alembic_cfg, revision)\n\n    # We want logging for the lookups, but alembic already sets\n    # it up in env.py, so set it up again separately for the syncing\n    with src.logging.init(\"sync_lookup_values\"):\n        sync_lookup_values()\n\n\ndef down(revision: str = \"-1\") -> None:\n    enable_query_logging()\n    command.downgrade(alembic_cfg, revision)\n\n\ndef downall(revision: str = \"base\") -> None:\n    enable_query_logging()\n    command.downgrade(alembic_cfg, revision)\n\n\ndef enable_query_logging() -> None:\n    \"\"\"Log each migration query as it happens along with timing.\n\n    Based on the example at https://docs.sqlalchemy.org/en/20/faq/performance.html#query-profiling\n    \"\"\"\n\n    @sqlalchemy.event.listens_for(sqlalchemy.engine.Engine, \"before_cursor_execute\", retval=True)\n    def before_execute(\n        conn: sqlalchemy.Connection,\n        _cursor: Any,\n        statement: str,\n        _parameters: Any,\n        _context: Any,\n        _executemany: bool,\n    ) -> tuple[str, Any]:\n        conn.info.setdefault(\"query_start_time\", []).append(time.monotonic())\n        logger.info(\"before execute\", extra={\"migrate.sql\": statement.strip()})\n        return statement, _parameters\n\n    @sqlalchemy.event.listens_for(sqlalchemy.engine.Engine, \"after_cursor_execute\")\n    def after_execute(\n        conn: sqlalchemy.Connection,\n        _cursor: Any,\n        statement: str,\n        _parameters: Any,\n        _context: Any,\n        _executemany: bool,\n    ) -> None:\n        total = int(1000 * (time.monotonic() - conn.info[\"query_start_time\"].pop(-1)))\n        logger.info(\n            \"after execute\", extra={\"migrate.sql\": statement.strip(), \"migrate.time_ms\": total}\n        )\n\n\ndef have_all_migrations_run(db_engine: sqlalchemy.engine.Engine) -> None:\n    directory = script.ScriptDirectory.from_config(alembic_cfg)\n    with db_engine.begin() as connection:\n        context = migration.MigrationContext.configure(connection)\n        current_heads = set(context.get_current_heads())\n        expected_heads = set(directory.get_heads())\n\n        # Only throw _if_ it's been migrated and doesn't match expectations.\n        # Otherwise, don't bother with this - most likely running in a testing environment.\n        if current_heads != expected_heads:\n            raise Exception(\n                (\n                    \"The database schema is not in sync with the migrations.\"\n                    \"Please verify that the migrations have been\"\n                    f\"run up to {expected_heads}; currently at {current_heads}\"\n                )\n            )\n\n        logger.info(\n            f\"The current migration head is up to date, {current_heads} and Alembic is expecting {expected_heads}\"\n        )"}
{"path":"documentation/api/monitoring-and-observability/logging-configuration.md","language":"markdown","type":"code","directory":"documentation/api/monitoring-and-observability","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/monitoring-and-observability/logging-configuration.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/script.py.mako\nLanguage: mako\nType: code\nDirectory: api/src/db/migrations\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/script.py.mako\nSize: 0.48 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/api/monitoring-and-observability/logging-conventions.md","language":"markdown","type":"code","directory":"documentation/api/monitoring-and-observability","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/monitoring-and-observability/logging-conventions.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: ${up_revision}\nRevises: ${down_revision | comma,n}\nCreate Date: ${create_date}\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n${imports if imports else \"\"}\n\n# revision identifiers, used by Alembic.\nrevision = ${repr(up_revision)}\ndown_revision = ${repr(down_revision)}\nbranch_labels = ${repr(branch_labels)}\ndepends_on = ${repr(depends_on)}\n\n\ndef upgrade():\n    ${upgrades if upgrades else \"pass\"}\n\n\ndef downgrade():\n    ${downgrades if downgrades else \"pass\"}"}
{"path":"documentation/api/package-dependency-management.md","language":"markdown","type":"code","directory":"documentation/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/package-dependency-management.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/setup_local_postgres_db.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/setup_local_postgres_db.py\nSize: 0.76 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/api/technical-overview.md","language":"markdown","type":"code","directory":"documentation/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/technical-overview.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"import sqlalchemy\n\nimport src.adapters.db as db\nimport src.logging\nfrom src.adapters.db import PostgresDBClient\nfrom src.constants.schema import Schemas\nfrom src.util.local import error_if_not_local\n\nlogger = logging.getLogger(__name__)\n\n\ndef setup_local_postgres_db() -> None:\n    with src.logging.init(__package__):\n        error_if_not_local()\n\n        db_client = PostgresDBClient()\n\n        with db_client.get_connection() as conn, conn.begin():\n            for schema in Schemas:\n                _create_schema(conn, schema)\n\n\ndef _create_schema(conn: db.Connection, schema_name: str) -> None:\n    logger.info(\"Creating schema %s if it does not already exist\", schema_name)\n    conn.execute(sqlalchemy.schema.CreateSchema(schema_name, if_not_exists=True))"}
{"path":"documentation/api/writing-tests.md","language":"markdown","type":"code","directory":"documentation/api","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/writing-tests.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2023_08_01_base_migration.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2023_08_01_base_migration.py\nSize: 0.28 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/architecture/README.md","language":"markdown","type":"code","directory":"documentation/architecture","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/architecture/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: 9fe657340f70\nRevises:\nCreate Date: 2023-08-01 12:00:00.0000000\n\n\"\"\"\n\n# revision identifiers, used by Alembic.\nrevision = \"9fe657340f70\"\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    pass\n\n\ndef downgrade():\n    pass"}
{"path":"documentation/frontend/README.md","language":"markdown","type":"code","directory":"documentation/frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/frontend/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2023_08_10_default_table_privileges.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2023_08_10_default_table_privileges.py\nSize: 1.19 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/frontend/analytics.md","language":"markdown","type":"code","directory":"documentation/frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/frontend/analytics.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: 3ed861176e3d\nRevises: 9fe657340f70\nCreate Date: 2023-08-10 15:52:10.626153\n\n\"\"\"\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"3ed861176e3d\"\ndown_revision = \"9fe657340f70\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # Change default privileges for future objects created by the `migrator`\n    # user to automatically be accessible by the `app` user.\n    # This needs to be done as a migration since default privileges for a role\n    # can only be altered by members of the role and not the superuser.\n    # Other permission changes should have already been taken care as part\n    # of the infrastructure setup (creating and configuring the the schema\n    # and roles)\n    op.execute(\"ALTER DEFAULT PRIVILEGES GRANT ALL ON TABLES TO app\")\n    op.execute(\"ALTER DEFAULT PRIVILEGES GRANT ALL ON SEQUENCES TO app\")\n    op.execute(\"ALTER DEFAULT PRIVILEGES GRANT ALL ON ROUTINES TO app\")\n\n\ndef downgrade():\n    op.execute(\"ALTER DEFAULT PRIVILEGES REVOKE ALL ON ROUTINES FROM app\")\n    op.execute(\"ALTER DEFAULT PRIVILEGES REVOKE ALL ON SEQUENCES FROM app\")\n    op.execute(\"ALTER DEFAULT PRIVILEGES REVOKE ALL ON TABLES FROM app\")"}
{"path":"documentation/frontend/development.md","language":"markdown","type":"code","directory":"documentation/frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/frontend/development.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2023_10_18_basic_opportunity_table.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2023_10_18_basic_opportunity_table.py\nSize: 1.42 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/frontend/environments.md","language":"markdown","type":"code","directory":"documentation/frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/frontend/environments.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: fdd9312633d8\nRevises: 3ed861176e3d\nCreate Date: 2023-10-18 16:37:18.252136\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"fdd9312633d8\"\ndown_revision = \"3ed861176e3d\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"topportunity\",\n        sa.Column(\"opportunity_id\", sa.Integer(), nullable=False),\n        sa.Column(\"oppnumber\", sa.Text(), nullable=True),\n        sa.Column(\"opptitle\", sa.Text(), nullable=True),\n        sa.Column(\"owningagency\", sa.Text(), nullable=True),\n        sa.Column(\"oppcategory\", sa.Text(), nullable=True),\n        sa.Column(\"is_draft\", sa.Boolean(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=op.f(\"topportunity_pkey\")),\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"topportunity\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/frontend/featureFlags.md","language":"markdown","type":"code","directory":"documentation/frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/frontend/featureFlags.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2023_11_27_rename_opportunity_table_prior_to_real_.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2023_11_27_rename_opportunity_table_prior_to_real_.py\nSize: 2.57 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/frontend/internationalization.md","language":"markdown","type":"code","directory":"documentation/frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/frontend/internationalization.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: dec1422eee27\nRevises: fdd9312633d8\nCreate Date: 2023-11-27 14:43:04.227044\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"dec1422eee27\"\ndown_revision = \"fdd9312633d8\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"opportunity\",\n        sa.Column(\"opportunity_id\", sa.Integer(), nullable=False),\n        sa.Column(\"opportunity_number\", sa.Text(), nullable=True),\n        sa.Column(\"opportunity_title\", sa.Text(), nullable=True),\n        sa.Column(\"agency\", sa.Text(), nullable=True),\n        sa.Column(\"category\", sa.Text(), nullable=True),\n        sa.Column(\"is_draft\", sa.Boolean(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=op.f(\"opportunity_pkey\")),\n        schema=\"api\",\n    )\n    op.drop_table(\"topportunity\", schema=\"api\")\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"topportunity\",\n        sa.Column(\"opportunity_id\", sa.INTEGER(), autoincrement=True, nullable=False),\n        sa.Column(\"oppnumber\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"opptitle\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"owningagency\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"oppcategory\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"is_draft\", sa.BOOLEAN(), autoincrement=False, nullable=False),\n        sa.Column(\n            \"created_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=\"topportunity_pkey\"),\n        schema=\"api\",\n    )\n    op.drop_table(\"opportunity\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/goals.md","language":"markdown","type":"code","directory":"documentation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/goals.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2023_12_11_add_rest_of_opportunity_table.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2023_12_11_add_rest_of_opportunity_table.py\nSize: 2.17 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/infra/README.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: 83964e6715a1\nRevises: dec1422eee27\nCreate Date: 2023-12-11 13:13:33.446941\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"83964e6715a1\"\ndown_revision = \"dec1422eee27\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"opportunity\", sa.Column(\"category_explanation\", sa.Text(), nullable=True), schema=\"api\"\n    )\n    op.add_column(\n        \"opportunity\", sa.Column(\"revision_number\", sa.Integer(), nullable=True), schema=\"api\"\n    )\n    op.add_column(\n        \"opportunity\", sa.Column(\"modified_comments\", sa.Text(), nullable=True), schema=\"api\"\n    )\n    op.add_column(\n        \"opportunity\", sa.Column(\"publisher_user_id\", sa.Integer(), nullable=True), schema=\"api\"\n    )\n    op.add_column(\n        \"opportunity\", sa.Column(\"publisher_profile_id\", sa.Integer(), nullable=True), schema=\"api\"\n    )\n    op.create_index(\n        op.f(\"opportunity_category_idx\"), \"opportunity\", [\"category\"], unique=False, schema=\"api\"\n    )\n    op.create_index(\n        op.f(\"opportunity_is_draft_idx\"), \"opportunity\", [\"is_draft\"], unique=False, schema=\"api\"\n    )\n    op.create_index(\n        op.f(\"opportunity_opportunity_title_idx\"),\n        \"opportunity\",\n        [\"opportunity_title\"],\n        unique=False,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"opportunity_opportunity_title_idx\"), table_name=\"opportunity\", schema=\"api\")\n    op.drop_index(op.f(\"opportunity_is_draft_idx\"), table_name=\"opportunity\", schema=\"api\")\n    op.drop_index(op.f(\"opportunity_category_idx\"), table_name=\"opportunity\", schema=\"api\")\n    op.drop_column(\"opportunity\", \"publisher_profile_id\", schema=\"api\")\n    op.drop_column(\"opportunity\", \"publisher_user_id\", schema=\"api\")\n    op.drop_column(\"opportunity\", \"modified_comments\", schema=\"api\")\n    op.drop_column(\"opportunity\", \"revision_number\", schema=\"api\")\n    op.drop_column(\"opportunity\", \"category_explanation\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/infra/destroy-infrastructure.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/destroy-infrastructure.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2024_01_29_add_topportunity_table_for_transfer.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_01_29_add_topportunity_table_for_transfer.py\nSize: 3.06 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/infra/environment-variables-and-secrets.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/environment-variables-and-secrets.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: b1eb1bd4a647\nRevises: 83964e6715a1\nCreate Date: 2024-01-29 14:07:42.665723\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"b1eb1bd4a647\"\ndown_revision = \"83964e6715a1\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"transfer_topportunity\",\n        sa.Column(\"opportunity_id\", sa.Integer(), nullable=False),\n        sa.Column(\"oppnumber\", sa.VARCHAR(length=160), nullable=True),\n        sa.Column(\"opptitle\", sa.VARCHAR(length=1020), nullable=True),\n        sa.Column(\"owningagency\", sa.VARCHAR(length=1020), nullable=True),\n        sa.Column(\"oppcategory\", sa.VARCHAR(length=4), nullable=True),\n        sa.Column(\"category_explanation\", sa.VARCHAR(length=1020), nullable=True),\n        sa.Column(\"is_draft\", sa.VARCHAR(length=4), nullable=False),\n        sa.Column(\"revision_number\", sa.Integer(), nullable=True),\n        sa.Column(\"modified_comments\", sa.VARCHAR(length=4000), nullable=True),\n        sa.Column(\"publisheruid\", sa.VARCHAR(length=1020), nullable=True),\n        sa.Column(\"publisher_profile_id\", sa.Integer(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.VARCHAR(length=200), nullable=True),\n        sa.Column(\"last_upd_date\", sa.Date(), nullable=True),\n        sa.Column(\"creator_id\", sa.VARCHAR(length=200), nullable=True),\n        sa.Column(\"created_date\", sa.Date(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=op.f(\"transfer_topportunity_pkey\")),\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"transfer_topportunity_is_draft_idx\"),\n        \"transfer_topportunity\",\n        [\"is_draft\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"transfer_topportunity_oppcategory_idx\"),\n        \"transfer_topportunity\",\n        [\"oppcategory\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"transfer_topportunity_opptitle_idx\"),\n        \"transfer_topportunity\",\n        [\"opptitle\"],\n        unique=False,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\n        op.f(\"transfer_topportunity_opptitle_idx\"), table_name=\"transfer_topportunity\", schema=\"api\"\n    )\n    op.drop_index(\n        op.f(\"transfer_topportunity_oppcategory_idx\"),\n        table_name=\"transfer_topportunity\",\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"transfer_topportunity_is_draft_idx\"), table_name=\"transfer_topportunity\", schema=\"api\"\n    )\n    op.drop_table(\"transfer_topportunity\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/infra/intro-to-terraform-workspaces.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/intro-to-terraform-workspaces.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2024_02_02_add_opportunity_category_table.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_02_02_add_opportunity_category_table.py\nSize: 2.67 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/infra/intro-to-terraform.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/intro-to-terraform.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: 479221fb8ba8\nRevises: b1eb1bd4a647\nCreate Date: 2024-02-02 11:36:33.241412\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"479221fb8ba8\"\ndown_revision = \"b1eb1bd4a647\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"lk_opportunity_category\",\n        sa.Column(\"opportunity_category_id\", sa.Integer(), nullable=False),\n        sa.Column(\"description\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_category_id\", name=op.f(\"lk_opportunity_category_pkey\")\n        ),\n        schema=\"api\",\n    )\n    op.add_column(\n        \"opportunity\",\n        sa.Column(\"opportunity_category_id\", sa.Integer(), nullable=True),\n        schema=\"api\",\n    )\n    op.drop_index(\"opportunity_category_idx\", table_name=\"opportunity\", schema=\"api\")\n    op.create_index(\n        op.f(\"opportunity_opportunity_category_id_idx\"),\n        \"opportunity\",\n        [\"opportunity_category_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_foreign_key(\n        op.f(\"opportunity_opportunity_category_id_lk_opportunity_category_fkey\"),\n        \"opportunity\",\n        \"lk_opportunity_category\",\n        [\"opportunity_category_id\"],\n        [\"opportunity_category_id\"],\n        source_schema=\"api\",\n        referent_schema=\"api\",\n    )\n    op.drop_column(\"opportunity\", \"category\", schema=\"api\")\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"opportunity\",\n        sa.Column(\"category\", sa.TEXT(), autoincrement=False, nullable=True),\n        schema=\"api\",\n    )\n    op.drop_constraint(\n        op.f(\"opportunity_opportunity_category_id_lk_opportunity_category_fkey\"),\n        \"opportunity\",\n        type_=\"foreignkey\",\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"opportunity_opportunity_category_id_idx\"), table_name=\"opportunity\", schema=\"api\"\n    )\n    op.create_index(\n        \"opportunity_category_idx\", \"opportunity\", [\"category\"], unique=False, schema=\"api\"\n    )\n    op.drop_column(\"opportunity\", \"opportunity_category_id\", schema=\"api\")\n    op.drop_table(\"lk_opportunity_category\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/infra/making-infra-changes.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/making-infra-changes.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2024_02_07_add_expanded_opportunity_models.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_02_07_add_expanded_opportunity_models.py\nSize: 12.01 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/infra/module-architecture.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/module-architecture.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: 8d2a88569e7e\nRevises: 479221fb8ba8\nCreate Date: 2024-02-07 12:16:16.564629\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"8d2a88569e7e\"\ndown_revision = \"479221fb8ba8\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"lk_applicant_type\",\n        sa.Column(\"applicant_type_id\", sa.Integer(), nullable=False),\n        sa.Column(\"description\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"applicant_type_id\", name=op.f(\"lk_applicant_type_pkey\")),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"lk_funding_category\",\n        sa.Column(\"funding_category_id\", sa.Integer(), nullable=False),\n        sa.Column(\"description\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"funding_category_id\", name=op.f(\"lk_funding_category_pkey\")),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"lk_funding_instrument\",\n        sa.Column(\"funding_instrument_id\", sa.Integer(), nullable=False),\n        sa.Column(\"description\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"funding_instrument_id\", name=op.f(\"lk_funding_instrument_pkey\")),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"lk_opportunity_status\",\n        sa.Column(\"opportunity_status_id\", sa.Integer(), nullable=False),\n        sa.Column(\"description\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_status_id\", name=op.f(\"lk_opportunity_status_pkey\")),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"link_applicant_type_opportunity\",\n        sa.Column(\"opportunity_id\", sa.Integer(), nullable=False),\n        sa.Column(\"applicant_type_id\", sa.Integer(), nullable=False),\n        sa.Column(\"updated_by\", sa.Text(), nullable=True),\n        sa.Column(\"created_by\", sa.Text(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"applicant_type_id\"],\n            [\"api.lk_applicant_type.applicant_type_id\"],\n            name=op.f(\"link_applicant_type_opportunity_applicant_type_id_lk_applicant_type_fkey\"),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=op.f(\"link_applicant_type_opportunity_opportunity_id_opportunity_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_id\", \"applicant_type_id\", name=op.f(\"link_applicant_type_opportunity_pkey\")\n        ),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"link_funding_category_opportunity\",\n        sa.Column(\"opportunity_id\", sa.Integer(), nullable=False),\n        sa.Column(\"funding_category_id\", sa.Integer(), nullable=False),\n        sa.Column(\"updated_by\", sa.Text(), nullable=True),\n        sa.Column(\"created_by\", sa.Text(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"funding_category_id\"],\n            [\"api.lk_funding_category.funding_category_id\"],\n            name=op.f(\n                \"link_funding_category_opportunity_funding_category_id_lk_funding_category_fkey\"\n            ),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=op.f(\"link_funding_category_opportunity_opportunity_id_opportunity_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_id\",\n            \"funding_category_id\",\n            name=op.f(\"link_funding_category_opportunity_pkey\"),\n        ),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"link_funding_instrument_opportunity\",\n        sa.Column(\"opportunity_id\", sa.Integer(), nullable=False),\n        sa.Column(\"funding_instrument_id\", sa.Integer(), nullable=False),\n        sa.Column(\"updated_by\", sa.Text(), nullable=True),\n        sa.Column(\"created_by\", sa.Text(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"funding_instrument_id\"],\n            [\"api.lk_funding_instrument.funding_instrument_id\"],\n            name=op.f(\n                \"link_funding_instrument_opportunity_funding_instrument_id_lk_funding_instrument_fkey\"\n            ),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=op.f(\"link_funding_instrument_opportunity_opportunity_id_opportunity_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_id\",\n            \"funding_instrument_id\",\n            name=op.f(\"link_funding_instrument_opportunity_pkey\"),\n        ),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"opportunity_assistance_listing\",\n        sa.Column(\"opportunity_assistance_listing_id\", sa.Integer(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.Integer(), nullable=False),\n        sa.Column(\"program_title\", sa.Text(), nullable=True),\n        sa.Column(\"assistance_listing_number\", sa.Text(), nullable=True),\n        sa.Column(\"updated_by\", sa.Text(), nullable=True),\n        sa.Column(\"created_by\", sa.Text(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=op.f(\"opportunity_assistance_listing_opportunity_id_opportunity_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_assistance_listing_id\", name=op.f(\"opportunity_assistance_listing_pkey\")\n        ),\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"opportunity_assistance_listing_opportunity_id_idx\"),\n        \"opportunity_assistance_listing\",\n        [\"opportunity_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_table(\n        \"opportunity_summary\",\n        sa.Column(\"opportunity_id\", sa.Integer(), nullable=False),\n        sa.Column(\"opportunity_status_id\", sa.Integer(), nullable=True),\n        sa.Column(\"summary_description\", sa.Text(), nullable=True),\n        sa.Column(\"is_cost_sharing\", sa.Boolean(), nullable=True),\n        sa.Column(\"close_date\", sa.Date(), nullable=True),\n        sa.Column(\"close_date_description\", sa.Text(), nullable=True),\n        sa.Column(\"post_date\", sa.Date(), nullable=True),\n        sa.Column(\"archive_date\", sa.Date(), nullable=True),\n        sa.Column(\"unarchive_date\", sa.Date(), nullable=True),\n        sa.Column(\"expected_number_of_awards\", sa.Integer(), nullable=True),\n        sa.Column(\"estimated_total_program_funding\", sa.Integer(), nullable=True),\n        sa.Column(\"award_floor\", sa.Integer(), nullable=True),\n        sa.Column(\"award_ceiling\", sa.Integer(), nullable=True),\n        sa.Column(\"additional_info_url\", sa.Text(), nullable=True),\n        sa.Column(\"additional_info_url_description\", sa.Text(), nullable=True),\n        sa.Column(\"version_number\", sa.Integer(), nullable=True),\n        sa.Column(\"modification_comments\", sa.Text(), nullable=True),\n        sa.Column(\"funding_category_description\", sa.Text(), nullable=True),\n        sa.Column(\"applicant_eligibility_description\", sa.Text(), nullable=True),\n        sa.Column(\"agency_code\", sa.Text(), nullable=True),\n        sa.Column(\"agency_name\", sa.Text(), nullable=True),\n        sa.Column(\"agency_phone_number\", sa.Text(), nullable=True),\n        sa.Column(\"agency_contact_description\", sa.Text(), nullable=True),\n        sa.Column(\"agency_email_address\", sa.Text(), nullable=True),\n        sa.Column(\"agency_email_address_description\", sa.Text(), nullable=True),\n        sa.Column(\"can_send_mail\", sa.Boolean(), nullable=True),\n        sa.Column(\"publisher_profile_id\", sa.Integer(), nullable=True),\n        sa.Column(\"publisher_user_id\", sa.Text(), nullable=True),\n        sa.Column(\"updated_by\", sa.Text(), nullable=True),\n        sa.Column(\"created_by\", sa.Text(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=op.f(\"opportunity_summary_opportunity_id_opportunity_fkey\"),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_status_id\"],\n            [\"api.lk_opportunity_status.opportunity_status_id\"],\n            name=op.f(\"opportunity_summary_opportunity_status_id_lk_opportunity_status_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=op.f(\"opportunity_summary_pkey\")),\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"opportunity_summary\", schema=\"api\")\n    op.drop_index(\n        op.f(\"opportunity_assistance_listing_opportunity_id_idx\"),\n        table_name=\"opportunity_assistance_listing\",\n        schema=\"api\",\n    )\n    op.drop_table(\"opportunity_assistance_listing\", schema=\"api\")\n    op.drop_table(\"link_funding_instrument_opportunity\", schema=\"api\")\n    op.drop_table(\"link_funding_category_opportunity\", schema=\"api\")\n    op.drop_table(\"link_applicant_type_opportunity\", schema=\"api\")\n    op.drop_table(\"lk_opportunity_status\", schema=\"api\")\n    op.drop_table(\"lk_funding_instrument\", schema=\"api\")\n    op.drop_table(\"lk_funding_category\", schema=\"api\")\n    op.drop_table(\"lk_applicant_type\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/infra/module-dependencies.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/module-dependencies.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2024_02_12_create_dms_exceptions_table.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_02_12_create_dms_exceptions_table.py\nSize: 0.91 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/infra/set-up-app-build-repository.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-app-build-repository.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: 86edfcb92a66\nRevises: 8d2a88569e7e\nCreate Date: 2024-02-12 15:58:54.880550\n\n\"\"\"\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"86edfcb92a66\"\ndown_revision = \"8d2a88569e7e\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.execute(\n        \"\"\"\n    CREATE TABLE IF NOT EXISTS api.awsdms_apply_exceptions (\n        ERROR_TIME timestamp  NOT NULL,\n        TASK_NAME varchar(128)  NOT NULL,\n        TABLE_OWNER varchar(128)  NOT NULL,\n        TABLE_NAME varchar(128)  NOT NULL,\n        STATEMENT text  NOT NULL,\n        ERROR text  NOT NULL\n    );\n    \"\"\"\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.execute(\"DROP TABLE IF EXISTS api.awsdms_apply_exceptions\")\n    # ### end Alembic commands ###"}
{"path":"documentation/infra/set-up-app-env.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-app-env.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2024_02_21_remove_dms_exceptions_table.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_02_21_remove_dms_exceptions_table.py\nSize: 0.94 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/infra/set-up-aws-account.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-aws-account.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: 5d58c38f2cac\nRevises: 86edfcb92a66\nCreate Date: 2024-02-21 13:44:50.932948\n\n\"\"\"\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"5d58c38f2cac\"\ndown_revision = \"86edfcb92a66\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.execute(\"DROP TABLE IF EXISTS api.awsdms_apply_exceptions\")\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.execute(\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS api.awsdms_apply_exceptions (\n            ERROR_TIME timestamp  NOT NULL,\n            TASK_NAME varchar(128)  NOT NULL,\n            TABLE_OWNER varchar(128)  NOT NULL,\n            TABLE_NAME varchar(128)  NOT NULL,\n            STATEMENT text  NOT NULL,\n            ERROR text  NOT NULL\n        );\n    \"\"\"\n    )\n    # ### end Alembic commands ###"}
{"path":"documentation/infra/set-up-database.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-database.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2024_03_07_drop_tables_to_remake.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_03_07_drop_tables_to_remake.py\nSize: 13.21 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/infra/set-up-infrastructure-tools.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-infrastructure-tools.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: ac80e949bcf8\nRevises: 5d58c38f2cac\nCreate Date: 2024-03-07 10:20:15.639825\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"ac80e949bcf8\"\ndown_revision = \"5d58c38f2cac\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"link_funding_instrument_opportunity\", schema=\"api\")\n    op.drop_index(\"opportunity_is_draft_idx\", table_name=\"opportunity\", schema=\"api\")\n    op.drop_index(\"opportunity_opportunity_category_id_idx\", table_name=\"opportunity\", schema=\"api\")\n    op.drop_index(\"opportunity_opportunity_title_idx\", table_name=\"opportunity\", schema=\"api\")\n\n    op.drop_table(\"link_funding_category_opportunity\", schema=\"api\")\n    op.drop_table(\"link_applicant_type_opportunity\", schema=\"api\")\n    op.drop_index(\n        \"opportunity_assistance_listing_opportunity_id_idx\",\n        table_name=\"opportunity_assistance_listing\",\n        schema=\"api\",\n    )\n    op.drop_table(\"opportunity_assistance_listing\", schema=\"api\")\n    op.drop_table(\"opportunity_summary\", schema=\"api\")\n    op.drop_table(\"opportunity\", schema=\"api\")\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"opportunity\",\n        sa.Column(\n            \"opportunity_id\",\n            sa.INTEGER(),\n            server_default=sa.text(\"nextval('opportunity_opportunity_id_seq'::regclass)\"),\n            autoincrement=True,\n            nullable=False,\n        ),\n        sa.Column(\"opportunity_number\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"opportunity_title\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"agency\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"is_draft\", sa.BOOLEAN(), autoincrement=False, nullable=False),\n        sa.Column(\n            \"created_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\"category_explanation\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"revision_number\", sa.INTEGER(), autoincrement=False, nullable=True),\n        sa.Column(\"modified_comments\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"publisher_user_id\", sa.INTEGER(), autoincrement=False, nullable=True),\n        sa.Column(\"publisher_profile_id\", sa.INTEGER(), autoincrement=False, nullable=True),\n        sa.Column(\"opportunity_category_id\", sa.INTEGER(), autoincrement=False, nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_category_id\"],\n            [\"api.lk_opportunity_category.opportunity_category_id\"],\n            name=\"opportunity_opportunity_category_id_lk_opportunity_cate_c6e9\",\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=\"opportunity_pkey\"),\n        postgresql_ignore_search_path=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        \"opportunity_opportunity_title_idx\",\n        \"opportunity\",\n        [\"opportunity_title\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        \"opportunity_opportunity_category_id_idx\",\n        \"opportunity\",\n        [\"opportunity_category_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        \"opportunity_is_draft_idx\", \"opportunity\", [\"is_draft\"], unique=False, schema=\"api\"\n    )\n    op.create_table(\n        \"opportunity_summary\",\n        sa.Column(\"opportunity_id\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"opportunity_status_id\", sa.INTEGER(), autoincrement=False, nullable=True),\n        sa.Column(\"summary_description\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"is_cost_sharing\", sa.BOOLEAN(), autoincrement=False, nullable=True),\n        sa.Column(\"close_date\", sa.DATE(), autoincrement=False, nullable=True),\n        sa.Column(\"close_date_description\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"post_date\", sa.DATE(), autoincrement=False, nullable=True),\n        sa.Column(\"archive_date\", sa.DATE(), autoincrement=False, nullable=True),\n        sa.Column(\"unarchive_date\", sa.DATE(), autoincrement=False, nullable=True),\n        sa.Column(\"expected_number_of_awards\", sa.INTEGER(), autoincrement=False, nullable=True),\n        sa.Column(\n            \"estimated_total_program_funding\", sa.INTEGER(), autoincrement=False, nullable=True\n        ),\n        sa.Column(\"award_floor\", sa.INTEGER(), autoincrement=False, nullable=True),\n        sa.Column(\"award_ceiling\", sa.INTEGER(), autoincrement=False, nullable=True),\n        sa.Column(\"additional_info_url\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"additional_info_url_description\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"version_number\", sa.INTEGER(), autoincrement=False, nullable=True),\n        sa.Column(\"modification_comments\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"funding_category_description\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\n            \"applicant_eligibility_description\", sa.TEXT(), autoincrement=False, nullable=True\n        ),\n        sa.Column(\"agency_code\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"agency_name\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"agency_phone_number\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"agency_contact_description\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"agency_email_address\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\n            \"agency_email_address_description\", sa.TEXT(), autoincrement=False, nullable=True\n        ),\n        sa.Column(\"can_send_mail\", sa.BOOLEAN(), autoincrement=False, nullable=True),\n        sa.Column(\"publisher_profile_id\", sa.INTEGER(), autoincrement=False, nullable=True),\n        sa.Column(\"publisher_user_id\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"updated_by\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"created_by\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\n            \"created_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=\"opportunity_summary_opportunity_id_opportunity_fkey\",\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_status_id\"],\n            [\"api.lk_opportunity_status.opportunity_status_id\"],\n            name=\"opportunity_summary_opportunity_status_id_lk_opportunit_ea00\",\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=\"opportunity_summary_pkey\"),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"opportunity_assistance_listing\",\n        sa.Column(\n            \"opportunity_assistance_listing_id\", sa.INTEGER(), autoincrement=True, nullable=False\n        ),\n        sa.Column(\"opportunity_id\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"program_title\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"assistance_listing_number\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"updated_by\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"created_by\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\n            \"created_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=\"opportunity_assistance_listing_opportunity_id_opportunity_fkey\",\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_assistance_listing_id\", name=\"opportunity_assistance_listing_pkey\"\n        ),\n        schema=\"api\",\n    )\n    op.create_index(\n        \"opportunity_assistance_listing_opportunity_id_idx\",\n        \"opportunity_assistance_listing\",\n        [\"opportunity_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_table(\n        \"link_applicant_type_opportunity\",\n        sa.Column(\"opportunity_id\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"applicant_type_id\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"updated_by\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"created_by\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\n            \"created_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"applicant_type_id\"],\n            [\"api.lk_applicant_type.applicant_type_id\"],\n            name=\"link_applicant_type_opportunity_applicant_type_id_lk_ap_7903\",\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=\"link_applicant_type_opportunity_opportunity_id_opportunity_fkey\",\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_id\", \"applicant_type_id\", name=\"link_applicant_type_opportunity_pkey\"\n        ),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"link_funding_category_opportunity\",\n        sa.Column(\"opportunity_id\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"funding_category_id\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"updated_by\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"created_by\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\n            \"created_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"funding_category_id\"],\n            [\"api.lk_funding_category.funding_category_id\"],\n            name=\"link_funding_category_opportunity_funding_category_id_l_4add\",\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=\"link_funding_category_opportunity_opportunity_id_opport_eb65\",\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_id\", \"funding_category_id\", name=\"link_funding_category_opportunity_pkey\"\n        ),\n        schema=\"api\",\n    )\n\n    op.create_table(\n        \"link_funding_instrument_opportunity\",\n        sa.Column(\"opportunity_id\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"funding_instrument_id\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"updated_by\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\"created_by\", sa.TEXT(), autoincrement=False, nullable=True),\n        sa.Column(\n            \"created_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"funding_instrument_id\"],\n            [\"api.lk_funding_instrument.funding_instrument_id\"],\n            name=\"link_funding_instrument_opportunity_funding_instrument__68d6\",\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=\"link_funding_instrument_opportunity_opportunity_id_oppo_9420\",\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_id\",\n            \"funding_instrument_id\",\n            name=\"link_funding_instrument_opportunity_pkey\",\n        ),\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###"}
{"path":"documentation/infra/set-up-monitoring-alerts.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-monitoring-alerts.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2024_03_07_updates_for_summary_tables.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_03_07_updates_for_summary_tables.py\nSize: 14.14 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/infra/set-up-network.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-network.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: 578c80acb29c\nRevises: ac80e949bcf8\nCreate Date: 2024-03-07 10:44:08.694002\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"578c80acb29c\"\ndown_revision = \"ac80e949bcf8\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"opportunity\",\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"opportunity_number\", sa.Text(), nullable=True),\n        sa.Column(\"opportunity_title\", sa.Text(), nullable=True),\n        sa.Column(\"agency\", sa.Text(), nullable=True),\n        sa.Column(\"opportunity_category_id\", sa.Integer(), nullable=True),\n        sa.Column(\"category_explanation\", sa.Text(), nullable=True),\n        sa.Column(\"is_draft\", sa.Boolean(), nullable=False),\n        sa.Column(\"revision_number\", sa.Text(), nullable=True),\n        sa.Column(\"modified_comments\", sa.Text(), nullable=True),\n        sa.Column(\"publisher_user_id\", sa.Text(), nullable=True),\n        sa.Column(\"publisher_profile_id\", sa.BigInteger(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_category_id\"],\n            [\"api.lk_opportunity_category.opportunity_category_id\"],\n            name=op.f(\"opportunity_opportunity_category_id_lk_opportunity_category_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=op.f(\"opportunity_pkey\")),\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"opportunity_is_draft_idx\"), \"opportunity\", [\"is_draft\"], unique=False, schema=\"api\"\n    )\n    op.create_index(\n        op.f(\"opportunity_opportunity_category_id_idx\"),\n        \"opportunity\",\n        [\"opportunity_category_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"opportunity_opportunity_title_idx\"),\n        \"opportunity\",\n        [\"opportunity_title\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_table(\n        \"opportunity_assistance_listing\",\n        sa.Column(\"opportunity_assistance_listing_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"assistance_listing_number\", sa.Text(), nullable=True),\n        sa.Column(\"program_title\", sa.Text(), nullable=True),\n        sa.Column(\"updated_by\", sa.Text(), nullable=True),\n        sa.Column(\"created_by\", sa.Text(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=op.f(\"opportunity_assistance_listing_opportunity_id_opportunity_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_assistance_listing_id\", name=op.f(\"opportunity_assistance_listing_pkey\")\n        ),\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"opportunity_assistance_listing_opportunity_id_idx\"),\n        \"opportunity_assistance_listing\",\n        [\"opportunity_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_table(\n        \"opportunity_summary\",\n        sa.Column(\"opportunity_summary_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"summary_description\", sa.Text(), nullable=True),\n        sa.Column(\"is_cost_sharing\", sa.Boolean(), nullable=True),\n        sa.Column(\"is_forecast\", sa.Boolean(), nullable=False),\n        sa.Column(\"post_date\", sa.Date(), nullable=True),\n        sa.Column(\"close_date\", sa.Date(), nullable=True),\n        sa.Column(\"close_date_description\", sa.Text(), nullable=True),\n        sa.Column(\"archive_date\", sa.Date(), nullable=True),\n        sa.Column(\"unarchive_date\", sa.Date(), nullable=True),\n        sa.Column(\"expected_number_of_awards\", sa.Integer(), nullable=True),\n        sa.Column(\"estimated_total_program_funding\", sa.BigInteger(), nullable=True),\n        sa.Column(\"award_floor\", sa.BigInteger(), nullable=True),\n        sa.Column(\"award_ceiling\", sa.BigInteger(), nullable=True),\n        sa.Column(\"additional_info_url\", sa.Text(), nullable=True),\n        sa.Column(\"additional_info_url_description\", sa.Text(), nullable=True),\n        sa.Column(\"forecasted_post_date\", sa.Date(), nullable=True),\n        sa.Column(\"forecasted_close_date\", sa.Date(), nullable=True),\n        sa.Column(\"forecasted_close_date_description\", sa.Text(), nullable=True),\n        sa.Column(\"forecasted_award_date\", sa.Date(), nullable=True),\n        sa.Column(\"forecasted_project_start_date\", sa.Date(), nullable=True),\n        sa.Column(\"fiscal_year\", sa.Integer(), nullable=True),\n        sa.Column(\"revision_number\", sa.Integer(), nullable=False),\n        sa.Column(\"modification_comments\", sa.Text(), nullable=True),\n        sa.Column(\"funding_category_description\", sa.Text(), nullable=True),\n        sa.Column(\"applicant_eligibility_description\", sa.Text(), nullable=True),\n        sa.Column(\"agency_code\", sa.Text(), nullable=True),\n        sa.Column(\"agency_name\", sa.Text(), nullable=True),\n        sa.Column(\"agency_phone_number\", sa.Text(), nullable=True),\n        sa.Column(\"agency_contact_description\", sa.Text(), nullable=True),\n        sa.Column(\"agency_email_address\", sa.Text(), nullable=True),\n        sa.Column(\"agency_email_address_description\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=True),\n        sa.Column(\"can_send_mail\", sa.Boolean(), nullable=True),\n        sa.Column(\"publisher_profile_id\", sa.BigInteger(), nullable=True),\n        sa.Column(\"publisher_user_id\", sa.Text(), nullable=True),\n        sa.Column(\"updated_by\", sa.Text(), nullable=True),\n        sa.Column(\"created_by\", sa.Text(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=op.f(\"opportunity_summary_opportunity_id_opportunity_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_summary_id\", name=op.f(\"opportunity_summary_pkey\")),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"current_opportunity_summary\",\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"opportunity_summary_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"opportunity_status_id\", sa.Integer(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=op.f(\"current_opportunity_summary_opportunity_id_opportunity_fkey\"),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_status_id\"],\n            [\"api.lk_opportunity_status.opportunity_status_id\"],\n            name=op.f(\n                \"current_opportunity_summary_opportunity_status_id_lk_opportunity_status_fkey\"\n            ),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_summary_id\"],\n            [\"api.opportunity_summary.opportunity_summary_id\"],\n            name=op.f(\n                \"current_opportunity_summary_opportunity_summary_id_opportunity_summary_fkey\"\n            ),\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_id\",\n            \"opportunity_summary_id\",\n            name=op.f(\"current_opportunity_summary_pkey\"),\n        ),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"link_opportunity_summary_applicant_type\",\n        sa.Column(\"opportunity_summary_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"applicant_type_id\", sa.Integer(), nullable=False),\n        sa.Column(\"legacy_applicant_type_id\", sa.Integer(), nullable=True),\n        sa.Column(\"updated_by\", sa.Text(), nullable=True),\n        sa.Column(\"created_by\", sa.Text(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"applicant_type_id\"],\n            [\"api.lk_applicant_type.applicant_type_id\"],\n            name=op.f(\n                \"link_opportunity_summary_applicant_type_applicant_type_id_lk_applicant_type_fkey\"\n            ),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_summary_id\"],\n            [\"api.opportunity_summary.opportunity_summary_id\"],\n            name=op.f(\n                \"link_opportunity_summary_applicant_type_opportunity_summary_id_opportunity_summary_fkey\"\n            ),\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_summary_id\",\n            \"applicant_type_id\",\n            name=op.f(\"link_opportunity_summary_applicant_type_pkey\"),\n        ),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"link_opportunity_summary_funding_category\",\n        sa.Column(\"opportunity_summary_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"funding_category_id\", sa.Integer(), nullable=False),\n        sa.Column(\"legacy_funding_category_id\", sa.Integer(), nullable=True),\n        sa.Column(\"updated_by\", sa.Text(), nullable=True),\n        sa.Column(\"created_by\", sa.Text(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"funding_category_id\"],\n            [\"api.lk_funding_category.funding_category_id\"],\n            name=op.f(\n                \"link_opportunity_summary_funding_category_funding_category_id_lk_funding_category_fkey\"\n            ),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_summary_id\"],\n            [\"api.opportunity_summary.opportunity_summary_id\"],\n            name=op.f(\n                \"link_opportunity_summary_funding_category_opportunity_summary_id_opportunity_summary_fkey\"\n            ),\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_summary_id\",\n            \"funding_category_id\",\n            name=op.f(\"link_opportunity_summary_funding_category_pkey\"),\n        ),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"link_opportunity_summary_funding_instrument\",\n        sa.Column(\"opportunity_summary_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"funding_instrument_id\", sa.Integer(), nullable=False),\n        sa.Column(\"legacy_funding_instrument_id\", sa.Integer(), nullable=True),\n        sa.Column(\"updated_by\", sa.Text(), nullable=True),\n        sa.Column(\"created_by\", sa.Text(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"funding_instrument_id\"],\n            [\"api.lk_funding_instrument.funding_instrument_id\"],\n            name=op.f(\n                \"link_opportunity_summary_funding_instrument_funding_instrument_id_lk_funding_instrument_fkey\"\n            ),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_summary_id\"],\n            [\"api.opportunity_summary.opportunity_summary_id\"],\n            name=op.f(\n                \"link_opportunity_summary_funding_instrument_opportunity_summary_id_opportunity_summary_fkey\"\n            ),\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_summary_id\",\n            \"funding_instrument_id\",\n            name=op.f(\"link_opportunity_summary_funding_instrument_pkey\"),\n        ),\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"link_opportunity_summary_funding_instrument\", schema=\"api\")\n    op.drop_table(\"link_opportunity_summary_funding_category\", schema=\"api\")\n    op.drop_table(\"link_opportunity_summary_applicant_type\", schema=\"api\")\n    op.drop_table(\"current_opportunity_summary\", schema=\"api\")\n    op.drop_table(\"opportunity_summary\", schema=\"api\")\n    op.drop_index(\n        op.f(\"opportunity_assistance_listing_opportunity_id_idx\"),\n        table_name=\"opportunity_assistance_listing\",\n        schema=\"api\",\n    )\n    op.drop_table(\"opportunity_assistance_listing\", schema=\"api\")\n    op.drop_index(op.f(\"opportunity_opportunity_title_idx\"), table_name=\"opportunity\", schema=\"api\")\n    op.drop_index(\n        op.f(\"opportunity_opportunity_category_id_idx\"), table_name=\"opportunity\", schema=\"api\"\n    )\n    op.drop_index(op.f(\"opportunity_is_draft_idx\"), table_name=\"opportunity\", schema=\"api\")\n    op.drop_table(\"opportunity\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/infra/vulnerability-management.md","language":"markdown","type":"code","directory":"documentation/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/vulnerability-management.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2024_03_12_add_indexes_for_search.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_03_12_add_indexes_for_search.py\nSize: 4.71 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/notice_of_funding_opportunity_prototypes/README.md","language":"markdown","type":"code","directory":"documentation/notice_of_funding_opportunity_prototypes","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/notice_of_funding_opportunity_prototypes/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: f8364285630d\nRevises: 578c80acb29c\nCreate Date: 2024-03-12 13:22:57.718265\n\n\"\"\"\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"f8364285630d\"\ndown_revision = \"578c80acb29c\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index(\n        op.f(\"current_opportunity_summary_opportunity_id_idx\"),\n        \"current_opportunity_summary\",\n        [\"opportunity_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"current_opportunity_summary_opportunity_status_id_idx\"),\n        \"current_opportunity_summary\",\n        [\"opportunity_status_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"current_opportunity_summary_opportunity_summary_id_idx\"),\n        \"current_opportunity_summary\",\n        [\"opportunity_summary_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"link_opportunity_summary_applicant_type_applicant_type_id_idx\"),\n        \"link_opportunity_summary_applicant_type\",\n        [\"applicant_type_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"link_opportunity_summary_applicant_type_opportunity_summary_id_idx\"),\n        \"link_opportunity_summary_applicant_type\",\n        [\"opportunity_summary_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"link_opportunity_summary_funding_category_funding_category_id_idx\"),\n        \"link_opportunity_summary_funding_category\",\n        [\"funding_category_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"link_opportunity_summary_funding_category_opportunity_summary_id_idx\"),\n        \"link_opportunity_summary_funding_category\",\n        [\"opportunity_summary_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"link_opportunity_summary_funding_instrument_funding_instrument_id_idx\"),\n        \"link_opportunity_summary_funding_instrument\",\n        [\"funding_instrument_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"link_opportunity_summary_funding_instrument_opportunity_summary_id_idx\"),\n        \"link_opportunity_summary_funding_instrument\",\n        [\"opportunity_summary_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"opportunity_agency_idx\"), \"opportunity\", [\"agency\"], unique=False, schema=\"api\"\n    )\n    op.create_index(\n        op.f(\"opportunity_summary_opportunity_id_idx\"),\n        \"opportunity_summary\",\n        [\"opportunity_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\n        op.f(\"opportunity_summary_opportunity_id_idx\"),\n        table_name=\"opportunity_summary\",\n        schema=\"api\",\n    )\n    op.drop_index(op.f(\"opportunity_agency_idx\"), table_name=\"opportunity\", schema=\"api\")\n    op.drop_index(\n        op.f(\"link_opportunity_summary_funding_instrument_opportunity_summary_id_idx\"),\n        table_name=\"link_opportunity_summary_funding_instrument\",\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"link_opportunity_summary_funding_instrument_funding_instrument_id_idx\"),\n        table_name=\"link_opportunity_summary_funding_instrument\",\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"link_opportunity_summary_funding_category_opportunity_summary_id_idx\"),\n        table_name=\"link_opportunity_summary_funding_category\",\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"link_opportunity_summary_funding_category_funding_category_id_idx\"),\n        table_name=\"link_opportunity_summary_funding_category\",\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"link_opportunity_summary_applicant_type_opportunity_summary_id_idx\"),\n        table_name=\"link_opportunity_summary_applicant_type\",\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"link_opportunity_summary_applicant_type_applicant_type_id_idx\"),\n        table_name=\"link_opportunity_summary_applicant_type\",\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"current_opportunity_summary_opportunity_summary_id_idx\"),\n        table_name=\"current_opportunity_summary\",\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"current_opportunity_summary_opportunity_status_id_idx\"),\n        table_name=\"current_opportunity_summary\",\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"current_opportunity_summary_opportunity_id_idx\"),\n        table_name=\"current_opportunity_summary\",\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###"}
{"path":"documentation/user-research/README.md","language":"markdown","type":"code","directory":"documentation/user-research","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/user-research/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2024_04_16_make_revision_number_nullable.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_04_16_make_revision_number_nullable.py\nSize: 0.86 KB\nLast Modified: 2025-02-14T17:08:26.440Z"}
{"path":"documentation/wiki/README.md","language":"markdown","type":"code","directory":"documentation/wiki","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: b26ea0f40066\nRevises: f8364285630d\nCreate Date: 2024-04-16 13:36:35.993325\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"b26ea0f40066\"\ndown_revision = \"f8364285630d\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"opportunity_summary\",\n        \"revision_number\",\n        existing_type=sa.INTEGER(),\n        nullable=True,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"opportunity_summary\",\n        \"revision_number\",\n        existing_type=sa.INTEGER(),\n        nullable=False,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/SUMMARY.md","language":"markdown","type":"code","directory":"documentation/wiki","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/SUMMARY.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2024_04_19_fix_column_type_and_add_version_number.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_04_19_fix_column_type_and_add_version_number.py\nSize: 1.24 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/about/team.md","language":"markdown","type":"code","directory":"documentation/wiki/about","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/about/team.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: c6878bae0c60\nRevises: b26ea0f40066\nCreate Date: 2024-04-19 13:41:34.017203\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"c6878bae0c60\"\ndown_revision = \"b26ea0f40066\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"opportunity\",\n        \"revision_number\",\n        existing_type=sa.TEXT(),\n        type_=sa.Integer(),\n        existing_nullable=True,\n        postgresql_using=\"revision_number::integer\",\n        schema=\"api\",\n    )\n    op.add_column(\n        \"opportunity_summary\",\n        sa.Column(\"version_number\", sa.Integer(), nullable=True),\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"opportunity_summary\", \"version_number\", schema=\"api\")\n    op.alter_column(\n        \"opportunity\",\n        \"revision_number\",\n        existing_type=sa.Integer(),\n        type_=sa.TEXT(),\n        existing_nullable=True,\n        postgresql_using=\"revision_number::TEXT\",\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/about/terminology.md","language":"markdown","type":"code","directory":"documentation/wiki/about","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/about/terminology.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"File: api/src/db/migrations/versions/2024_04_24_add_staging_tables.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_04_24_add_staging_tables.py\nSize: 31.36 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/design-and-research/brand-guidelines/README.md","language":"markdown","type":"code","directory":"documentation/wiki/design-and-research/brand-guidelines","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.129Z","content":"Revision ID: e3a1be603d26\nRevises: c6878bae0c60\nCreate Date: 2024-04-24 15:27:13.602523\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"e3a1be603d26\"\ndown_revision = \"c6878bae0c60\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"tapplicanttypes_forecast\",\n        sa.Column(\"at_frcst_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"at_id\", sa.Text(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"at_frcst_id\", \"at_id\", \"opportunity_id\", name=op.f(\"tapplicanttypes_forecast_pkey\")\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tapplicanttypes_forecast_transformed_at_idx\"),\n        \"tapplicanttypes_forecast\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tapplicanttypes_forecast_hist\",\n        sa.Column(\"at_frcst_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"at_id\", sa.Text(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"revision_number\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"at_frcst_id\",\n            \"at_id\",\n            \"opportunity_id\",\n            \"revision_number\",\n            name=op.f(\"tapplicanttypes_forecast_hist_pkey\"),\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tapplicanttypes_forecast_hist_transformed_at_idx\"),\n        \"tapplicanttypes_forecast_hist\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tapplicanttypes_synopsis\",\n        sa.Column(\"at_syn_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"at_id\", sa.Text(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"at_syn_id\", \"at_id\", \"opportunity_id\", name=op.f(\"tapplicanttypes_synopsis_pkey\")\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tapplicanttypes_synopsis_transformed_at_idx\"),\n        \"tapplicanttypes_synopsis\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tapplicanttypes_synopsis_hist\",\n        sa.Column(\"at_syn_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"at_id\", sa.Text(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"revision_number\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"at_syn_id\",\n            \"at_id\",\n            \"opportunity_id\",\n            \"revision_number\",\n            name=op.f(\"tapplicanttypes_synopsis_hist_pkey\"),\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tapplicanttypes_synopsis_hist_transformed_at_idx\"),\n        \"tapplicanttypes_synopsis_hist\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tforecast\",\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"version_nbr\", sa.BigInteger(), nullable=False),\n        sa.Column(\"posting_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"archive_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"forecast_desc\", sa.Text(), nullable=True),\n        sa.Column(\"oth_cat_fa_desc\", sa.Text(), nullable=True),\n        sa.Column(\"cost_sharing\", sa.Text(), nullable=True),\n        sa.Column(\"number_of_awards\", sa.Text(), nullable=True),\n        sa.Column(\"est_funding\", sa.Text(), nullable=True),\n        sa.Column(\"award_ceiling\", sa.Text(), nullable=True),\n        sa.Column(\"award_floor\", sa.Text(), nullable=True),\n        sa.Column(\"fd_link_url\", sa.Text(), nullable=True),\n        sa.Column(\"fd_link_desc\", sa.Text(), nullable=True),\n        sa.Column(\"ac_name\", sa.Text(), nullable=True),\n        sa.Column(\"ac_phone\", sa.Text(), nullable=True),\n        sa.Column(\"ac_email_addr\", sa.Text(), nullable=True),\n        sa.Column(\"ac_email_desc\", sa.Text(), nullable=True),\n        sa.Column(\"agency_code\", sa.Text(), nullable=True),\n        sa.Column(\"sendmail\", sa.Text(), nullable=True),\n        sa.Column(\"applicant_elig_desc\", sa.Text(), nullable=True),\n        sa.Column(\"est_synopsis_posting_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"est_appl_response_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"est_appl_response_date_desc\", sa.Text(), nullable=True),\n        sa.Column(\"est_award_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"est_project_start_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"fiscal_year\", sa.BigInteger(), nullable=True),\n        sa.Column(\"modification_comments\", sa.Text(), nullable=True),\n        sa.Column(\"create_ts\", sa.TIMESTAMP(timezone=True), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"publisheruid\", sa.Text(), nullable=True),\n        sa.Column(\"publisher_profile_id\", sa.BigInteger(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=op.f(\"tforecast_pkey\")),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tforecast_transformed_at_idx\"),\n        \"tforecast\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tforecast_hist\",\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"revision_number\", sa.BigInteger(), nullable=False),\n        sa.Column(\"version_nbr\", sa.BigInteger(), nullable=False),\n        sa.Column(\"posting_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"archive_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"forecast_desc\", sa.Text(), nullable=True),\n        sa.Column(\"oth_cat_fa_desc\", sa.Text(), nullable=True),\n        sa.Column(\"cost_sharing\", sa.Text(), nullable=True),\n        sa.Column(\"number_of_awards\", sa.Text(), nullable=True),\n        sa.Column(\"est_funding\", sa.Text(), nullable=True),\n        sa.Column(\"award_ceiling\", sa.Text(), nullable=True),\n        sa.Column(\"award_floor\", sa.Text(), nullable=True),\n        sa.Column(\"fd_link_url\", sa.Text(), nullable=True),\n        sa.Column(\"fd_link_desc\", sa.Text(), nullable=True),\n        sa.Column(\"ac_name\", sa.Text(), nullable=True),\n        sa.Column(\"ac_phone\", sa.Text(), nullable=True),\n        sa.Column(\"ac_email_addr\", sa.Text(), nullable=True),\n        sa.Column(\"ac_email_desc\", sa.Text(), nullable=True),\n        sa.Column(\"agency_code\", sa.Text(), nullable=True),\n        sa.Column(\"sendmail\", sa.Text(), nullable=True),\n        sa.Column(\"applicant_elig_desc\", sa.Text(), nullable=True),\n        sa.Column(\"est_synopsis_posting_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"est_appl_response_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"est_appl_response_date_desc\", sa.Text(), nullable=True),\n        sa.Column(\"est_award_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"est_project_start_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"fiscal_year\", sa.BigInteger(), nullable=True),\n        sa.Column(\"modification_comments\", sa.Text(), nullable=True),\n        sa.Column(\"action_type\", sa.Text(), nullable=True),\n        sa.Column(\"action_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"create_ts\", sa.TIMESTAMP(timezone=True), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"publisheruid\", sa.Text(), nullable=True),\n        sa.Column(\"publisher_profile_id\", sa.BigInteger(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_id\", \"revision_number\", name=op.f(\"tforecast_hist_pkey\")\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tforecast_hist_transformed_at_idx\"),\n        \"tforecast_hist\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tfundactcat_forecast\",\n        sa.Column(\"fac_frcst_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"fac_id\", sa.Text(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"fac_frcst_id\", \"fac_id\", \"opportunity_id\", name=op.f(\"tfundactcat_forecast_pkey\")\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tfundactcat_forecast_transformed_at_idx\"),\n        \"tfundactcat_forecast\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tfundactcat_forecast_hist\",\n        sa.Column(\"fac_frcst_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"fac_id\", sa.Text(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"revision_number\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"fac_frcst_id\",\n            \"fac_id\",\n            \"opportunity_id\",\n            \"revision_number\",\n            name=op.f(\"tfundactcat_forecast_hist_pkey\"),\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tfundactcat_forecast_hist_transformed_at_idx\"),\n        \"tfundactcat_forecast_hist\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tfundactcat_synopsis\",\n        sa.Column(\"fac_syn_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"fac_id\", sa.Text(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"fac_syn_id\", \"fac_id\", \"opportunity_id\", name=op.f(\"tfundactcat_synopsis_pkey\")\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tfundactcat_synopsis_transformed_at_idx\"),\n        \"tfundactcat_synopsis\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tfundactcat_synopsis_hist\",\n        sa.Column(\"fac_syn_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"fac_id\", sa.Text(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"revision_number\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"fac_syn_id\",\n            \"fac_id\",\n            \"opportunity_id\",\n            \"revision_number\",\n            name=op.f(\"tfundactcat_synopsis_hist_pkey\"),\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tfundactcat_synopsis_hist_transformed_at_idx\"),\n        \"tfundactcat_synopsis_hist\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tfundinstr_forecast\",\n        sa.Column(\"fi_frcst_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"fi_id\", sa.Text(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"fi_frcst_id\", \"fi_id\", \"opportunity_id\", name=op.f(\"tfundinstr_forecast_pkey\")\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tfundinstr_forecast_transformed_at_idx\"),\n        \"tfundinstr_forecast\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tfundinstr_forecast_hist\",\n        sa.Column(\"fi_frcst_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"fi_id\", sa.Text(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"revision_number\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"fi_frcst_id\",\n            \"fi_id\",\n            \"opportunity_id\",\n            \"revision_number\",\n            name=op.f(\"tfundinstr_forecast_hist_pkey\"),\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tfundinstr_forecast_hist_transformed_at_idx\"),\n        \"tfundinstr_forecast_hist\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tfundinstr_synopsis\",\n        sa.Column(\"fi_syn_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"fi_id\", sa.Text(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"fi_syn_id\", \"fi_id\", \"opportunity_id\", name=op.f(\"tfundinstr_synopsis_pkey\")\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tfundinstr_synopsis_transformed_at_idx\"),\n        \"tfundinstr_synopsis\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tfundinstr_synopsis_hist\",\n        sa.Column(\"fi_syn_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"fi_id\", sa.Text(), nullable=False),\n        sa.Column(\"revision_number\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"fi_syn_id\",\n            \"opportunity_id\",\n            \"fi_id\",\n            \"revision_number\",\n            name=op.f(\"tfundinstr_synopsis_hist_pkey\"),\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tfundinstr_synopsis_hist_transformed_at_idx\"),\n        \"tfundinstr_synopsis_hist\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"topportunity\",\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"oppnumber\", sa.Text(), nullable=True),\n        sa.Column(\"revision_number\", sa.BigInteger(), nullable=True),\n        sa.Column(\"opptitle\", sa.Text(), nullable=True),\n        sa.Column(\"owningagency\", sa.Text(), nullable=True),\n        sa.Column(\"publisheruid\", sa.Text(), nullable=True),\n        sa.Column(\"listed\", sa.Text(), nullable=True),\n        sa.Column(\"oppcategory\", sa.Text(), nullable=True),\n        sa.Column(\"initial_opportunity_id\", sa.BigInteger(), nullable=True),\n        sa.Column(\"modified_comments\", sa.Text(), nullable=True),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"flag_2006\", sa.Text(), nullable=True),\n        sa.Column(\"category_explanation\", sa.Text(), nullable=True),\n        sa.Column(\"publisher_profile_id\", sa.BigInteger(), nullable=True),\n        sa.Column(\"is_draft\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=op.f(\"topportunity_pkey\")),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"topportunity_transformed_at_idx\"),\n        \"topportunity\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"topportunity_cfda\",\n        sa.Column(\"opp_cfda_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"cfdanumber\", sa.Text(), nullable=True),\n        sa.Column(\"programtitle\", sa.Text(), nullable=True),\n        sa.Column(\"origtoppid\", sa.BigInteger(), nullable=True),\n        sa.Column(\"oppidcfdanum\", sa.Text(), nullable=True),\n        sa.Column(\"origoppnum\", sa.Text(), nullable=True),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\"opp_cfda_id\", name=op.f(\"topportunity_cfda_pkey\")),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"topportunity_cfda_transformed_at_idx\"),\n        \"topportunity_cfda\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tsynopsis\",\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"posting_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"response_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"archive_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"unarchive_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"syn_desc\", sa.Text(), nullable=True),\n        sa.Column(\"oth_cat_fa_desc\", sa.Text(), nullable=True),\n        sa.Column(\"agency_addr_desc\", sa.Text(), nullable=True),\n        sa.Column(\"cost_sharing\", sa.Text(), nullable=True),\n        sa.Column(\"number_of_awards\", sa.Text(), nullable=True),\n        sa.Column(\"est_funding\", sa.Text(), nullable=True),\n        sa.Column(\"award_ceiling\", sa.Text(), nullable=True),\n        sa.Column(\"award_floor\", sa.Text(), nullable=True),\n        sa.Column(\"fd_link_url\", sa.Text(), nullable=True),\n        sa.Column(\"fd_link_desc\", sa.Text(), nullable=True),\n        sa.Column(\"agency_contact_desc\", sa.Text(), nullable=True),\n        sa.Column(\"ac_email_addr\", sa.Text(), nullable=True),\n        sa.Column(\"ac_email_desc\", sa.Text(), nullable=True),\n        sa.Column(\"agency_name\", sa.Text(), nullable=True),\n        sa.Column(\"agency_phone\", sa.Text(), nullable=True),\n        sa.Column(\"a_sa_code\", sa.Text(), nullable=True),\n        sa.Column(\"ac_phone_number\", sa.Text(), nullable=True),\n        sa.Column(\"ac_name\", sa.Text(), nullable=True),\n        sa.Column(\"create_ts\", sa.TIMESTAMP(timezone=True), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"sendmail\", sa.Text(), nullable=True),\n        sa.Column(\"response_date_desc\", sa.Text(), nullable=True),\n        sa.Column(\"applicant_elig_desc\", sa.Text(), nullable=True),\n        sa.Column(\"version_nbr\", sa.BigInteger(), nullable=True),\n        sa.Column(\"modification_comments\", sa.Text(), nullable=True),\n        sa.Column(\"publisheruid\", sa.Text(), nullable=True),\n        sa.Column(\"publisher_profile_id\", sa.BigInteger(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=op.f(\"tsynopsis_pkey\")),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tsynopsis_transformed_at_idx\"),\n        \"tsynopsis\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    op.create_table(\n        \"tsynopsis_hist\",\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"revision_number\", sa.BigInteger(), nullable=False),\n        sa.Column(\"posting_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"response_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"archive_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"unarchive_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"syn_desc\", sa.Text(), nullable=True),\n        sa.Column(\"oth_cat_fa_desc\", sa.Text(), nullable=True),\n        sa.Column(\"agency_addr_desc\", sa.Text(), nullable=True),\n        sa.Column(\"cost_sharing\", sa.Text(), nullable=True),\n        sa.Column(\"number_of_awards\", sa.Text(), nullable=True),\n        sa.Column(\"est_funding\", sa.Text(), nullable=True),\n        sa.Column(\"award_ceiling\", sa.Text(), nullable=True),\n        sa.Column(\"award_floor\", sa.Text(), nullable=True),\n        sa.Column(\"fd_link_url\", sa.Text(), nullable=True),\n        sa.Column(\"fd_link_desc\", sa.Text(), nullable=True),\n        sa.Column(\"agency_contact_desc\", sa.Text(), nullable=True),\n        sa.Column(\"ac_email_addr\", sa.Text(), nullable=True),\n        sa.Column(\"ac_email_desc\", sa.Text(), nullable=True),\n        sa.Column(\"agency_name\", sa.Text(), nullable=True),\n        sa.Column(\"agency_phone\", sa.Text(), nullable=True),\n        sa.Column(\"a_sa_code\", sa.Text(), nullable=True),\n        sa.Column(\"ac_phone_number\", sa.Text(), nullable=True),\n        sa.Column(\"ac_name\", sa.Text(), nullable=True),\n        sa.Column(\"create_ts\", sa.TIMESTAMP(timezone=True), nullable=False),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"sendmail\", sa.Text(), nullable=True),\n        sa.Column(\"response_date_desc\", sa.Text(), nullable=True),\n        sa.Column(\"applicant_elig_desc\", sa.Text(), nullable=True),\n        sa.Column(\"action_type\", sa.Text(), nullable=True),\n        sa.Column(\"action_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"version_nbr\", sa.BigInteger(), nullable=False),\n        sa.Column(\"modification_comments\", sa.Text(), nullable=True),\n        sa.Column(\"publisheruid\", sa.Text(), nullable=True),\n        sa.Column(\"publisher_profile_id\", sa.BigInteger(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_id\", \"revision_number\", name=op.f(\"tsynopsis_hist_pkey\")\n        ),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tsynopsis_hist_transformed_at_idx\"),\n        \"tsynopsis_hist\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\n        op.f(\"tsynopsis_hist_transformed_at_idx\"), table_name=\"tsynopsis_hist\", schema=\"staging\"\n    )\n    op.drop_table(\"tsynopsis_hist\", schema=\"staging\")\n    op.drop_index(op.f(\"tsynopsis_transformed_at_idx\"), table_name=\"tsynopsis\", schema=\"staging\")\n    op.drop_table(\"tsynopsis\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"topportunity_cfda_transformed_at_idx\"),\n        table_name=\"topportunity_cfda\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"topportunity_cfda\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"topportunity_transformed_at_idx\"), table_name=\"topportunity\", schema=\"staging\"\n    )\n    op.drop_table(\"topportunity\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tfundinstr_synopsis_hist_transformed_at_idx\"),\n        table_name=\"tfundinstr_synopsis_hist\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tfundinstr_synopsis_hist\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tfundinstr_synopsis_transformed_at_idx\"),\n        table_name=\"tfundinstr_synopsis\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tfundinstr_synopsis\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tfundinstr_forecast_hist_transformed_at_idx\"),\n        table_name=\"tfundinstr_forecast_hist\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tfundinstr_forecast_hist\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tfundinstr_forecast_transformed_at_idx\"),\n        table_name=\"tfundinstr_forecast\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tfundinstr_forecast\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tfundactcat_synopsis_hist_transformed_at_idx\"),\n        table_name=\"tfundactcat_synopsis_hist\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tfundactcat_synopsis_hist\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tfundactcat_synopsis_transformed_at_idx\"),\n        table_name=\"tfundactcat_synopsis\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tfundactcat_synopsis\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tfundactcat_forecast_hist_transformed_at_idx\"),\n        table_name=\"tfundactcat_forecast_hist\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tfundactcat_forecast_hist\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tfundactcat_forecast_transformed_at_idx\"),\n        table_name=\"tfundactcat_forecast\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tfundactcat_forecast\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tforecast_hist_transformed_at_idx\"), table_name=\"tforecast_hist\", schema=\"staging\"\n    )\n    op.drop_table(\"tforecast_hist\", schema=\"staging\")\n    op.drop_index(op.f(\"tforecast_transformed_at_idx\"), table_name=\"tforecast\", schema=\"staging\")\n    op.drop_table(\"tforecast\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tapplicanttypes_synopsis_hist_transformed_at_idx\"),\n        table_name=\"tapplicanttypes_synopsis_hist\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tapplicanttypes_synopsis_hist\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tapplicanttypes_synopsis_transformed_at_idx\"),\n        table_name=\"tapplicanttypes_synopsis\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tapplicanttypes_synopsis\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tapplicanttypes_forecast_hist_transformed_at_idx\"),\n        table_name=\"tapplicanttypes_forecast_hist\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tapplicanttypes_forecast_hist\", schema=\"staging\")\n    op.drop_index(\n        op.f(\"tapplicanttypes_forecast_transformed_at_idx\"),\n        table_name=\"tapplicanttypes_forecast\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tapplicanttypes_forecast\", schema=\"staging\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/design-and-research/brand-guidelines/colors.md","language":"markdown","type":"code","directory":"documentation/wiki/design-and-research/brand-guidelines","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/colors.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_05_01_add_created_at_updated_at_and_deleted_.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_05_01_add_created_at_updated_at_and_deleted_.py\nSize: 1.85 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/design-and-research/brand-guidelines/grid-and-composition.md","language":"markdown","type":"code","directory":"documentation/wiki/design-and-research/brand-guidelines","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/grid-and-composition.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 1ddd1d051a99\nRevises: e3a1be603d26\nCreate Date: 2024-05-01 11:14:34.332661\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"1ddd1d051a99\"\ndown_revision = \"e3a1be603d26\"\nbranch_labels = None\ndepends_on = None\n\n\nTABLES = (\n    \"tapplicanttypes_forecast\",\n    \"tapplicanttypes_forecast_hist\",\n    \"tapplicanttypes_synopsis\",\n    \"tapplicanttypes_synopsis_hist\",\n    \"tforecast\",\n    \"tforecast_hist\",\n    \"tfundactcat_forecast\",\n    \"tfundactcat_forecast_hist\",\n    \"tfundactcat_synopsis\",\n    \"tfundactcat_synopsis_hist\",\n    \"tfundinstr_forecast\",\n    \"tfundinstr_forecast_hist\",\n    \"tfundinstr_synopsis\",\n    \"tfundinstr_synopsis_hist\",\n    \"topportunity\",\n    \"topportunity_cfda\",\n    \"tsynopsis\",\n    \"tsynopsis_hist\",\n)\n\n\ndef upgrade():\n    for table_name in TABLES:\n        op.add_column(\n            table_name,\n            sa.Column(\n                \"created_at\",\n                sa.TIMESTAMP(timezone=True),\n                server_default=sa.text(\"now()\"),\n                nullable=False,\n            ),\n            schema=\"staging\",\n        )\n        op.add_column(\n            table_name,\n            sa.Column(\n                \"updated_at\",\n                sa.TIMESTAMP(timezone=True),\n                server_default=sa.text(\"now()\"),\n                nullable=False,\n            ),\n            schema=\"staging\",\n        )\n        op.add_column(\n            table_name,\n            sa.Column(\"deleted_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n            schema=\"staging\",\n        )\n\n\ndef downgrade():\n    for table_name in TABLES:\n        op.drop_column(table_name, \"deleted_at\", schema=\"staging\")\n        op.drop_column(table_name, \"updated_at\", schema=\"staging\")\n        op.drop_column(table_name, \"created_at\", schema=\"staging\")"}
{"path":"documentation/wiki/design-and-research/brand-guidelines/iconography.md","language":"markdown","type":"code","directory":"documentation/wiki/design-and-research/brand-guidelines","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/iconography.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_05_02_add_unique_constraint_for_summary.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_05_02_add_unique_constraint_for_summary.py\nSize: 0.92 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/design-and-research/brand-guidelines/logo.md","language":"markdown","type":"code","directory":"documentation/wiki/design-and-research/brand-guidelines","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/logo.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 24061ff82646\nRevises: 1ddd1d051a99\nCreate Date: 2024-05-02 10:11:35.832837\n\n\"\"\"\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"24061ff82646\"\ndown_revision = \"1ddd1d051a99\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_unique_constraint(\n        op.f(\"opportunity_summary_is_forecast_uniq\"),\n        \"opportunity_summary\",\n        [\"is_forecast\", \"revision_number\", \"opportunity_id\"],\n        schema=\"api\",\n        postgresql_nulls_not_distinct=True,\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_constraint(\n        op.f(\"opportunity_summary_is_forecast_uniq\"),\n        \"opportunity_summary\",\n        schema=\"api\",\n        type_=\"unique\",\n    )\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/design-and-research/brand-guidelines/photos-and-illustrations.md","language":"markdown","type":"code","directory":"documentation/wiki/design-and-research/brand-guidelines","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/photos-and-illustrations.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_05_07_add_unique_constraint_link_table_legacy_.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_05_07_add_unique_constraint_link_table_legacy_.py\nSize: 1.94 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/design-and-research/brand-guidelines/typography.md","language":"markdown","type":"code","directory":"documentation/wiki/design-and-research/brand-guidelines","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/typography.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: f97987d087b5\nRevises: 24061ff82646\nCreate Date: 2024-05-07 14:41:19.401963\n\n\"\"\"\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"f97987d087b5\"\ndown_revision = \"24061ff82646\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_unique_constraint(\n        op.f(\"link_opportunity_summary_applicant_type_opportunity_summary_id_uniq\"),\n        \"link_opportunity_summary_applicant_type\",\n        [\"opportunity_summary_id\", \"legacy_applicant_type_id\"],\n        schema=\"api\",\n    )\n    op.create_unique_constraint(\n        op.f(\"link_opportunity_summary_funding_category_opportunity_summary_id_uniq\"),\n        \"link_opportunity_summary_funding_category\",\n        [\"opportunity_summary_id\", \"legacy_funding_category_id\"],\n        schema=\"api\",\n    )\n    op.create_unique_constraint(\n        op.f(\"link_opportunity_summary_funding_instrument_opportunity_summary_id_uniq\"),\n        \"link_opportunity_summary_funding_instrument\",\n        [\"opportunity_summary_id\", \"legacy_funding_instrument_id\"],\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_constraint(\n        op.f(\"link_opportunity_summary_funding_instrument_opportunity_summary_id_uniq\"),\n        \"link_opportunity_summary_funding_instrument\",\n        schema=\"api\",\n        type_=\"unique\",\n    )\n    op.drop_constraint(\n        op.f(\"link_opportunity_summary_funding_category_opportunity_summary_id_uniq\"),\n        \"link_opportunity_summary_funding_category\",\n        schema=\"api\",\n        type_=\"unique\",\n    )\n    op.drop_constraint(\n        op.f(\"link_opportunity_summary_applicant_type_opportunity_summary_id_uniq\"),\n        \"link_opportunity_summary_applicant_type\",\n        schema=\"api\",\n        type_=\"unique\",\n    )\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/design-and-research/user-research/README.md","language":"markdown","type":"code","directory":"documentation/wiki/design-and-research/user-research","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/user-research/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_05_09_add_transformation_notes_to_staging_.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_05_09_add_transformation_notes_to_staging_.py\nSize: 4.68 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/design-and-research/user-research/grants.gov-archetypes.md","language":"markdown","type":"code","directory":"documentation/wiki/design-and-research/user-research","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/user-research/grants.gov-archetypes.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 61c58638e56b\nRevises: f97987d087b5\nCreate Date: 2024-05-09 15:06:48.010975\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"61c58638e56b\"\ndown_revision = \"f97987d087b5\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"tapplicanttypes_forecast\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tapplicanttypes_forecast_hist\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tapplicanttypes_synopsis\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tapplicanttypes_synopsis_hist\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tforecast\", sa.Column(\"transformation_notes\", sa.Text(), nullable=True), schema=\"staging\"\n    )\n    op.add_column(\n        \"tforecast_hist\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tfundactcat_forecast\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tfundactcat_forecast_hist\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tfundactcat_synopsis\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tfundactcat_synopsis_hist\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tfundinstr_forecast\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tfundinstr_forecast_hist\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tfundinstr_synopsis\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tfundinstr_synopsis_hist\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"topportunity\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"topportunity_cfda\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    op.add_column(\n        \"tsynopsis\", sa.Column(\"transformation_notes\", sa.Text(), nullable=True), schema=\"staging\"\n    )\n    op.add_column(\n        \"tsynopsis_hist\",\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        schema=\"staging\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"tsynopsis_hist\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tsynopsis\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"topportunity_cfda\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"topportunity\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tfundinstr_synopsis_hist\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tfundinstr_synopsis\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tfundinstr_forecast_hist\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tfundinstr_forecast\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tfundactcat_synopsis_hist\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tfundactcat_synopsis\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tfundactcat_forecast_hist\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tfundactcat_forecast\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tforecast_hist\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tforecast\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tapplicanttypes_synopsis_hist\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tapplicanttypes_synopsis\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tapplicanttypes_forecast_hist\", \"transformation_notes\", schema=\"staging\")\n    op.drop_column(\"tapplicanttypes_forecast\", \"transformation_notes\", schema=\"staging\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/design-and-research/voice-and-tone-guide.md","language":"markdown","type":"code","directory":"documentation/wiki/design-and-research","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/voice-and-tone-guide.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_07_08_add_agency_related_tables.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_07_08_add_agency_related_tables.py\nSize: 8.51 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/get-involved/communication-channels/README.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/communication-channels","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/communication-channels/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 4f7acbb61548\nRevises: 61c58638e56b\nCreate Date: 2024-07-08 12:43:45.240782\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"4f7acbb61548\"\ndown_revision = \"61c58638e56b\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"agency_contact_info\",\n        sa.Column(\"agency_contact_info_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"contact_name\", sa.Text(), nullable=False),\n        sa.Column(\"address_line_1\", sa.Text(), nullable=False),\n        sa.Column(\"address_line_2\", sa.Text(), nullable=True),\n        sa.Column(\"city\", sa.Text(), nullable=False),\n        sa.Column(\"state\", sa.Text(), nullable=False),\n        sa.Column(\"zip_code\", sa.Text(), nullable=False),\n        sa.Column(\"phone_number\", sa.Text(), nullable=False),\n        sa.Column(\"primary_email\", sa.Text(), nullable=False),\n        sa.Column(\"secondary_email\", sa.Text(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"agency_contact_info_id\", name=op.f(\"agency_contact_info_pkey\")),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"lk_agency_download_file_type\",\n        sa.Column(\"agency_download_file_type_id\", sa.Integer(), nullable=False),\n        sa.Column(\"description\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\n            \"agency_download_file_type_id\", name=op.f(\"lk_agency_download_file_type_pkey\")\n        ),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"lk_agency_submission_notification_setting\",\n        sa.Column(\"agency_submission_notification_setting_id\", sa.Integer(), nullable=False),\n        sa.Column(\"description\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\n            \"agency_submission_notification_setting_id\",\n            name=op.f(\"lk_agency_submission_notification_setting_pkey\"),\n        ),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"agency\",\n        sa.Column(\"agency_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"agency_name\", sa.Text(), nullable=False),\n        sa.Column(\"agency_code\", sa.Text(), nullable=False),\n        sa.Column(\"sub_agency_code\", sa.Text(), nullable=True),\n        sa.Column(\"assistance_listing_number\", sa.Text(), nullable=False),\n        sa.Column(\"agency_submission_notification_setting_id\", sa.Integer(), nullable=False),\n        sa.Column(\"agency_contact_info_id\", sa.BigInteger(), nullable=True),\n        sa.Column(\"is_test_agency\", sa.Boolean(), nullable=False),\n        sa.Column(\"ldap_group\", sa.Text(), nullable=False),\n        sa.Column(\"description\", sa.Text(), nullable=False),\n        sa.Column(\"label\", sa.Text(), nullable=False),\n        sa.Column(\"is_multilevel_agency\", sa.Boolean(), nullable=False),\n        sa.Column(\"is_multiproject\", sa.Boolean(), nullable=False),\n        sa.Column(\"has_system_to_system_certificate\", sa.Boolean(), nullable=False),\n        sa.Column(\"can_view_packages_in_grace_period\", sa.Boolean(), nullable=False),\n        sa.Column(\"is_image_workspace_enabled\", sa.Boolean(), nullable=False),\n        sa.Column(\"is_validation_workspace_enabled\", sa.Boolean(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"agency_contact_info_id\"],\n            [\"api.agency_contact_info.agency_contact_info_id\"],\n            name=op.f(\"agency_agency_contact_info_id_agency_contact_info_fkey\"),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"agency_submission_notification_setting_id\"],\n            [\n                \"api.lk_agency_submission_notification_setting.agency_submission_notification_setting_id\"\n            ],\n            name=op.f(\n                \"agency_agency_submission_notification_setting_id_lk_agency_submission_notification_setting_fkey\"\n            ),\n        ),\n        sa.PrimaryKeyConstraint(\"agency_id\", name=op.f(\"agency_pkey\")),\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"agency_agency_code_idx\"), \"agency\", [\"agency_code\"], unique=True, schema=\"api\"\n    )\n    op.create_table(\n        \"link_agency_download_file_type\",\n        sa.Column(\"agency_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"agency_download_file_type_id\", sa.Integer(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"agency_download_file_type_id\"],\n            [\"api.lk_agency_download_file_type.agency_download_file_type_id\"],\n            name=op.f(\n                \"link_agency_download_file_type_agency_download_file_type_id_lk_agency_download_file_type_fkey\"\n            ),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"agency_id\"],\n            [\"api.agency.agency_id\"],\n            name=op.f(\"link_agency_download_file_type_agency_id_agency_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\n            \"agency_id\",\n            \"agency_download_file_type_id\",\n            name=op.f(\"link_agency_download_file_type_pkey\"),\n        ),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"tgroups\",\n        sa.Column(\"keyfield\", sa.Text(), nullable=False),\n        sa.Column(\"value\", sa.Text(), nullable=True),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\"deleted_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        sa.PrimaryKeyConstraint(\"keyfield\", name=op.f(\"tgroups_pkey\")),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tgroups_transformed_at_idx\"),\n        \"tgroups\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"tgroups_transformed_at_idx\"), table_name=\"tgroups\", schema=\"staging\")\n    op.drop_table(\"tgroups\", schema=\"staging\")\n    op.drop_table(\"link_agency_download_file_type\", schema=\"api\")\n    op.drop_index(op.f(\"agency_agency_code_idx\"), table_name=\"agency\", schema=\"api\")\n    op.drop_table(\"agency\", schema=\"api\")\n    op.drop_table(\"lk_agency_submission_notification_setting\", schema=\"api\")\n    op.drop_table(\"lk_agency_download_file_type\", schema=\"api\")\n    op.drop_table(\"agency_contact_info\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/get-involved/communication-channels/slack-community-chat/README.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/communication-channels/slack-community-chat","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/communication-channels/slack-community-chat/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_10_01_make_number_of_awards_a_big_int.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_01_make_number_of_awards_a_big_int.py\nSize: 0.96 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/get-involved/communication-channels/slack-community-chat/installing-slack.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/communication-channels/slack-community-chat","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/communication-channels/slack-community-chat/installing-slack.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: fa38970d0cef\nRevises: 4f7acbb61548\nCreate Date: 2024-10-01 14:06:12.736411\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"fa38970d0cef\"\ndown_revision = \"4f7acbb61548\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"opportunity_summary\",\n        \"expected_number_of_awards\",\n        existing_type=sa.INTEGER(),\n        type_=sa.BigInteger(),\n        existing_nullable=True,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"opportunity_summary\",\n        \"expected_number_of_awards\",\n        existing_type=sa.BigInteger(),\n        type_=sa.INTEGER(),\n        existing_nullable=True,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/get-involved/communication-channels/slack-community-chat/naming-conventions.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/communication-channels/slack-community-chat","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/communication-channels/slack-community-chat/naming-conventions.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_10_16_add_opportunity_attachment_tables.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_16_add_opportunity_attachment_tables.py\nSize: 3.81 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/get-involved/communication-channels/slack-community-chat/recommended-channels.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/communication-channels/slack-community-chat","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/communication-channels/slack-community-chat/recommended-channels.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 56448a3ecb8f\nRevises: fa38970d0cef\nCreate Date: 2024-10-16 22:00:42.274537\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"56448a3ecb8f\"\ndown_revision = \"fa38970d0cef\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"lk_opportunity_attachment_type\",\n        sa.Column(\"opportunity_attachment_type_id\", sa.Integer(), nullable=False),\n        sa.Column(\"description\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_attachment_type_id\", name=op.f(\"lk_opportunity_attachment_type_pkey\")\n        ),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"opportunity_attachment\",\n        sa.Column(\"attachment_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"opportunity_attachment_type_id\", sa.Integer(), nullable=False),\n        sa.Column(\"file_location\", sa.Text(), nullable=False),\n        sa.Column(\"mime_type\", sa.Text(), nullable=False),\n        sa.Column(\"file_name\", sa.Text(), nullable=False),\n        sa.Column(\"file_description\", sa.Text(), nullable=False),\n        sa.Column(\"file_size_bytes\", sa.BigInteger(), nullable=False),\n        sa.Column(\"created_by\", sa.Text(), nullable=True),\n        sa.Column(\"updated_by\", sa.Text(), nullable=True),\n        sa.Column(\"legacy_folder_id\", sa.BigInteger(), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_attachment_type_id\"],\n            [\"api.lk_opportunity_attachment_type.opportunity_attachment_type_id\"],\n            name=op.f(\n                \"opportunity_attachment_opportunity_attachment_type_id_lk_opportunity_attachment_type_fkey\"\n            ),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=op.f(\"opportunity_attachment_opportunity_id_opportunity_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\"attachment_id\", name=op.f(\"opportunity_attachment_pkey\")),\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"opportunity_attachment_opportunity_attachment_type_id_idx\"),\n        \"opportunity_attachment\",\n        [\"opportunity_attachment_type_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"opportunity_attachment_opportunity_id_idx\"),\n        \"opportunity_attachment\",\n        [\"opportunity_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\n        op.f(\"opportunity_attachment_opportunity_id_idx\"),\n        table_name=\"opportunity_attachment\",\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"opportunity_attachment_opportunity_attachment_type_id_idx\"),\n        table_name=\"opportunity_attachment\",\n        schema=\"api\",\n    )\n    op.drop_table(\"opportunity_attachment\", schema=\"api\")\n    op.drop_table(\"lk_opportunity_attachment_type\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/get-involved/communication-channels/zoom-public-meetings.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/communication-channels","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/communication-channels/zoom-public-meetings.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_10_17_add_top_level_agency_field.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_17_add_top_level_agency_field.py\nSize: 1.08 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/get-involved/community-events/README.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/community-events","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/community-events/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 39f7f941fc6c\nRevises: 558ad9563e9a\nCreate Date: 2024-10-17 20:08:55.193636\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"39f7f941fc6c\"\ndown_revision = \"558ad9563e9a\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"agency\", sa.Column(\"top_level_agency_id\", sa.BigInteger(), nullable=True), schema=\"api\"\n    )\n    op.create_foreign_key(\n        op.f(\"agency_top_level_agency_id_agency_fkey\"),\n        \"agency\",\n        \"agency\",\n        [\"top_level_agency_id\"],\n        [\"agency_id\"],\n        source_schema=\"api\",\n        referent_schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_constraint(\n        op.f(\"agency_top_level_agency_id_agency_fkey\"), \"agency\", schema=\"api\", type_=\"foreignkey\"\n    )\n    op.drop_column(\"agency\", \"top_level_agency_id\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/get-involved/community-events/fall-2024-coding-challenge.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/community-events","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/community-events/fall-2024-coding-challenge.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_10_17_legacy_null_agency.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_17_legacy_null_agency.py\nSize: 1.10 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/get-involved/get-involved.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/get-involved.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 558ad9563e9a\nRevises: 56448a3ecb8f\nCreate Date: 2024-10-17 13:54:32.420425\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"558ad9563e9a\"\ndown_revision = \"56448a3ecb8f\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\"agency\", \"ldap_group\", existing_type=sa.TEXT(), nullable=True, schema=\"api\")\n    op.alter_column(\"agency\", \"description\", existing_type=sa.TEXT(), nullable=True, schema=\"api\")\n    op.alter_column(\"agency\", \"label\", existing_type=sa.TEXT(), nullable=True, schema=\"api\")\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\"agency\", \"label\", existing_type=sa.TEXT(), nullable=False, schema=\"api\")\n    op.alter_column(\"agency\", \"description\", existing_type=sa.TEXT(), nullable=False, schema=\"api\")\n    op.alter_column(\"agency\", \"ldap_group\", existing_type=sa.TEXT(), nullable=False, schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/get-involved/github-code.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/github-code.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_10_28_add_opportunity_search_index_queue_table.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_28_add_opportunity_search_index_queue_table.py\nSize: 1.84 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/get-involved/github-planning/README.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/github-planning","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/github-planning/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: a2e9144cdc6b\nRevises: 39f7f941fc6c\nCreate Date: 2024-10-28 17:09:18.569197\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"a2e9144cdc6b\"\ndown_revision = \"39f7f941fc6c\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"opportunity_search_index_queue\",\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"has_update\", sa.Boolean(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=op.f(\"opportunity_search_index_queue_opportunity_id_opportunity_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=op.f(\"opportunity_search_index_queue_pkey\")),\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"opportunity_search_index_queue_opportunity_id_idx\"),\n        \"opportunity_search_index_queue\",\n        [\"opportunity_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\n        op.f(\"opportunity_search_index_queue_opportunity_id_idx\"),\n        table_name=\"opportunity_search_index_queue\",\n        schema=\"api\",\n    )\n    op.drop_table(\"opportunity_search_index_queue\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/get-involved/github-planning/filling-out-a-bug-report.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/github-planning","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/github-planning/filling-out-a-bug-report.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_10_28_add_opportunity_table_triggers.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_28_add_opportunity_table_triggers.py\nSize: 2.53 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/get-involved/github-planning/filling-out-a-feature-request.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/github-planning","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/github-planning/filling-out-a-feature-request.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: a8ebde13a18a\nRevises: a2e9144cdc6b\nCreate Date: 2024-10-28 17:48:02.678523\n\n\"\"\"\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"a8ebde13a18a\"\ndown_revision = \"a2e9144cdc6b\"\nbranch_labels = None\ndepends_on = None\n\ncreate_trigger_function = \"\"\"\nCREATE OR REPLACE FUNCTION update_opportunity_search_queue()\nRETURNS TRIGGER AS $$\nDECLARE\n    opp_id bigint;\nBEGIN\n    -- Determine the opportunity_id based on the table\n    CASE TG_TABLE_NAME\n        WHEN 'link_opportunity_summary_funding_instrument' THEN\n            opp_id := (SELECT opportunity_id FROM api.opportunity_summary WHERE opportunity_summary_id = NEW.opportunity_summary_id);\n        WHEN 'link_opportunity_summary_funding_category' THEN\n            opp_id := (SELECT opportunity_id FROM api.opportunity_summary WHERE opportunity_summary_id = NEW.opportunity_summary_id);\n        WHEN 'link_opportunity_summary_applicant_type' THEN\n            opp_id := (SELECT opportunity_id FROM api.opportunity_summary WHERE opportunity_summary_id = NEW.opportunity_summary_id);\n        WHEN 'opportunity_summary' THEN\n            opp_id := NEW.opportunity_id;\n        WHEN 'current_opportunity_summary' THEN\n            opp_id := NEW.opportunity_id;\n        ELSE\n            opp_id := NEW.opportunity_id;\n    END CASE;\n\n    INSERT INTO api.opportunity_search_index_queue (opportunity_id, has_update)\n    VALUES (opp_id, TRUE)\n    ON CONFLICT (opportunity_id)\n    DO UPDATE SET has_update = TRUE, updated_at = CURRENT_TIMESTAMP;\n\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\"\"\"\n\n# List of tables that need triggers\ntables = [\n    \"opportunity\",\n    \"opportunity_assistance_listing\",\n    \"current_opportunity_summary\",\n    \"opportunity_summary\",\n    \"link_opportunity_summary_funding_instrument\",\n    \"link_opportunity_summary_funding_category\",\n    \"link_opportunity_summary_applicant_type\",\n    \"opportunity_attachment\",\n]\n\n\ndef upgrade():\n    # Create the trigger function\n    op.execute(create_trigger_function)\n\n    # Create triggers for each table\n    for table in tables:\n        op.execute(\n            f\"\"\"\n            CREATE TRIGGER {table}_queue_trigger\n            AFTER INSERT OR UPDATE ON api.{table}\n            FOR EACH ROW EXECUTE FUNCTION update_opportunity_search_queue();\n        \"\"\"\n        )\n\n\ndef downgrade():\n    # Drop triggers\n    for table in tables:\n        op.execute(f\"DROP TRIGGER IF EXISTS {table}_queue_trigger ON api.{table};\")\n\n    # Drop the trigger function\n    op.execute(\"DROP FUNCTION IF EXISTS update_opportunity_search_queue();\")"}
{"path":"documentation/wiki/get-involved/github-planning/reporting-a-security-vulnerability.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/github-planning","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/github-planning/reporting-a-security-vulnerability.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_10_31_remove_has_update_column_from_.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_10_31_remove_has_update_column_from_.py\nSize: 3.23 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/get-involved/policies-and-guidelines/README.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/policies-and-guidelines","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/policies-and-guidelines/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 8b96ade6f6a2\nRevises: a8ebde13a18a\nCreate Date: 2024-10-31 16:57:43.256710\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"8b96ade6f6a2\"\ndown_revision = \"a8ebde13a18a\"\nbranch_labels = None\ndepends_on = None\n\n\ncreate_old_trigger_function = \"\"\"\nCREATE OR REPLACE FUNCTION update_opportunity_search_queue()\nRETURNS TRIGGER AS $$\nDECLARE\n    opp_id bigint;\nBEGIN\n    -- Determine the opportunity_id based on the table\n    CASE TG_TABLE_NAME\n        WHEN 'link_opportunity_summary_funding_instrument' THEN\n            opp_id := (SELECT opportunity_id FROM api.opportunity_summary WHERE opportunity_summary_id = NEW.opportunity_summary_id);\n        WHEN 'link_opportunity_summary_funding_category' THEN\n            opp_id := (SELECT opportunity_id FROM api.opportunity_summary WHERE opportunity_summary_id = NEW.opportunity_summary_id);\n        WHEN 'link_opportunity_summary_applicant_type' THEN\n            opp_id := (SELECT opportunity_id FROM api.opportunity_summary WHERE opportunity_summary_id = NEW.opportunity_summary_id);\n        WHEN 'opportunity_summary' THEN\n            opp_id := NEW.opportunity_id;\n        WHEN 'current_opportunity_summary' THEN\n            opp_id := NEW.opportunity_id;\n        ELSE\n            opp_id := NEW.opportunity_id;\n    END CASE;\n\n    INSERT INTO api.opportunity_search_index_queue (opportunity_id, has_update)\n    VALUES (opp_id, TRUE)\n    ON CONFLICT (opportunity_id)\n    DO UPDATE SET has_update = TRUE, updated_at = CURRENT_TIMESTAMP;\n\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\"\"\"\n\ncreate_trigger_function = \"\"\"\nCREATE OR REPLACE FUNCTION update_opportunity_search_queue()\nRETURNS TRIGGER AS $$\nDECLARE\n    opp_id bigint;\nBEGIN\n    -- Determine the opportunity_id based on the table\n    CASE TG_TABLE_NAME\n        WHEN 'link_opportunity_summary_funding_instrument' THEN\n            opp_id := (SELECT opportunity_id FROM api.opportunity_summary WHERE opportunity_summary_id = NEW.opportunity_summary_id);\n        WHEN 'link_opportunity_summary_funding_category' THEN\n            opp_id := (SELECT opportunity_id FROM api.opportunity_summary WHERE opportunity_summary_id = NEW.opportunity_summary_id);\n        WHEN 'link_opportunity_summary_applicant_type' THEN\n            opp_id := (SELECT opportunity_id FROM api.opportunity_summary WHERE opportunity_summary_id = NEW.opportunity_summary_id);\n        WHEN 'opportunity_summary' THEN\n            opp_id := NEW.opportunity_id;\n        WHEN 'current_opportunity_summary' THEN\n            opp_id := NEW.opportunity_id;\n        ELSE\n            opp_id := NEW.opportunity_id;\n    END CASE;\n\n    INSERT INTO api.opportunity_search_index_queue (opportunity_id)\n    VALUES (opp_id)\n    ON CONFLICT (opportunity_id)\n    DO NOTHING;\n\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\"\"\"\n\n\ndef upgrade():\n    # Update the trigger function\n    op.execute(create_trigger_function)\n\n    op.drop_column(\"opportunity_search_index_queue\", \"has_update\", schema=\"api\")\n\n\ndef downgrade():\n    op.execute(create_old_trigger_function)\n\n    op.add_column(\n        \"opportunity_search_index_queue\",\n        sa.Column(\"has_update\", sa.BOOLEAN(), autoincrement=False, nullable=False),\n        schema=\"api\",\n    )"}
{"path":"documentation/wiki/get-involved/policies-and-guidelines/code-of-conduct.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/policies-and-guidelines","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/policies-and-guidelines/code-of-conduct.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_11_01_rename_agency_column.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_11_01_rename_agency_column.py\nSize: 1.13 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/get-involved/policies-and-guidelines/content-privacy.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/policies-and-guidelines","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/policies-and-guidelines/content-privacy.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 3640e31e6a85\nRevises: 8b96ade6f6a2\nCreate Date: 2024-11-01 12:57:11.887858\n\n\"\"\"\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"3640e31e6a85\"\ndown_revision = \"8b96ade6f6a2\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"opportunity_agency_idx\", table_name=\"opportunity\", schema=\"api\")\n    op.alter_column(\"opportunity\", \"agency\", new_column_name=\"agency_code\", schema=\"api\")\n    op.create_index(\n        op.f(\"opportunity_agency_code_idx\"),\n        \"opportunity\",\n        [\"agency_code\"],\n        unique=False,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"opportunity_agency_code_idx\"), table_name=\"opportunity\", schema=\"api\")\n    op.alter_column(\"opportunity\", \"agency_code\", new_column_name=\"agency\", schema=\"api\")\n    op.create_index(\n        \"opportunity_agency_idx\", \"opportunity\", [\"agency_code\"], unique=False, schema=\"api\"\n    )\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/get-involved/policies-and-guidelines/incident-response-protocol.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/policies-and-guidelines","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/policies-and-guidelines/incident-response-protocol.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_11_12_add_basic_user_tables.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_11_12_add_basic_user_tables.py\nSize: 3.97 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/get-involved/policies-and-guidelines/reporting-and-removing-content.md","language":"markdown","type":"code","directory":"documentation/wiki/get-involved/policies-and-guidelines","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/policies-and-guidelines/reporting-and-removing-content.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 65e962033cc6\nRevises: 3640e31e6a85\nCreate Date: 2024-11-12 17:14:12.947794\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"65e962033cc6\"\ndown_revision = \"3640e31e6a85\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"lk_external_user_type\",\n        sa.Column(\"external_user_type_id\", sa.Integer(), nullable=False),\n        sa.Column(\"description\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"external_user_type_id\", name=op.f(\"lk_external_user_type_pkey\")),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"user\",\n        sa.Column(\"user_id\", sa.UUID(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"user_id\", name=op.f(\"user_pkey\")),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"link_external_user\",\n        sa.Column(\"link_external_user_id\", sa.UUID(), nullable=False),\n        sa.Column(\"external_user_id\", sa.Text(), nullable=False),\n        sa.Column(\"user_id\", sa.UUID(), nullable=False),\n        sa.Column(\"external_user_type_id\", sa.Integer(), nullable=False),\n        sa.Column(\"email\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"external_user_type_id\"],\n            [\"api.lk_external_user_type.external_user_type_id\"],\n            name=op.f(\"link_external_user_external_user_type_id_lk_external_user_type_fkey\"),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"], [\"api.user.user_id\"], name=op.f(\"link_external_user_user_id_user_fkey\")\n        ),\n        sa.PrimaryKeyConstraint(\"link_external_user_id\", name=op.f(\"link_external_user_pkey\")),\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"link_external_user_external_user_id_idx\"),\n        \"link_external_user\",\n        [\"external_user_id\"],\n        unique=True,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"link_external_user_external_user_type_id_idx\"),\n        \"link_external_user\",\n        [\"external_user_type_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"link_external_user_user_id_idx\"),\n        \"link_external_user\",\n        [\"user_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\n        op.f(\"link_external_user_user_id_idx\"), table_name=\"link_external_user\", schema=\"api\"\n    )\n    op.drop_index(\n        op.f(\"link_external_user_external_user_type_id_idx\"),\n        table_name=\"link_external_user\",\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"link_external_user_external_user_id_idx\"),\n        table_name=\"link_external_user\",\n        schema=\"api\",\n    )\n    op.drop_table(\"link_external_user\", schema=\"api\")\n    op.drop_table(\"user\", schema=\"api\")\n    op.drop_table(\"lk_external_user_type\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/README.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_11_15_add_metadata_extract_table.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_11_15_add_metadata_extract_table.py\nSize: 2.28 KB\nLast Modified: 2025-02-14T17:08:26.441Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-06-26-recording-architecture-decisions.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-06-26-recording-architecture-decisions.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 7346f6b52c3d\nRevises: 65e962033cc6\nCreate Date: 2024-11-15 20:06:40.422630\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"7346f6b52c3d\"\ndown_revision = \"65e962033cc6\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"lk_extract_type\",\n        sa.Column(\"extract_type_id\", sa.Integer(), nullable=False),\n        sa.Column(\"description\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"extract_type_id\", name=op.f(\"lk_extract_type_pkey\")),\n        schema=\"api\",\n    )\n\n    op.create_table(\n        \"extract_metadata\",\n        sa.Column(\"extract_metadata_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"extract_type_id\", sa.Integer(), nullable=False),\n        sa.Column(\"file_name\", sa.Text(), nullable=False),\n        sa.Column(\"file_path\", sa.Text(), nullable=False),\n        sa.Column(\"file_size_bytes\", sa.BigInteger(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"extract_type_id\"],\n            [\"api.lk_extract_type.extract_type_id\"],\n            name=op.f(\"extract_metadata_extract_type_id_lk_extract_type_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\"extract_metadata_id\", name=op.f(\"extract_metadata_pkey\")),\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"extract_metadata\", schema=\"api\")\n    op.drop_table(\"lk_extract_type\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-06-29-ci-cd-task-runner.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-06-29-ci-cd-task-runner.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_11_18_user_session_table.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_11_18_user_session_table.py\nSize: 1.47 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-06-30-api-language.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-06-30-api-language.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 16eaca2334c9\nRevises: 7346f6b52c3d\nCreate Date: 2024-11-18 13:10:37.039657\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"16eaca2334c9\"\ndown_revision = \"7346f6b52c3d\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"user_token_session\",\n        sa.Column(\"user_id\", sa.UUID(), nullable=False),\n        sa.Column(\"token_id\", sa.UUID(), nullable=False),\n        sa.Column(\"expires_at\", sa.TIMESTAMP(timezone=True), nullable=False),\n        sa.Column(\"is_valid\", sa.Boolean(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"], [\"api.user.user_id\"], name=op.f(\"user_token_session_user_id_user_fkey\")\n        ),\n        sa.PrimaryKeyConstraint(\"user_id\", \"token_id\", name=op.f(\"user_token_session_pkey\")),\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"user_token_session\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-03-design-prototyping-tool.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-03-design-prototyping-tool.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_12_04_login_gov_state.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_12_04_login_gov_state.py\nSize: 1.19 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-05-chat-adr.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-05-chat-adr.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 6a23520d2c3c\nRevises: 16eaca2334c9\nCreate Date: 2024-12-04 16:35:29.200758\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"6a23520d2c3c\"\ndown_revision = \"16eaca2334c9\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"login_gov_state\",\n        sa.Column(\"login_gov_state_id\", sa.UUID(), nullable=False),\n        sa.Column(\"nonce\", sa.UUID(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"login_gov_state_id\", name=op.f(\"login_gov_state_pkey\")),\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"login_gov_state\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-05-db-choices.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-05-db-choices.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_12_19_add_attachment_table_staging_and_foreign.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_12_19_add_attachment_table_staging_and_foreign.py\nSize: 2.75 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-07-api-framework.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-07-api-framework.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: f8058a6c0a66\nRevises: 6a23520d2c3c\nCreate Date: 2024-12-19 19:27:50.327943\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"f8058a6c0a66\"\ndown_revision = \"6a23520d2c3c\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"tsynopsisattachment\",\n        sa.Column(\"syn_att_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\"att_revision_number\", sa.BigInteger(), nullable=True),\n        sa.Column(\"att_type\", sa.Text(), nullable=True),\n        sa.Column(\"mime_type\", sa.Text(), nullable=True),\n        sa.Column(\"link_url\", sa.Text(), nullable=True),\n        sa.Column(\"file_name\", sa.Text(), nullable=True),\n        sa.Column(\"file_desc\", sa.Text(), nullable=True),\n        sa.Column(\"file_lob\", sa.LargeBinary(), nullable=True),\n        sa.Column(\"file_lob_size\", sa.BigInteger(), nullable=True),\n        sa.Column(\"create_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"created_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"last_upd_date\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"creator_id\", sa.Text(), nullable=True),\n        sa.Column(\"last_upd_id\", sa.Text(), nullable=True),\n        sa.Column(\"syn_att_folder_id\", sa.BigInteger(), nullable=True),\n        sa.Column(\"is_deleted\", sa.Boolean(), nullable=False),\n        sa.Column(\"transformed_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\"deleted_at\", sa.TIMESTAMP(timezone=True), nullable=True),\n        sa.Column(\"transformation_notes\", sa.Text(), nullable=True),\n        sa.PrimaryKeyConstraint(\"syn_att_id\", name=op.f(\"tsynopsisattachment_pkey\")),\n        schema=\"staging\",\n    )\n    op.create_index(\n        op.f(\"tsynopsisattachment_transformed_at_idx\"),\n        \"tsynopsisattachment\",\n        [\"transformed_at\"],\n        unique=False,\n        schema=\"staging\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\n        op.f(\"tsynopsisattachment_transformed_at_idx\"),\n        table_name=\"tsynopsisattachment\",\n        schema=\"staging\",\n    )\n    op.drop_table(\"tsynopsisattachment\", schema=\"staging\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-07-backend-tooling.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-07-backend-tooling.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2024_12_19_create_user_saved_opportunity_table.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2024_12_19_create_user_saved_opportunity_table.py\nSize: 1.60 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-10-front-end-language.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-10-front-end-language.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 232a9223ed9b\nRevises: 6a23520d2c3c\nCreate Date: 2024-12-19 22:41:02.487364\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"232a9223ed9b\"\ndown_revision = \"f8058a6c0a66\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"user_saved_opportunity\",\n        sa.Column(\"user_id\", sa.UUID(), nullable=False),\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"], [\"api.user.user_id\"], name=op.f(\"user_saved_opportunity_user_id_user_fkey\")\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=op.f(\"user_saved_opportunity_opportunity_id_opportunity_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\n            \"user_id\", \"opportunity_id\", name=op.f(\"user_saved_opportunity_pkey\")\n        ),\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"user_saved_opportunity\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-10-wiki-platform.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-10-wiki-platform.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2025_01_06_remove_transfertopportunity_models.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_01_06_remove_transfertopportunity_models.py\nSize: 3.45 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-11-design-diagramming-tool.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-11-design-diagramming-tool.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 99bb8e01ad38\nRevises: 232a9223ed9b\nCreate Date: 2025-01-06 21:25:41.127922\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"99bb8e01ad38\"\ndown_revision = \"cc2912373d5b\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\n        \"transfer_topportunity_is_draft_idx\", table_name=\"transfer_topportunity\", schema=\"api\"\n    )\n    op.drop_index(\n        \"transfer_topportunity_oppcategory_idx\", table_name=\"transfer_topportunity\", schema=\"api\"\n    )\n    op.drop_index(\n        \"transfer_topportunity_opptitle_idx\", table_name=\"transfer_topportunity\", schema=\"api\"\n    )\n    op.drop_table(\"transfer_topportunity\", schema=\"api\")\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"transfer_topportunity\",\n        sa.Column(\"opportunity_id\", sa.INTEGER(), autoincrement=True, nullable=False),\n        sa.Column(\"oppnumber\", sa.VARCHAR(length=160), autoincrement=False, nullable=True),\n        sa.Column(\"opptitle\", sa.VARCHAR(length=1020), autoincrement=False, nullable=True),\n        sa.Column(\"owningagency\", sa.VARCHAR(length=1020), autoincrement=False, nullable=True),\n        sa.Column(\"oppcategory\", sa.VARCHAR(length=4), autoincrement=False, nullable=True),\n        sa.Column(\n            \"category_explanation\", sa.VARCHAR(length=1020), autoincrement=False, nullable=True\n        ),\n        sa.Column(\"is_draft\", sa.VARCHAR(length=4), autoincrement=False, nullable=False),\n        sa.Column(\"revision_number\", sa.INTEGER(), autoincrement=False, nullable=True),\n        sa.Column(\"modified_comments\", sa.VARCHAR(length=4000), autoincrement=False, nullable=True),\n        sa.Column(\"publisheruid\", sa.VARCHAR(length=1020), autoincrement=False, nullable=True),\n        sa.Column(\"publisher_profile_id\", sa.INTEGER(), autoincrement=False, nullable=True),\n        sa.Column(\"last_upd_id\", sa.VARCHAR(length=200), autoincrement=False, nullable=True),\n        sa.Column(\"last_upd_date\", sa.DATE(), autoincrement=False, nullable=True),\n        sa.Column(\"creator_id\", sa.VARCHAR(length=200), autoincrement=False, nullable=True),\n        sa.Column(\"created_date\", sa.DATE(), autoincrement=False, nullable=True),\n        sa.Column(\n            \"created_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=\"transfer_topportunity_pkey\"),\n        schema=\"api\",\n    )\n    op.create_index(\n        \"transfer_topportunity_opptitle_idx\",\n        \"transfer_topportunity\",\n        [\"opptitle\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        \"transfer_topportunity_oppcategory_idx\",\n        \"transfer_topportunity\",\n        [\"oppcategory\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.create_index(\n        \"transfer_topportunity_is_draft_idx\",\n        \"transfer_topportunity\",\n        [\"is_draft\"],\n        unique=False,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-11-ticket-tracking.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-11-ticket-tracking.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2025_01_08_create_saved_searches_table.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_01_08_create_saved_searches_table.py\nSize: 1.80 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-14-front-end-framework.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-14-front-end-framework.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: cc2912373d5b\nRevises: 232a9223ed9b\nCreate Date: 2025-01-08 16:20:40.898481\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"cc2912373d5b\"\ndown_revision = \"232a9223ed9b\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"user_saved_search\",\n        sa.Column(\"saved_search_id\", sa.UUID(), nullable=False),\n        sa.Column(\"user_id\", sa.UUID(), nullable=False),\n        sa.Column(\"search_query\", postgresql.JSONB(astext_type=sa.Text()), nullable=False),\n        sa.Column(\"name\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"], [\"api.user.user_id\"], name=op.f(\"user_saved_search_user_id_user_fkey\")\n        ),\n        sa.PrimaryKeyConstraint(\"saved_search_id\", name=op.f(\"user_saved_search_pkey\")),\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"user_saved_search_user_id_idx\"),\n        \"user_saved_search\",\n        [\"user_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\n        op.f(\"user_saved_search_user_id_idx\"), table_name=\"user_saved_search\", schema=\"api\"\n    )\n    op.drop_table(\"user_saved_search\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-17-frontend-tooling.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-17-frontend-tooling.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2025_01_16_rename_tables_and_create_job_table.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_01_16_rename_tables_and_create_job_table.py\nSize: 7.14 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-18-frontend-testing.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-18-frontend-testing.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: dc04ce955a9a\nRevises: 99bb8e01ad38\nCreate Date: 2025-01-16 18:34:48.013913\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\nfrom sqlalchemy.sql import text\n\n# revision identifiers, used by Alembic.\nrevision = \"dc04ce955a9a\"\ndown_revision = \"fe052c05c757\"\nbranch_labels = None\ndepends_on = None\n\n\ncreate_trigger_function = \"\"\"\nCREATE OR REPLACE FUNCTION update_opportunity_search_queue()\nRETURNS TRIGGER AS $$\nDECLARE\n    opp_id bigint;\nBEGIN\n    -- Determine the opportunity_id based on the table\n    CASE TG_TABLE_NAME\n        WHEN 'link_opportunity_summary_funding_instrument' THEN\n            opp_id := (SELECT opportunity_id FROM api.opportunity_summary WHERE opportunity_summary_id = NEW.opportunity_summary_id);\n        WHEN 'link_opportunity_summary_funding_category' THEN\n            opp_id := (SELECT opportunity_id FROM api.opportunity_summary WHERE opportunity_summary_id = NEW.opportunity_summary_id);\n        WHEN 'link_opportunity_summary_applicant_type' THEN\n            opp_id := (SELECT opportunity_id FROM api.opportunity_summary WHERE opportunity_summary_id = NEW.opportunity_summary_id);\n        WHEN 'opportunity_summary' THEN\n            opp_id := NEW.opportunity_id;\n        WHEN 'current_opportunity_summary' THEN\n            opp_id := NEW.opportunity_id;\n        ELSE\n            opp_id := NEW.opportunity_id;\n    END CASE;\n\n    INSERT INTO api.opportunity_change_audit (opportunity_id)\n    VALUES (opp_id)\n    ON CONFLICT (opportunity_id)\n    DO UPDATE SET updated_at = CURRENT_TIMESTAMP;\n\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\"\"\"\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"job_log\",\n        sa.Column(\"job_id\", sa.UUID(), nullable=False),\n        sa.Column(\"job_type\", sa.Text(), nullable=False),\n        sa.Column(\n            \"job_status\",\n            sa.Text(),\n            nullable=False,\n        ),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"job_id\", name=op.f(\"job_pkey\")),\n        schema=\"api\",\n    )\n    op.create_table(\n        \"opportunity_change_audit\",\n        sa.Column(\"opportunity_id\", sa.BigInteger(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=op.f(\"opportunity_change_audit_opportunity_id_opportunity_fkey\"),\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=op.f(\"opportunity_change_audit_pkey\")),\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"opportunity_change_audit_opportunity_id_idx\"),\n        \"opportunity_change_audit\",\n        [\"opportunity_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n\n    op.execute(create_trigger_function)\n\n    # Insert all existing opportunities into the audit table\n    op.execute(\n        text(\n            \"\"\"\n            INSERT INTO api.opportunity_change_audit (opportunity_id, created_at, updated_at)\n            SELECT\n                opportunity_id,\n                CURRENT_TIMESTAMP as created_at,\n                CURRENT_TIMESTAMP as updated_at\n            FROM api.opportunity\n            ON CONFLICT (opportunity_id) DO NOTHING\n            \"\"\"\n        )\n    )\n\n    op.drop_index(\n        \"opportunity_search_index_queue_opportunity_id_idx\",\n        table_name=\"opportunity_search_index_queue\",\n        schema=\"api\",\n    )\n    op.drop_table(\"opportunity_search_index_queue\", schema=\"api\")\n\n    op.create_table(\n        \"lk_job_status\",\n        sa.Column(\"job_status_id\", sa.Integer(), nullable=False),\n        sa.Column(\"description\", sa.Text(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\"job_status_id\", name=op.f(\"lk_job_status_pkey\")),\n        schema=\"api\",\n    )\n    op.add_column(\"job_log\", sa.Column(\"job_status_id\", sa.Integer(), nullable=False), schema=\"api\")\n    op.add_column(\n        \"job_log\",\n        sa.Column(\"metrics\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        schema=\"api\",\n    )\n    op.create_foreign_key(\n        op.f(\"job_log_job_status_id_lk_job_status_fkey\"),\n        \"job_log\",\n        \"lk_job_status\",\n        [\"job_status_id\"],\n        [\"job_status_id\"],\n        source_schema=\"api\",\n        referent_schema=\"api\",\n    )\n    op.drop_column(\"job_log\", \"job_status\", schema=\"api\")\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"job_log\",\n        sa.Column(\"job_status\", sa.TEXT(), autoincrement=False, nullable=False),\n        schema=\"api\",\n    )\n    op.drop_constraint(\n        op.f(\"job_job_status_id_lk_job_status_fkey\"), \"job_log\", schema=\"api\", type_=\"foreignkey\"\n    )\n    op.drop_column(\"job_log\", \"metrics\", schema=\"api\")\n    op.drop_column(\"job_log\", \"job_status_id\", schema=\"api\")\n    op.drop_table(\"lk_job_status\", schema=\"api\")\n    op.create_table(\n        \"opportunity_search_index_queue\",\n        sa.Column(\"opportunity_id\", sa.BIGINT(), autoincrement=False, nullable=False),\n        sa.Column(\n            \"created_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"opportunity_id\"],\n            [\"api.opportunity.opportunity_id\"],\n            name=\"opportunity_search_index_queue_opportunity_id_opportunity_fkey\",\n        ),\n        sa.PrimaryKeyConstraint(\"opportunity_id\", name=\"opportunity_search_index_queue_pkey\"),\n        schema=\"api\",\n    )\n    op.create_index(\n        \"opportunity_search_index_queue_opportunity_id_idx\",\n        \"opportunity_search_index_queue\",\n        [\"opportunity_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    op.drop_index(\n        op.f(\"opportunity_change_audit_opportunity_id_idx\"),\n        table_name=\"opportunity_change_audit\",\n        schema=\"api\",\n    )\n    op.drop_table(\"opportunity_change_audit\", schema=\"api\")\n    op.drop_table(\"job_log\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-19-backend-api-type.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-19-backend-api-type.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2025_01_22_notification_table.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_01_22_notification_table.py\nSize: 1.81 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-19-backend-testing.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-19-backend-testing.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: fe052c05c757\nRevises: 99bb8e01ad38\nCreate Date: 2025-01-22 13:01:54.955189\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"fe052c05c757\"\ndown_revision = \"99bb8e01ad38\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"user_notification_log\",\n        sa.Column(\"user_notification_log_id\", sa.UUID(), nullable=False),\n        sa.Column(\"user_id\", sa.UUID(), nullable=False),\n        sa.Column(\"notification_reason\", sa.Text(), nullable=False),\n        sa.Column(\"notification_sent\", sa.Boolean(), nullable=False),\n        sa.Column(\n            \"created_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"], [\"api.user.user_id\"], name=op.f(\"user_notification_log_user_id_user_fkey\")\n        ),\n        sa.PrimaryKeyConstraint(\n            \"user_notification_log_id\", name=op.f(\"user_notification_log_pkey\")\n        ),\n        schema=\"api\",\n    )\n    op.create_index(\n        op.f(\"user_notification_log_user_id_idx\"),\n        \"user_notification_log\",\n        [\"user_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\n        op.f(\"user_notification_log_user_id_idx\"), table_name=\"user_notification_log\", schema=\"api\"\n    )\n    op.drop_table(\"user_notification_log\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-20-deployment-strategy.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-20-deployment-strategy.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2025_01_24_add_last_notified_at_to_user_saved_.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_01_24_add_last_notified_at_to_user_saved_.py\nSize: 0.84 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-20-fe-design-system.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-20-fe-design-system.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 43b179a7c92e\nRevises: dc04ce955a9a\nCreate Date: 2025-01-24 17:15:14.064880\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"43b179a7c92e\"\ndown_revision = \"9e7fc937646a\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"user_saved_opportunity\",\n        sa.Column(\n            \"last_notified_at\", sa.TIMESTAMP(timezone=True), server_default=\"NOW()\", nullable=False\n        ),\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"user_saved_opportunity\", \"last_notified_at\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-20-fe-server-rendering.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-20-fe-server-rendering.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2025_01_28_add_searched_opportunity_ids_and_last_.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_01_28_add_searched_opportunity_ids_and_last_.py\nSize: 1.19 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-20-fe-use-npm.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-20-fe-use-npm.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 9e7fc937646a\nRevises: dc04ce955a9a\nCreate Date: 2025-01-28 19:06:19.397240\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"9e7fc937646a\"\ndown_revision = \"dc04ce955a9a\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"user_saved_search\",\n        sa.Column(\n            \"last_notified_at\",\n            sa.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            nullable=False,\n        ),\n        schema=\"api\",\n    )\n    op.add_column(\n        \"user_saved_search\",\n        sa.Column(\"searched_opportunity_ids\", postgresql.ARRAY(sa.BigInteger()), nullable=False),\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"user_saved_search\", \"searched_opportunity_ids\", schema=\"api\")\n    op.drop_column(\"user_saved_search\", \"last_notified_at\", schema=\"api\")\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-20-fe-uswds-in-react.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-20-fe-uswds-in-react.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2025_02_07_remove_opportunity_attachment_type.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_02_07_remove_opportunity_attachment_type.py\nSize: 1.71 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-24-video-conferencing.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-24-video-conferencing.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: f99f1a5aee8b\nRevises: 43b179a7c92e\nCreate Date: 2025-02-07 20:01:38.893554\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"f99f1a5aee8b\"\ndown_revision = \"43b179a7c92e\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\n        \"opportunity_attachment_opportunity_attachment_type_id_idx\",\n        table_name=\"opportunity_attachment\",\n        schema=\"api\",\n    )\n    op.drop_constraint(\n        \"opportunity_attachment_opportunity_attachment_type_id_l_e60d\",\n        \"opportunity_attachment\",\n        schema=\"api\",\n        type_=\"foreignkey\",\n    )\n    op.drop_column(\"opportunity_attachment\", \"opportunity_attachment_type_id\", schema=\"api\")\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"opportunity_attachment\",\n        sa.Column(\n            \"opportunity_attachment_type_id\", sa.INTEGER(), autoincrement=False, nullable=False\n        ),\n        schema=\"api\",\n    )\n    op.create_foreign_key(\n        \"opportunity_attachment_opportunity_attachment_type_id_l_e60d\",\n        \"opportunity_attachment\",\n        \"lk_opportunity_attachment_type\",\n        [\"opportunity_attachment_type_id\"],\n        [\"opportunity_attachment_type_id\"],\n        source_schema=\"api\",\n        referent_schema=\"api\",\n    )\n    op.create_index(\n        \"opportunity_attachment_opportunity_attachment_type_id_idx\",\n        \"opportunity_attachment\",\n        [\"opportunity_attachment_type_id\"],\n        unique=False,\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-07-26-backend-prod-server.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-07-26-backend-prod-server.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/migrations/versions/2025_02_07_remove_opportunity_attachment_type_.py\nLanguage: py\nType: code\nDirectory: api/src/db/migrations/versions\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/migrations/versions/2025_02_07_remove_opportunity_attachment_type_.py\nSize: 1.50 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-08-01-analytics-platform.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-08-01-analytics-platform.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"Revision ID: 56d129425397\nRevises: f99f1a5aee8b\nCreate Date: 2025-02-07 21:20:08.955913\n\n\"\"\"\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"56d129425397\"\ndown_revision = \"f99f1a5aee8b\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"lk_opportunity_attachment_type\", schema=\"api\")\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"lk_opportunity_attachment_type\",\n        sa.Column(\n            \"opportunity_attachment_type_id\", sa.INTEGER(), autoincrement=True, nullable=False\n        ),\n        sa.Column(\"description\", sa.TEXT(), autoincrement=False, nullable=False),\n        sa.Column(\n            \"created_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\n            \"updated_at\",\n            postgresql.TIMESTAMP(timezone=True),\n            server_default=sa.text(\"now()\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.PrimaryKeyConstraint(\n            \"opportunity_attachment_type_id\", name=\"lk_opportunity_attachment_type_pkey\"\n        ),\n        schema=\"api\",\n    )\n    # ### end Alembic commands ###"}
{"path":"documentation/wiki/product/decisions/adr/2023-08-21-branch-conv-release-workflow.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-08-21-branch-conv-release-workflow.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/__init__.py\nLanguage: py\nType: model\nDirectory: api/src/db/models\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/__init__.py\nSize: 0.45 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-08-21-cloud-platform.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-08-21-cloud-platform.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"from . import (\n    agency_models,\n    base,\n    extract_models,\n    lookup_models,\n    opportunity_models,\n    task_models,\n    user_models,\n)\n\nlogger = logging.getLogger(__name__)\n\n# Re-export metadata\n# This is used by tests to create the test database.\nmetadata = base.metadata\n\n__all__ = [\n    \"metadata\",\n    \"opportunity_models\",\n    \"lookup_models\",\n    \"agency_models\",\n    \"user_models\",\n    \"extract_models\",\n    \"task_models\",\n]"}
{"path":"documentation/wiki/product/decisions/adr/2023-08-21-infrastructure-as-code-tool.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-08-21-infrastructure-as-code-tool.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/agency_models.py\nLanguage: py\nType: model\nDirectory: api/src/db/models\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/agency_models.py\nSize: 4.47 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-09-07-data-replication-tool.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-09-07-data-replication-tool.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"from src.adapters.db.type_decorators.postgres_type_decorators import LookupColumn\nfrom src.constants.lookup_constants import (\n    AgencyDownloadFileType,\n    AgencySubmissionNotificationSetting,\n)\nfrom src.db.models.base import ApiSchemaTable, TimestampMixin\nfrom src.db.models.lookup_models import (\n    LkAgencyDownloadFileType,\n    LkAgencySubmissionNotificationSetting,\n)\n\n\nclass AgencyContactInfo(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"agency_contact_info\"\n\n    agency_contact_info_id: Mapped[int] = mapped_column(BigInteger, primary_key=True)\n\n    contact_name: Mapped[str]\n\n    address_line_1: Mapped[str]\n    address_line_2: Mapped[str | None]\n    city: Mapped[str]\n\n    # Note that while it would make sense to do an enum for state\n    # it doesn't look to be limited to US states and includes some foreign states\n    # as well as numbers(?) in the existing system\n    state: Mapped[str]\n    zip_code: Mapped[str]\n    phone_number: Mapped[str]\n    primary_email: Mapped[str]\n    secondary_email: Mapped[str | None]\n\n\nclass Agency(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"agency\"\n\n    agency_id: Mapped[int] = mapped_column(BigInteger, primary_key=True)\n\n    agency_name: Mapped[str]\n\n    agency_code: Mapped[str] = mapped_column(index=True, unique=True)\n    sub_agency_code: Mapped[str | None]\n\n    assistance_listing_number: Mapped[str]\n\n    agency_submission_notification_setting: Mapped[AgencySubmissionNotificationSetting] = (\n        mapped_column(\n            \"agency_submission_notification_setting_id\",\n            LookupColumn(LkAgencySubmissionNotificationSetting),\n            ForeignKey(\n                LkAgencySubmissionNotificationSetting.agency_submission_notification_setting_id\n            ),\n        )\n    )\n\n    agency_contact_info_id: Mapped[BigInteger | None] = mapped_column(\n        BigInteger, ForeignKey(AgencyContactInfo.agency_contact_info_id)\n    )\n    agency_contact_info: Mapped[AgencyContactInfo | None] = relationship(AgencyContactInfo)\n\n    # There are several agencies in the data we're ingesting that\n    # are clearly meant for testing, I'm not certain we want to flag\n    # them in this way, but adding it for now - can revisit later\n    # From the legacy system configurations, this should be the following agencies\n    # GDIT,IVV,IVPDF,0001,FGLT,NGMS,NGMS-Sub1,SECSCAN\n    # including any subagencies\n    is_test_agency: Mapped[bool]\n\n    # These values come from the legacy system, but their exact usage isn't entirely\n    # clear at this point in time.\n    ldap_group: Mapped[str | None]\n    description: Mapped[str | None]\n    label: Mapped[str | None]\n\n    is_multilevel_agency: Mapped[bool] = mapped_column(default=False)\n    is_multiproject: Mapped[bool] = mapped_column(default=False)\n    has_system_to_system_certificate: Mapped[bool] = mapped_column(default=False)\n    can_view_packages_in_grace_period: Mapped[bool] = mapped_column(default=False)\n    is_image_workspace_enabled: Mapped[bool] = mapped_column(default=False)\n    is_validation_workspace_enabled: Mapped[bool] = mapped_column(default=False)\n\n    link_agency_download_file_types: Mapped[list[\"LinkAgencyDownloadFileType\"]] = relationship(\n        back_populates=\"agency\", uselist=True, cascade=\"all, delete-orphan\"\n    )\n\n    agency_download_file_types: AssociationProxy[set[AgencyDownloadFileType]] = association_proxy(\n        \"link_agency_download_file_types\",\n        \"agency_download_file_type\",\n        creator=lambda obj: LinkAgencyDownloadFileType(agency_download_file_type=obj),\n    )\n\n    top_level_agency_id: Mapped[int | None] = mapped_column(\n        BigInteger,\n        ForeignKey(agency_id),\n        nullable=True,\n    )\n    top_level_agency: Mapped[\"Agency | None\"] = relationship(\n        lambda: Agency,\n        remote_side=[agency_id],\n    )\n\n\nclass LinkAgencyDownloadFileType(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"link_agency_download_file_type\"\n\n    agency_id: Mapped[int] = mapped_column(\n        BigInteger,\n        ForeignKey(Agency.agency_id),\n        primary_key=True,\n    )\n    agency: Mapped[Agency] = relationship(Agency)\n\n    agency_download_file_type: Mapped[AgencyDownloadFileType] = mapped_column(\n        \"agency_download_file_type_id\",\n        LookupColumn(LkAgencyDownloadFileType),\n        ForeignKey(LkAgencyDownloadFileType.agency_download_file_type_id),\n        primary_key=True,\n    )"}
{"path":"documentation/wiki/product/decisions/adr/2023-09-22-hhs-comms-site.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-09-22-hhs-comms-site.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/base.py\nLanguage: py\nType: model\nDirectory: api/src/db/models\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/base.py\nSize: 4.50 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-10-16-email-marketing.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-10-16-email-marketing.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"from sqlalchemy import TIMESTAMP, MetaData, Text, inspect\nfrom sqlalchemy.dialects import postgresql\nfrom sqlalchemy.orm import DeclarativeBase, Mapped, declarative_mixin, mapped_column\nfrom sqlalchemy.sql.functions import now as sqlnow\n\nfrom src.constants.schema import Schemas\nfrom src.util import datetime_util\n\n# Override the default naming of constraints\n# to use suffixes instead:\n# https://stackoverflow.com/questions/4107915/postgresql-default-constraint-names/4108266#4108266\nmetadata = MetaData(\n    naming_convention={\n        \"ix\": \"%(table_name)s_%(column_0_name)s_idx\",\n        \"uq\": \"%(table_name)s_%(column_0_name)s_uniq\",\n        \"ck\": \"%(table_name)s_`%(constraint_name)s_check`\",\n        \"fk\": \"%(table_name)s_%(column_0_name)s_%(referred_table_name)s_fkey\",\n        \"pk\": \"%(table_name)s_pkey\",\n    }\n)\n\n\nclass Base(DeclarativeBase):\n    # Attach the metadata to the Base class so all tables automatically get added to the metadata\n    metadata = metadata\n\n    # Override the default type that SQLAlchemy will map python types to.\n    # This is used if you simply define a column like:\n    #\n    #   my_column: Mapped[str]\n    #\n    # If you provide a mapped_column attribute you can override these values\n    #\n    # See: https://docs.sqlalchemy.org/en/20/orm/declarative_tables.html#mapped-column-derives-the-datatype-and-nullability-from-the-mapped-annotation\n    #      for the default mappings\n    #\n    # See: https://docs.sqlalchemy.org/en/20/orm/declarative_tables.html#orm-declarative-mapped-column-type-map\n    #      for details on setting up this configuration.\n    type_annotation_map = {\n        # Always include a timezone for datetimes\n        datetime: TIMESTAMP(timezone=True),\n        # Explicitly use the Text column type for strings\n        str: Text,\n        # Always use the Postgres UUID column type\n        uuid.UUID: postgresql.UUID(as_uuid=True),\n    }\n\n    @classmethod\n    def get_table_name(cls) -> str:\n        return cls.__tablename__\n\n    def _dict(self) -> dict:\n        return {c.key: getattr(self, c.key) for c in inspect(self).mapper.column_attrs}\n\n    def for_json(self) -> dict:\n        json_valid_dict = {}\n        dictionary = self._dict()\n        for key, value in dictionary.items():\n            if isinstance(value, UUID) or isinstance(value, Decimal):\n                json_valid_dict[key] = str(value)\n            elif isinstance(value, date) or isinstance(value, datetime):\n                json_valid_dict[key] = value.isoformat()\n            else:\n                json_valid_dict[key] = value\n\n        return json_valid_dict\n\n    def copy(self, **kwargs: dict[str, Any]) -> \"Base\":\n        # TODO - Python 3.11 will let us make the return Self instead\n        table = self.__table__\n        non_pk_columns = [\n            k for k in table.columns.keys() if k not in table.primary_key.columns.keys()  # type: ignore\n        ]\n        data = {c: getattr(self, c) for c in non_pk_columns}\n        data.update(kwargs)\n        copy = self.__class__(**data)\n        return copy\n\n    def __repr__(self) -> str:\n        values = []\n        for k, v in self.for_json().items():\n            values.append(f\"{k}={v!r}\")\n\n        return f\"<{self.__class__.__name__}({','.join(values)})\"\n\n    def __rich_repr__(self) -> Iterable[tuple[str, Any]]:\n        \"\"\"Rich repr for interactive console.\n\n        See https://rich.readthedocs.io/en/latest/pretty.html#rich-repr-protocol\n        \"\"\"\n        return self._dict().items()\n\n\nclass ApiSchemaTable(Base):\n    __abstract__ = True\n\n    __table_args__: Any = {\"schema\": Schemas.API}\n\n\n@declarative_mixin\nclass IdMixin:\n    \"\"\"Mixin to add a UUID id primary key column to a model\n    https://docs.sqlalchemy.org/en/20/orm/declarative_mixins.html\n    \"\"\"\n\n    id: Mapped[uuid.UUID] = mapped_column(primary_key=True, default=uuid.uuid4)\n\n\ndef same_as_created_at(context: Any) -> Any:\n    return context.get_current_parameters()[\"created_at\"]\n\n\n@declarative_mixin\nclass TimestampMixin:\n    \"\"\"Mixin to add created_at and updated_at columns to a model\n    https://docs.sqlalchemy.org/en/20/orm/declarative_mixins.html#mixing-in-columns\n    \"\"\"\n\n    created_at: Mapped[datetime] = mapped_column(\n        nullable=False,\n        default=datetime_util.utcnow,\n        server_default=sqlnow(),\n    )\n\n    updated_at: Mapped[datetime] = mapped_column(\n        nullable=False,\n        default=same_as_created_at,\n        onupdate=datetime_util.utcnow,\n        server_default=sqlnow(),\n    )"}
{"path":"documentation/wiki/product/decisions/adr/2023-10-16-listserv.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-10-16-listserv.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/extract_models.py\nLanguage: py\nType: model\nDirectory: api/src/db/models\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/extract_models.py\nSize: 0.79 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-11-22-uptime-monitoring.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-11-22-uptime-monitoring.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"from src.adapters.db.type_decorators.postgres_type_decorators import LookupColumn\nfrom src.constants.lookup_constants import ExtractType\nfrom src.db.models.base import ApiSchemaTable, TimestampMixin\nfrom src.db.models.lookup_models import LkExtractType\n\n\nclass ExtractMetadata(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"extract_metadata\"\n\n    extract_metadata_id = mapped_column(BigInteger, primary_key=True)\n    extract_type: Mapped[ExtractType] = mapped_column(\n        \"extract_type_id\",\n        LookupColumn(LkExtractType),\n        ForeignKey(LkExtractType.extract_type_id),\n    )\n    file_name: Mapped[str]\n    file_path: Mapped[str]\n    file_size_bytes: Mapped[int] = mapped_column(BigInteger)"}
{"path":"documentation/wiki/product/decisions/adr/2023-12-06-database-migrations.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-12-06-database-migrations.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/foreign/__init__.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/foreign\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/__init__.py\nSize: 0.24 KB\nLast Modified: 2025-02-14T17:08:26.442Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-12-15-deliverable-reporting-strategy.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-12-15-deliverable-reporting-strategy.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"from . import attachment, forecast, foreignbase, opportunity, synopsis, tgroups\n\nmetadata = foreignbase.metadata\n\n__all__ = [\"metadata\", \"forecast\", \"opportunity\", \"synopsis\", \"tgroups\", \"attachment\"]"}
{"path":"documentation/wiki/product/decisions/adr/2023-12-18-measurement-dashboard-architecture.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-12-18-measurement-dashboard-architecture.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/foreign/attachment.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/foreign\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/attachment.py\nSize: 0.76 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/adr/2023-12-20-contact-us-email.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2023-12-20-contact-us-email.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"from sqlalchemy.orm import Mapped, foreign, relationship\n\nfrom src.db.models.legacy_mixin import synopsis_mixin\n\nfrom . import foreignbase\nfrom .opportunity import Topportunity\n\n\nclass TsynopsisAttachment(foreignbase.ForeignBase, synopsis_mixin.TsynopsisAttachmentMixin):\n    __tablename__ = \"tsynopsisattachment\"\n\n    opportunity: Mapped[Topportunity | None] = relationship(\n        Topportunity,\n        primaryjoin=lambda: TsynopsisAttachment.opportunity_id\n        == foreign(Topportunity.opportunity_id),\n        uselist=False,\n        overlaps=\"opportunity\",\n    )"}
{"path":"documentation/wiki/product/decisions/adr/2024-02-26-e2e-integration-testing-framework.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2024-02-26-e2e-integration-testing-framework.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/foreign/dialect.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/foreign\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/dialect.py\nSize: 1.49 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/adr/2024-03-04-logging-monitoring.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2024-03-04-logging-monitoring.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"import re\n\nimport sqlalchemy\nimport sqlalchemy.dialects.postgresql\n\n\nclass ForeignTableDDLCompiler(sqlalchemy.sql.compiler.DDLCompiler):\n    \"\"\"SQLAlchemy compiler for creating foreign tables.\"\"\"\n\n    def create_table_constraints(self, _table, _include_foreign_key_constraints=None, **kw):\n        return \"\"  # Don't generate any constraints.\n\n    def visit_create_table(self, create, **kw):\n        table = create.element\n        table._prefixes = (\"FOREIGN\",)  # Add \"FOREIGN\" before \"TABLE\".\n        sql = super().visit_create_table(create, **kw)\n        table._prefixes = ()\n        return sql\n\n    def post_create_table(self, table):\n        # Add foreign options at the end.\n        return (\n            f\" SERVER grants OPTIONS (schema 'EGRANTSADMIN', table '{table.name.upper()}', \"\n            \"readonly 'true', prefetch '1000')\"\n        )\n\n    def visit_create_column(self, create, first_pk=False, **kw):\n        column = create.element\n        sql = super().visit_create_column(create, first_pk, **kw)\n        if sql and column.primary_key:\n            # Add \"OPTIONS ...\" to primary key column.\n            sql = re.sub(r\"^(.*?)( NOT NULL)?$\", r\"\\1 OPTIONS (key 'true')\\2\", sql)\n        return sql\n\n\nclass ForeignTableDialect(sqlalchemy.dialects.postgresql.dialect):\n    \"\"\"SQLAlchemy dialect for creating foreign tables.\n\n    See https://docs.sqlalchemy.org/en/20/dialects/\n    \"\"\"\n\n    ddl_compiler = ForeignTableDDLCompiler"}
{"path":"documentation/wiki/product/decisions/adr/2024-03-19-dashboard-storage.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2024-03-19-dashboard-storage.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/foreign/forecast.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/foreign\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/forecast.py\nSize: 3.75 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/adr/2024-04-10-dashboard-tool.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2024-04-10-dashboard-tool.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"from ..legacy_mixin import forecast_mixin\nfrom . import foreignbase\nfrom .opportunity import Topportunity\n\n\nclass Tforecast(foreignbase.ForeignBase, forecast_mixin.TforecastMixin):\n    __tablename__ = \"tforecast\"\n\n    opportunity: Mapped[Topportunity | None] = relationship(\n        Topportunity,\n        primaryjoin=lambda: Tforecast.opportunity_id == foreign(Topportunity.opportunity_id),\n        uselist=False,\n        overlaps=\"opportunity\",\n    )\n\n\nclass TforecastHist(foreignbase.ForeignBase, forecast_mixin.TforecastHistMixin):\n    __tablename__ = \"tforecast_hist\"\n\n    opportunity: Mapped[Topportunity | None] = relationship(\n        Topportunity,\n        primaryjoin=lambda: TforecastHist.opportunity_id == foreign(Topportunity.opportunity_id),\n        uselist=False,\n        overlaps=\"opportunity\",\n    )\n\n\nclass TapplicanttypesForecast(foreignbase.ForeignBase, forecast_mixin.TapplicanttypesForecastMixin):\n    __tablename__ = \"tapplicanttypes_forecast\"\n\n    forecast: Mapped[Tforecast | None] = relationship(\n        Tforecast,\n        primaryjoin=lambda: TapplicanttypesForecast.opportunity_id\n        == foreign(Tforecast.opportunity_id),\n        uselist=False,\n        overlaps=\"forecast\",\n    )\n\n\nclass TapplicanttypesForecastHist(\n    foreignbase.ForeignBase, forecast_mixin.TapplicanttypesForecastHistMixin\n):\n    __tablename__ = \"tapplicanttypes_forecast_hist\"\n\n    forecast: Mapped[TforecastHist | None] = relationship(\n        TforecastHist,\n        primaryjoin=lambda: and_(\n            TapplicanttypesForecastHist.opportunity_id == foreign(TforecastHist.opportunity_id),\n            TapplicanttypesForecastHist.revision_number == foreign(TforecastHist.revision_number),\n        ),\n        uselist=False,\n        overlaps=\"forecast\",\n    )\n\n\nclass TfundactcatForecast(foreignbase.ForeignBase, forecast_mixin.TfundactcatForecastMixin):\n    __tablename__ = \"tfundactcat_forecast\"\n\n    forecast: Mapped[Tforecast | None] = relationship(\n        Tforecast,\n        primaryjoin=lambda: TfundactcatForecast.opportunity_id == foreign(Tforecast.opportunity_id),\n        uselist=False,\n        overlaps=\"forecast\",\n    )\n\n\nclass TfundactcatForecastHist(foreignbase.ForeignBase, forecast_mixin.TfundactcatForecastHistMixin):\n    __tablename__ = \"tfundactcat_forecast_hist\"\n\n    forecast: Mapped[TforecastHist | None] = relationship(\n        TforecastHist,\n        primaryjoin=lambda: and_(\n            TfundactcatForecastHist.opportunity_id == foreign(TforecastHist.opportunity_id),\n            TfundactcatForecastHist.revision_number == foreign(TforecastHist.revision_number),\n        ),\n        uselist=False,\n        overlaps=\"forecast\",\n    )\n\n\nclass TfundinstrForecast(foreignbase.ForeignBase, forecast_mixin.TfundinstrForecastMixin):\n    __tablename__ = \"tfundinstr_forecast\"\n\n    forecast: Mapped[Tforecast | None] = relationship(\n        Tforecast,\n        primaryjoin=lambda: TfundinstrForecast.opportunity_id == foreign(Tforecast.opportunity_id),\n        uselist=False,\n        overlaps=\"forecast\",\n    )\n\n\nclass TfundinstrForecastHist(foreignbase.ForeignBase, forecast_mixin.TfundinstrForecastHistMixin):\n    __tablename__ = \"tfundinstr_forecast_hist\"\n\n    forecast: Mapped[TforecastHist | None] = relationship(\n        TforecastHist,\n        primaryjoin=lambda: and_(\n            TfundinstrForecastHist.opportunity_id == foreign(TforecastHist.opportunity_id),\n            TfundinstrForecastHist.revision_number == foreign(TforecastHist.revision_number),\n        ),\n        uselist=False,\n        overlaps=\"forecast\",\n    )"}
{"path":"documentation/wiki/product/decisions/adr/2024-10-02-search-engine.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2024-10-02-search-engine.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/foreign/foreignbase.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/foreign\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/foreignbase.py\nSize: 1.58 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/adr/2024-10-18-document-storage.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2024-10-18-document-storage.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"import datetime\nfrom typing import Any, Iterable\n\nimport sqlalchemy\n\nfrom src.constants.schema import Schemas\n\nmetadata = sqlalchemy.MetaData()\n\n\nclass ForeignBase(sqlalchemy.orm.DeclarativeBase):\n    metadata = metadata\n\n    __table_args__ = {\"schema\": Schemas.LEGACY}\n\n    # These types are selected so that the underlying Oracle types are mapped to a more general\n    # type. For example all CHAR and VARCHAR types can be mapped to TEXT for simplicity. See\n    # https://github.com/laurenz/oracle_fdw?tab=readme-ov-file#data-types\n    type_annotation_map = {\n        int: sqlalchemy.BigInteger,\n        str: sqlalchemy.Text,\n        datetime.datetime: sqlalchemy.TIMESTAMP(timezone=True),\n    }\n\n    def _dict(self) -> dict:\n        return {c.key: getattr(self, c.key) for c in sqlalchemy.inspect(self).mapper.column_attrs}\n\n    def __repr__(self) -> str:\n        return f\"<{type(self).__name__}({self._dict()!r})\"\n\n    def __rich_repr__(self) -> Iterable[tuple[str, Any]]:\n        \"\"\"Rich repr for interactive console.\n        See https://rich.readthedocs.io/en/latest/pretty.html#rich-repr-protocol\n        \"\"\"\n        return self._dict().items()"}
{"path":"documentation/wiki/product/decisions/adr/2024-11-14-document-sharing.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2024-11-14-document-sharing.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/foreign/opportunity.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/foreign\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/opportunity.py\nSize: 0.96 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/adr/2024-11-20-internal-wiki.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2024-11-20-internal-wiki.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"from src.db.models.legacy_mixin import opportunity_mixin\n\nfrom . import foreignbase\n\n\nclass Topportunity(foreignbase.ForeignBase, opportunity_mixin.TopportunityMixin):\n    __tablename__ = \"topportunity\"\n\n    cfdas: Mapped[list[\"TopportunityCfda\"]] = relationship(\n        primaryjoin=lambda: Topportunity.opportunity_id == foreign(TopportunityCfda.opportunity_id),\n        uselist=True,\n    )\n\n\nclass TopportunityCfda(foreignbase.ForeignBase, opportunity_mixin.TopportunityCfdaMixin):\n    __tablename__ = \"topportunity_cfda\"\n\n    opportunity: Mapped[Topportunity | None] = relationship(\n        primaryjoin=lambda: TopportunityCfda.opportunity_id == foreign(Topportunity.opportunity_id),\n        uselist=False,\n    )"}
{"path":"documentation/wiki/product/decisions/adr/2024-12-05-shared-team-calendar-platform.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2024-12-05-shared-team-calendar-platform.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/foreign/synopsis.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/foreign\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/synopsis.py\nSize: 3.77 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/adr/2024-12-06-team-health-survey-tool.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2024-12-06-team-health-survey-tool.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"from src.db.models.legacy_mixin import synopsis_mixin\n\nfrom . import foreignbase\nfrom .opportunity import Topportunity\n\n\nclass Tsynopsis(foreignbase.ForeignBase, synopsis_mixin.TsynopsisMixin):\n    __tablename__ = \"tsynopsis\"\n\n    opportunity: Mapped[Topportunity | None] = relationship(\n        Topportunity,\n        primaryjoin=lambda: Tsynopsis.opportunity_id == foreign(Topportunity.opportunity_id),\n        uselist=False,\n        overlaps=\"opportunity\",\n    )\n\n\nclass TsynopsisHist(foreignbase.ForeignBase, synopsis_mixin.TsynopsisHistMixin):\n    __tablename__ = \"tsynopsis_hist\"\n\n    opportunity: Mapped[Topportunity | None] = relationship(\n        Topportunity,\n        primaryjoin=lambda: TsynopsisHist.opportunity_id == foreign(Topportunity.opportunity_id),\n        uselist=False,\n        overlaps=\"opportunity\",\n    )\n\n\nclass TapplicanttypesSynopsis(foreignbase.ForeignBase, synopsis_mixin.TapplicanttypesSynopsisMixin):\n    __tablename__ = \"tapplicanttypes_synopsis\"\n\n    synopsis: Mapped[Tsynopsis | None] = relationship(\n        Tsynopsis,\n        primaryjoin=lambda: TapplicanttypesSynopsis.opportunity_id\n        == foreign(Tsynopsis.opportunity_id),\n        uselist=False,\n        overlaps=\"synopsis\",\n    )\n\n\nclass TapplicanttypesSynopsisHist(\n    foreignbase.ForeignBase, synopsis_mixin.TapplicanttypesSynopsisHistMixin\n):\n    __tablename__ = \"tapplicanttypes_synopsis_hist\"\n\n    synopsis: Mapped[TsynopsisHist | None] = relationship(\n        TsynopsisHist,\n        primaryjoin=lambda: and_(\n            TapplicanttypesSynopsisHist.opportunity_id == foreign(TsynopsisHist.opportunity_id),\n            TapplicanttypesSynopsisHist.revision_number == foreign(TsynopsisHist.revision_number),\n        ),\n        uselist=False,\n        overlaps=\"synopsis\",\n    )\n\n\nclass TfundactcatSynopsis(foreignbase.ForeignBase, synopsis_mixin.TfundactcatSynopsisMixin):\n    __tablename__ = \"tfundactcat_synopsis\"\n\n    synopsis: Mapped[Tsynopsis | None] = relationship(\n        Tsynopsis,\n        primaryjoin=lambda: TfundactcatSynopsis.opportunity_id == foreign(Tsynopsis.opportunity_id),\n        uselist=False,\n        overlaps=\"synopsis\",\n    )\n\n\nclass TfundactcatSynopsisHist(foreignbase.ForeignBase, synopsis_mixin.TfundactcatSynopsisHistMixin):\n    __tablename__ = \"tfundactcat_synopsis_hist\"\n\n    synopsis: Mapped[TsynopsisHist | None] = relationship(\n        TsynopsisHist,\n        primaryjoin=lambda: and_(\n            TfundactcatSynopsisHist.opportunity_id == foreign(TsynopsisHist.opportunity_id),\n            TfundactcatSynopsisHist.revision_number == foreign(TsynopsisHist.revision_number),\n        ),\n        uselist=False,\n        overlaps=\"synopsis\",\n    )\n\n\nclass TfundinstrSynopsis(foreignbase.ForeignBase, synopsis_mixin.TfundinstrSynopsisMixin):\n    __tablename__ = \"tfundinstr_synopsis\"\n\n    synopsis: Mapped[Tsynopsis | None] = relationship(\n        Tsynopsis,\n        primaryjoin=lambda: TfundinstrSynopsis.opportunity_id == foreign(Tsynopsis.opportunity_id),\n        uselist=False,\n        overlaps=\"synopsis\",\n    )\n\n\nclass TfundinstrSynopsisHist(foreignbase.ForeignBase, synopsis_mixin.TfundinstrSynopsisHistMixin):\n    __tablename__ = \"tfundinstr_synopsis_hist\"\n\n    synopsis: Mapped[TsynopsisHist | None] = relationship(\n        TsynopsisHist,\n        primaryjoin=lambda: and_(\n            TfundinstrSynopsisHist.opportunity_id == foreign(TsynopsisHist.opportunity_id),\n            TfundinstrSynopsisHist.revision_number == foreign(TsynopsisHist.revision_number),\n        ),\n        uselist=False,\n        overlaps=\"synopsis\",\n    )"}
{"path":"documentation/wiki/product/decisions/adr/2024-12-17-adding-slack-users.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2024-12-17-adding-slack-users.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/foreign/tgroups.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/foreign\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/foreign/tgroups.py\nSize: 0.38 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/adr/2025-01-02-repo-organization.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/2025-01-02-repo-organization.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"from src.db.models.legacy_mixin import tgroups_mixin\n\nfrom . import foreignbase\n\n\nclass Tgroups(foreignbase.ForeignBase, tgroups_mixin.TGroupsMixin):\n    __tablename__ = \"tgroups\""}
{"path":"documentation/wiki/product/decisions/adr/README.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/legacy_mixin/__init__.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/legacy_mixin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/legacy_mixin/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/adr/use-ethnio-for-design-research.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/adr","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/adr/use-ethnio-for-design-research.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":""}
{"path":"documentation/wiki/product/decisions/infra/0000-use-markdown-architectural-decision-records.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/infra/0000-use-markdown-architectural-decision-records.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/legacy_mixin/forecast_mixin.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/legacy_mixin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/legacy_mixin/forecast_mixin.py\nSize: 6.02 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/infra/0001-ci-cd-interface.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/infra/0001-ci-cd-interface.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"import datetime\n\nfrom sqlalchemy.orm import Mapped, declarative_mixin, mapped_column\n\n\n@declarative_mixin\nclass TforecastMixin:\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    version_nbr: Mapped[int]\n    posting_date: Mapped[datetime.datetime | None]\n    archive_date: Mapped[datetime.datetime | None]\n    forecast_desc: Mapped[str | None]\n    oth_cat_fa_desc: Mapped[str | None]\n    cost_sharing: Mapped[str | None]\n    number_of_awards: Mapped[str | None]\n    est_funding: Mapped[str | None]\n    award_ceiling: Mapped[str | None]\n    award_floor: Mapped[str | None]\n    fd_link_url: Mapped[str | None]\n    fd_link_desc: Mapped[str | None]\n    ac_name: Mapped[str | None]\n    ac_phone: Mapped[str | None]\n    ac_email_addr: Mapped[str | None]\n    ac_email_desc: Mapped[str | None]\n    agency_code: Mapped[str | None]\n    sendmail: Mapped[str | None]\n    applicant_elig_desc: Mapped[str | None]\n    est_synopsis_posting_date: Mapped[datetime.datetime | None]\n    est_appl_response_date: Mapped[datetime.datetime | None]\n    est_appl_response_date_desc: Mapped[str | None]\n    est_award_date: Mapped[datetime.datetime | None]\n    est_project_start_date: Mapped[datetime.datetime | None]\n    fiscal_year: Mapped[int | None]\n    modification_comments: Mapped[str | None]\n    create_ts: Mapped[datetime.datetime]\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n    publisheruid: Mapped[str | None]\n    publisher_profile_id: Mapped[int | None]\n\n\n@declarative_mixin\nclass TforecastHistMixin:\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    revision_number: Mapped[int] = mapped_column(primary_key=True)\n    version_nbr: Mapped[int]\n    posting_date: Mapped[datetime.datetime | None]\n    archive_date: Mapped[datetime.datetime | None]\n    forecast_desc: Mapped[str | None]\n    oth_cat_fa_desc: Mapped[str | None]\n    cost_sharing: Mapped[str | None]\n    number_of_awards: Mapped[str | None]\n    est_funding: Mapped[str | None]\n    award_ceiling: Mapped[str | None]\n    award_floor: Mapped[str | None]\n    fd_link_url: Mapped[str | None]\n    fd_link_desc: Mapped[str | None]\n    ac_name: Mapped[str | None]\n    ac_phone: Mapped[str | None]\n    ac_email_addr: Mapped[str | None]\n    ac_email_desc: Mapped[str | None]\n    agency_code: Mapped[str | None]\n    sendmail: Mapped[str | None]\n    applicant_elig_desc: Mapped[str | None]\n    est_synopsis_posting_date: Mapped[datetime.datetime | None]\n    est_appl_response_date: Mapped[datetime.datetime | None]\n    est_appl_response_date_desc: Mapped[str | None]\n    est_award_date: Mapped[datetime.datetime | None]\n    est_project_start_date: Mapped[datetime.datetime | None]\n    fiscal_year: Mapped[int | None]\n    modification_comments: Mapped[str | None]\n    action_type: Mapped[str | None]\n    action_date: Mapped[datetime.datetime | None]\n    create_ts: Mapped[datetime.datetime]\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n    publisheruid: Mapped[str | None]\n    publisher_profile_id: Mapped[int | None]\n\n\n@declarative_mixin\nclass TapplicanttypesForecastMixin:\n    at_frcst_id: Mapped[int] = mapped_column(primary_key=True)\n    at_id: Mapped[str] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n\n\n@declarative_mixin\nclass TapplicanttypesForecastHistMixin:\n    at_frcst_id: Mapped[int] = mapped_column(primary_key=True)\n    at_id: Mapped[str] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    revision_number: Mapped[int] = mapped_column(primary_key=True)\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n\n\n@declarative_mixin\nclass TfundactcatForecastMixin:\n    fac_frcst_id: Mapped[int] = mapped_column(primary_key=True)\n    fac_id: Mapped[str] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n\n\n@declarative_mixin\nclass TfundactcatForecastHistMixin:\n    fac_frcst_id: Mapped[int] = mapped_column(primary_key=True)\n    fac_id: Mapped[str] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    revision_number: Mapped[int] = mapped_column(primary_key=True)\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n\n\n@declarative_mixin\nclass TfundinstrForecastMixin:\n    fi_frcst_id: Mapped[int] = mapped_column(primary_key=True)\n    fi_id: Mapped[str] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n\n\n@declarative_mixin\nclass TfundinstrForecastHistMixin:\n    fi_frcst_id: Mapped[int] = mapped_column(primary_key=True)\n    fi_id: Mapped[str] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    revision_number: Mapped[int] = mapped_column(primary_key=True)\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]"}
{"path":"documentation/wiki/product/decisions/infra/0002-use-custom-implementation-of-github-oidc.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/infra/0002-use-custom-implementation-of-github-oidc.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/legacy_mixin/opportunity_mixin.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/legacy_mixin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/legacy_mixin/opportunity_mixin.py\nSize: 1.54 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/infra/0003-manage-ecr-in-prod-account-module.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/infra/0003-manage-ecr-in-prod-account-module.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"import datetime\n\nfrom sqlalchemy.orm import Mapped, declarative_mixin, mapped_column\n\n\n@declarative_mixin\nclass TopportunityMixin:\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    oppnumber: Mapped[str | None]\n    revision_number: Mapped[int | None]\n    opptitle: Mapped[str | None]\n    owningagency: Mapped[str | None]\n    publisheruid: Mapped[str | None]\n    listed: Mapped[str | None]\n    oppcategory: Mapped[str | None]\n    initial_opportunity_id: Mapped[int | None]\n    modified_comments: Mapped[str | None]\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n    flag_2006: Mapped[str | None]\n    category_explanation: Mapped[str | None]\n    publisher_profile_id: Mapped[int | None]\n    is_draft: Mapped[str | None]\n\n\n@declarative_mixin\nclass TopportunityCfdaMixin:\n    opp_cfda_id: Mapped[int] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int]\n    cfdanumber: Mapped[str | None]\n    programtitle: Mapped[str | None]\n    origtoppid: Mapped[int | None]\n    oppidcfdanum: Mapped[str | None]\n    origoppnum: Mapped[str | None]\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]"}
{"path":"documentation/wiki/product/decisions/infra/0004-separate-terraform-backend-configs-into-separate-config-files.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/infra/0004-separate-terraform-backend-configs-into-separate-config-files.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"File: api/src/db/models/legacy_mixin/synopsis_mixin.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/legacy_mixin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/legacy_mixin/synopsis_mixin.py\nSize: 6.63 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/infra/0005-database-module-design.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/infra/0005-database-module-design.md","size":894508,"lastModified":"2025-02-14T17:08:31.130Z","content":"import datetime\n\nfrom sqlalchemy.orm import Mapped, declarative_mixin, mapped_column\n\n\n@declarative_mixin\nclass TsynopsisMixin:\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    posting_date: Mapped[datetime.datetime | None]\n    response_date: Mapped[datetime.datetime | None]\n    archive_date: Mapped[datetime.datetime | None]\n    unarchive_date: Mapped[datetime.datetime | None]\n    syn_desc: Mapped[str | None]\n    oth_cat_fa_desc: Mapped[str | None]\n    agency_addr_desc: Mapped[str | None]\n    cost_sharing: Mapped[str | None]\n    number_of_awards: Mapped[str | None]\n    est_funding: Mapped[str | None]\n    award_ceiling: Mapped[str | None]\n    award_floor: Mapped[str | None]\n    fd_link_url: Mapped[str | None]\n    fd_link_desc: Mapped[str | None]\n    agency_contact_desc: Mapped[str | None]\n    ac_email_addr: Mapped[str | None]\n    ac_email_desc: Mapped[str | None]\n    agency_name: Mapped[str | None]\n    agency_phone: Mapped[str | None]\n    a_sa_code: Mapped[str | None]\n    ac_phone_number: Mapped[str | None]\n    ac_name: Mapped[str | None]\n    create_ts: Mapped[datetime.datetime]\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n    sendmail: Mapped[str | None]\n    response_date_desc: Mapped[str | None]\n    applicant_elig_desc: Mapped[str | None]\n    version_nbr: Mapped[int | None]\n    modification_comments: Mapped[str | None]\n    publisheruid: Mapped[str | None]\n    publisher_profile_id: Mapped[int | None]\n\n\n@declarative_mixin\nclass TsynopsisHistMixin:\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    revision_number: Mapped[int] = mapped_column(primary_key=True)\n    posting_date: Mapped[datetime.datetime | None]\n    response_date: Mapped[datetime.datetime | None]\n    archive_date: Mapped[datetime.datetime | None]\n    unarchive_date: Mapped[datetime.datetime | None]\n    syn_desc: Mapped[str | None]\n    oth_cat_fa_desc: Mapped[str | None]\n    agency_addr_desc: Mapped[str | None]\n    cost_sharing: Mapped[str | None]\n    number_of_awards: Mapped[str | None]\n    est_funding: Mapped[str | None]\n    award_ceiling: Mapped[str | None]\n    award_floor: Mapped[str | None]\n    fd_link_url: Mapped[str | None]\n    fd_link_desc: Mapped[str | None]\n    agency_contact_desc: Mapped[str | None]\n    ac_email_addr: Mapped[str | None]\n    ac_email_desc: Mapped[str | None]\n    agency_name: Mapped[str | None]\n    agency_phone: Mapped[str | None]\n    a_sa_code: Mapped[str | None]\n    ac_phone_number: Mapped[str | None]\n    ac_name: Mapped[str | None]\n    create_ts: Mapped[datetime.datetime]\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n    sendmail: Mapped[str | None]\n    response_date_desc: Mapped[str | None]\n    applicant_elig_desc: Mapped[str | None]\n    action_type: Mapped[str | None]\n    action_date: Mapped[datetime.datetime | None]\n    version_nbr: Mapped[int]\n    modification_comments: Mapped[str | None]\n    publisheruid: Mapped[str | None]\n    publisher_profile_id: Mapped[int | None]\n\n\n@declarative_mixin\nclass TapplicanttypesSynopsisMixin:\n    at_syn_id: Mapped[int] = mapped_column(primary_key=True)\n    at_id: Mapped[str] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n\n\n@declarative_mixin\nclass TapplicanttypesSynopsisHistMixin:\n    at_syn_id: Mapped[int] = mapped_column(primary_key=True)\n    at_id: Mapped[str] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    revision_number: Mapped[int] = mapped_column(primary_key=True)\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n\n\n@declarative_mixin\nclass TfundactcatSynopsisMixin:\n    fac_syn_id: Mapped[int] = mapped_column(primary_key=True)\n    fac_id: Mapped[str] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n\n\n@declarative_mixin\nclass TfundactcatSynopsisHistMixin:\n    fac_syn_id: Mapped[int] = mapped_column(primary_key=True)\n    fac_id: Mapped[str] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    revision_number: Mapped[int] = mapped_column(primary_key=True)\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n\n\n@declarative_mixin\nclass TfundinstrSynopsisMixin:\n    fi_syn_id: Mapped[int] = mapped_column(primary_key=True)\n    fi_id: Mapped[str] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n\n\n@declarative_mixin\nclass TfundinstrSynopsisHistMixin:\n    fi_syn_id: Mapped[int] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(primary_key=True)\n    fi_id: Mapped[str] = mapped_column(primary_key=True)\n    revision_number: Mapped[int] = mapped_column(primary_key=True)\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n\n\nclass TsynopsisAttachmentMixin:\n    syn_att_id: Mapped[int] = mapped_column(primary_key=True)\n    opportunity_id: Mapped[int]\n    att_revision_number: Mapped[int | None]\n    att_type: Mapped[str | None]\n    mime_type: Mapped[str | None]\n    link_url: Mapped[str | None]\n    file_name: Mapped[str | None]\n    file_desc: Mapped[str | None]\n    file_lob: Mapped[bytes | None]\n    file_lob_size: Mapped[int | None]\n    create_date: Mapped[datetime.datetime | None]\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]\n    syn_att_folder_id: Mapped[int | None]"}
{"path":"documentation/wiki/product/decisions/infra/0006-provision-database-users-with-serverless-function.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/infra/0006-provision-database-users-with-serverless-function.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/legacy_mixin/tgroups_mixin.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/legacy_mixin\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/legacy_mixin/tgroups_mixin.py\nSize: 0.59 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/infra/0007-database-migration-architecture.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/infra/0007-database-migration-architecture.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"import datetime\n\nfrom sqlalchemy.orm import Mapped, declarative_mixin, mapped_column\n\n\n@declarative_mixin\nclass TGroupsMixin:\n    keyfield: Mapped[str] = mapped_column(primary_key=True)\n    value: Mapped[str | None]\n    created_date: Mapped[datetime.datetime | None]\n    last_upd_date: Mapped[datetime.datetime | None]\n    creator_id: Mapped[str | None]\n    last_upd_id: Mapped[str | None]"}
{"path":"documentation/wiki/product/decisions/infra/0008-consolidate-infra-config-from-tfvars-files-into-config-module.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/infra/0008-consolidate-infra-config-from-tfvars-files-into-config-module.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/lookup/__init__.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/lookup\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/lookup/__init__.py\nSize: 0.34 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/infra/0009-environment-use-cases.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/infra/0009-environment-use-cases.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"__all__ = [\n    \"Lookup\",\n    \"LookupInt\",\n    \"LookupStr\",\n    \"LookupConfig\",\n    \"LookupTable\",\n    \"LookupRegistry\",\n    \"sync_lookup_values\",\n]"}
{"path":"documentation/wiki/product/decisions/infra/0010-production-networking-long-term-state.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/infra/0010-production-networking-long-term-state.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/lookup/lookup.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/lookup\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/lookup/lookup.py\nSize: 3.90 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/decisions/infra/README.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions/infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/infra/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"T = TypeVar(\"T\", StrEnum, IntEnum)\n\n\n@dataclasses.dataclass\nclass Lookup(Generic[T], ABC, metaclass=ABCMeta):\n    \"\"\"\n    A class which handles mapping a specific enum\n    member to additional metadata.\n\n    At the moment, it only specifies a lookup value in\n    the DB, but we can expand this to include configuration\n    for additional member-specific fields like whether the\n    field is deprecated, or should be excluded from the API schema\n    \"\"\"\n\n    lookup_enum: T\n    lookup_val: int\n\n    @abstractmethod\n    def get_description(self) -> str:\n        pass\n\n\nclass LookupStr(Lookup[StrEnum]):\n    def get_description(self) -> str:\n        return self.lookup_enum\n\n\nclass LookupInt(Lookup[IntEnum]):\n    def get_description(self) -> str:\n        return self.lookup_enum.name\n\n\nclass LookupConfig(Generic[T]):\n    \"\"\"\n    Configuration object for storing lookup mapping\n    information. Helps with the conversion of our\n    enums to lookup integers in the DB, and vice-versa.\n    \"\"\"\n\n    _enums: Tuple[Type[T], ...]\n    _enum_to_lookup_map: dict[T, Lookup]\n    _int_to_lookup_map: dict[int, Lookup]\n\n    def __init__(self, lookups: list[Lookup]) -> None:\n        enum_types_seen: set[Type[T]] = set()\n        _enum_to_lookup_map: dict[T, Lookup] = {}\n        _int_to_lookup_map: dict[int, Lookup] = {}\n\n        for lookup in lookups:\n            if lookup.lookup_enum in _enum_to_lookup_map:\n                raise AttributeError(\n                    f\"Duplicate lookup_enum {lookup.lookup_enum} defined, {lookup} + {_enum_to_lookup_map[lookup.lookup_enum]}\"\n                )\n            _enum_to_lookup_map[lookup.lookup_enum] = lookup\n\n            if lookup.lookup_val <= 0:\n                raise AttributeError(\n                    f\"Only positive lookup_val values are allowed, {lookup} not allowed\"\n                )\n\n            if lookup.lookup_val in _int_to_lookup_map:\n                raise AttributeError(\n                    f\"Duplicate lookup_val {lookup.lookup_val} defined, {lookup} + {_int_to_lookup_map[lookup.lookup_val]}\"\n                )\n            _int_to_lookup_map[lookup.lookup_val] = lookup\n\n            enum_types_seen.add(lookup.lookup_enum.__class__)\n\n        # Verify that for each enum in the config\n        # that all of the values were mapped\n        expected_enum_members = set()\n        for enum_type_seen in enum_types_seen:\n            expected_enum_members.update([e for e in enum_type_seen])\n\n        diff = expected_enum_members.difference(_enum_to_lookup_map)\n        if len(diff) > 0:\n            raise AttributeError(\n                f\"Lookup config must define a mapping for all enum values, the following were missing: {diff}\"\n            )\n\n        self._enums: Tuple[Type[T], ...] = tuple(enum_types_seen)\n        self._enum_to_lookup_map: dict[T, Lookup] = _enum_to_lookup_map\n        self._int_to_lookup_map: dict[int, Lookup] = _int_to_lookup_map\n\n    def get_enums(self) -> Tuple[Type[T], ...]:\n        return self._enums\n\n    def get_lookups(self) -> list[Lookup]:\n        return [lk for lk in self._enum_to_lookup_map.values()]\n\n    def get_int_for_enum(self, e: T) -> Optional[int]:\n        \"\"\"\n        Given an enum, get the lookup int for it in the DB\n        \"\"\"\n        lookup = self._enum_to_lookup_map.get(e)\n        if lookup is None:\n            return None\n\n        return lookup.lookup_val\n\n    def get_lookup_for_int(self, num: int) -> Optional[Lookup]:\n        \"\"\"\n        Given a lookup int, get the lookup for it\n        \"\"\"\n        return self._int_to_lookup_map.get(num)\n\n    def get_enum_for_int(self, num: int) -> Optional[T]:\n        \"\"\"\n        Given a lookup int, get the enum for it (via the lookup object)\n        \"\"\"\n        lookup = self.get_lookup_for_int(num)\n        if lookup is None:\n            return None\n        return lookup.lookup_enum"}
{"path":"documentation/wiki/product/decisions/template.md","language":"markdown","type":"code","directory":"documentation/wiki/product/decisions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/decisions/template.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/lookup/lookup_registry.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/lookup\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/lookup/lookup_registry.py\nSize: 3.00 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/deliverables/README.md","language":"markdown","type":"code","directory":"documentation/wiki/product/deliverables","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/deliverables/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"from src.db.models.lookup.lookup import LookupConfig\nfrom src.db.models.lookup.lookup_table import LookupTable\n\nlogger = logging.getLogger(__name__)\n\nL = TypeVar(\"L\", bound=LookupTable)\n\n\nclass LookupRegistry:\n    _lookup_registry: dict[Type[LookupTable], LookupConfig] = {}\n\n    @classmethod\n    def register_lookup(cls, lookup: LookupConfig) -> Callable[[Type[L]], Type[L]]:\n        \"\"\"\n        Attach the lookup class mapping to a particular lookup table.\n\n        Can be used as::\n\n        @LookupRegistry.register_lookup(lookup_constants.MY_LOOKUP_CONFIG)\n        class LkMyLookup(LookupTable):\n            pass\n\n        \"\"\"\n\n        def decorator(lookup_table: Type[L]) -> Type[L]:\n            if lookup_table in cls._lookup_registry:\n                raise Exception(\n                    f\"Cannot attach lookup mapping to table {lookup_table.get_table_name()}, table already registered\"\n                )\n\n            cls._lookup_registry[lookup_table] = lookup\n\n            return lookup_table\n\n        return decorator\n\n    @classmethod\n    def _get_lookup_config(cls, lookup_table: Type[LookupTable]) -> LookupConfig:\n        lookup_config = cls._lookup_registry.get(lookup_table)\n        if lookup_config is None:\n            raise Exception(\n                f\"Table {lookup_table.get_table_name()} does not have a registered lookup_config via register_lookup\"\n            )\n        return lookup_config\n\n    @classmethod\n    def get_lookup_int_for_enum(\n        cls, lookup_table: Type[LookupTable], lookup_enum: StrEnum | IntEnum | None\n    ) -> int | None:\n        \"\"\"\n        Given a Lookup Table + Enum, get the lookup int value to store in the DB\n        \"\"\"\n        if lookup_enum is None:\n            return None\n\n        lookup_config = cls._get_lookup_config(lookup_table)\n\n        return lookup_config.get_int_for_enum(lookup_enum)\n\n    @classmethod\n    def get_enum_for_lookup_int(\n        cls, lookup_table: Type[LookupTable], lookup_val: int | None\n    ) -> Enum | None:\n        \"\"\"\n        Given a Lookup Table + lookup int, get the enum that is mapped to it\n        \"\"\"\n        if lookup_val is None:\n            return None\n\n        lookup_config = cls._get_lookup_config(lookup_table)\n\n        return lookup_config.get_enum_for_int(lookup_val)\n\n    @classmethod\n    def is_valid_type_for_table(\n        cls, lookup_table: Type[LookupTable], lookup_val: Any | None\n    ) -> bool:\n        \"\"\"\n        Given a Lookup Table + a value, return whether it is of a type configured for the that table.\n\n        This makes sure we only try to write enums configured for a certain table to that table.\n        \"\"\"\n\n        # None is always valid\n        if lookup_val is None:\n            return True\n\n        lookup_config = cls._get_lookup_config(lookup_table)\n\n        return isinstance(lookup_val, lookup_config.get_enums())\n\n    @classmethod\n    def get_sync_values(cls) -> dict[Type[LookupTable], LookupConfig]:\n        return cls._lookup_registry"}
{"path":"documentation/wiki/product/deliverables/co-design-group.md","language":"markdown","type":"code","directory":"documentation/wiki/product/deliverables","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/deliverables/co-design-group.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/lookup/lookup_table.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/lookup\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/lookup/lookup_table.py\nSize: 0.37 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/deliverables/get-opportunities.md","language":"markdown","type":"code","directory":"documentation/wiki/product/deliverables","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/deliverables/get-opportunities.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"from src.db.models.base import ApiSchemaTable\nfrom src.db.models.lookup import Lookup\n\nL = TypeVar(\"L\", bound=\"LookupTable\")\n\n\nclass LookupTable(ApiSchemaTable):\n    __abstract__ = True\n\n    @classmethod\n    def from_lookup(cls: Type[L], lookup: Lookup) -> L:\n        raise NotImplementedError(f\"from_lookup must be implemented by {cls.__name__}\")"}
{"path":"documentation/wiki/product/deliverables/open-source-onboarding.md","language":"markdown","type":"code","directory":"documentation/wiki/product/deliverables","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/deliverables/open-source-onboarding.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/lookup/sync_lookup_values.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/lookup\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/lookup/sync_lookup_values.py\nSize: 2.18 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/deliverables/static-site-public-launch.md","language":"markdown","type":"code","directory":"documentation/wiki/product/deliverables","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/deliverables/static-site-public-launch.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"import src.adapters.db as db\nfrom src.adapters.db import PostgresDBClient\nfrom src.adapters.db.clients.postgres_config import get_db_config\nfrom src.db.models.lookup import Lookup, LookupRegistry, LookupTable\n\nlogger = logging.getLogger(__name__)\n\n\ndef sync_lookup_values(db_client: Optional[PostgresDBClient] = None) -> None:\n    \"\"\"\n    Sync lookup values to the DB, adding or updating any\n    values that aren't already present.\n\n    Sync is based on the primary key integer of the lookup\n    tables, so changing the description will work, and adding\n    new ones is possible, but you cannot reuse existing numbers\n    which the utilities prevent anyways.\n    \"\"\"\n    logger.info(\"Beginning sync of lookup values to DB\")\n\n    if not db_client:\n        db_client = PostgresDBClient(get_db_config())\n\n    with db_client.get_session() as db_session, db_session.begin():\n        sync_values = LookupRegistry.get_sync_values()\n\n        for table, lookup_config in sync_values.items():\n            _sync_lookup_for_table(table, lookup_config.get_lookups(), db_session)\n\n\ndef _sync_lookup_for_table(\n    table: Type[LookupTable], lookups: list[Lookup], db_session: db.Session\n) -> None:\n    logger.info(\"Syncing lookup values for table %s\", table.get_table_name())\n\n    # Optimization: Read all rows into the db_session's identity map. This makes\n    # the select query that merge does reference the identity map cache instead of\n    # making a query individually for every lookup value. Locally this brought the\n    # runtime down from 3500ms to 180ms for ~20 tables & ~400 lookup values\n    _ = db_session.query(table).all()\n\n    has_modification = False\n    for lookup in lookups:\n        instance = db_session.merge(table.from_lookup(lookup))\n        if db_session.is_modified(instance):\n            logger.info(\"Updated lookup value in table %s to %r\", table.get_table_name(), lookup)\n            has_modification = True\n\n    if not has_modification:\n        # This is just to make the logs clearer instead of seeing\n        # several \"Syncing lookup values for table ..\" and then nothing in-between\n        logger.info(\"No modified lookup values for table %s\", table.get_table_name())"}
{"path":"documentation/wiki/product/deliverables/static-site-soft-launch.md","language":"markdown","type":"code","directory":"documentation/wiki/product/deliverables","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/deliverables/static-site-soft-launch.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/lookup_models.py\nLanguage: py\nType: model\nDirectory: api/src/db/models\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/lookup_models.py\nSize: 9.67 KB\nLast Modified: 2025-02-14T17:08:26.443Z"}
{"path":"documentation/wiki/product/product-roadmap.md","language":"markdown","type":"code","directory":"documentation/wiki/product","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/product-roadmap.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"from src.constants.lookup_constants import (\n    AgencyDownloadFileType,\n    AgencySubmissionNotificationSetting,\n    ApplicantType,\n    ExternalUserType,\n    ExtractType,\n    FundingCategory,\n    FundingInstrument,\n    JobStatus,\n    OpportunityCategory,\n    OpportunityStatus,\n)\nfrom src.db.models.base import TimestampMixin\nfrom src.db.models.lookup import Lookup, LookupConfig, LookupRegistry, LookupStr, LookupTable\n\nOPPORTUNITY_STATUS_CONFIG = LookupConfig(\n    [\n        LookupStr(OpportunityStatus.FORECASTED, 1),\n        LookupStr(OpportunityStatus.POSTED, 2),\n        LookupStr(OpportunityStatus.CLOSED, 3),\n        LookupStr(OpportunityStatus.ARCHIVED, 4),\n    ]\n)\n\nOPPORTUNITY_CATEGORY_CONFIG = LookupConfig(\n    [\n        LookupStr(OpportunityCategory.DISCRETIONARY, 1),\n        LookupStr(OpportunityCategory.MANDATORY, 2),\n        LookupStr(OpportunityCategory.CONTINUATION, 3),\n        LookupStr(OpportunityCategory.EARMARK, 4),\n        LookupStr(OpportunityCategory.OTHER, 5),\n    ]\n)\n\nAPPLICANT_TYPE_CONFIG = LookupConfig(\n    [\n        LookupStr(ApplicantType.STATE_GOVERNMENTS, 1),\n        LookupStr(ApplicantType.COUNTY_GOVERNMENTS, 2),\n        LookupStr(ApplicantType.CITY_OR_TOWNSHIP_GOVERNMENTS, 3),\n        LookupStr(ApplicantType.SPECIAL_DISTRICT_GOVERNMENTS, 4),\n        LookupStr(ApplicantType.INDEPENDENT_SCHOOL_DISTRICTS, 5),\n        LookupStr(ApplicantType.PUBLIC_AND_STATE_INSTITUTIONS_OF_HIGHER_EDUCATION, 6),\n        LookupStr(ApplicantType.PRIVATE_INSTITUTIONS_OF_HIGHER_EDUCATION, 7),\n        LookupStr(ApplicantType.FEDERALLY_RECOGNIZED_NATIVE_AMERICAN_TRIBAL_GOVERNMENTS, 8),\n        LookupStr(ApplicantType.OTHER_NATIVE_AMERICAN_TRIBAL_ORGANIZATIONS, 9),\n        LookupStr(ApplicantType.PUBLIC_AND_INDIAN_HOUSING_AUTHORITIES, 10),\n        LookupStr(ApplicantType.NONPROFITS_NON_HIGHER_EDUCATION_WITH_501C3, 11),\n        LookupStr(ApplicantType.NONPROFITS_NON_HIGHER_EDUCATION_WITHOUT_501C3, 12),\n        LookupStr(ApplicantType.INDIVIDUALS, 13),\n        LookupStr(ApplicantType.FOR_PROFIT_ORGANIZATIONS_OTHER_THAN_SMALL_BUSINESSES, 14),\n        LookupStr(ApplicantType.SMALL_BUSINESSES, 15),\n        LookupStr(ApplicantType.OTHER, 16),\n        LookupStr(ApplicantType.UNRESTRICTED, 17),\n    ]\n)\n\n\nFUNDING_CATEGORY_CONFIG = LookupConfig(\n    [\n        LookupStr(FundingCategory.RECOVERY_ACT, 1),\n        LookupStr(FundingCategory.AGRICULTURE, 2),\n        LookupStr(FundingCategory.ARTS, 3),\n        LookupStr(FundingCategory.BUSINESS_AND_COMMERCE, 4),\n        LookupStr(FundingCategory.COMMUNITY_DEVELOPMENT, 5),\n        LookupStr(FundingCategory.CONSUMER_PROTECTION, 6),\n        LookupStr(FundingCategory.DISASTER_PREVENTION_AND_RELIEF, 7),\n        LookupStr(FundingCategory.EDUCATION, 8),\n        LookupStr(FundingCategory.EMPLOYMENT_LABOR_AND_TRAINING, 9),\n        LookupStr(FundingCategory.ENERGY, 10),\n        LookupStr(FundingCategory.ENVIRONMENT, 11),\n        LookupStr(FundingCategory.FOOD_AND_NUTRITION, 12),\n        LookupStr(FundingCategory.HEALTH, 13),\n        LookupStr(FundingCategory.HOUSING, 14),\n        LookupStr(FundingCategory.HUMANITIES, 15),\n        LookupStr(FundingCategory.INFRASTRUCTURE_INVESTMENT_AND_JOBS_ACT, 16),\n        LookupStr(FundingCategory.INFORMATION_AND_STATISTICS, 17),\n        LookupStr(FundingCategory.INCOME_SECURITY_AND_SOCIAL_SERVICES, 18),\n        LookupStr(FundingCategory.LAW_JUSTICE_AND_LEGAL_SERVICES, 19),\n        LookupStr(FundingCategory.NATURAL_RESOURCES, 20),\n        LookupStr(FundingCategory.OPPORTUNITY_ZONE_BENEFITS, 21),\n        LookupStr(FundingCategory.REGIONAL_DEVELOPMENT, 22),\n        LookupStr(FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT, 23),\n        LookupStr(FundingCategory.TRANSPORTATION, 24),\n        LookupStr(FundingCategory.AFFORDABLE_CARE_ACT, 25),\n        LookupStr(FundingCategory.OTHER, 26),\n    ]\n)\n\nFUNDING_INSTRUMENT_CONFIG = LookupConfig(\n    [\n        LookupStr(FundingInstrument.COOPERATIVE_AGREEMENT, 1),\n        LookupStr(FundingInstrument.GRANT, 2),\n        LookupStr(FundingInstrument.PROCUREMENT_CONTRACT, 3),\n        LookupStr(FundingInstrument.OTHER, 4),\n    ]\n)\n\nAGENCY_DOWNLOAD_FILE_TYPE_CONFIG = LookupConfig(\n    [LookupStr(AgencyDownloadFileType.XML, 1), LookupStr(AgencyDownloadFileType.PDF, 2)]\n)\n\nAGENCY_SUBMISSION_NOTIFICATION_SETTING_CONFIG = LookupConfig(\n    [\n        LookupStr(AgencySubmissionNotificationSetting.NEVER, 1),\n        LookupStr(AgencySubmissionNotificationSetting.FIRST_APPLICATION_ONLY, 2),\n        LookupStr(AgencySubmissionNotificationSetting.ALWAYS, 3),\n    ]\n)\n\nJOB_STATUS_CONFIG = LookupConfig(\n    [\n        LookupStr(JobStatus.STARTED, 1),\n        LookupStr(JobStatus.COMPLETED, 2),\n        LookupStr(JobStatus.FAILED, 3),\n    ]\n)\n\nEXTERNAL_USER_TYPE_CONFIG = LookupConfig([LookupStr(ExternalUserType.LOGIN_GOV, 1)])\n\nEXTRACT_TYPE_CONFIG = LookupConfig(\n    [\n        LookupStr(ExtractType.OPPORTUNITIES_JSON, 1),\n        LookupStr(ExtractType.OPPORTUNITIES_CSV, 2),\n    ]\n)\n\n\n@LookupRegistry.register_lookup(OPPORTUNITY_CATEGORY_CONFIG)\nclass LkOpportunityCategory(LookupTable, TimestampMixin):\n    __tablename__ = \"lk_opportunity_category\"\n\n    opportunity_category_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str]\n\n    @classmethod\n    def from_lookup(cls, lookup: Lookup) -> \"LkOpportunityCategory\":\n        return LkOpportunityCategory(\n            opportunity_category_id=lookup.lookup_val, description=lookup.get_description()\n        )\n\n\n@LookupRegistry.register_lookup(APPLICANT_TYPE_CONFIG)\nclass LkApplicantType(LookupTable, TimestampMixin):\n    __tablename__ = \"lk_applicant_type\"\n\n    applicant_type_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str]\n\n    @classmethod\n    def from_lookup(cls, lookup: Lookup) -> \"LkApplicantType\":\n        return LkApplicantType(\n            applicant_type_id=lookup.lookup_val, description=lookup.get_description()\n        )\n\n\n@LookupRegistry.register_lookup(FUNDING_CATEGORY_CONFIG)\nclass LkFundingCategory(LookupTable, TimestampMixin):\n    __tablename__ = \"lk_funding_category\"\n\n    funding_category_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str]\n\n    @classmethod\n    def from_lookup(cls, lookup: Lookup) -> \"LkFundingCategory\":\n        return LkFundingCategory(\n            funding_category_id=lookup.lookup_val, description=lookup.get_description()\n        )\n\n\n@LookupRegistry.register_lookup(FUNDING_INSTRUMENT_CONFIG)\nclass LkFundingInstrument(LookupTable, TimestampMixin):\n    __tablename__ = \"lk_funding_instrument\"\n\n    funding_instrument_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str]\n\n    @classmethod\n    def from_lookup(cls, lookup: Lookup) -> \"LkFundingInstrument\":\n        return LkFundingInstrument(\n            funding_instrument_id=lookup.lookup_val, description=lookup.get_description()\n        )\n\n\n@LookupRegistry.register_lookup(OPPORTUNITY_STATUS_CONFIG)\nclass LkOpportunityStatus(LookupTable, TimestampMixin):\n    __tablename__ = \"lk_opportunity_status\"\n\n    opportunity_status_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str]\n\n    @classmethod\n    def from_lookup(cls, lookup: Lookup) -> \"LkOpportunityStatus\":\n        return LkOpportunityStatus(\n            opportunity_status_id=lookup.lookup_val, description=lookup.get_description()\n        )\n\n\n@LookupRegistry.register_lookup(AGENCY_DOWNLOAD_FILE_TYPE_CONFIG)\nclass LkAgencyDownloadFileType(LookupTable, TimestampMixin):\n    __tablename__ = \"lk_agency_download_file_type\"\n\n    agency_download_file_type_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str]\n\n    @classmethod\n    def from_lookup(cls, lookup: Lookup) -> \"LkAgencyDownloadFileType\":\n        return LkAgencyDownloadFileType(\n            agency_download_file_type_id=lookup.lookup_val, description=lookup.get_description()\n        )\n\n\n@LookupRegistry.register_lookup(AGENCY_SUBMISSION_NOTIFICATION_SETTING_CONFIG)\nclass LkAgencySubmissionNotificationSetting(LookupTable, TimestampMixin):\n    __tablename__ = \"lk_agency_submission_notification_setting\"\n\n    agency_submission_notification_setting_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str]\n\n    @classmethod\n    def from_lookup(cls, lookup: Lookup) -> \"LkAgencySubmissionNotificationSetting\":\n        return LkAgencySubmissionNotificationSetting(\n            agency_submission_notification_setting_id=lookup.lookup_val,\n            description=lookup.get_description(),\n        )\n\n\n@LookupRegistry.register_lookup(EXTERNAL_USER_TYPE_CONFIG)\nclass LkExternalUserType(LookupTable, TimestampMixin):\n    __tablename__ = \"lk_external_user_type\"\n\n    external_user_type_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str]\n\n    @classmethod\n    def from_lookup(cls, lookup: Lookup) -> \"LkExternalUserType\":\n        return LkExternalUserType(\n            external_user_type_id=lookup.lookup_val, description=lookup.get_description()\n        )\n\n\n@LookupRegistry.register_lookup(EXTRACT_TYPE_CONFIG)\nclass LkExtractType(LookupTable, TimestampMixin):\n    __tablename__ = \"lk_extract_type\"\n\n    extract_type_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str]\n\n    @classmethod\n    def from_lookup(cls, lookup: Lookup) -> \"LkExtractType\":\n        return LkExtractType(\n            extract_type_id=lookup.lookup_val, description=lookup.get_description()\n        )\n\n\n@LookupRegistry.register_lookup(JOB_STATUS_CONFIG)\nclass LkJobStatus(LookupTable, TimestampMixin):\n    __tablename__ = \"lk_job_status\"\n\n    job_status_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str]\n\n    @classmethod\n    def from_lookup(cls, lookup: Lookup) -> \"LkJobStatus\":\n        return LkJobStatus(job_status_id=lookup.lookup_val, description=lookup.get_description())"}
{"path":"documentation/wiki/product/simpler-grants.gov-analytics/README.md","language":"markdown","type":"code","directory":"documentation/wiki/product/simpler-grants.gov-analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/simpler-grants.gov-analytics/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/opportunity_models.py\nLanguage: py\nType: model\nDirectory: api/src/db/models\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/opportunity_models.py\nSize: 16.86 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"documentation/wiki/product/simpler-grants.gov-analytics/api-metrics.md","language":"markdown","type":"code","directory":"documentation/wiki/product/simpler-grants.gov-analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/simpler-grants.gov-analytics/api-metrics.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"from sqlalchemy import BigInteger, ForeignKey, UniqueConstraint\nfrom sqlalchemy.ext.associationproxy import AssociationProxy, association_proxy\nfrom sqlalchemy.orm import Mapped, mapped_column, relationship\n\nfrom src.adapters.db.type_decorators.postgres_type_decorators import LookupColumn\nfrom src.constants.lookup_constants import (\n    ApplicantType,\n    FundingCategory,\n    FundingInstrument,\n    OpportunityCategory,\n    OpportunityStatus,\n)\nfrom src.db.models.agency_models import Agency\nfrom src.db.models.base import ApiSchemaTable, TimestampMixin\nfrom src.db.models.lookup_models import (\n    LkApplicantType,\n    LkFundingCategory,\n    LkFundingInstrument,\n    LkOpportunityCategory,\n    LkOpportunityStatus,\n)\n\nif TYPE_CHECKING:\n    from src.db.models.user_models import UserSavedOpportunity\n\n\nclass Opportunity(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"opportunity\"\n\n    opportunity_id: Mapped[int] = mapped_column(BigInteger, primary_key=True)\n\n    opportunity_number: Mapped[str | None]\n    opportunity_title: Mapped[str | None] = mapped_column(index=True)\n    agency_code: Mapped[str | None] = mapped_column(index=True)\n\n    @property\n    def agency(self) -> str | None:\n        # TODO - this is temporary until the frontend no longer needs this name\n        return self.agency_code\n\n    category: Mapped[OpportunityCategory | None] = mapped_column(\n        \"opportunity_category_id\",\n        LookupColumn(LkOpportunityCategory),\n        ForeignKey(LkOpportunityCategory.opportunity_category_id),\n        index=True,\n    )\n    category_explanation: Mapped[str | None]\n\n    is_draft: Mapped[bool] = mapped_column(index=True)\n\n    revision_number: Mapped[int | None]\n    modified_comments: Mapped[str | None]\n\n    # These presumably refer to the TUSER_ACCOUNT, and TUSER_PROFILE tables\n    # although the legacy DB does not have them setup as foreign keys\n    publisher_user_id: Mapped[str | None]\n    publisher_profile_id: Mapped[int | None] = mapped_column(BigInteger)\n\n    opportunity_attachments: Mapped[list[\"OpportunityAttachment\"]] = relationship(\n        back_populates=\"opportunity\", uselist=True, cascade=\"all, delete-orphan\"\n    )\n\n    opportunity_assistance_listings: Mapped[list[\"OpportunityAssistanceListing\"]] = relationship(\n        back_populates=\"opportunity\", uselist=True, cascade=\"all, delete-orphan\"\n    )\n\n    opportunity_change_audit: Mapped[\"OpportunityChangeAudit | None\"] = relationship(\n        back_populates=\"opportunity\", single_parent=True, cascade=\"all, delete-orphan\"\n    )\n\n    current_opportunity_summary: Mapped[\"CurrentOpportunitySummary | None\"] = relationship(\n        back_populates=\"opportunity\", single_parent=True, cascade=\"all, delete-orphan\"\n    )\n\n    all_opportunity_summaries: Mapped[list[\"OpportunitySummary\"]] = relationship(\n        back_populates=\"opportunity\", uselist=True, cascade=\"all, delete-orphan\"\n    )\n\n    saved_opportunities_by_users: Mapped[list[\"UserSavedOpportunity\"]] = relationship(\n        \"UserSavedOpportunity\",\n        back_populates=\"opportunity\",\n        uselist=True,\n        cascade=\"all, delete-orphan\",\n    )\n\n    agency_record: Mapped[Agency | None] = relationship(\n        Agency,\n        primaryjoin=\"Opportunity.agency_code == foreign(Agency.agency_code)\",\n        uselist=False,\n        viewonly=True,\n    )\n\n    @property\n    def top_level_agency_name(self) -> str | None:\n        if self.agency_record is not None and self.agency_record.top_level_agency is not None:\n            return self.agency_record.top_level_agency.agency_name\n\n        return self.agency_name\n\n    @property\n    def agency_name(self) -> str | None:\n        # Fetch the agency name from the agency table record (if one was found)\n        if self.agency_record is not None:\n            return self.agency_record.agency_name\n\n        return None\n\n    @property\n    def summary(self) -> \"OpportunitySummary | None\":\n        \"\"\"\n        Utility getter method for converting an Opportunity in our endpoints\n\n        This handles mapping the current opportunity summary to the \"summary\" object\n         in our API responses - handling nullablity as well.\n        \"\"\"\n        if self.current_opportunity_summary is None:\n            return None\n\n        return self.current_opportunity_summary.opportunity_summary\n\n    @property\n    def opportunity_status(self) -> OpportunityStatus | None:\n        if self.current_opportunity_summary is None:\n            return None\n\n        return self.current_opportunity_summary.opportunity_status\n\n    @property\n    def all_forecasts(self) -> list[\"OpportunitySummary\"]:\n        # Utility method for getting all forecasted summary records attached to the opportunity\n        # Note this will include historical and deleted records.\n        return [summary for summary in self.all_opportunity_summaries if summary.is_forecast]\n\n    @property\n    def all_non_forecasts(self) -> list[\"OpportunitySummary\"]:\n        # Utility method for getting all forecasted summary records attached to the opportunity\n        # Note this will include historical and deleted records.\n        return [summary for summary in self.all_opportunity_summaries if not summary.is_forecast]\n\n\nclass OpportunitySummary(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"opportunity_summary\"\n\n    __table_args__ = (\n        # nulls not distinct makes it so nulls work in the unique constraint\n        UniqueConstraint(\n            \"is_forecast\", \"revision_number\", \"opportunity_id\", postgresql_nulls_not_distinct=True\n        ),\n        # Need to define the table args like this to inherit whatever we set on the super table\n        # otherwise we end up overwriting things and Alembic remakes the whole table\n        ApiSchemaTable.__table_args__,\n    )\n\n    opportunity_summary_id: Mapped[int] = mapped_column(BigInteger, primary_key=True)\n\n    opportunity_id: Mapped[int] = mapped_column(\n        BigInteger, ForeignKey(Opportunity.opportunity_id), index=True\n    )\n    opportunity: Mapped[Opportunity] = relationship(Opportunity)\n\n    summary_description: Mapped[str | None]\n\n    is_cost_sharing: Mapped[bool | None]\n    is_forecast: Mapped[bool]\n\n    post_date: Mapped[date | None]\n    close_date: Mapped[date | None]\n    close_date_description: Mapped[str | None]\n    archive_date: Mapped[date | None]\n    unarchive_date: Mapped[date | None]\n\n    # The award amounts can be for several billion requiring us to use BigInteger\n    expected_number_of_awards: Mapped[int | None] = mapped_column(BigInteger)\n    estimated_total_program_funding: Mapped[int | None] = mapped_column(BigInteger)\n    award_floor: Mapped[int | None] = mapped_column(BigInteger)\n    award_ceiling: Mapped[int | None] = mapped_column(BigInteger)\n\n    additional_info_url: Mapped[str | None]\n    additional_info_url_description: Mapped[str | None]\n\n    # Only if the summary is forecasted\n    forecasted_post_date: Mapped[date | None]\n    forecasted_close_date: Mapped[date | None]\n    forecasted_close_date_description: Mapped[str | None]\n    forecasted_award_date: Mapped[date | None]\n    forecasted_project_start_date: Mapped[date | None]\n    fiscal_year: Mapped[int | None]\n\n    revision_number: Mapped[int | None]\n    modification_comments: Mapped[str | None]\n\n    funding_category_description: Mapped[str | None]\n    applicant_eligibility_description: Mapped[str | None]\n\n    agency_phone_number: Mapped[str | None]\n    agency_contact_description: Mapped[str | None]\n    agency_email_address: Mapped[str | None]\n    agency_email_address_description: Mapped[str | None]\n\n    is_deleted: Mapped[bool | None]\n\n    version_number: Mapped[int | None]\n    can_send_mail: Mapped[bool | None]\n    publisher_profile_id: Mapped[int | None] = mapped_column(BigInteger)\n    publisher_user_id: Mapped[str | None]\n    updated_by: Mapped[str | None]\n    created_by: Mapped[str | None]\n\n    # Do not use these agency fields, they're kept for now, but\n    # are simply copying behavior from the legacy system - prefer\n    # the same named values in the opportunity itself\n    agency_code: Mapped[str | None]\n    agency_name: Mapped[str | None]\n\n    link_funding_instruments: Mapped[list[\"LinkOpportunitySummaryFundingInstrument\"]] = (\n        relationship(\n            back_populates=\"opportunity_summary\", uselist=True, cascade=\"all, delete-orphan\"\n        )\n    )\n    link_funding_categories: Mapped[list[\"LinkOpportunitySummaryFundingCategory\"]] = relationship(\n        back_populates=\"opportunity_summary\", uselist=True, cascade=\"all, delete-orphan\"\n    )\n    link_applicant_types: Mapped[list[\"LinkOpportunitySummaryApplicantType\"]] = relationship(\n        back_populates=\"opportunity_summary\", uselist=True, cascade=\"all, delete-orphan\"\n    )\n\n    # Create an association proxy for each of the link table relationships\n    # https://docs.sqlalchemy.org/en/20/orm/extensions/associationproxy.html\n    #\n    # This lets us use these values as if they were just ordinary lists on a python\n    # object. For example::\n    #\n    #   opportunity.funding_instruments.add(FundingInstrument.GRANT)\n    #\n    # will add a row to the link_opportunity_summary_funding_instrument table itself\n    # and is still capable of using all of our column mapping code uneventfully.\n    funding_instruments: AssociationProxy[set[FundingInstrument]] = association_proxy(\n        \"link_funding_instruments\",\n        \"funding_instrument\",\n        creator=lambda obj: LinkOpportunitySummaryFundingInstrument(funding_instrument=obj),\n    )\n    funding_categories: AssociationProxy[set[FundingCategory]] = association_proxy(\n        \"link_funding_categories\",\n        \"funding_category\",\n        creator=lambda obj: LinkOpportunitySummaryFundingCategory(funding_category=obj),\n    )\n    applicant_types: AssociationProxy[set[ApplicantType]] = association_proxy(\n        \"link_applicant_types\",\n        \"applicant_type\",\n        creator=lambda obj: LinkOpportunitySummaryApplicantType(applicant_type=obj),\n    )\n\n    # We configure a relationship from a summary to the current opportunity summary\n    # Just in case we delete this record, we can cascade to deleting the current_opportunity_summary\n    # record as well automatically.\n    current_opportunity_summary: Mapped[\"CurrentOpportunitySummary | None\"] = relationship(\n        back_populates=\"opportunity_summary\", single_parent=True, cascade=\"delete\"\n    )\n\n    def for_json(self) -> dict:\n        json_valid_dict = super().for_json()\n\n        # The proxy values don't end up in the JSON as they aren't columns\n        # so manually add them.\n        json_valid_dict[\"funding_instruments\"] = self.funding_instruments\n        json_valid_dict[\"funding_categories\"] = self.funding_categories\n        json_valid_dict[\"applicant_types\"] = self.applicant_types\n\n        return json_valid_dict\n\n    def can_summary_be_public(self, current_date: date) -> bool:\n        \"\"\"\n        Utility method to check whether a summary object\n        \"\"\"\n        if self.is_deleted:\n            return False\n\n        if self.post_date is None or self.post_date > current_date:\n            return False\n\n        return True\n\n\nclass OpportunityAssistanceListing(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"opportunity_assistance_listing\"\n\n    opportunity_assistance_listing_id: Mapped[int] = mapped_column(BigInteger, primary_key=True)\n\n    opportunity_id: Mapped[int] = mapped_column(\n        BigInteger, ForeignKey(Opportunity.opportunity_id), index=True\n    )\n    opportunity: Mapped[Opportunity] = relationship(Opportunity)\n\n    assistance_listing_number: Mapped[str | None]\n    program_title: Mapped[str | None]\n\n    updated_by: Mapped[str | None]\n    created_by: Mapped[str | None]\n\n\nclass LinkOpportunitySummaryFundingInstrument(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"link_opportunity_summary_funding_instrument\"\n\n    __table_args__ = (\n        # We want a unique constraint so that legacy IDs are unique for a given opportunity summary\n        UniqueConstraint(\"opportunity_summary_id\", \"legacy_funding_instrument_id\"),\n        # Need to define the table args like this to inherit whatever we set on the super table\n        # otherwise we end up overwriting things and Alembic remakes the whole table\n        ApiSchemaTable.__table_args__,\n    )\n\n    opportunity_summary_id: Mapped[int] = mapped_column(\n        BigInteger,\n        ForeignKey(OpportunitySummary.opportunity_summary_id),\n        primary_key=True,\n        index=True,\n    )\n    opportunity_summary: Mapped[OpportunitySummary] = relationship(OpportunitySummary)\n\n    funding_instrument: Mapped[FundingInstrument] = mapped_column(\n        \"funding_instrument_id\",\n        LookupColumn(LkFundingInstrument),\n        ForeignKey(LkFundingInstrument.funding_instrument_id),\n        primary_key=True,\n        index=True,\n    )\n\n    legacy_funding_instrument_id: Mapped[int | None]\n\n    updated_by: Mapped[str | None]\n    created_by: Mapped[str | None]\n\n\nclass LinkOpportunitySummaryFundingCategory(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"link_opportunity_summary_funding_category\"\n\n    __table_args__ = (\n        # We want a unique constraint so that legacy IDs are unique for a given opportunity summary\n        UniqueConstraint(\"opportunity_summary_id\", \"legacy_funding_category_id\"),\n        # Need to define the table args like this to inherit whatever we set on the super table\n        # otherwise we end up overwriting things and Alembic remakes the whole table\n        ApiSchemaTable.__table_args__,\n    )\n\n    opportunity_summary_id: Mapped[int] = mapped_column(\n        BigInteger,\n        ForeignKey(OpportunitySummary.opportunity_summary_id),\n        primary_key=True,\n        index=True,\n    )\n    opportunity_summary: Mapped[OpportunitySummary] = relationship(OpportunitySummary)\n\n    funding_category: Mapped[FundingCategory] = mapped_column(\n        \"funding_category_id\",\n        LookupColumn(LkFundingCategory),\n        ForeignKey(LkFundingCategory.funding_category_id),\n        primary_key=True,\n        index=True,\n    )\n\n    legacy_funding_category_id: Mapped[int | None]\n\n    updated_by: Mapped[str | None]\n    created_by: Mapped[str | None]\n\n\nclass LinkOpportunitySummaryApplicantType(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"link_opportunity_summary_applicant_type\"\n\n    __table_args__ = (\n        # We want a unique constraint so that legacy IDs are unique for a given opportunity summary\n        UniqueConstraint(\"opportunity_summary_id\", \"legacy_applicant_type_id\"),\n        # Need to define the table args like this to inherit whatever we set on the super table\n        # otherwise we end up overwriting things and Alembic remakes the whole table\n        ApiSchemaTable.__table_args__,\n    )\n\n    opportunity_summary_id: Mapped[int] = mapped_column(\n        BigInteger,\n        ForeignKey(OpportunitySummary.opportunity_summary_id),\n        primary_key=True,\n        index=True,\n    )\n    opportunity_summary: Mapped[OpportunitySummary] = relationship(OpportunitySummary)\n\n    applicant_type: Mapped[ApplicantType] = mapped_column(\n        \"applicant_type_id\",\n        LookupColumn(LkApplicantType),\n        ForeignKey(LkApplicantType.applicant_type_id),\n        primary_key=True,\n        index=True,\n    )\n\n    legacy_applicant_type_id: Mapped[int | None]\n\n    updated_by: Mapped[str | None]\n    created_by: Mapped[str | None]\n\n\nclass CurrentOpportunitySummary(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"current_opportunity_summary\"\n\n    opportunity_id: Mapped[int] = mapped_column(\n        BigInteger, ForeignKey(Opportunity.opportunity_id), primary_key=True, index=True\n    )\n    opportunity: Mapped[Opportunity] = relationship(single_parent=True)\n\n    opportunity_summary_id: Mapped[int] = mapped_column(\n        BigInteger,\n        ForeignKey(OpportunitySummary.opportunity_summary_id),\n        primary_key=True,\n        index=True,\n    )\n    opportunity_summary: Mapped[OpportunitySummary] = relationship(single_parent=True)\n\n    opportunity_status: Mapped[OpportunityStatus] = mapped_column(\n        \"opportunity_status_id\",\n        LookupColumn(LkOpportunityStatus),\n        ForeignKey(LkOpportunityStatus.opportunity_status_id),\n        index=True,\n    )\n\n\nclass OpportunityAttachment(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"opportunity_attachment\"\n\n    attachment_id: Mapped[int] = mapped_column(BigInteger, primary_key=True)\n\n    opportunity_id: Mapped[int] = mapped_column(\n        BigInteger, ForeignKey(Opportunity.opportunity_id), index=True\n    )\n    opportunity: Mapped[Opportunity] = relationship(Opportunity)\n\n    file_location: Mapped[str]\n    mime_type: Mapped[str]\n    file_name: Mapped[str]\n    file_description: Mapped[str]\n    file_size_bytes: Mapped[int] = mapped_column(BigInteger)\n    created_by: Mapped[str | None]\n    updated_by: Mapped[str | None]\n    legacy_folder_id: Mapped[int | None] = mapped_column(BigInteger)\n\n\nclass OpportunityChangeAudit(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"opportunity_change_audit\"\n\n    opportunity_id: Mapped[int] = mapped_column(\n        BigInteger, ForeignKey(Opportunity.opportunity_id), primary_key=True, index=True\n    )\n    opportunity: Mapped[Opportunity] = relationship(Opportunity)"}
{"path":"documentation/wiki/product/simpler-grants.gov-analytics/open-source-community-metrics.md","language":"markdown","type":"code","directory":"documentation/wiki/product/simpler-grants.gov-analytics","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/product/simpler-grants.gov-analytics/open-source-community-metrics.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/staging/__init__.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/staging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/__init__.py\nSize: 0.20 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/.dockerignore","language":"unknown","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/.dockerignore","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"metadata = staging_base.metadata\n\n__all__ = [\"metadata\", \"opportunity\", \"forecast\", \"synopsis\", \"tgroups\", \"attachment\"]"}
{"path":"frontend/.env.development","language":"unknown","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/.env.development","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/staging/attachment.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/staging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/attachment.py\nSize: 0.62 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/.env.production","language":"unknown","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/.env.production","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"from src.db.models.legacy_mixin import synopsis_mixin\nfrom src.db.models.staging.staging_base import StagingBase, StagingParamMixin\n\nfrom .opportunity import Topportunity\n\n\nclass TsynopsisAttachment(StagingBase, synopsis_mixin.TsynopsisAttachmentMixin, StagingParamMixin):\n    __tablename__ = \"tsynopsisattachment\"\n\n    opportunity: Mapped[Topportunity | None] = relationship(\n        Topportunity,\n        primaryjoin=lambda: TsynopsisAttachment.opportunity_id\n        == foreign(Topportunity.opportunity_id),\n        uselist=False,\n        overlaps=\"opportunity\",\n    )"}
{"path":"frontend/.eslintrc.js","language":"javascript","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/.eslintrc.js","size":1644826,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/staging/forecast.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/staging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/forecast.py\nSize: 5.78 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/.pa11yci-desktop.json","language":"json","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/.pa11yci-desktop.json","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"from src.db.models.staging.staging_base import StagingBase, StagingParamMixin\n\nfrom ..legacy_mixin import forecast_mixin\nfrom .opportunity import Topportunity\n\n\nclass Tforecast(StagingBase, forecast_mixin.TforecastMixin, StagingParamMixin):\n    __tablename__ = \"tforecast\"\n\n    opportunity: Mapped[Topportunity | None] = relationship(\n        Topportunity,\n        primaryjoin=\"Tforecast.opportunity_id == foreign(Topportunity.opportunity_id)\",\n        uselist=False,\n        overlaps=\"opportunity\",\n    )\n\n    @property\n    def is_forecast(self) -> bool:\n        return True\n\n    @property\n    def is_historical_table(self) -> bool:\n        return False\n\n    @property\n    def description(self) -> str | None:\n        return self.forecast_desc\n\n    @property\n    def agency_phone_number(self) -> str | None:\n        return self.ac_phone\n\n\nclass TforecastHist(StagingBase, forecast_mixin.TforecastHistMixin, StagingParamMixin):\n    __tablename__ = \"tforecast_hist\"\n\n    opportunity: Mapped[Topportunity | None] = relationship(\n        Topportunity,\n        primaryjoin=\"TforecastHist.opportunity_id == foreign(Topportunity.opportunity_id)\",\n        uselist=False,\n        overlaps=\"opportunity\",\n    )\n\n    @property\n    def is_forecast(self) -> bool:\n        return True\n\n    @property\n    def is_historical_table(self) -> bool:\n        return True\n\n    @property\n    def description(self) -> str | None:\n        return self.forecast_desc\n\n    @property\n    def agency_phone_number(self) -> str | None:\n        return self.ac_phone\n\n\nclass TapplicanttypesForecast(\n    StagingBase, forecast_mixin.TapplicanttypesForecastMixin, StagingParamMixin\n):\n    __tablename__ = \"tapplicanttypes_forecast\"\n\n    forecast: Mapped[Tforecast | None] = relationship(\n        Tforecast,\n        primaryjoin=\"TapplicanttypesForecast.opportunity_id == foreign(Tforecast.opportunity_id)\",\n        uselist=False,\n        overlaps=\"forecast\",\n    )\n\n    @property\n    def legacy_applicant_type_id(self) -> int:\n        return self.at_frcst_id\n\n    @property\n    def is_forecast(self) -> bool:\n        return True\n\n    @property\n    def revision_number(self) -> None:\n        return None\n\n    @property\n    def is_historical_table(self) -> bool:\n        return False\n\n\nclass TapplicanttypesForecastHist(\n    StagingBase, forecast_mixin.TapplicanttypesForecastHistMixin, StagingParamMixin\n):\n    __tablename__ = \"tapplicanttypes_forecast_hist\"\n\n    forecast: Mapped[TforecastHist | None] = relationship(\n        TforecastHist,\n        primaryjoin=\"and_(TapplicanttypesForecastHist.opportunity_id == foreign(TforecastHist.opportunity_id), TapplicanttypesForecastHist.revision_number == foreign(TforecastHist.revision_number))\",\n        uselist=False,\n        overlaps=\"forecast\",\n    )\n\n    @property\n    def legacy_applicant_type_id(self) -> int:\n        return self.at_frcst_id\n\n    @property\n    def is_forecast(self) -> bool:\n        return True\n\n    @property\n    def is_historical_table(self) -> bool:\n        return True\n\n\nclass TfundactcatForecast(StagingBase, forecast_mixin.TfundactcatForecastMixin, StagingParamMixin):\n    __tablename__ = \"tfundactcat_forecast\"\n\n    forecast: Mapped[Tforecast | None] = relationship(\n        Tforecast,\n        primaryjoin=\"TfundactcatForecast.opportunity_id == foreign(Tforecast.opportunity_id)\",\n        uselist=False,\n        overlaps=\"forecast\",\n    )\n\n    @property\n    def legacy_funding_category_id(self) -> int:\n        return self.fac_frcst_id\n\n    @property\n    def is_forecast(self) -> bool:\n        return True\n\n    @property\n    def revision_number(self) -> None:\n        return None\n\n    @property\n    def is_historical_table(self) -> bool:\n        return False\n\n\nclass TfundactcatForecastHist(\n    StagingBase, forecast_mixin.TfundactcatForecastHistMixin, StagingParamMixin\n):\n    __tablename__ = \"tfundactcat_forecast_hist\"\n\n    forecast: Mapped[TforecastHist | None] = relationship(\n        TforecastHist,\n        primaryjoin=\"and_(TfundactcatForecastHist.opportunity_id == foreign(TforecastHist.opportunity_id), TfundactcatForecastHist.revision_number == foreign(TforecastHist.revision_number))\",\n        uselist=False,\n        overlaps=\"forecast\",\n    )\n\n    @property\n    def legacy_funding_category_id(self) -> int:\n        return self.fac_frcst_id\n\n    @property\n    def is_forecast(self) -> bool:\n        return True\n\n    @property\n    def is_historical_table(self) -> bool:\n        return True\n\n\nclass TfundinstrForecast(StagingBase, forecast_mixin.TfundinstrForecastMixin, StagingParamMixin):\n    __tablename__ = \"tfundinstr_forecast\"\n\n    forecast: Mapped[Tforecast | None] = relationship(\n        Tforecast,\n        primaryjoin=\"TfundinstrForecast.opportunity_id == foreign(Tforecast.opportunity_id)\",\n        uselist=False,\n        overlaps=\"forecast\",\n    )\n\n    @property\n    def legacy_funding_instrument_id(self) -> int:\n        return self.fi_frcst_id\n\n    @property\n    def is_forecast(self) -> bool:\n        return True\n\n    @property\n    def revision_number(self) -> None:\n        return None\n\n    @property\n    def is_historical_table(self) -> bool:\n        return False\n\n\nclass TfundinstrForecastHist(\n    StagingBase, forecast_mixin.TfundinstrForecastHistMixin, StagingParamMixin\n):\n    __tablename__ = \"tfundinstr_forecast_hist\"\n\n    forecast: Mapped[TforecastHist | None] = relationship(\n        TforecastHist,\n        primaryjoin=\"and_(TfundinstrForecastHist.opportunity_id == foreign(TforecastHist.opportunity_id), TfundinstrForecastHist.revision_number == foreign(TforecastHist.revision_number))\",\n        uselist=False,\n        overlaps=\"forecast\",\n    )\n\n    @property\n    def legacy_funding_instrument_id(self) -> int:\n        return self.fi_frcst_id\n\n    @property\n    def is_forecast(self) -> bool:\n        return True\n\n    @property\n    def is_historical_table(self) -> bool:\n        return True"}
{"path":"frontend/.pa11yci-mobile.json","language":"json","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/.pa11yci-mobile.json","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/staging/opportunity.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/staging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/opportunity.py\nSize: 0.80 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/.prettierignore","language":"unknown","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/.prettierignore","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"from src.db.models.legacy_mixin import opportunity_mixin\nfrom src.db.models.staging.staging_base import StagingBase, StagingParamMixin\n\n\nclass Topportunity(StagingBase, opportunity_mixin.TopportunityMixin, StagingParamMixin):\n    __tablename__ = \"topportunity\"\n\n    cfdas: Mapped[list[\"TopportunityCfda\"]] = relationship(\n        primaryjoin=\"Topportunity.opportunity_id == foreign(TopportunityCfda.opportunity_id)\",\n        uselist=True,\n    )\n\n\nclass TopportunityCfda(StagingBase, opportunity_mixin.TopportunityCfdaMixin, StagingParamMixin):\n    __tablename__ = \"topportunity_cfda\"\n\n    opportunity: Mapped[Topportunity | None] = relationship(\n        primaryjoin=\"TopportunityCfda.opportunity_id == foreign(Topportunity.opportunity_id)\",\n        uselist=False,\n    )"}
{"path":"frontend/.prettierrc.js","language":"javascript","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/.prettierrc.js","size":1644826,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/staging/staging_base.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/staging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/staging_base.py\nSize: 2.46 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/.storybook/I18nStoryWrapper.tsx","language":"typescript","type":"code","directory":"frontend/.storybook","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/.storybook/I18nStoryWrapper.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"import sqlalchemy\nfrom sqlalchemy.orm import Mapped, declarative_mixin, mapped_column\n\nfrom src.constants.schema import Schemas\nfrom src.util import datetime_util\n\nmetadata = sqlalchemy.MetaData(\n    naming_convention={\n        \"ix\": \"%(table_name)s_%(column_0_name)s_idx\",\n        \"uq\": \"%(table_name)s_%(column_0_name)s_uniq\",\n        \"ck\": \"%(table_name)s_`%(constraint_name)s_check`\",\n        \"fk\": \"%(table_name)s_%(column_0_name)s_%(referred_table_name)s_fkey\",\n        \"pk\": \"%(table_name)s_pkey\",\n    }\n)\n\n\nclass StagingBase(sqlalchemy.orm.DeclarativeBase):\n    metadata = metadata\n\n    __table_args__ = {\"schema\": Schemas.STAGING}\n\n    # These types are selected so that the underlying Oracle types are mapped to a more general\n    # type. For example all CHAR and VARCHAR types can be mapped to TEXT for simplicity. See\n    # https://github.com/laurenz/oracle_fdw?tab=readme-ov-file#data-types\n    type_annotation_map = {\n        int: sqlalchemy.BigInteger,\n        str: sqlalchemy.Text,\n        datetime.datetime: sqlalchemy.TIMESTAMP(timezone=True),\n    }\n\n    def _dict(self) -> dict:\n        return {c.key: getattr(self, c.key) for c in sqlalchemy.inspect(self).mapper.column_attrs}\n\n    def __repr__(self) -> str:\n        return f\"<{type(self).__name__}({self._dict()!r})\"\n\n    def __rich_repr__(self) -> Iterable[tuple[str, Any]]:\n        \"\"\"Rich repr for interactive console.\n        See https://rich.readthedocs.io/en/latest/pretty.html#rich-repr-protocol\n        \"\"\"\n        return self._dict().items()\n\n\ndef same_as_created_at(context: Any) -> Any:\n    return context.get_current_parameters()[\"created_at\"]\n\n\n@declarative_mixin\nclass StagingParamMixin:\n    is_deleted: Mapped[bool]\n    transformed_at: Mapped[datetime.datetime | None] = mapped_column(index=True)\n\n    created_at: Mapped[datetime.datetime] = mapped_column(\n        nullable=False,\n        default=datetime_util.utcnow,\n        server_default=sqlalchemy.sql.functions.now(),\n    )\n\n    updated_at: Mapped[datetime.datetime] = mapped_column(\n        nullable=False,\n        default=same_as_created_at,\n        onupdate=datetime_util.utcnow,\n        server_default=sqlalchemy.sql.functions.now(),\n    )\n\n    deleted_at: Mapped[datetime.datetime | None] = mapped_column(\n        nullable=True,\n        default=None,\n        server_default=None,\n    )\n\n    transformation_notes: Mapped[str | None]\n\n    @property\n    def is_modified(self) -> bool:\n        return self.transformed_at is None"}
{"path":"frontend/.storybook/main.js","language":"javascript","type":"code","directory":"frontend/.storybook","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/.storybook/main.js","size":1644826,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/staging/synopsis.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/staging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/synopsis.py\nSize: 5.96 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/.storybook/preview.tsx","language":"typescript","type":"code","directory":"frontend/.storybook","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/.storybook/preview.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"from src.db.models.legacy_mixin import synopsis_mixin\nfrom src.db.models.staging.staging_base import StagingBase, StagingParamMixin\n\nfrom .opportunity import Topportunity\n\n\nclass Tsynopsis(StagingBase, synopsis_mixin.TsynopsisMixin, StagingParamMixin):\n    __tablename__ = \"tsynopsis\"\n\n    opportunity: Mapped[Topportunity | None] = relationship(\n        Topportunity,\n        primaryjoin=\"Tsynopsis.opportunity_id == foreign(Topportunity.opportunity_id)\",\n        uselist=False,\n        overlaps=\"opportunity\",\n    )\n\n    @property\n    def is_forecast(self) -> bool:\n        return False\n\n    @property\n    def is_historical_table(self) -> bool:\n        return False\n\n    @property\n    def description(self) -> str | None:\n        return self.syn_desc\n\n    @property\n    def agency_code(self) -> str | None:\n        return self.a_sa_code\n\n    @property\n    def agency_phone_number(self) -> str | None:\n        return self.ac_phone_number\n\n\nclass TsynopsisHist(StagingBase, synopsis_mixin.TsynopsisHistMixin, StagingParamMixin):\n    __tablename__ = \"tsynopsis_hist\"\n\n    opportunity: Mapped[Topportunity | None] = relationship(\n        Topportunity,\n        primaryjoin=\"TsynopsisHist.opportunity_id == foreign(Topportunity.opportunity_id)\",\n        uselist=False,\n        overlaps=\"opportunity\",\n    )\n\n    @property\n    def is_forecast(self) -> bool:\n        return False\n\n    @property\n    def is_historical_table(self) -> bool:\n        return True\n\n    @property\n    def description(self) -> str | None:\n        return self.syn_desc\n\n    @property\n    def agency_code(self) -> str | None:\n        return self.a_sa_code\n\n    @property\n    def agency_phone_number(self) -> str | None:\n        return self.ac_phone_number\n\n\nclass TapplicanttypesSynopsis(\n    StagingBase, synopsis_mixin.TapplicanttypesSynopsisMixin, StagingParamMixin\n):\n    __tablename__ = \"tapplicanttypes_synopsis\"\n\n    synopsis: Mapped[Tsynopsis | None] = relationship(\n        Tsynopsis,\n        primaryjoin=\"TapplicanttypesSynopsis.opportunity_id == foreign(Tsynopsis.opportunity_id)\",\n        uselist=False,\n        overlaps=\"synopsis\",\n    )\n\n    @property\n    def legacy_applicant_type_id(self) -> int:\n        return self.at_syn_id\n\n    @property\n    def is_forecast(self) -> bool:\n        return False\n\n    @property\n    def revision_number(self) -> None:\n        return None\n\n    @property\n    def is_historical_table(self) -> bool:\n        return False\n\n\nclass TapplicanttypesSynopsisHist(\n    StagingBase, synopsis_mixin.TapplicanttypesSynopsisHistMixin, StagingParamMixin\n):\n    __tablename__ = \"tapplicanttypes_synopsis_hist\"\n\n    synopsis: Mapped[TsynopsisHist | None] = relationship(\n        TsynopsisHist,\n        primaryjoin=\"and_(TapplicanttypesSynopsisHist.opportunity_id == foreign(TsynopsisHist.opportunity_id), TapplicanttypesSynopsisHist.revision_number == foreign(TsynopsisHist.revision_number))\",\n        uselist=False,\n        overlaps=\"synopsis\",\n    )\n\n    @property\n    def legacy_applicant_type_id(self) -> int:\n        return self.at_syn_id\n\n    @property\n    def is_forecast(self) -> bool:\n        return False\n\n    @property\n    def is_historical_table(self) -> bool:\n        return True\n\n\nclass TfundactcatSynopsis(StagingBase, synopsis_mixin.TfundactcatSynopsisMixin, StagingParamMixin):\n    __tablename__ = \"tfundactcat_synopsis\"\n\n    synopsis: Mapped[Tsynopsis | None] = relationship(\n        Tsynopsis,\n        primaryjoin=\"TfundactcatSynopsis.opportunity_id == foreign(Tsynopsis.opportunity_id)\",\n        uselist=False,\n        overlaps=\"synopsis\",\n    )\n\n    @property\n    def legacy_funding_category_id(self) -> int:\n        return self.fac_syn_id\n\n    @property\n    def is_forecast(self) -> bool:\n        return False\n\n    @property\n    def revision_number(self) -> None:\n        return None\n\n    @property\n    def is_historical_table(self) -> bool:\n        return False\n\n\nclass TfundactcatSynopsisHist(\n    StagingBase, synopsis_mixin.TfundactcatSynopsisHistMixin, StagingParamMixin\n):\n    __tablename__ = \"tfundactcat_synopsis_hist\"\n\n    synopsis: Mapped[TsynopsisHist | None] = relationship(\n        TsynopsisHist,\n        primaryjoin=\"and_(TfundactcatSynopsisHist.opportunity_id == foreign(TsynopsisHist.opportunity_id), TfundactcatSynopsisHist.revision_number == foreign(TsynopsisHist.revision_number))\",\n        uselist=False,\n        overlaps=\"synopsis\",\n    )\n\n    @property\n    def legacy_funding_category_id(self) -> int:\n        return self.fac_syn_id\n\n    @property\n    def is_forecast(self) -> bool:\n        return False\n\n    @property\n    def is_historical_table(self) -> bool:\n        return True\n\n\nclass TfundinstrSynopsis(StagingBase, synopsis_mixin.TfundinstrSynopsisMixin, StagingParamMixin):\n    __tablename__ = \"tfundinstr_synopsis\"\n\n    synopsis: Mapped[Tsynopsis | None] = relationship(\n        Tsynopsis,\n        primaryjoin=\"TfundinstrSynopsis.opportunity_id == foreign(Tsynopsis.opportunity_id)\",\n        uselist=False,\n        overlaps=\"synopsis\",\n    )\n\n    @property\n    def legacy_funding_instrument_id(self) -> int:\n        return self.fi_syn_id\n\n    @property\n    def is_forecast(self) -> bool:\n        return False\n\n    @property\n    def revision_number(self) -> None:\n        return None\n\n    @property\n    def is_historical_table(self) -> bool:\n        return False\n\n\nclass TfundinstrSynopsisHist(\n    StagingBase, synopsis_mixin.TfundinstrSynopsisHistMixin, StagingParamMixin\n):\n    __tablename__ = \"tfundinstr_synopsis_hist\"\n\n    synopsis: Mapped[TsynopsisHist | None] = relationship(\n        TsynopsisHist,\n        primaryjoin=\"and_(TfundinstrSynopsisHist.opportunity_id == foreign(TsynopsisHist.opportunity_id), TfundinstrSynopsisHist.revision_number == foreign(TsynopsisHist.revision_number))\",\n        uselist=False,\n        overlaps=\"synopsis\",\n    )\n\n    @property\n    def legacy_funding_instrument_id(self) -> int:\n        return self.fi_syn_id\n\n    @property\n    def is_forecast(self) -> bool:\n        return False\n\n    @property\n    def is_historical_table(self) -> bool:\n        return True"}
{"path":"frontend/Dockerfile","language":"unknown","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/Dockerfile","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/staging/tgroups.py\nLanguage: py\nType: model\nDirectory: api/src/db/models/staging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/staging/tgroups.py\nSize: 0.65 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/Makefile","language":"unknown","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/Makefile","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"class Tgroups(StagingBase, tgroups_mixin.TGroupsMixin, StagingParamMixin):\n    __tablename__ = \"tgroups\"\n\n    def get_agency_code(self) -> str:\n        # The keyfield is formatted as:\n        # Agency-<AGENCY CODE>-<field name>\n        # so to get the agency code, we need to parse out the middle bit\n        # so we split and drop the first + last field and rejoin it.\n        tokens = self.keyfield.split(\"-\")\n        return \"-\".join(tokens[1:-1])\n\n    def get_field_name(self) -> str:\n        return self.keyfield.split(\"-\")[-1]"}
{"path":"frontend/README.md","language":"markdown","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/task_models.py\nLanguage: py\nType: model\nDirectory: api/src/db/models\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/task_models.py\nSize: 0.76 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/artillery-load-test.yml","language":"yaml","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/artillery-load-test.yml","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"from sqlalchemy import ForeignKey\nfrom sqlalchemy.dialects.postgresql import JSONB, UUID\nfrom sqlalchemy.orm import Mapped, mapped_column\n\nfrom src.adapters.db.type_decorators.postgres_type_decorators import LookupColumn\nfrom src.db.models.base import ApiSchemaTable, TimestampMixin\nfrom src.db.models.lookup_models import JobStatus, LkJobStatus\n\n\nclass JobLog(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"job_log\"\n\n    job_id: Mapped[uuid.UUID] = mapped_column(UUID, primary_key=True, default=uuid.uuid4)\n    job_type: Mapped[str]\n    job_status: Mapped[JobStatus] = mapped_column(\n        \"job_status_id\",\n        LookupColumn(LkJobStatus),\n        ForeignKey(LkJobStatus.job_status_id),\n    )\n    metrics: Mapped[dict | None] = mapped_column(JSONB)"}
{"path":"frontend/bin/wait-for-frontend.sh","language":"unknown","type":"code","directory":"frontend/bin","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/bin/wait-for-frontend.sh","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/db/models/user_models.py\nLanguage: py\nType: model\nDirectory: api/src/db/models\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/db/models/user_models.py\nSize: 4.90 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/docker-compose-release.yml","language":"yaml","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/docker-compose-release.yml","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"from sqlalchemy import BigInteger, ForeignKey, and_\nfrom sqlalchemy.dialects.postgresql import ARRAY, JSONB, UUID\nfrom sqlalchemy.orm import Mapped, mapped_column, relationship\nfrom sqlalchemy.sql.functions import now as sqlnow\n\nfrom src.adapters.db.type_decorators.postgres_type_decorators import LookupColumn\nfrom src.constants.lookup_constants import ExternalUserType\nfrom src.db.models.base import ApiSchemaTable, TimestampMixin\nfrom src.db.models.lookup_models import LkExternalUserType\nfrom src.db.models.opportunity_models import Opportunity\nfrom src.util import datetime_util\n\n\nclass User(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"user\"\n\n    user_id: Mapped[uuid.UUID] = mapped_column(UUID, primary_key=True, default=uuid.uuid4)\n\n    saved_opportunities: Mapped[list[\"UserSavedOpportunity\"]] = relationship(\n        \"UserSavedOpportunity\",\n        back_populates=\"user\",\n        uselist=True,\n        cascade=\"all, delete-orphan\",\n    )\n\n    saved_searches: Mapped[list[\"UserSavedSearch\"]] = relationship(\n        \"UserSavedSearch\", back_populates=\"user\", uselist=True, cascade=\"all, delete-orphan\"\n    )\n\n    linked_login_gov_external_user: Mapped[\"LinkExternalUser | None\"] = relationship(\n        \"LinkExternalUser\",\n        primaryjoin=lambda: and_(\n            LinkExternalUser.user_id == User.user_id,\n            LinkExternalUser.external_user_type == ExternalUserType.LOGIN_GOV,\n        ),\n        uselist=False,\n        viewonly=True,\n    )\n\n    @property\n    def email(self) -> str | None:\n        if self.linked_login_gov_external_user is not None:\n            return self.linked_login_gov_external_user.email\n        return None\n\n\nclass LinkExternalUser(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"link_external_user\"\n\n    link_external_user_id: Mapped[uuid.UUID] = mapped_column(primary_key=True, default=uuid.uuid4)\n\n    external_user_id: Mapped[str] = mapped_column(index=True, unique=True)\n\n    external_user_type: Mapped[ExternalUserType] = mapped_column(\n        \"external_user_type_id\",\n        LookupColumn(LkExternalUserType),\n        ForeignKey(LkExternalUserType.external_user_type_id),\n        index=True,\n    )\n\n    user_id: Mapped[uuid.UUID] = mapped_column(ForeignKey(User.user_id), index=True)\n    user: Mapped[User] = relationship(User)\n\n    email: Mapped[str]\n\n\nclass UserTokenSession(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"user_token_session\"\n\n    user_id: Mapped[uuid.UUID] = mapped_column(ForeignKey(User.user_id), primary_key=True)\n    user: Mapped[User] = relationship(User)\n\n    token_id: Mapped[uuid.UUID] = mapped_column(primary_key=True)\n\n    expires_at: Mapped[datetime]\n\n    # When a user logs out, we set this flag to False.\n    is_valid: Mapped[bool] = mapped_column(default=True)\n\n\nclass LoginGovState(ApiSchemaTable, TimestampMixin):\n    \"\"\"Table used to store temporary state during the OAuth login flow\"\"\"\n\n    __tablename__ = \"login_gov_state\"\n\n    login_gov_state_id: Mapped[uuid.UUID] = mapped_column(UUID, primary_key=True)\n\n    # https://openid.net/specs/openid-connect-core-1_0.html#NonceNotes\n    nonce: Mapped[uuid.UUID]\n\n\nclass UserSavedOpportunity(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"user_saved_opportunity\"\n\n    user_id: Mapped[uuid.UUID] = mapped_column(ForeignKey(User.user_id), primary_key=True)\n    opportunity_id: Mapped[int] = mapped_column(\n        BigInteger, ForeignKey(Opportunity.opportunity_id), primary_key=True\n    )\n\n    last_notified_at: Mapped[datetime] = mapped_column(\n        default=datetime_util.utcnow, server_default=\"NOW()\", nullable=False\n    )\n\n    user: Mapped[User] = relationship(User, back_populates=\"saved_opportunities\")\n    opportunity: Mapped[Opportunity] = relationship(\n        \"Opportunity\", back_populates=\"saved_opportunities_by_users\"\n    )\n\n\nclass UserSavedSearch(ApiSchemaTable, TimestampMixin):\n    \"\"\"Table for storing saved search queries for users\"\"\"\n\n    __tablename__ = \"user_saved_search\"\n\n    saved_search_id: Mapped[uuid.UUID] = mapped_column(UUID, primary_key=True, default=uuid.uuid4)\n\n    user_id: Mapped[uuid.UUID] = mapped_column(ForeignKey(User.user_id), index=True)\n    user: Mapped[User] = relationship(User, back_populates=\"saved_searches\")\n\n    search_query: Mapped[dict] = mapped_column(JSONB)\n\n    name: Mapped[str]\n\n    last_notified_at: Mapped[datetime] = mapped_column(\n        nullable=False,\n        default=datetime_util.utcnow,\n        server_default=sqlnow(),\n    )\n    searched_opportunity_ids: Mapped[list[int]] = mapped_column(ARRAY(BigInteger))\n\n\nclass UserNotificationLog(ApiSchemaTable, TimestampMixin):\n    __tablename__ = \"user_notification_log\"\n\n    user_notification_log_id: Mapped[uuid.UUID] = mapped_column(\n        UUID, primary_key=True, default=uuid.uuid4\n    )\n\n    user_id: Mapped[uuid.UUID] = mapped_column(ForeignKey(User.user_id), index=True)\n    user: Mapped[User] = relationship(User)\n\n    notification_reason: Mapped[str]\n    notification_sent: Mapped[bool]"}
{"path":"frontend/docker-compose.yml","language":"yaml","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/docker-compose.yml","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/logging/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/__init__.py\nSize: 0.86 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/jest.config.js","language":"javascript","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/jest.config.js","size":1644826,"lastModified":"2025-02-14T17:08:31.131Z","content":"There are two formatters for the log messages: human-readable and JSON.\nThe formatter that is used is determined by the environment variable\nLOG_FORMAT. If the environment variable is not set, the JSON formatter\nis used by default. See src.logging.formatters for more information.\n\nThe logger also adds a PII mask filter to the root logger. See\nsrc.logging.pii for more information.\n\nUsage:\n    import src.logging\n\n    with src.logging.init(\"program name\"):\n        ...\n\nOnce the module has been initialized, the standard logging module can be\nused to log messages:\n\nExample:\n    import logging\n\n    logger = logging.getLogger(__name__)\n    logger.info(\"message\")\n\"\"\"\n\nimport src.logging.config as config\n\n\ndef init(program_name: str) -> config.LoggingContext:\n    return config.LoggingContext(program_name)"}
{"path":"frontend/next-env.d.ts","language":"typescript","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/next-env.d.ts","size":324715,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/logging/audit.py\nLanguage: py\nType: code\nDirectory: api/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/audit.py\nSize: 4.63 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/next.config.js","language":"javascript","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/next.config.js","size":1644826,"lastModified":"2025-02-14T17:08:31.131Z","content":"logger = logging.getLogger(__name__)\n\nAUDIT = 32\nlogging.addLevelName(AUDIT, \"AUDIT\")\n\n\ndef init() -> None:\n    \"\"\"Initialize the audit logging module to start\n    logging security audit events.\"\"\"\n    sys.addaudithook(handle_audit_event)\n\n\ndef handle_audit_event(event_name: str, args: tuple[Any, ...]) -> None:\n    # Define events to log and the arguments to log for each event.\n    # For more information about these events and what they mean, see https://peps.python.org/pep-0578/#suggested-audit-hook-locations\n    # For the full list of auditable events, see https://docs.python.org/3/library/audit_events.html\n    # Define this variable locally so it can't be modified by other modules.\n\n    EVENTS_TO_LOG = {\n        # Detect dynamic execution of code objects. This only occurs for explicit\n        # calls, and is not raised for normal function invocation.\n        \"exec\": (\"code_object\",),  # TODO - this can't be logged as a code object isn't serializable\n        # Detect when a file is about to be opened. path and mode are the usual\n        # parameters to open if available, while flags is provided instead of\n        # mode in some cases.\n        \"open\": (\"path\", \"mode\", \"flags\"),\n        # Detect when a signal is sent to a process.\n        \"os.kill\": (\"pid\", \"sig\"),\n        # Detect when a file is renamed.\n        \"os.rename\": (\"src\", \"dst\", \"src_dir_fd\", \"dst_dir_fd\"),\n        # Detect when a subprocess is started.\n        \"subprocess.Popen\": (\"executable\", \"args\", \"cwd\", \"_\"),\n        # Detect access to network resources. The address is unmodified from the original call.\n        \"socket.connect\": (\"socket\", \"address\"),\n        \"socket.getaddrinfo\": (\"host\", \"port\", \"family\", \"type\", \"protocol\"),\n        # Detect when new audit hooks are being added.\n        \"sys.addaudithook\": (),\n        # Detects URL requests.\n        # Don't log data or headers because they may contain sensitive information.\n        \"urllib.Request\": (\"url\", \"_\", \"_\", \"method\"),\n    }\n\n    if event_name not in EVENTS_TO_LOG:\n        return\n\n    arg_names = EVENTS_TO_LOG[event_name]\n    log_audit_event(event_name, args, arg_names)\n\n\n# Set the audit hook to be traceable so that coverage module can track calls to it\n# The coverage module relies on Python's trace hooks\n# (See https://coverage.readthedocs.io/en/7.1.0/howitworks.html#execution)\n# According to the docs for sys.addaudithook, the audit hook is only traced if the callable\n# has a __cantrace__ member that is set to a true value.\n# (See https://docs.python.org/3/library/sys.html#sys.addaudithook)\nhandle_audit_event.__cantrace__ = True  # type: ignore\n\n\ndef log_audit_event(event_name: str, args: Sequence[Any], arg_names: Sequence[str]) -> None:\n    \"\"\"Log a message but only log recently repeated messages at intervals.\"\"\"\n    extra = {\n        f\"audit.args.{arg_name}\": arg\n        for arg_name, arg in zip(arg_names, args, strict=True)\n        if arg_name != \"_\"\n    }\n\n    key = (event_name, repr(args))\n    if key not in audit_message_count:\n        count = 1\n    else:\n        count = audit_message_count[key] + 1\n    audit_message_count[key] = count\n\n    if count > 100 and count % 100 != 0:\n        return\n\n    if count > 10 and count % 10 != 0:\n        return\n\n    extra[\"count\"] = count\n\n    logger.log(AUDIT, event_name, extra=extra)\n\n\nclass LeastRecentlyUsedDict(collections.OrderedDict):\n    \"\"\"A dict with a maximum size, evicting the least recently written key when full.\n\n    Getting a key that is not present returns a default value of 0.\n\n    Setting a key marks it as most recently used and removes the oldest key if full.\n\n    May be useful for tracking the count of items where limited memory usage is needed even if\n    the set of items can be unlimited.\n\n    Based on the example at\n    https://docs.python.org/3/library/collections.html#ordereddict-examples-and-recipes\n    \"\"\"\n\n    def __init__(self, maxsize: int = 128, *args: Any, **kwargs: Any) -> None:\n        self.maxsize = maxsize\n        super().__init__(*args, **kwargs)\n\n    def __getitem__(self, key: Hashable) -> int:\n        if key in self:\n            return super().__getitem__(key)\n        return 0\n\n    def __setitem__(self, key: Hashable, value: int) -> None:\n        if key in self:\n            self.move_to_end(key)\n        super().__setitem__(key, value)\n        if self.maxsize < len(self):\n            self.popitem(last=False)\n\n\naudit_message_count = LeastRecentlyUsedDict()"}
{"path":"frontend/package.json","language":"json","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/package.json","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/logging/config.py\nLanguage: py\nType: code\nDirectory: api/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/config.py\nSize: 6.25 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/postcss.config.js","language":"javascript","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/postcss.config.js","size":1644826,"lastModified":"2025-02-14T17:08:31.131Z","content":"from pydantic_settings import SettingsConfigDict\n\nimport src.logging.audit\nimport src.logging.formatters as formatters\nimport src.logging.pii as pii\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\n_original_argv = tuple(sys.argv)\n\n\nclass HumanReadableFormatterConfig(PydanticBaseEnvConfig):\n    message_width: int = formatters.HUMAN_READABLE_FORMATTER_DEFAULT_MESSAGE_WIDTH\n\n\nclass LoggingConfig(PydanticBaseEnvConfig):\n    model_config = SettingsConfigDict(env_prefix=\"log_\", env_nested_delimiter=\"__\")\n\n    format: str = \"json\"\n    level: str = \"INFO\"\n    enable_audit: bool = False\n    human_readable_formatter: HumanReadableFormatterConfig = HumanReadableFormatterConfig()\n\n    # Specify logging_level_overrides formatted as \"<logger>=<level>\" like \"newrelic=INFO,something.else=ERROR\"\n    level_overrides: str | None = None\n\n\nclass LoggingContext(ContextManager[None]):\n    \"\"\"\n    A context manager for handling setting up the logging stream.\n\n    To help facillitate being able to test logging, we need to be able\n    to easily create temporary output streams and then tear them down.\n\n    When this context manager is torn down, the stream handler created\n    with it will be removed.\n\n    For example:\n    ```py\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    with LoggingContext(\"example_program_name\"):\n        # This log message will go to stdout\n        logger.info(\"example log message\")\n\n    # This log message won't go to stdout as the\n    # handler will have been removed\n    logger.info(\"example log message\")\n    ```\n    Note that any other handlers added to the root logger won't be affected\n    and calling this multiple times before exit would result in duplicate logs.\n    \"\"\"\n\n    def __init__(self, program_name: str) -> None:\n        self._configure_logging()\n        log_program_info(program_name)\n\n    def __enter__(self) -> None:\n        pass\n\n    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:\n        # Remove the console handler to stop logs from being sent to stdout\n        # This is useful in the test suite, since multiple tests may initialize\n        # separate duplicate handlers. This allows for easier cleanup for each\n        # of those tests.\n        logging.root.removeHandler(self.console_handler)\n\n    def _configure_logging(self) -> None:\n        \"\"\"Configure logging for the application.\n\n        Configures the root module logger to log to stdout.\n        Adds a PII mask filter to the root logger.\n        Also configures log levels third party packages.\n        \"\"\"\n        config = LoggingConfig()\n\n        # Loggers can be configured using config functions defined\n        # in logging.config or by directly making calls to the main API\n        # of the logging module (see https://docs.python.org/3/library/logging.config.html)\n        # We opt to use the main API using functions like `addHandler` which is\n        # non-destructive, i.e. it does not overwrite any existing handlers.\n        # In contrast, logging.config.dictConfig() would overwrite any existing loggers.\n        # This is important during testing, since fixtures like `caplog` add handlers that would\n        # get overwritten if we call logging.config.dictConfig() during the scope of the test.\n        self.console_handler = logging.StreamHandler(sys.stdout)\n        formatter = get_formatter(config)\n        self.console_handler.setFormatter(formatter)\n        self.console_handler.addFilter(pii.mask_pii)\n        logging.root.addHandler(self.console_handler)\n        logging.root.setLevel(config.level)\n\n        if config.enable_audit:\n            src.logging.audit.init()\n\n        # Configure loggers for third party packages\n        logging.getLogger(\"alembic\").setLevel(logging.INFO)\n        logging.getLogger(\"werkzeug\").setLevel(logging.WARN)\n        logging.getLogger(\"sqlalchemy.pool\").setLevel(logging.INFO)\n        logging.getLogger(\"sqlalchemy.dialects.postgresql\").setLevel(logging.INFO)\n\n        # Allow an env var to override logging config, mostly for development purposes\n        # Parsing string formatted like \"logger1=INFO,logger2=ERROR\"\n        if config.level_overrides is not None:\n            for override in config.level_overrides.split(\",\"):\n                logger_override, level_override = override.split(\"=\")\n                logging.getLogger(logger_override).setLevel(level_override)\n\n\ndef get_formatter(config: LoggingConfig) -> logging.Formatter:\n    \"\"\"Return the formatter used by the root logger.\n\n    The formatter is determined by the environment variable LOG_FORMAT. If the\n    environment variable is not set, the JSON formatter is used by default.\n    \"\"\"\n    if config.format == \"human-readable\":\n        return get_human_readable_formatter(config.human_readable_formatter)\n    return formatters.JsonFormatter()\n\n\ndef log_program_info(program_name: str) -> None:\n    logger.info(\n        \"start %s: %s %s %s, hostname %s, pid %i, user %i(%s)\",\n        program_name,\n        platform.python_implementation(),\n        platform.python_version(),\n        platform.system(),\n        platform.node(),\n        os.getpid(),\n        os.getuid(),\n        pwd.getpwuid(os.getuid()).pw_name,\n        extra={\n            \"hostname\": platform.node(),\n            \"cpu_count\": os.cpu_count(),\n            # If mypy is run on a mac, it will throw a module has no attribute error, even though\n            # we never actually access it with the conditional.\n            #\n            # However, we can't just silence this error, because on linux (e.g. CI/CD) that will\n            # throw an unused â€œtype: ignoreâ€ comment error. Casting to Any instead ensures this\n            # passes regardless of where mypy is being run\n            \"cpu_usable\": (\n                len(cast(Any, os).sched_getaffinity(0))\n                if \"sched_getaffinity\" in dir(os)\n                else \"unknown\"\n            ),\n        },\n    )\n    logger.info(\"invoked as: %s\", \" \".join(_original_argv))\n\n\ndef get_human_readable_formatter(\n    config: HumanReadableFormatterConfig,\n) -> formatters.HumanReadableFormatter:\n    \"\"\"Return the human readable formatter used by the root logger.\"\"\"\n    return formatters.HumanReadableFormatter(message_width=config.message_width)"}
{"path":"frontend/public/img/logo-white-lg.webp","language":"unknown","type":"code","directory":"frontend/public/img","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/public/img/logo-white-lg.webp","size":0,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/logging/decodelog.py\nLanguage: py\nType: code\nDirectory: api/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/decodelog.py\nSize: 4.04 KB\nLast Modified: 2025-02-14T17:08:26.444Z"}
{"path":"frontend/scripts/postinstall.js","language":"javascript","type":"code","directory":"frontend/scripts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/scripts/postinstall.js","size":1644826,"lastModified":"2025-02-14T17:08:31.131Z","content":"import datetime\nimport json\nimport sys\nfrom typing import Mapping, Optional\n\nRED = \"\\033[31m\"\nGREEN = \"\\033[32m\"\nBLUE = \"\\033[34m\"\nORANGE = \"\\033[38;5;208m\"\nRESET = \"\\033[0m\"\nNO_COLOUR = \"\"\n\nDEFAULT_MESSAGE_WIDTH = 50\n\noutput_dates = None\n\n\ndef main() -> None:\n    \"\"\"Main entry point when used as a script.\"\"\"\n    for line in sys.stdin:\n        processed = process_line(line)\n        if processed is not None:\n            sys.stdout.write(processed)\n            sys.stdout.write(\"\\r\\n\")\n\n\ndef process_line(line: str) -> Optional[str]:\n    \"\"\"Process a line of the log and return the reformatted line.\"\"\"\n    line = line.rstrip()\n    if line and line[0] == \"{\":\n        # JSON format\n        return decode_json_line(line)\n    elif \"| {\" in line:\n        # `docker-compose logs ...` format\n        return decode_json_line(line[line.find(\"| {\") + 2 :])\n    # Anything else is left alone\n    return line\n\n\ndef decode_json_line(line: str) -> Optional[str]:\n    \"\"\"Decode a JSON log line and return the reformatted line.\"\"\"\n    try:\n        data = json.loads(line)\n    except json.decoder.JSONDecodeError:\n        return line\n\n    name = data.pop(\"name\", \"-\")\n    level = data.pop(\"levelname\", \"-\")\n    func_name = data.pop(\"funcName\", \"-\")\n    created = datetime.datetime.utcfromtimestamp(float(data.pop(\"created\", 0)))\n    message = data.pop(\"message\", \"-\")\n\n    if level == \"AUDIT\":\n        return None\n\n    return format_line(created, name, func_name, level, message, data)\n\n\ndef format_line(\n    created: datetime.datetime,\n    logger_name: str,\n    func_name: str,\n    level: str,\n    message: str,\n    extra: Mapping[str, str],\n    message_width: int = DEFAULT_MESSAGE_WIDTH,\n) -> str:\n    \"\"\"Format log fields as a coloured string.\"\"\"\n    logger_name_color = color_for_name(logger_name)\n    level_color = color_for_level(level)\n    return \"{created}  {logger_name} {func_name:<28} {level} {message} {extra}\".format(\n        created=format_datetime(created),\n        logger_name=colorize(logger_name.ljust(36), logger_name_color),\n        func_name=func_name,\n        level=colorize(level.ljust(8), level_color),\n        message=colorize(message.ljust(message_width), level_color),\n        extra=colorize(format_extra(extra), BLUE),\n    )\n\n\ndef colorize(text: str, color: str) -> str:\n    return f\"{color}{text}{RESET}\"\n\n\ndef color_for_name(name: str) -> str:\n    if name.startswith(\"src\"):\n        return GREEN\n    elif name.startswith(\"sqlalchemy\"):\n        return ORANGE\n    return NO_COLOUR\n\n\ndef color_for_level(level: str) -> str:\n    if level in (\"WARNING\", \"ERROR\", \"CRITICAL\"):\n        return RED\n    return NO_COLOUR\n\n\ndef format_datetime(created: datetime.datetime) -> str:\n    global output_dates\n    if output_dates is None:\n        # Check first line - if over 10h ago, output dates as well as time.\n        output_dates = 36000 < (datetime.datetime.now() - created).total_seconds()\n    if output_dates:\n        return created.isoformat(timespec=\"milliseconds\")\n    else:\n        return created.time().isoformat(timespec=\"milliseconds\")\n\n\nEXCLUDE_EXTRA = {\n    \"args\",\n    \"created\",\n    \"entity.guid\",\n    \"entity.name\",\n    \"entity.type\",\n    \"exc_info\",\n    \"filename\",\n    \"funcName\",\n    \"levelname\",\n    \"levelno\",\n    \"lineno\",\n    \"message\",\n    \"module\",\n    \"msecs\",\n    \"msg\",\n    \"name\",\n    \"pathname\",\n    \"process\",\n    \"processName\",\n    \"relativeCreated\",\n    \"span.id\",\n    \"thread\",\n    \"threadName\",\n    \"trace.id\",\n    \"traceId\",\n    \"deploy_github_ref\",\n    \"deploy_github_sha\",\n}\n\n\ndef format_extra(data: Mapping[str, str]) -> str:\n    return \" \".join(\n        \"%s=%s\" % (key, value)\n        for key, value in data.items()\n        if key not in EXCLUDE_EXTRA and value is not None\n    )\n\n\nif __name__ == \"__main__\":\n    main()"}
{"path":"frontend/scripts/sassOptions.js","language":"javascript","type":"code","directory":"frontend/scripts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/scripts/sassOptions.js","size":1644826,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/logging/flask_logger.py\nLanguage: py\nType: code\nDirectory: api/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/flask_logger.py\nSize: 6.31 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/[...not-found]/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/[...not-found]","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/[...not-found]/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"This module configures an application's logger to add extra data\nto all log messages. Flask application context data such as the\napp name and request context data such as the request method, request url\nrule, and query parameters are added to the log record.\n\nThis module also configures the Flask application to log every\nnon-404 request.\n\nUsage:\n    import src.logging.flask_logger as flask_logger\n\n    logger = logging.getLogger(__name__)\n    app = create_app()\n\n    flask_logger.init_app(logger, app)\n\"\"\"\n\nimport logging\nimport os\nimport time\nimport uuid\n\nimport flask\n\nfrom src.util.deploy_metadata import get_deploy_metadata_config\n\nlogger = logging.getLogger(__name__)\nEXTRA_LOG_DATA_ATTR = \"extra_log_data\"\n\n_GLOBAL_LOG_CONTEXT: dict = {}\n\n\ndef init_app(app_logger: logging.Logger, app: flask.Flask) -> None:\n    \"\"\"Initialize the Flask app logger.\n\n    Adds Flask app context data and Flask request context data\n    to every log record using log filters.\n    See https://docs.python.org/3/howto/logging-cookbook.html#using-filters-to-impart-contextual-information\n\n    Also configures the app to log every non-404 request using the given logger.\n\n    Usage:\n        import src.logging.flask_logger as flask_logger\n\n        logger = logging.getLogger(__name__)\n        app = create_app()\n\n        flask_logger.init_app(logger, app)\n    \"\"\"\n\n    # Need to add filters to each of the handlers rather than to the logger itself, since\n    # messages are passed directly to the ancestor loggersâ€™ handlers bypassing any filters\n    # set on the ancestors.\n    # See https://docs.python.org/3/library/logging.html#logging.Logger.propagate\n    for handler in app_logger.handlers:\n        handler.addFilter(_add_global_context_info_to_log_record)\n        handler.addFilter(_add_request_context_info_to_log_record)\n\n    # Add request context data to every log record for the current request\n    # such as request id, request method, request path, and the matching Flask request url rule\n    app.before_request(\n        lambda: add_extra_data_to_current_request_logs(_get_request_context_info(flask.request))\n    )\n\n    app.before_request(_track_request_start_time)\n    app.before_request(_log_start_request)\n    app.after_request(_log_end_request)\n\n    deploy_metadata = get_deploy_metadata_config()\n\n    # Add some metadata to all log messages globally\n    add_extra_data_to_global_logs(\n        {\n            \"app.name\": app.name,\n            \"environment\": os.environ.get(\"ENVIRONMENT\"),\n            \"deploy_github_ref\": deploy_metadata.deploy_github_ref,\n            \"deploy_github_sha\": deploy_metadata.deploy_github_sha,\n            \"deploy_whoami\": deploy_metadata.deploy_whoami,\n        }\n    )\n\n    app_logger.info(\"initialized flask logger\")\n\n\ndef add_extra_data_to_current_request_logs(\n    data: dict[str, str | int | float | bool | None]\n) -> None:\n    \"\"\"Add data to every log record for the current request.\"\"\"\n    assert flask.has_request_context(), \"Must be in a request context\"\n\n    extra_log_data = getattr(flask.g, EXTRA_LOG_DATA_ATTR, {})\n    extra_log_data.update(data)\n    setattr(flask.g, EXTRA_LOG_DATA_ATTR, extra_log_data)\n\n\ndef add_extra_data_to_global_logs(data: dict[str, str | int | float | bool | None]) -> None:\n    \"\"\"Add metadata to all logs for the rest of the lifecycle of this app process\"\"\"\n    global _GLOBAL_LOG_CONTEXT\n    _GLOBAL_LOG_CONTEXT.update(data)\n\n\ndef _track_request_start_time() -> None:\n    \"\"\"Store the request start time in flask.g\"\"\"\n    flask.g.request_start_time = time.perf_counter()\n\n\ndef _log_start_request() -> None:\n    \"\"\"Log the start of a request.\n\n    This function handles the Flask's before_request event.\n    See https://tedboy.github.io/flask/interface_src.application_object.html#flask.Flask.before_request\n\n    Additional info about the request will be in the `extra` field\n    added by `_add_request_context_info_to_log_record`\n    \"\"\"\n    logger.info(\"start request\")\n\n\ndef _log_end_request(response: flask.Response) -> flask.Response:\n    \"\"\"Log the end of a request.\n\n    This function handles the Flask's after_request event.\n    See https://tedboy.github.io/flask/interface_src.application_object.html#flask.Flask.after_request\n\n    Additional info about the request will be in the `extra` field\n    added by `_add_request_context_info_to_log_record`\n    \"\"\"\n\n    logger.info(\n        \"end request\",\n        extra={\n            \"response.status_code\": response.status_code,\n            \"response.content_length\": response.content_length,\n            \"response.content_type\": response.content_type,\n            \"response.mimetype\": response.mimetype,\n            \"response.time_ms\": (time.perf_counter() - flask.g.request_start_time) * 1000,\n        },\n    )\n    return response\n\n\ndef _add_request_context_info_to_log_record(record: logging.LogRecord) -> bool:\n    \"\"\"Add request context data to the log record.\n\n    If there is no request context, then do not add any data.\n    \"\"\"\n    if not flask.has_request_context():\n        return True\n\n    assert flask.request is not None\n    extra_log_data: dict[str, str] = getattr(flask.g, EXTRA_LOG_DATA_ATTR, {})\n    record.__dict__.update(extra_log_data)\n\n    return True\n\n\ndef _add_global_context_info_to_log_record(record: logging.LogRecord) -> bool:\n    global _GLOBAL_LOG_CONTEXT\n    record.__dict__ |= _GLOBAL_LOG_CONTEXT\n\n    return True\n\n\ndef _get_request_context_info(request: flask.Request) -> dict:\n    internal_request_id = str(uuid.uuid4())\n    flask.g.internal_request_id = internal_request_id\n\n    data = {\n        \"request.id\": request.headers.get(\"x-amzn-requestid\", \"\"),\n        \"request.method\": request.method,\n        \"request.path\": request.path,\n        \"request.url_rule\": str(request.url_rule),\n        # This ID is used to group all logs for a given request\n        # and is returned in the API response for any 4xx/5xx scenarios\n        \"request.internal_id\": internal_request_id,\n    }\n\n    # Add query parameter data in the format request.query.<key> = <value>\n    # For example, the query string ?foo=bar&baz=qux would be added as\n    # request.query.foo = bar and request.query.baz = qux\n    # PII should be kept out of the URL, as URLs are logged in access logs.\n    # With that assumption, it is safe to log query parameters.\n    for key, value in request.args.items():\n        data[f\"request.query.{key}\"] = value\n    return data"}
{"path":"frontend/src/app/[locale]/dev/feature-flags/FeatureFlagsTable.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/dev/feature-flags","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/dev/feature-flags/FeatureFlagsTable.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/logging/formatters.py\nLanguage: py\nType: code\nDirectory: api/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/formatters.py\nSize: 3.30 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/dev/feature-flags/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/dev/feature-flags","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/dev/feature-flags/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"This module defines two formatters, JsonFormatter for machine-readable logs to\nbe used in production, and HumanReadableFormatter for human readable logs to\nbe used used during development.\n\nSee https://docs.python.org/3/library/logging.html#formatter-objects\n\"\"\"\n\nimport json\nimport logging\nfrom datetime import date, datetime\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any, Callable, Type, TypeVar\nfrom uuid import UUID\n\nimport src.logging.decodelog as decodelog\n\nT = TypeVar(\"T\")\n\n\n# identity returns an unmodified object\ndef identity(obj: T) -> T:\n    return obj\n\n\n# Mapping of types to functions for conversion\n# when writing logs to JSON\nENCODERS_BY_TYPE: dict[Type[Any], Callable[[Any], Any]] = {\n    # JSONEncoder handles these properly already:\n    # https://docs.python.org/3/library/json.html#json.JSONEncoder\n    str: identity,\n    int: identity,\n    float: identity,\n    bool: identity,\n    list: identity,\n    datetime: lambda d: d.isoformat(),\n    date: lambda d: d.isoformat(),\n    Enum: lambda e: e.value,\n    set: lambda s: list(s),\n    # The fallback below would do these,\n    # but making it explicit that these\n    # types are supported for logging.\n    Decimal: str,\n    UUID: str,\n    Exception: str,\n}\n\n\ndef json_encoder(obj: Any) -> Any:\n    \"\"\"\n    Handle conversion of various types when logs\n    are serialized into JSON. If not specified\n    will attempt to convert using str() on the object\n    \"\"\"\n\n    _type = type(obj)\n    encode = ENCODERS_BY_TYPE.get(_type, str)\n\n    \"\"\"\n    The recommended approach from the JSON docs\n    is to call the default method from JSONEncoder\n    to allow it to error anything not defined, we\n    choose not to do that as we want to give a best\n    effort for every value to be serialized for the logs\n    https://docs.python.org/3/library/json.html\n\n    If a field you are trying to log doesn't make sense\n    to format as a string then please add it above, but be\n    aware that the format needs to be parseable by whatever\n    tools you are using to ingest logs and metrics.\n    \"\"\"\n    return encode(obj)\n\n\nclass JsonFormatter(logging.Formatter):\n    \"\"\"A logging formatter which formats each line as JSON.\"\"\"\n\n    def format(self, record: logging.LogRecord) -> str:\n        # logging.Formatter.format adds the `message` attribute to the LogRecord\n        # see https://github.com/python/cpython/blob/main/Lib/logging/__init__.py#L690-L720\n        super().format(record)\n\n        return json.dumps(record.__dict__, separators=(\",\", \":\"), default=json_encoder)\n\n\nHUMAN_READABLE_FORMATTER_DEFAULT_MESSAGE_WIDTH = decodelog.DEFAULT_MESSAGE_WIDTH\n\n\nclass HumanReadableFormatter(logging.Formatter):\n    \"\"\"A logging formatter which formats each line\n    as color-code human readable text\n    \"\"\"\n\n    message_width: int\n\n    def __init__(self, message_width: int = HUMAN_READABLE_FORMATTER_DEFAULT_MESSAGE_WIDTH):\n        super().__init__()\n        self.message_width = message_width\n\n    def format(self, record: logging.LogRecord) -> str:\n        message = super().format(record)\n        return decodelog.format_line(\n            datetime.fromtimestamp(record.created),\n            record.name,\n            record.funcName,\n            record.levelname,\n            message,\n            record.__dict__,\n            message_width=self.message_width,\n        )"}
{"path":"frontend/src/app/[locale]/error/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/error","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/error/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/logging/pii.py\nLanguage: py\nType: code\nDirectory: api/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/logging/pii.py\nSize: 3.07 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/health/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/health","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/health/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"This module defines a filter that can be attached to a logger to mask PII\nfrom log records. The filter is applied to all log records, and masks PII\nthat looks like social security numbers.\n\nYou can add the filter to a handler:\n\nExample:\n    import logging\n    import src.logging.pii as pii\n\n    handler = logging.StreamHandler()\n    handler.addFilter(pii.mask_pii)\n    logger = logging.getLogger(__name__)\n    logger.addHandler(handler)\n\nOr you can add the filter directly to a logger.\nIf adding the filter directly to a logger, take note that the filter\nwill not be called for child loggers.\nSee https://docs.python.org/3/library/logging.html#logging.Logger.propagate\n\nExample:\n    import logging\n    import src.logging.pii as pii\n\n    logger = logging.getLogger(__name__)\n    logger.addFilter(pii.mask_pii)\n\"\"\"\n\nimport logging\nimport re\nfrom typing import Any, Optional\n\n\ndef mask_pii(record: logging.LogRecord) -> bool:\n    # Loop through all entries in the record's __dict__\n    # attribute and mask any things that look like PII.\n    # We will mask positional args separately below.\n    record.__dict__ |= {\n        key: _mask_pii_for_key(key, value)\n        for key, value in record.__dict__.items()\n        if key != \"args\"  # Handle positional \"args\" separately\n    }\n\n    # record.__dict__[\"args\"] will contain positional arguments to logging calls.\n    # For example, a call like logger.info(\"%s %s\", \"foo\", \"bar\") will result in a LogRecord\n    # with record.__dict__[\"args\"] == (\"foo\", \"bar\")\n    # We want to mask the PII on each argument separately rather than trying to do a PII regex\n    # match on the entire args tuple.\n    args = record.__dict__[\"args\"]\n    record.__dict__[\"args\"] = tuple(map(_mask_pii, args))\n    return True\n\n\n# Regular expression to match a tax identifier (SSN), 9 digits with optional dashes.\n# Matches between word boundaries (\\b), except when:\n#  - Preceded by word character and dash (e.g. \"ip-10-11-12-134\")\n#  - Preceded by or followed by a decimal point (for floating point numbers)\nTIN_RE = re.compile(\n    r\"\"\"\n        \\b          # word boundary\n        (?<!\\w-)    # not preceded by word character and dash\n        (?<!\\.)     # not preceded by decimal point\n        (\\d-?){8}   # digit then optional dash, 8 times\n        \\d          # last digit\n        \\b          # word boundary\n        (?!\\.\\d)    # not followed by decimal point and digit (for decimal numbers)\n    \"\"\",\n    re.ASCII | re.VERBOSE,\n)\n\nALLOW_NO_MASK = {\n    \"account_key\",\n    \"count\",\n    \"created\",\n    \"hostname\",\n    \"process\",\n    \"thread\",\n}\n\n\ndef _mask_pii_for_key(key: str, value: Optional[Any]) -> Optional[Any]:\n    \"\"\"\n    Mask the given value if it has the pattern of a tax identifier\n    unless its key is one of the allowed values to avoid masking\n    something that looks like an SSN but is known to be safe (like a timestamp)\n    \"\"\"\n    if key in ALLOW_NO_MASK:\n        return value\n    return _mask_pii(value)\n\n\ndef _mask_pii(value: Optional[Any]) -> Optional[Any]:\n    if TIN_RE.search(str(value)):\n        return TIN_RE.sub(\"*********\", str(value))\n    return value"}
{"path":"frontend/src/app/[locale]/layout.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/layout.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/pagination/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/pagination\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/pagination/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/maintenance/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/maintenance","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/maintenance/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":""}
{"path":"frontend/src/app/[locale]/not-found.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/not-found.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/pagination/pagination_models.py\nLanguage: py\nType: code\nDirectory: api/src/pagination\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/pagination/pagination_models.py\nSize: 1.34 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/opportunity/[id]/error.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/opportunity/[id]","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/opportunity/[id]/error.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"from pydantic import BaseModel, Field\n\nfrom src.pagination.paginator import Paginator\n\n\nclass SortDirection(StrEnum):\n    ASCENDING = \"ascending\"\n    DESCENDING = \"descending\"\n\n    def short_form(self) -> str:\n        if self == SortDirection.DESCENDING:\n            return \"desc\"\n        return \"asc\"\n\n\nclass SortOrderParams(BaseModel):\n    order_by: str\n    sort_direction: SortDirection\n\n\nclass PaginationParams(BaseModel):\n    page_offset: int\n    page_size: int\n\n    sort_order: list[SortOrderParams] = Field(default_factory=list)\n\n\n@dataclasses.dataclass\nclass SortOrder:\n    order_by: str\n    sort_direction: SortDirection\n\n\n@dataclasses.dataclass\nclass PaginationInfo:\n    page_offset: int\n    page_size: int\n\n    total_records: int\n    total_pages: int\n\n    sort_order: list[SortOrder]\n\n    @classmethod\n    def from_pagination_params(\n        cls, pagination_params: PaginationParams, paginator: Paginator\n    ) -> Self:\n        return cls(\n            page_offset=pagination_params.page_offset,\n            page_size=pagination_params.page_size,\n            total_records=paginator.total_records,\n            total_pages=paginator.total_pages,\n            sort_order=[\n                SortOrder(p.order_by, p.sort_direction) for p in pagination_params.sort_order\n            ],\n        )"}
{"path":"frontend/src/app/[locale]/opportunity/[id]/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/opportunity/[id]","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/opportunity/[id]/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/pagination/pagination_schema.py\nLanguage: py\nType: code\nDirectory: api/src/pagination\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/pagination/pagination_schema.py\nSize: 5.06 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"from marshmallow import pre_load\n\nfrom src.api.schemas.extension import Schema, fields, validators\nfrom src.pagination.pagination_models import SortDirection\n\n\nclass BasePaginationSchema(Schema):\n\n    @pre_load\n    def before_load(self, item: dict, many: bool, **kwargs: dict) -> dict:\n        # If sort_order is used, don't change anything\n        # We'll assume they've migrated properly\n        if item.get(\"sort_order\") is not None:\n            return item\n\n        # While we wait for the frontend to start using the new multi-sort, automatically\n        # setup a monosort for them from the old fields.\n        if item.get(\"order_by\") is not None and item.get(\"sort_direction\") is not None:\n            item[\"sort_order\"] = [\n                {\"order_by\": item[\"order_by\"], \"sort_direction\": item[\"sort_direction\"]}\n            ]\n        return item\n\n\ndef generate_pagination_schema(\n    cls_name: str,\n    order_by_fields: list[str],\n    max_page_size: int = 5000,\n    default_sort_order: list[dict] | None = None,\n) -> Type[Schema]:\n    \"\"\"\n    Generate a schema that describes the pagination for a pagination endpoint.\n\n        cls_name will be what the model is named internally by Marshmallow and what OpenAPI shows.\n        order_by_fields can be a list of fields that the endpoint allows you to sort the response by\n\n    This is functionally equivalent to specifying your own class like so:\n\n        class MyPaginationSchema(Schema):\n            order_by = fields.String(\n                validate=[validators.OneOf([\"id\",\"created_at\",\"updated_at\"])],\n                required=True,\n                metadata={\"description\": \"The field to sort the response by\"}\n            )\n            sort_direction = fields.Enum(\n                SortDirection,\n                required=True,\n                metadata={\"description\": \"Whether to sort the response ascending or descending\"},\n            )\n            page_size = fields.Integer(\n                required=True,\n                validate=[validators.Range(min=1)],\n                metadata={\"description\": \"The size of the page to fetch\", \"example\": 25},\n            )\n            page_offset = fields.Integer(\n                required=True,\n                validate=[validators.Range(min=1)],\n                metadata={\"description\": \"The page number to fetch, starts counting from 1\", \"example\": 1},\n            )\n\n    \"\"\"\n\n    sort_order_schema = Schema.from_dict(\n        {\n            \"order_by\": fields.String(\n                validate=[validators.OneOf(order_by_fields)],\n                required=True,\n                metadata={\"description\": \"The field to sort the response by\"},\n            ),\n            \"sort_direction\": fields.Enum(\n                SortDirection,\n                required=True,\n                metadata={\"description\": \"Whether to sort the response ascending or descending\"},\n            ),\n        },\n        name=f\"SortOrder{cls_name}\",\n    )\n\n    additional_sort_order_params: dict = {}\n    if default_sort_order is not None:\n        additional_sort_order_params[\"load_default\"] = default_sort_order\n    else:\n        additional_sort_order_params[\"required\"] = True\n\n    pagination_schema_fields = {\n        \"sort_order\": fields.List(\n            fields.Nested(sort_order_schema()),\n            metadata={\"description\": \"The list of sorting rules\"},\n            validate=[validators.Length(min=1, max=5)],\n            **additional_sort_order_params,\n        ),\n        \"page_size\": fields.Integer(\n            required=True,\n            validate=[validators.Range(min=1, max=max_page_size)],\n            metadata={\"description\": \"The size of the page to fetch\", \"example\": 25},\n        ),\n        \"page_offset\": fields.Integer(\n            required=True,\n            validate=[validators.Range(min=1)],\n            metadata={\n                \"description\": \"The page number to fetch, starts counting from 1\",\n                \"example\": 1,\n            },\n        ),\n    }\n    return BasePaginationSchema.from_dict(pagination_schema_fields, name=cls_name)  # type: ignore\n\n\nclass SortOrderSchema(Schema):\n    order_by = fields.String(\n        metadata={\"description\": \"The field that the records were sorted by\", \"example\": \"id\"}\n    )\n    sort_direction = fields.Enum(\n        SortDirection,\n        metadata={\"description\": \"The direction the records are sorted\"},\n    )\n\n\nclass PaginationInfoSchema(Schema):\n    # This is part of the response schema to provide all pagination information back to a user\n\n    page_offset = fields.Integer(\n        metadata={\"description\": \"The page number that was fetched\", \"example\": 1}\n    )\n    page_size = fields.Integer(\n        metadata={\"description\": \"The size of the page fetched\", \"example\": 25}\n    )\n    total_records = fields.Integer(\n        metadata={\"description\": \"The total number of records fetchable\", \"example\": 42}\n    )\n    total_pages = fields.Integer(\n        metadata={\"description\": \"The total number of pages that can be fetched\", \"example\": 2}\n    )\n\n    sort_order = fields.List(\n        fields.Nested(SortOrderSchema()),\n        metadata={\"description\": \"The sort order passed in originally\"},\n    )"}
{"path":"frontend/src/app/[locale]/process/ProcessIntro.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/process","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/process/ProcessIntro.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/pagination/paginator.py\nLanguage: py\nType: code\nDirectory: api/src/pagination\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/pagination/paginator.py\nSize: 2.61 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/process/ProcessInvolved.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/process","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/process/ProcessInvolved.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"from sqlalchemy import Select, func, inspect\n\nimport src.adapters.db as db\nfrom src.db.models.base import Base\n\nDEFAULT_PAGE_SIZE = 25\n\n\nT = TypeVar(\"T\", bound=Base)\n\n\nclass Paginator(Generic[T]):\n    \"\"\"\n    DB select statement paginator that helps with setting up queries\n    that you want to paginate into chunks.\n\n    Any usage of this should make sure that the select query passed in\n    contains sorting information otherwise results may not be expected.\n\n    Expected usage::\n        from sqlalchemy import desc, select\n\n        from src.db.models.opportunity_models import Opportunity\n        from src.pagination.paginator import Paginator\n\n        # Create a select statement that includes ordering and sorting\n        stmt = select(User).order_by(desc(\"opportunity_id\"))\n\n        # Add any filters\n        stmt = stmt.where(Opportunity.agency_code == \"US-XYZ\")\n\n        # Use the paginator to get a specific page (page 2 in this case)\n        paginator: Paginator[Opportunity] = Paginator(stmt, db_session, page_size=10)\n        users: list[Opportunity] = paginator.page_at(page_offset=2)\n\n    \"\"\"\n\n    def __init__(\n        self, table_model: Type[Base], stmt: Select, db_session: db.Session, page_size: int = 25\n    ):\n        self.table_model = table_model\n        self.stmt = stmt\n        self.db_session = db_session\n\n        if page_size <= 0:\n            raise ValueError(\"Page size must be at least 1\")\n\n        self.page_size = page_size\n\n        self.total_records = _get_record_count(self.table_model, self.db_session, self.stmt)\n        self.total_pages = int(math.ceil(self.total_records / self.page_size))\n\n    def page_at(self, page_offset: int) -> Sequence[T]:\n        \"\"\"\n        Get a specific page for pagination\n        \"\"\"\n        if page_offset <= 0 or page_offset > self.total_pages:\n            return []\n\n        offset = self.page_size * (page_offset - 1)\n\n        return (\n            self.db_session.execute(self.stmt.offset(offset).limit(self.page_size))\n            .unique()\n            .scalars()\n            .all()\n        )\n\n\ndef _get_record_count(table_model: Type[Base], db_session: db.Session, stmt: Select) -> int:\n    # Simplify the query to instead be select count(DISTINCT(<primary key>)) from <whatever the query was>\n    # and remove the order_by as we won't care for this query and it would just make it slower.\n\n    primary_key = inspect(table_model).primary_key[0]\n\n    count_stmt = stmt.order_by(None).with_only_columns(\n        func.count(primary_key.distinct()), maintain_column_froms=True\n    )\n    return db_session.execute(count_stmt).scalar_one()"}
{"path":"frontend/src/app/[locale]/process/ProcessNext.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/process","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/process/ProcessNext.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/search/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/process/ProcessProgress.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/process","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/process/ProcessProgress.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":""}
{"path":"frontend/src/app/[locale]/process/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/process","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/process/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/search/backend/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/search/backend\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/backend/__init__.py\nSize: 0.12 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/research/ResearchArchetypes.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/research","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/research/ResearchArchetypes.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":""}
{"path":"frontend/src/app/[locale]/research/ResearchImpact.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/research","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/research/ResearchImpact.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"File: api/src/search/backend/load_opportunities_to_index.py\nLanguage: py\nType: code\nDirectory: api/src/search/backend\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/backend/load_opportunities_to_index.py\nSize: 13.84 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/research/ResearchIntro.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/research","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/research/ResearchIntro.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.131Z","content":"from opensearchpy.exceptions import ConnectionTimeout, TransportError\nfrom pydantic import Field\nfrom pydantic_settings import SettingsConfigDict\nfrom sqlalchemy import select, update\nfrom sqlalchemy.orm import noload, selectinload\nfrom tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_fixed\n\nimport src.adapters.db as db\nimport src.adapters.search as search\nfrom src.api.opportunities_v1.opportunity_schemas import OpportunityV1Schema\nfrom src.db.models.agency_models import Agency\nfrom src.db.models.lookup_models import JobStatus\nfrom src.db.models.opportunity_models import (\n    CurrentOpportunitySummary,\n    Opportunity,\n    OpportunityAttachment,\n    OpportunityChangeAudit,\n)\nfrom src.db.models.task_models import JobLog\nfrom src.task.task import Task\nfrom src.util import datetime_util, file_util\nfrom src.util.datetime_util import get_now_us_eastern_datetime\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\nALLOWED_ATTACHMENT_SUFFIXES = set(\n    [\"txt\", \"pdf\", \"docx\", \"doc\", \"xlsx\", \"xlsm\", \"html\", \"htm\", \"pptx\", \"ppt\", \"rtf\"]\n)\n\n\nclass LoadOpportunitiesToIndexConfig(PydanticBaseEnvConfig):\n    model_config = SettingsConfigDict(env_prefix=\"LOAD_OPP_SEARCH_\")\n\n    shard_count: int = Field(default=1)  # LOAD_OPP_SEARCH_SHARD_COUNT\n    replica_count: int = Field(default=1)  # LOAD_OPP_SEARCH_REPLICA_COUNT\n\n    # TODO - these might make sense to come from some sort of opportunity-search-index-config?\n    # look into this a bit more when we setup the search endpoint itself.\n    alias_name: str = Field(default=\"opportunity-index-alias\")  # LOAD_OPP_SEARCH_ALIAS_NAME\n    index_prefix: str = Field(default=\"opportunity-index\")  # LOAD_OPP_INDEX_PREFIX\n\n    enable_opportunity_attachment_pipeline: bool = Field(\n        default=False, alias=\"ENABLE_OPPORTUNITY_ATTACHMENT_PIPELINE\"\n    )\n\n\nclass LoadOpportunitiesToIndex(Task):\n    class Metrics(StrEnum):\n        RECORDS_LOADED = \"records_loaded\"\n        TEST_RECORDS_SKIPPED = \"test_records_skipped\"\n\n    def __init__(\n        self,\n        db_session: db.Session,\n        search_client: search.SearchClient,\n        is_full_refresh: bool = True,\n        config: LoadOpportunitiesToIndexConfig | None = None,\n    ) -> None:\n        super().__init__(db_session)\n\n        self.search_client = search_client\n        self.is_full_refresh = is_full_refresh\n        if config is None:\n            config = LoadOpportunitiesToIndexConfig()\n        self.config = config\n\n        if is_full_refresh:\n            current_timestamp = get_now_us_eastern_datetime().strftime(\"%Y-%m-%d_%H-%M-%S\")\n            self.index_name = f\"{self.config.index_prefix}-{current_timestamp}\"\n        else:\n            self.index_name = self.config.alias_name\n        self.set_metrics({\"index_name\": self.index_name})\n\n    def run_task(self) -> None:\n        logger.info(\"Creating multi-attachment pipeline\")\n        self._create_multi_attachment_pipeline()\n        if self.is_full_refresh:\n            logger.info(\"Running full refresh\")\n            self.full_refresh()\n        else:\n            logger.info(\"Running incremental load\")\n            self.incremental_updates_and_deletes()\n\n    def _create_multi_attachment_pipeline(self) -> None:\n        \"\"\"\n        Create multi-attachment processor\n        \"\"\"\n        pipeline = {\n            \"description\": \"Extract attachment information\",\n            \"processors\": [\n                {\n                    \"foreach\": {\n                        \"field\": \"attachments\",\n                        \"processor\": {\n                            \"attachment\": {\n                                \"target_field\": \"_ingest._value.attachment\",\n                                \"field\": \"_ingest._value.data\",\n                            }\n                        },\n                        \"ignore_missing\": True,\n                    }\n                }\n            ],\n        }\n\n        self.search_client.put_pipeline(pipeline, \"multi-attachment\")\n\n    def incremental_updates_and_deletes(self) -> None:\n        existing_opportunity_ids = self.fetch_existing_opportunity_ids_in_index()\n\n        # Handle updates/inserts\n        self._handle_incremental_upserts(existing_opportunity_ids)\n\n        # Handle deletes\n        self._handle_incremental_delete(existing_opportunity_ids)\n\n    def _handle_incremental_upserts(self, existing_opportunity_ids: set[int]) -> None:\n        \"\"\"Handle updates/inserts of opportunities into the search index when running incrementally\"\"\"\n\n        # Get last successful job timestamp\n        last_successful_job = (\n            self.db_session.query(JobLog)\n            .filter(\n                JobLog.job_type == self.cls_name(),\n                JobLog.job_status == JobStatus.COMPLETED,\n            )\n            .order_by(JobLog.created_at.desc())\n            .first()\n        )\n\n        # Fetch opportunities that need processing from the queue\n        query = (\n            select(Opportunity)\n            .join(OpportunityChangeAudit)\n            .join(CurrentOpportunitySummary)\n            .where(\n                Opportunity.is_draft.is_(False),\n                CurrentOpportunitySummary.opportunity_status.isnot(None),\n            )\n            .options(selectinload(\"*\"), noload(Opportunity.all_opportunity_summaries))\n        )\n\n        # Add timestamp filter\n        if last_successful_job:\n            query = query.where(OpportunityChangeAudit.updated_at > last_successful_job.created_at)\n\n        queued_opportunities = self.db_session.execute(query).scalars().all()\n\n        # Process updates and inserts\n        processed_opportunity_ids = set()\n        opportunities_to_index = []\n\n        for opportunity in queued_opportunities:\n            logger.info(\n                \"Processing queued opportunity\",\n                extra={\n                    \"opportunity_id\": opportunity.opportunity_id,\n                    \"status\": (\n                        \"update\"\n                        if opportunity.opportunity_id in existing_opportunity_ids\n                        else \"insert\"\n                    ),\n                },\n            )\n\n            # Add to index batch if it's indexable\n            opportunities_to_index.append(opportunity)\n            processed_opportunity_ids.add(opportunity.opportunity_id)\n\n        # Bulk index the opportunities (handles both inserts and updates)\n        if opportunities_to_index:\n            loaded_ids = self.load_records(opportunities_to_index)\n            logger.info(f\"Indexed {len(loaded_ids)} opportunities\")\n\n            # Update updated_at timestamp instead of deleting records\n            self.db_session.execute(\n                update(OpportunityChangeAudit)\n                .where(OpportunityChangeAudit.opportunity_id.in_(processed_opportunity_ids))\n                .values(updated_at=datetime_util.utcnow())\n            )\n\n    def _handle_incremental_delete(self, existing_opportunity_ids: set[int]) -> None:\n        \"\"\"Handle deletion of opportunities when running incrementally\n\n        Scenarios in which we delete an opportunity from the index:\n        * An opportunity is no longer in our database\n        * An opportunity is a draft (unlikely to ever happen, would require published->draft)\n        * An opportunity loses its opportunity status\n        * An opportunity has a test agency\n        \"\"\"\n\n        # Fetch the opportunity IDs of opportunities we would expect to be in the index\n        opportunity_ids_we_want_in_search: set[int] = set(\n            self.db_session.execute(\n                select(Opportunity.opportunity_id)\n                .join(CurrentOpportunitySummary)\n                .join(Agency, Opportunity.agency_code == Agency.agency_code, isouter=True)\n                .where(\n                    Opportunity.is_draft.is_(False),\n                    CurrentOpportunitySummary.opportunity_status.isnot(None),\n                    # We treat a null agency as fine\n                    # We only want to filter out if is_test_agency=True specifically\n                    Agency.is_test_agency.isnot(True),\n                )\n            )\n            .scalars()\n            .all()\n        )\n\n        opportunity_ids_to_delete = existing_opportunity_ids - opportunity_ids_we_want_in_search\n\n        for opportunity_id in opportunity_ids_to_delete:\n            logger.info(\n                \"Deleting opportunity from search\",\n                extra={\"opportunity_id\": opportunity_id, \"status\": \"delete\"},\n            )\n\n        if opportunity_ids_to_delete:\n            self.search_client.bulk_delete(self.index_name, opportunity_ids_to_delete)\n\n    def full_refresh(self) -> None:\n        # create the index\n        self.search_client.create_index(\n            self.index_name,\n            shard_count=self.config.shard_count,\n            replica_count=self.config.replica_count,\n        )\n\n        # load the records\n        for opp_batch in self.fetch_opportunities():\n            self.load_records(opp_batch)\n\n        # handle aliasing of endpoints\n        self.search_client.swap_alias_index(\n            self.index_name,\n            self.config.alias_name,\n        )\n\n        # cleanup old indexes\n        self.search_client.cleanup_old_indices(self.config.index_prefix, [self.index_name])\n\n    def fetch_opportunities(self) -> Iterator[Sequence[Opportunity]]:\n        \"\"\"\n        Fetch the opportunities in batches. The iterator returned\n        will give you each individual batch to be processed.\n\n        Fetches all opportunities where:\n            * is_draft = False\n            * current_opportunity_summary is not None\n        \"\"\"\n        return (\n            self.db_session.execute(\n                select(Opportunity)\n                .join(CurrentOpportunitySummary)\n                .where(\n                    Opportunity.is_draft.is_(False),\n                    CurrentOpportunitySummary.opportunity_status.isnot(None),\n                )\n                .options(selectinload(\"*\"), noload(Opportunity.all_opportunity_summaries))\n                # Top level agency won't be automatically fetched up front unless we add this\n                # due to the odd nature of the relationship we have setup for the agency table\n                # Adding it here improves performance when serializing to JSON as we won't need to\n                # call out to the DB repeatedly.\n                .options(\n                    selectinload(Opportunity.agency_record).selectinload(Agency.top_level_agency)\n                )\n                .execution_options(yield_per=1000)\n            )\n            .scalars()\n            .partitions()\n        )\n\n    def fetch_existing_opportunity_ids_in_index(self) -> set[int]:\n        if not self.search_client.alias_exists(self.index_name):\n            raise RuntimeError(\n                \"Alias %s does not exist, please run the full refresh job before the incremental job\"\n                % self.index_name\n            )\n\n        opportunity_ids: set[int] = set()\n\n        for response in self.search_client.scroll(\n            self.config.alias_name,\n            {\"size\": 10000, \"_source\": [\"opportunity_id\"]},\n            include_scores=False,\n        ):\n            for record in response.records:\n                opportunity_ids.add(record[\"opportunity_id\"])\n\n        return opportunity_ids\n\n    def filter_attachment(self, attachment: OpportunityAttachment) -> bool:\n        file_suffix = attachment.file_name.lower().split(\".\")[-1]\n        return file_suffix in ALLOWED_ATTACHMENT_SUFFIXES\n\n    def get_attachment_json_for_opportunity(\n        self, opp_attachments: list[OpportunityAttachment]\n    ) -> list[dict]:\n\n        attachments = []\n        for att in opp_attachments:\n            if self.filter_attachment(att):\n                with file_util.open_stream(\n                    att.file_location,\n                    \"rb\",\n                ) as file:\n                    file_content = file.read()\n                    attachments.append(\n                        {\n                            \"filename\": att.file_name,\n                            \"data\": base64.b64encode(file_content).decode(\"utf-8\"),\n                        }\n                    )\n\n        return attachments\n\n    @retry(\n        stop=stop_after_attempt(3),  # Retry up to 3 times\n        wait=wait_fixed(2),  # Wait 2 seconds between retries\n        retry=retry_if_exception_type(\n            (TransportError, ConnectionTimeout)\n        ),  # Retry on TransportError (including timeouts)\n    )\n    def load_records(self, records: Sequence[Opportunity]) -> set[int]:\n        logger.info(\"Loading batch of opportunities...\")\n\n        schema = OpportunityV1Schema()\n        json_records = []\n\n        loaded_opportunity_ids = set()\n\n        for record in records:\n            log_extra = {\n                \"opportunity_id\": record.opportunity_id,\n                \"opportunity_status\": record.opportunity_status,\n            }\n            logger.info(\"Preparing opportunity for upload to search index\", extra=log_extra)\n\n            # If the opportunity has a test agency, skip uploading it to the index\n            if record.agency_record and record.agency_record.is_test_agency:\n                logger.info(\n                    \"Skipping upload of opportunity as agency is a test agency\",\n                    extra=log_extra | {\"agency\": record.agency_code},\n                )\n                self.increment(self.Metrics.TEST_RECORDS_SKIPPED)\n                continue\n\n            json_record = schema.dump(record)\n            if self.config.enable_opportunity_attachment_pipeline:\n                json_record[\"attachments\"] = self.get_attachment_json_for_opportunity(\n                    record.opportunity_attachments\n                )\n\n            json_records.append(json_record)\n            self.increment(self.Metrics.RECORDS_LOADED)\n\n            loaded_opportunity_ids.add(record.opportunity_id)\n\n        self.search_client.bulk_upsert(\n            self.index_name, json_records, \"opportunity_id\", pipeline=\"multi-attachment\"\n        )\n\n        return loaded_opportunity_ids"}
{"path":"frontend/src/app/[locale]/research/ResearchMethodology.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/research","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/research/ResearchMethodology.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/search/backend/load_search_data.py\nLanguage: py\nType: code\nDirectory: api/src/search/backend\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/backend/load_search_data.py\nSize: 1.00 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/research/ResearchThemes.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/research","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/research/ResearchThemes.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"import src.adapters.db as db\nimport src.adapters.search as search\nfrom src.adapters.db import flask_db\nfrom src.adapters.search import flask_opensearch\nfrom src.search.backend.load_opportunities_to_index import LoadOpportunitiesToIndex\nfrom src.search.backend.load_search_data_blueprint import load_search_data_blueprint\nfrom src.task.ecs_background_task import ecs_background_task\n\n\n@load_search_data_blueprint.cli.command(\n    \"load-opportunity-data\", help=\"Load opportunity data from our database to the search index\"\n)\n@click.option(\n    \"--full-refresh/--incremental\",\n    default=True,\n    help=\"Whether to run a full refresh, or only incrementally update oppportunities\",\n)\n@flask_db.with_db_session()\n@flask_opensearch.with_search_client()\n@ecs_background_task(task_name=\"load-opportunity-data-opensearch\")\ndef load_opportunity_data(\n    search_client: search.SearchClient, db_session: db.Session, full_refresh: bool\n) -> None:\n    LoadOpportunitiesToIndex(db_session, search_client, full_refresh).run()"}
{"path":"frontend/src/app/[locale]/research/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/research","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/research/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/search/backend/load_search_data_blueprint.py\nLanguage: py\nType: code\nDirectory: api/src/search/backend\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/backend/load_search_data_blueprint.py\nSize: 0.16 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/saved-grants/layout.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/saved-grants","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/saved-grants/layout.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"load_search_data_blueprint = APIBlueprint(\n    \"load-search-data\", __name__, enable_openapi=False, cli_group=\"load-search-data\"\n)"}
{"path":"frontend/src/app/[locale]/saved-grants/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/saved-grants","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/saved-grants/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/search/search_config.py\nLanguage: py\nType: code\nDirectory: api/src/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/search_config.py\nSize: 0.41 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/search/QueryProvider.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/search/QueryProvider.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from src.util.env_config import PydanticBaseEnvConfig\n\n\nclass SearchConfig(PydanticBaseEnvConfig):\n    opportunity_search_index_alias: str = Field(default=\"opportunity-index-alias\")\n\n\n_search_config: SearchConfig | None = None\n\n\ndef get_search_config() -> SearchConfig:\n    global _search_config\n\n    if _search_config is None:\n        _search_config = SearchConfig()\n\n    return _search_config"}
{"path":"frontend/src/app/[locale]/search/error.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/search/error.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/search/search_models.py\nLanguage: py\nType: code\nDirectory: api/src/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/search/search_models.py\nSize: 0.47 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/search/layout.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/search/layout.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from pydantic import BaseModel\n\n\nclass StrSearchFilter(BaseModel):\n    one_of: list[str] | None = None\n\n\nclass BoolSearchFilter(BaseModel):\n    one_of: list[bool] | None = None\n\n\nclass IntSearchFilter(BaseModel):\n    min: int | None = None\n    max: int | None = None\n\n\nclass DateSearchFilter(BaseModel):\n    start_date: date | None = None\n    end_date: date | None = None\n    start_date_relative: int | None = None\n    end_date_relative: int | None = None"}
{"path":"frontend/src/app/[locale]/search/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/search/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/agencies_v1/get_agencies.py\nLanguage: py\nType: service\nDirectory: api/src/services/agencies_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/agencies_v1/get_agencies.py\nSize: 1.61 KB\nLast Modified: 2025-02-14T17:08:26.445Z"}
{"path":"frontend/src/app/[locale]/subscribe/actions.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/subscribe","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/subscribe/actions.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from pydantic import BaseModel, Field\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import joinedload\n\nimport src.adapters.db as db\nfrom src.db.models.agency_models import Agency\nfrom src.pagination.pagination_models import PaginationInfo, PaginationParams\nfrom src.pagination.paginator import Paginator\nfrom src.services.service_utils import apply_sorting\n\nlogger = logging.getLogger(__name__)\n\n\nclass AgencyFilters(BaseModel):\n    agency_id: int | None = None\n    agency_name: str | None = None\n\n\nclass AgencyListParams(BaseModel):\n    pagination: PaginationParams\n\n    filters: AgencyFilters | None = Field(default_factory=AgencyFilters)\n\n\ndef get_agencies(\n    db_session: db.Session, list_params: AgencyListParams\n) -> Tuple[Sequence[Agency], PaginationInfo]:\n\n    stmt = (\n        select(Agency).options(joinedload(Agency.top_level_agency), joinedload(\"*\"))\n        # Exclude test agencies\n        .where(Agency.is_test_agency.isnot(True))\n    )\n\n    stmt = apply_sorting(stmt, Agency, list_params.pagination.sort_order)\n\n    if list_params.filters:\n        if list_params.filters.agency_name:\n            stmt = stmt.where(Agency.agency_name == list_params.filters.agency_name)\n\n    # Apply pagination after processing\n    paginator: Paginator[Agency] = Paginator(\n        Agency, stmt, db_session, page_size=list_params.pagination.page_size\n    )\n\n    paginated_agencies = paginator.page_at(page_offset=list_params.pagination.page_offset)\n    pagination_info = PaginationInfo.from_pagination_params(list_params.pagination, paginator)\n\n    return paginated_agencies, pagination_info"}
{"path":"frontend/src/app/[locale]/subscribe/confirmation/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/subscribe/confirmation","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/subscribe/confirmation/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/extracts_v1/get_extracts.py\nLanguage: py\nType: service\nDirectory: api/src/services/extracts_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/extracts_v1/get_extracts.py\nSize: 3.15 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/app/[locale]/subscribe/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/subscribe","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/subscribe/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from pydantic import BaseModel, Field\nfrom sqlalchemy import asc, desc, select\n\nimport src.adapters.db as db\nfrom src.constants.lookup_constants import ExtractType\nfrom src.db.models.extract_models import ExtractMetadata\nfrom src.db.models.lookup_models import LkExtractType\nfrom src.pagination.pagination_models import PaginationInfo, PaginationParams, SortDirection\nfrom src.pagination.paginator import Paginator\nfrom src.search.search_models import DateSearchFilter\nfrom src.util import datetime_util\nfrom src.util.file_util import pre_sign_file_location\n\nlogger = logging.getLogger(__name__)\n\n\nclass ExtractFilters(BaseModel):\n    extract_type: ExtractType | None = None\n    # Default to last 7 days if no date range is provided\n    created_at: DateSearchFilter | None = Field(\n        default_factory=lambda: DateSearchFilter(\n            start_date=(datetime_util.utcnow() - timedelta(days=7)).date(),\n            end_date=datetime_util.utcnow().date() + timedelta(days=1),\n        )\n    )\n\n\nclass ExtractListParams(BaseModel):\n    pagination: PaginationParams\n\n    filters: ExtractFilters | None = Field(default_factory=ExtractFilters)\n\n\ndef get_extracts(\n    db_session: db.Session, list_params: ExtractListParams\n) -> Tuple[Sequence[ExtractMetadata], PaginationInfo]:\n    stmt = select(ExtractMetadata)\n\n    # Apply sorting from pagination params\n    for sort_order in list_params.pagination.sort_order:\n        if sort_order.order_by == \"extract_type\":\n            # Join with lookup table for extract type description\n            stmt = stmt.join(\n                LkExtractType, ExtractMetadata.extract_type == LkExtractType.extract_type_id\n            )\n            sort_column = LkExtractType.description\n        else:\n            sort_column = getattr(ExtractMetadata, sort_order.order_by)\n\n        if sort_order.sort_direction == SortDirection.ASCENDING:\n            stmt = stmt.order_by(asc(sort_column))\n        else:\n            stmt = stmt.order_by(desc(sort_column))\n\n    if list_params.filters:\n        if list_params.filters.extract_type:\n            stmt = stmt.where(ExtractMetadata.extract_type == list_params.filters.extract_type)\n\n        if list_params.filters.created_at:\n            if list_params.filters.created_at.start_date:\n                stmt = stmt.where(\n                    ExtractMetadata.created_at >= list_params.filters.created_at.start_date\n                )\n            if list_params.filters.created_at.end_date:\n                stmt = stmt.where(\n                    ExtractMetadata.created_at <= list_params.filters.created_at.end_date\n                )\n\n    # Apply pagination\n    paginator: Paginator[ExtractMetadata] = Paginator(\n        ExtractMetadata, stmt, db_session, page_size=list_params.pagination.page_size\n    )\n\n    extracts = paginator.page_at(page_offset=list_params.pagination.page_offset)\n    pagination_info = PaginationInfo.from_pagination_params(list_params.pagination, paginator)\n\n    for extract in extracts:\n        file_loc = extract.file_path\n        setattr(extract, \"download_path\", pre_sign_file_location(file_loc))  # noqa: B010\n\n    return extracts, pagination_info"}
{"path":"frontend/src/app/[locale]/subscribe/unsubscribe/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/subscribe/unsubscribe","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/subscribe/unsubscribe/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/opportunities_v1/__init__.py\nLanguage: py\nType: service\nDirectory: api/src/services/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunities_v1/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/app/[locale]/unauthenticated/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/unauthenticated","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/unauthenticated/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":""}
{"path":"frontend/src/app/[locale]/unauthorized/page.tsx","language":"typescript","type":"code","directory":"frontend/src/app/[locale]/unauthorized","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/[locale]/unauthorized/page.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/opportunities_v1/experimental_constant.py\nLanguage: py\nType: service\nDirectory: api/src/services/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunities_v1/experimental_constant.py\nSize: 1.36 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/app/api/auth/callback/route.ts","language":"typescript","type":"code","directory":"frontend/src/app/api/auth/callback","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/api/auth/callback/route.ts","size":324715,"lastModified":"2025-02-14T17:08:31.132Z","content":"DEFAULT = [\n    # Note that we do keyword & non-keyword for agency & opportunity number\n    # as we don't want to compare to a tokenized value which\n    # may have split on the dashes, but also still support prefixing (eg. USAID-*)\n    \"agency^16\",\n    \"agency.keyword^16\",\n    \"opportunity_title^2\",\n    \"opportunity_number^12\",\n    \"opportunity_number.keyword^12\",\n    \"summary.summary_description\",\n    \"opportunity_assistance_listings.assistance_listing_number^10\",\n    \"opportunity_assistance_listings.program_title^4\",\n]\n\nEXPANDED = [\n    \"agency\",\n    \"agency.keyword\",\n    \"agency_name\",\n    \"top_level_agency_name\",\n    \"opportunity_title\",\n    \"opportunity_number\",\n    \"opportunity_number.keyword\",\n    \"category_explanation\",\n    \"summary.summary_description\",\n    \"summary.applicant_eligibility_description\",\n    \"summary.funding_category_description\",\n    \"opportunity_assistance_listings.assistance_listing_number\",\n    \"opportunity_assistance_listings.program_title\",\n]\n\n\nAGENCY = [\n    \"agency\",\n    \"agency.keyword\",\n    \"agency_name\",\n    \"top_level_agency_name\",\n    \"summary.agency_contact_description\",\n    \"summary.agency_email_address.keyword\",\n    \"summary.agency_email_address_description\",\n    \"summary.agency_phone_number.keyword\",\n]\n\n\nclass ScoringRule(StrEnum):\n    DEFAULT = \"default\"\n    EXPANDED = \"expanded\"\n    AGENCY = \"agency\""}
{"path":"frontend/src/app/api/auth/login/route.ts","language":"typescript","type":"code","directory":"frontend/src/app/api/auth/login","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/api/auth/login/route.ts","size":324715,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/opportunities_v1/get_opportunity.py\nLanguage: py\nType: service\nDirectory: api/src/services/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunities_v1/get_opportunity.py\nSize: 2.42 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/app/api/auth/logout/route.ts","language":"typescript","type":"code","directory":"frontend/src/app/api/auth/logout","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/api/auth/logout/route.ts","size":324715,"lastModified":"2025-02-14T17:08:31.132Z","content":"import src.adapters.db as db\nfrom src.adapters.aws import S3Config\nfrom src.api.route_utils import raise_flask_error\nfrom src.db.models.agency_models import Agency\nfrom src.db.models.opportunity_models import Opportunity, OpportunityAttachment\nfrom src.util.env_config import PydanticBaseEnvConfig\nfrom src.util.file_util import convert_public_s3_to_cdn_url, pre_sign_file_location\n\n\nclass AttachmentConfig(PydanticBaseEnvConfig):\n    # If the CDN URL is set, we'll use it instead of pre-signing the file locations\n    cdn_url: str | None = None\n\n\ndef _fetch_opportunity(\n    db_session: db.Session, opportunity_id: int, load_all_opportunity_summaries: bool\n) -> Opportunity:\n    stmt = (\n        select(Opportunity)\n        .where(Opportunity.opportunity_id == opportunity_id)\n        .where(Opportunity.is_draft.is_(False))\n        .options(selectinload(\"*\"))\n        # To get the top_level_agency field set properly upfront,\n        # we need to explicitly join here as the \"*\" approach doesn't\n        # seem to work with the way our agency relationships are setup\n        .options(selectinload(Opportunity.agency_record).selectinload(Agency.top_level_agency))\n    )\n\n    if not load_all_opportunity_summaries:\n        stmt = stmt.options(noload(Opportunity.all_opportunity_summaries))\n\n    opportunity = db_session.execute(stmt).unique().scalar_one_or_none()\n\n    if opportunity is None:\n        raise_flask_error(404, message=f\"Could not find Opportunity with ID {opportunity_id}\")\n\n    return opportunity\n\n\ndef pre_sign_opportunity_file_location(\n    opp_atts: list,\n) -> list[OpportunityAttachment]:\n    for opp_att in opp_atts:\n        opp_att.download_path = pre_sign_file_location(opp_att.file_location)\n\n    return opp_atts\n\n\ndef get_opportunity(db_session: db.Session, opportunity_id: int) -> Opportunity:\n    opportunity = _fetch_opportunity(\n        db_session, opportunity_id, load_all_opportunity_summaries=False\n    )\n\n    attachment_config = AttachmentConfig()\n    if attachment_config.cdn_url is not None:\n        s3_config = S3Config()\n        for opp_att in opportunity.opportunity_attachments:\n            opp_att.download_path = convert_public_s3_to_cdn_url(  # type: ignore\n                opp_att.file_location, attachment_config.cdn_url, s3_config\n            )\n    else:\n        pre_sign_opportunity_file_location(opportunity.opportunity_attachments)\n\n    return opportunity"}
{"path":"frontend/src/app/api/auth/session/route.ts","language":"typescript","type":"code","directory":"frontend/src/app/api/auth/session","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/api/auth/session/route.ts","size":324715,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/opportunities_v1/opportunity_to_csv.py\nLanguage: py\nType: service\nDirectory: api/src/services/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunities_v1/opportunity_to_csv.py\nSize: 2.41 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/app/api/mock/APIMockResponse.json","language":"json","type":"code","directory":"frontend/src/app/api/mock","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/api/mock/APIMockResponse.json","size":0,"lastModified":"2025-02-14T17:08:31.132Z","content":"from src.util.dict_util import flatten_dict\n\nCSV_FIELDS = [\n    \"opportunity_id\",\n    \"opportunity_number\",\n    \"opportunity_title\",\n    \"opportunity_status\",\n    \"agency_code\",\n    \"category\",\n    \"category_explanation\",\n    \"post_date\",\n    \"close_date\",\n    \"close_date_description\",\n    \"archive_date\",\n    \"is_cost_sharing\",\n    \"expected_number_of_awards\",\n    \"estimated_total_program_funding\",\n    \"award_floor\",\n    \"award_ceiling\",\n    \"additional_info_url\",\n    \"additional_info_url_description\",\n    \"opportunity_assistance_listings\",\n    \"funding_instruments\",\n    \"funding_categories\",\n    \"funding_category_description\",\n    \"applicant_types\",\n    \"applicant_eligibility_description\",\n    \"agency_name\",\n    \"top_level_agency_name\",\n    \"agency_contact_description\",\n    \"agency_email_address\",\n    \"agency_email_address_description\",\n    \"is_forecast\",\n    \"forecasted_post_date\",\n    \"forecasted_close_date\",\n    \"forecasted_close_date_description\",\n    \"forecasted_award_date\",\n    \"forecasted_project_start_date\",\n    \"fiscal_year\",\n    \"created_at\",\n    \"updated_at\",\n    # We put the description at the end as it's the longest value\n    # which can help improve readability of other fields\n    \"summary_description\",\n]\n# Same as above, but faster lookup\nCSV_FIELDS_SET = set(CSV_FIELDS)\n\n\ndef _process_assistance_listing(assistance_listings: list[dict]) -> str:\n    return \";\".join(\n        [f\"{a['assistance_listing_number']}|{a['program_title']}\" for a in assistance_listings]\n    )\n\n\ndef opportunities_to_csv(opportunities: Sequence[dict], output: io.StringIO) -> None:\n    writer = csv.DictWriter(output, fieldnames=CSV_FIELDS, quoting=csv.QUOTE_ALL)\n    writer.writeheader()\n\n    for opportunity in opportunities:\n        opp = flatten_dict(opportunity)\n\n        out_opportunity = {}\n        for k, v in opp.items():\n            # Remove prefixes from nested data structures\n            k = k.removeprefix(\"summary.\")\n            k = k.removeprefix(\"assistance_listings.\")\n\n            # Remove fields we haven't configured\n            if k not in CSV_FIELDS_SET:\n                continue\n\n            if k == \"opportunity_assistance_listings\":\n                v = _process_assistance_listing(v)\n\n            if k in [\"funding_instruments\", \"funding_categories\", \"applicant_types\"]:\n                v = \";\".join(v)\n\n            out_opportunity[k] = v\n\n        writer.writerow(out_opportunity)"}
{"path":"frontend/src/app/api/search/export/route.ts","language":"typescript","type":"code","directory":"frontend/src/app/api/search/export","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/api/search/export/route.ts","size":324715,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/opportunities_v1/search_opportunities.py\nLanguage: py\nType: service\nDirectory: api/src/services/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunities_v1/search_opportunities.py\nSize: 9.16 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/app/api/user/saved-opportunities/[id]/route.ts","language":"typescript","type":"code","directory":"frontend/src/app/api/user/saved-opportunities/[id]","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/api/user/saved-opportunities/[id]/route.ts","size":324715,"lastModified":"2025-02-14T17:08:31.132Z","content":"from pydantic import BaseModel, Field\n\nimport src.adapters.search as search\nfrom src.adapters.search.opensearch_response import SearchResponse\nfrom src.api.opportunities_v1.opportunity_schemas import OpportunityV1Schema, SearchQueryOperator\nfrom src.pagination.pagination_models import (\n    PaginationInfo,\n    PaginationParams,\n    SortDirection,\n    SortOrder,\n)\nfrom src.search.search_config import get_search_config\nfrom src.search.search_models import (\n    BoolSearchFilter,\n    DateSearchFilter,\n    IntSearchFilter,\n    StrSearchFilter,\n)\nfrom src.services.opportunities_v1.experimental_constant import (\n    AGENCY,\n    DEFAULT,\n    EXPANDED,\n    ScoringRule,\n)\n\nlogger = logging.getLogger(__name__)\n\n# To assist with mapping field names from our API requests\n# to what they are called in the search index, this mapping\n# can be used. Note that in many cases its just adjusting paths\n# or for text based fields adding \".keyword\" to the end to tell\n# the query we want to use the raw value rather than the tokenized one\n# See: https://opensearch.org/docs/latest/field-types/supported-field-types/keyword/\nREQUEST_FIELD_NAME_MAPPING = {\n    \"opportunity_number\": \"opportunity_number.keyword\",\n    \"opportunity_title\": \"opportunity_title.keyword\",\n    \"post_date\": \"summary.post_date\",\n    \"close_date\": \"summary.close_date\",\n    \"agency_code\": \"agency_code.keyword\",\n    \"agency\": \"agency_code.keyword\",\n    \"agency_name\": \"agency_name.keyword\",\n    \"top_level_agency_name\": \"top_level_agency_name.keyword\",\n    \"opportunity_status\": \"opportunity_status.keyword\",\n    \"funding_instrument\": \"summary.funding_instruments.keyword\",\n    \"funding_category\": \"summary.funding_categories.keyword\",\n    \"applicant_type\": \"summary.applicant_types.keyword\",\n    \"is_cost_sharing\": \"summary.is_cost_sharing\",\n    \"expected_number_of_awards\": \"summary.expected_number_of_awards\",\n    \"award_floor\": \"summary.award_floor\",\n    \"award_ceiling\": \"summary.award_ceiling\",\n    \"estimated_total_program_funding\": \"summary.estimated_total_program_funding\",\n}\n\nFILTER_RULE_MAPPING = {\n    ScoringRule.EXPANDED: EXPANDED,\n    ScoringRule.AGENCY: AGENCY,\n    ScoringRule.DEFAULT: DEFAULT,\n}\n\nSTATIC_PAGINATION = {\n    \"pagination\": {\n        \"page_offset\": 1,\n        \"page_size\": 1000,\n        \"sort_order\": [\n            {\n                \"order_by\": \"post_date\",\n                \"sort_direction\": \"descending\",\n            }\n        ],\n    }\n}\n\nSCHEMA = OpportunityV1Schema()\n\n\nclass OpportunityFilters(BaseModel):\n    applicant_type: StrSearchFilter | None = None\n    funding_instrument: StrSearchFilter | None = None\n    funding_category: StrSearchFilter | None = None\n    funding_applicant_type: StrSearchFilter | None = None\n    opportunity_status: StrSearchFilter | None = None\n    agency: StrSearchFilter | None = None\n    assistance_listing_number: StrSearchFilter | None = None\n\n    is_cost_sharing: BoolSearchFilter | None = None\n\n    expected_number_of_awards: IntSearchFilter | None = None\n    award_floor: IntSearchFilter | None = None\n    award_ceiling: IntSearchFilter | None = None\n    estimated_total_program_funding: IntSearchFilter | None = None\n\n    post_date: DateSearchFilter | None = None\n    close_date: DateSearchFilter | None = None\n\n\nclass Experimental(BaseModel):\n    scoring_rule: ScoringRule = Field(default=ScoringRule.DEFAULT)\n\n\nclass SearchOpportunityParams(BaseModel):\n    pagination: PaginationParams\n\n    query: str | None = Field(default=None)\n    query_operator: str = Field(default=SearchQueryOperator.AND)\n    filters: OpportunityFilters | None = Field(default=None)\n    experimental: Experimental = Field(default=Experimental())\n\n\ndef _adjust_field_name(field: str) -> str:\n    return REQUEST_FIELD_NAME_MAPPING.get(field, field)\n\n\ndef _get_sort_by(pagination: PaginationParams) -> list[tuple[str, SortDirection]]:\n    sort_by: list[tuple[str, SortDirection]] = []\n\n    for sort_order in pagination.sort_order:\n        sort_by.append((_adjust_field_name(sort_order.order_by), sort_order.sort_direction))\n\n    return sort_by\n\n\ndef _add_search_filters(\n    builder: search.SearchQueryBuilder, filters: OpportunityFilters | None\n) -> None:\n    if filters is None:\n        return\n\n    for field in filters.model_fields_set:\n        field_filters = getattr(filters, field)\n        field_name = _adjust_field_name(field)\n\n        # We use the type of the search filter to determine what methods\n        # we call on the builder. This way we can make sure we have the proper\n        # type mappings.\n        if isinstance(field_filters, StrSearchFilter) and field_filters.one_of:\n            builder.filter_terms(field_name, field_filters.one_of)\n\n        elif isinstance(field_filters, BoolSearchFilter) and field_filters.one_of:\n            builder.filter_terms(field_name, field_filters.one_of)\n\n        elif isinstance(field_filters, IntSearchFilter):\n            builder.filter_int_range(field_name, field_filters.min, field_filters.max)\n\n        elif isinstance(field_filters, DateSearchFilter):\n            start_date = (\n                field_filters.start_date\n                if field_filters.start_date\n                else field_filters.start_date_relative\n            )\n            end_date = (\n                field_filters.end_date\n                if field_filters.end_date\n                else field_filters.end_date_relative\n            )\n            builder.filter_date_range(\n                field_name,\n                start_date,\n                end_date,\n            )\n\n\ndef _add_aggregations(builder: search.SearchQueryBuilder) -> None:\n    # TODO - we'll likely want to adjust the total number of values returned, especially\n    # for agency as there could be hundreds of different agencies, and currently it's limited to 25.\n    builder.aggregation_terms(\"opportunity_status\", _adjust_field_name(\"opportunity_status\"))\n    builder.aggregation_terms(\"applicant_type\", _adjust_field_name(\"applicant_type\"))\n    builder.aggregation_terms(\"funding_instrument\", _adjust_field_name(\"funding_instrument\"))\n    builder.aggregation_terms(\"funding_category\", _adjust_field_name(\"funding_category\"))\n    builder.aggregation_terms(\"agency\", _adjust_field_name(\"agency_code\"), size=1000)\n\n\ndef _get_search_request(params: SearchOpportunityParams, aggregation: bool = True) -> dict:\n    builder = search.SearchQueryBuilder()\n\n    # Make sure total hit count gets counted for more than 10k records\n    builder.track_total_hits(True)\n\n    # Pagination\n    builder.pagination(\n        page_size=params.pagination.page_size, page_number=params.pagination.page_offset\n    )\n\n    # Sorting\n    builder.sort_by(_get_sort_by(params.pagination))\n\n    # Query\n    if params.query:\n        filter_rule = FILTER_RULE_MAPPING.get(params.experimental.scoring_rule, DEFAULT)\n        builder.simple_query(params.query, filter_rule, params.query_operator)\n\n    # Filters\n    _add_search_filters(builder, params.filters)\n\n    if aggregation:\n        # Aggregations / Facet / Filter Counts\n        _add_aggregations(builder)\n\n    return builder.build()\n\n\ndef _search_opportunities(\n    search_client: search.SearchClient,\n    search_params: SearchOpportunityParams,\n    includes: list | None = None,\n) -> SearchResponse:\n    search_request = _get_search_request(search_params)\n\n    index_alias = get_search_config().opportunity_search_index_alias\n    logger.info(\n        \"Querying search index alias %s\", index_alias, extra={\"search_index_alias\": index_alias}\n    )\n\n    response = search_client.search(\n        index_alias, search_request, includes=includes, excludes=[\"attachments\"]\n    )\n\n    return response\n\n\ndef search_opportunities(\n    search_client: search.SearchClient, raw_search_params: dict\n) -> Tuple[Sequence[dict], dict, PaginationInfo]:\n\n    search_params = SearchOpportunityParams.model_validate(raw_search_params)\n    response = _search_opportunities(search_client, search_params)\n\n    pagination_info = PaginationInfo(\n        page_offset=search_params.pagination.page_offset,\n        page_size=search_params.pagination.page_size,\n        total_records=response.total_records,\n        total_pages=int(math.ceil(response.total_records / search_params.pagination.page_size)),\n        sort_order=[\n            SortOrder(order_by=p.order_by, sort_direction=p.sort_direction)\n            for p in search_params.pagination.sort_order\n        ],\n    )\n\n    # While the data returned is already JSON/dicts like we want to return\n    # APIFlask will try to run whatever we return through the deserializers\n    # which means anything that requires conversions like timestamps end up failing\n    # as they don't need to be converted. So, we convert everything to those types (serialize)\n    # so that deserialization won't fail.\n    records = SCHEMA.load(response.records, many=True)\n\n    return records, response.aggregations, pagination_info\n\n\ndef search_opportunities_id(search_client: search.SearchClient, search_query: dict) -> list:\n    # Override pagination when calling opensearch\n    updated_search_query = search_query | STATIC_PAGINATION\n    search_params = SearchOpportunityParams.model_validate(updated_search_query)\n\n    response = _search_opportunities(search_client, search_params, includes=[\"opportunity_id\"])\n\n    return [opp[\"opportunity_id\"] for opp in response.records]"}
{"path":"frontend/src/app/api/user/saved-opportunities/route.ts","language":"typescript","type":"code","directory":"frontend/src/app/api/user/saved-opportunities","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/api/user/saved-opportunities/route.ts","size":324715,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/opportunity_attachments/__init__.py\nLanguage: py\nType: service\nDirectory: api/src/services/opportunity_attachments\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunity_attachments/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/app/robots.ts","language":"typescript","type":"code","directory":"frontend/src/app","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/robots.ts","size":324715,"lastModified":"2025-02-14T17:08:31.132Z","content":""}
{"path":"frontend/src/app/sitemap.ts","language":"typescript","type":"code","directory":"frontend/src/app","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/app/sitemap.ts","size":324715,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/opportunity_attachments/attachment_util.py\nLanguage: py\nType: service\nDirectory: api/src/services/opportunity_attachments\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/opportunity_attachments/attachment_util.py\nSize: 1.53 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/components/BetaAlert.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/BetaAlert.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from src.adapters.aws import S3Config\nfrom src.db.models.opportunity_models import Opportunity\nfrom src.util import file_util\n\n\ndef get_s3_attachment_path(\n    file_name: str, opportunity_attachment_id: int, opportunity: Opportunity, s3_config: S3Config\n) -> str:\n    \"\"\"Construct a path to the attachments on s3\n\n    Will be formatted like:\n\n        s3://<bucket>/opportunities/<opportunity_id>/attachments/<attachment_id>/<file_name>\n\n    Note that we store the files under a \"folder\" with the attachment ID as\n    the legacy system doesn't guarantee file names are unique within an opportunity.\n    \"\"\"\n\n    base_path = (\n        s3_config.draft_files_bucket_path\n        if opportunity.is_draft\n        else s3_config.public_files_bucket_path\n    )\n\n    return file_util.join(\n        base_path,\n        \"opportunities\",\n        str(opportunity.opportunity_id),\n        \"attachments\",\n        str(opportunity_attachment_id),\n        file_name,\n    )\n\n\ndef adjust_legacy_file_name(existing_file_name: str) -> str:\n    \"\"\"Correct the file names to remove any characters problematic for URL/s3 processing.\n\n    We only keep the following characters:\n    * A-Z\n    * a-z\n    * 0-9\n    * _\n    * -\n    * ~\n    * .\n\n    Whitespace will be replaced with underscores.\n\n    All other characters will be removed.\n    \"\"\"\n\n    # Replace one-or-more whitespace with a single underscore\n    file_name = re.sub(r\"\\s+\", \"_\", existing_file_name)\n\n    # Remove all non-accepted characters\n    file_name = re.sub(r\"[^a-zA-Z0-9_.\\-~]\", \"\", file_name)\n\n    return file_name"}
{"path":"frontend/src/components/Breadcrumbs.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/Breadcrumbs.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/service_utils.py\nLanguage: py\nType: service\nDirectory: api/src/services\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/service_utils.py\nSize: 0.97 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/components/ClientSideUrlUpdater.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/ClientSideUrlUpdater.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from sqlalchemy import asc, desc\nfrom sqlalchemy.sql import Select\n\nfrom src.pagination.pagination_models import SortDirection\n\n\ndef apply_sorting(stmt: Select, model: Type, sort_order: list) -> Select:\n    \"\"\"\n    Applies sorting to a SQLAlchemy select statement based on the provided sorting orders.\n\n    :param stmt: The SQLAlchemy query statement to which sorting should be applied.\n    :param model: The model class on which the sorting should be applied.\n    :param sort_order: A list of object describing the sorting order for a column.\n    :return: The modified query statement with the applied sorting.\n    \"\"\"\n\n    order_cols: list = []\n    for order in sort_order:\n        column = getattr(model, order.order_by)\n        if order.sort_direction == SortDirection.ASCENDING:\n            order_cols.append(asc(column))\n        elif order.sort_direction == SortDirection.DESCENDING:\n            order_cols.append(desc(column))\n\n    return stmt.order_by(*order_cols)"}
{"path":"frontend/src/components/CollapsableContent.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/CollapsableContent.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/users/README.md\nLanguage: md\nType: service\nDirectory: api/src/services/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/README.md\nSize: 0.13 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/components/ContentDisplayToggle.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/ContentDisplayToggle.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":""}
{"path":"frontend/src/components/ContentLayout.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/ContentLayout.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/users/__init__.py\nLanguage: py\nType: service\nDirectory: api/src/services/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/components/FilterCheckbox.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/FilterCheckbox.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":""}
{"path":"frontend/src/components/Footer.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/Footer.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/users/create_saved_search.py\nLanguage: py\nType: service\nDirectory: api/src/services/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/create_saved_search.py\nSize: 0.76 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/components/FullWidthAlert.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/FullWidthAlert.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from src.adapters import db, search\nfrom src.db.models.user_models import UserSavedSearch\nfrom src.services.opportunities_v1.search_opportunities import search_opportunities_id\n\nlogger = logging.getLogger(__name__)\n\n\ndef create_saved_search(\n    search_client: search.SearchClient, db_session: db.Session, user_id: UUID, json_data: dict\n) -> UserSavedSearch:\n\n    # Retrieve opportunity IDs\n    opportunity_ids = search_opportunities_id(search_client, json_data[\"search_query\"])\n\n    saved_search = UserSavedSearch(\n        user_id=user_id,\n        name=json_data[\"name\"],\n        search_query=json_data[\"search_query\"],\n        searched_opportunity_ids=opportunity_ids,\n    )\n\n    db_session.add(saved_search)\n\n    return saved_search"}
{"path":"frontend/src/components/GrantsIdentifier.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/GrantsIdentifier.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/users/delete_saved_opportunity.py\nLanguage: py\nType: service\nDirectory: api/src/services/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/delete_saved_opportunity.py\nSize: 0.58 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/components/Header.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/Header.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from sqlalchemy import delete\n\nfrom src.adapters import db\nfrom src.api.route_utils import raise_flask_error\nfrom src.db.models.user_models import UserSavedOpportunity\n\n\ndef delete_saved_opportunity(db_session: db.Session, user_id: UUID, opportunity_id: int) -> None:\n    result = db_session.execute(\n        delete(UserSavedOpportunity).where(\n            UserSavedOpportunity.user_id == user_id,\n            UserSavedOpportunity.opportunity_id == opportunity_id,\n        )\n    )\n\n    if result.rowcount == 0:\n        raise_flask_error(404, \"Saved opportunity not found\")"}
{"path":"frontend/src/components/Hero.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/Hero.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/users/delete_saved_search.py\nLanguage: py\nType: service\nDirectory: api/src/services/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/delete_saved_search.py\nSize: 0.60 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/components/Layout.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/Layout.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from sqlalchemy import select\n\nfrom src.adapters import db\nfrom src.api.route_utils import raise_flask_error\nfrom src.db.models.user_models import UserSavedSearch\n\n\ndef delete_saved_search(db_session: db.Session, user_id: UUID, saved_search_id: UUID) -> None:\n    saved_search = db_session.execute(\n        select(UserSavedSearch).where(\n            UserSavedSearch.saved_search_id == saved_search_id, UserSavedSearch.user_id == user_id\n        )\n    ).scalar_one_or_none()\n\n    if not saved_search:\n        raise_flask_error(404, \"Saved search not found\")\n\n    db_session.delete(saved_search)"}
{"path":"frontend/src/components/Loading.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/Loading.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/users/get_saved_opportunities.py\nLanguage: py\nType: service\nDirectory: api/src/services/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/get_saved_opportunities.py\nSize: 0.75 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/components/LoginButtonModal.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/LoginButtonModal.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from sqlalchemy import select\nfrom sqlalchemy.orm import selectinload\n\nfrom src.adapters import db\nfrom src.db.models.opportunity_models import Opportunity\nfrom src.db.models.user_models import UserSavedOpportunity\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_saved_opportunities(db_session: db.Session, user_id: UUID) -> list[Opportunity]:\n    logger.info(f\"Getting saved opportunities for user {user_id}\")\n\n    saved_opportunities = (\n        db_session.execute(\n            select(Opportunity)\n            .join(UserSavedOpportunity)\n            .where(UserSavedOpportunity.user_id == user_id)\n            .options(selectinload(\"*\"))\n        )\n        .scalars()\n        .all()\n    )\n    return list(saved_opportunities)"}
{"path":"frontend/src/components/LoginModal.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/LoginModal.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/users/get_saved_searches.py\nLanguage: py\nType: service\nDirectory: api/src/services/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/get_saved_searches.py\nSize: 1.24 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/components/SaveButton.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/SaveButton.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from pydantic import BaseModel\nfrom sqlalchemy import select\n\nfrom src.adapters import db\nfrom src.db.models.user_models import UserSavedSearch\nfrom src.pagination.pagination_models import PaginationInfo, PaginationParams\nfrom src.pagination.paginator import Paginator\nfrom src.services.service_utils import apply_sorting\n\n\nclass SavedSearchListParams(BaseModel):\n    pagination: PaginationParams\n\n\ndef get_saved_searches(\n    db_session: db.Session, user_id: UUID, raw_search_params: dict\n) -> Tuple[Sequence[UserSavedSearch], PaginationInfo]:\n    \"\"\"Get all saved searches for a user\"\"\"\n\n    search_params = SavedSearchListParams.model_validate(raw_search_params)\n\n    stmt = select(UserSavedSearch).where(UserSavedSearch.user_id == user_id)\n\n    stmt = apply_sorting(stmt, UserSavedSearch, search_params.pagination.sort_order)\n\n    paginator: Paginator[UserSavedSearch] = Paginator(\n        UserSavedSearch, stmt, db_session, page_size=search_params.pagination.page_size\n    )\n\n    paginated_search = paginator.page_at(page_offset=search_params.pagination.page_offset)\n\n    pagination_info = PaginationInfo.from_pagination_params(search_params.pagination, paginator)\n\n    return paginated_search, pagination_info"}
{"path":"frontend/src/components/ServerErrorAlert.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/ServerErrorAlert.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/users/get_user.py\nLanguage: py\nType: service\nDirectory: api/src/services/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/get_user.py\nSize: 0.49 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/components/Spinner.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/Spinner.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from sqlalchemy import select\n\nfrom src.adapters import db\nfrom src.db.models.user_models import LinkExternalUser\n\n\ndef _fetch_user(db_session: db.Session, user_id: UUID) -> LinkExternalUser | None:\n    stmt = select(LinkExternalUser).where(LinkExternalUser.user_id == user_id)\n\n    user = db_session.execute(stmt).scalar_one_or_none()\n\n    return user\n\n\ndef get_user(db_session: db.Session, user_id: UUID) -> LinkExternalUser | None:\n    return _fetch_user(db_session, user_id)"}
{"path":"frontend/src/components/USWDSIcon.tsx","language":"typescript","type":"code","directory":"frontend/src/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/USWDSIcon.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/users/login_gov_callback_handler.py\nLanguage: py\nType: service\nDirectory: api/src/services/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/login_gov_callback_handler.py\nSize: 6.69 KB\nLast Modified: 2025-02-14T17:08:26.446Z"}
{"path":"frontend/src/components/content/IndexGoalContent.tsx","language":"typescript","type":"code","directory":"frontend/src/components/content","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/content/IndexGoalContent.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from pydantic import BaseModel\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import selectinload\n\nimport src.adapters.db as db\nfrom src.adapters.oauth.login_gov.login_gov_oauth_client import LoginGovOauthClient\nfrom src.adapters.oauth.oauth_client_models import OauthTokenRequest\nfrom src.api.route_utils import raise_flask_error\nfrom src.auth.api_jwt_auth import create_jwt_for_user\nfrom src.auth.auth_errors import JwtValidationError\nfrom src.auth.login_gov_jwt_auth import get_login_gov_client_assertion, validate_token\nfrom src.constants.lookup_constants import ExternalUserType\nfrom src.db.models.user_models import LinkExternalUser, LoginGovState, User\nfrom src.util.string_utils import is_valid_uuid\n\nlogger = logging.getLogger(__name__)\n\n\nclass CallbackParams(BaseModel):\n    code: str\n    state: str\n    error: str | None = None\n    error_description: str | None = None\n\n\n@dataclass\nclass LoginGovDataContainer:\n    \"\"\"Holds various login gov related fields we want to pass around\"\"\"\n\n    code: str\n    nonce: str\n\n\n@dataclass\nclass LoginGovCallbackResponse:\n    token: str\n    is_user_new: bool\n\n\ndef get_login_gov_client() -> LoginGovOauthClient:\n    \"\"\"Get the login.gov client, in a method to be overridable in tests\"\"\"\n    return LoginGovOauthClient()\n\n\ndef handle_login_gov_callback_request(\n    query_data: dict, db_session: db.Session\n) -> LoginGovDataContainer:\n    \"\"\"Handle the callback from login.gov after calling the authenticate endpoint\n\n    NOTE: Any errors thrown here will actually lead to a redirect due to the\n          with_login_redirect_error_handler handler we have attached to the route\n    \"\"\"\n    # Process the data coming back from login.gov via the redirect query params\n    # see: https://developers.login.gov/oidc/authorization/#authorization-response\n    callback_params = CallbackParams.model_validate(query_data)\n\n    # If we got an error back in the callback, raise an exception\n    # The only two documented error values are access_denied and invalid_request\n    # which would both indicate an issue in our configuration and we'll treat as a 5xx internal error\n    if callback_params.error is not None:\n        error_message = f\"{callback_params.error} {callback_params.error_description}\"\n        raise_flask_error(500, error_message)\n\n    # If the state value we received isn't a valid UUID\n    # then it's likely someone randomly calling the endpoint\n    # We don't want this validation on the schema as it would\n    # occur before our error catching that handles redirects\n    if not is_valid_uuid(callback_params.state):\n        raise_flask_error(422, \"Invalid OAuth state value\")\n\n    login_gov_state = db_session.execute(\n        select(LoginGovState).where(LoginGovState.login_gov_state_id == callback_params.state)\n    ).scalar_one_or_none()\n\n    # If we don't have the state value in our DB, that either means:\n    # * login.gov is very broken and sending us bad data\n    # * Someone called this endpoint directly with a random value\n    #\n    # There isn't a way to truly separate those here, so we'll assume the more likely second one\n    # and raise a 404 to say we have no idea what they passed us.\n    if login_gov_state is None:\n        raise_flask_error(404, \"OAuth state not found\")\n\n    # We do not want the login_gov_state to be reusable - so delete it\n    # even if we later error to avoid any replay attacks.\n    db_session.delete(login_gov_state)\n\n    return LoginGovDataContainer(code=callback_params.code, nonce=str(login_gov_state.nonce))\n\n\ndef handle_login_gov_token(\n    db_session: db.Session, login_gov_data: LoginGovDataContainer\n) -> LoginGovCallbackResponse:\n    \"\"\"Fetch user info from login gov, and handle user creation\n\n    NOTE: Any errors thrown here will actually lead to a redirect due to the\n          with_login_redirect_error_handler handler we have attached to the route\n    \"\"\"\n\n    # call the token endpoint (make a client)\n    # https://developers.login.gov/oidc/token/\n    client = get_login_gov_client()\n    response = client.get_token(\n        OauthTokenRequest(\n            code=login_gov_data.code, client_assertion=get_login_gov_client_assertion()\n        )\n    )\n\n    # If this request failed, we'll assume we're the issue and 500\n    if response.is_error_response():\n        raise_flask_error(500, response.error_description)\n\n    # Process the token response from login.gov\n    # which will create/update a user in the DB\n    return _process_token(db_session, response.id_token, login_gov_data.nonce)\n\n\ndef _process_token(db_session: db.Session, token: str, nonce: str) -> LoginGovCallbackResponse:\n    \"\"\"Process the token from login.gov and generate our own token for auth\"\"\"\n    try:\n        login_gov_user = validate_token(token, nonce)\n    except JwtValidationError as e:\n        logger.info(\"Login.gov token validation failed\", extra={\"auth.issue\": e.message})\n        raise_flask_error(401, e.message)\n\n    external_user: LinkExternalUser | None = db_session.execute(\n        select(LinkExternalUser)\n        .where(LinkExternalUser.external_user_id == login_gov_user.user_id)\n        # We only support login.gov right now, so this does nothing, but let's\n        # be explicit just in case.\n        .where(LinkExternalUser.external_user_type == ExternalUserType.LOGIN_GOV)\n        .options(selectinload(\"*\"))\n    ).scalar()\n\n    is_user_new = external_user is None\n\n    # If we didn't find anything, we want to create the user\n    if external_user is None:\n        external_user = _create_login_gov_user(login_gov_user.user_id, db_session)\n\n    # Update fields on the external user table\n    external_user.email = login_gov_user.email\n\n    # Flush the records to the DB so any auto-generated IDs and similar are populated\n    # prior to us trying to work with the user further.\n    # NOTE: This doesn't commit yet - but effectively moves the cache from memory to the DB transaction\n    db_session.flush()\n\n    token, user_token_session = create_jwt_for_user(\n        external_user.user, db_session, email=external_user.email\n    )\n\n    logger.info(\n        \"Generated token for user\",\n        extra={\n            \"user_token_session.token_id\": str(user_token_session.token_id),\n            \"user_token_session.user_id\": str(user_token_session.user_id),\n        },\n    )\n\n    return LoginGovCallbackResponse(token=token, is_user_new=is_user_new)\n\n\ndef _create_login_gov_user(external_user_id: str, db_session: db.Session) -> LinkExternalUser:\n    user = User()\n    db_session.add(user)\n\n    external_user = LinkExternalUser(\n        user=user,\n        external_user_type=ExternalUserType.LOGIN_GOV,\n        external_user_id=external_user_id,\n        # note we set other params in the calling method to also handle updates\n    )\n    db_session.add(external_user)\n\n    return external_user"}
{"path":"frontend/src/components/content/ProcessAndResearchContent.tsx","language":"typescript","type":"code","directory":"frontend/src/components/content","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/content/ProcessAndResearchContent.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/services/users/update_saved_searches.py\nLanguage: py\nType: service\nDirectory: api/src/services/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/services/users/update_saved_searches.py\nSize: 0.87 KB\nLast Modified: 2025-02-14T17:08:26.447Z"}
{"path":"frontend/src/components/opportunity/OpportunityAwardGridRow.tsx","language":"typescript","type":"code","directory":"frontend/src/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/opportunity/OpportunityAwardGridRow.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"from pydantic import BaseModel\nfrom sqlalchemy import select\n\nfrom src.adapters import db\nfrom src.api.route_utils import raise_flask_error\nfrom src.db.models.user_models import UserSavedSearch\n\n\nclass UpdateSavedSearchInput(BaseModel):\n    name: str\n\n\ndef update_saved_search(\n    db_session: db.Session, user_id: UUID, saved_search_id: UUID, json_data: dict\n) -> UserSavedSearch:\n    \"\"\"Update saved search for a user\"\"\"\n    update_input = UpdateSavedSearchInput.model_validate(json_data)\n\n    saved_search = db_session.execute(\n        select(UserSavedSearch).where(\n            UserSavedSearch.saved_search_id == saved_search_id, UserSavedSearch.user_id == user_id\n        )\n    ).scalar_one_or_none()\n\n    if not saved_search:\n        raise_flask_error(404, \"Saved search not found\")\n\n    # Update\n    saved_search.name = update_input.name\n\n    return saved_search"}
{"path":"frontend/src/components/opportunity/OpportunityAwardInfo.tsx","language":"typescript","type":"code","directory":"frontend/src/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/opportunity/OpportunityAwardInfo.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.132Z","content":"File: api/src/static/swagger-ui-bundle.js\nLanguage: js\nType: code\nDirectory: api/src/static\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/static/swagger-ui-bundle.js\nSize: 1370.16 KB\nLast Modified: 2025-02-14T17:08:26.453Z"}
{"path":"frontend/src/components/opportunity/OpportunityCTA.tsx","language":"typescript","type":"code","directory":"frontend/src/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/opportunity/OpportunityCTA.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":""}
{"path":"frontend/src/components/opportunity/OpportunityDescription.tsx","language":"typescript","type":"code","directory":"frontend/src/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/opportunity/OpportunityDescription.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/static/swagger-ui-standalone-preset.js\nLanguage: js\nType: code\nDirectory: api/src/static\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/static/swagger-ui-standalone-preset.js\nSize: 225.37 KB\nLast Modified: 2025-02-14T17:08:26.455Z"}
{"path":"frontend/src/components/opportunity/OpportunityDocuments.tsx","language":"typescript","type":"code","directory":"frontend/src/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/opportunity/OpportunityDocuments.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":""}
{"path":"frontend/src/components/opportunity/OpportunityDownload.tsx","language":"typescript","type":"code","directory":"frontend/src/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/opportunity/OpportunityDownload.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/task/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/task\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/__init__.py\nSize: 0.50 KB\nLast Modified: 2025-02-14T17:08:26.455Z"}
{"path":"frontend/src/components/opportunity/OpportunityHistory.tsx","language":"typescript","type":"code","directory":"frontend/src/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/opportunity/OpportunityHistory.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"# import any of the other files so they get initialized and attached to the blueprint\nimport src.task.opportunities.set_current_opportunities_task  # noqa: F401 E402 isort:skip\nimport src.task.notifications.generate_notifications  # noqa: F401 E402 isort:skip\nimport src.task.opportunities.export_opportunity_data_task  # noqa: F401 E402 isort:skip\nimport src.task.analytics.create_analytics_db_csvs  # noqa: F401 E402 isort:skip\n\n__all__ = [\"task_blueprint\"]"}
{"path":"frontend/src/components/opportunity/OpportunityIntro.tsx","language":"typescript","type":"code","directory":"frontend/src/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/opportunity/OpportunityIntro.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/task/analytics/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/task/analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/analytics/__init__.py\nSize: 0.06 KB\nLast Modified: 2025-02-14T17:08:26.455Z"}
{"path":"frontend/src/components/opportunity/OpportunityLink.tsx","language":"typescript","type":"code","directory":"frontend/src/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/opportunity/OpportunityLink.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":""}
{"path":"frontend/src/components/opportunity/OpportunityStatusWidget.tsx","language":"typescript","type":"code","directory":"frontend/src/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/opportunity/OpportunityStatusWidget.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/task/analytics/create_analytics_db_csvs.py\nLanguage: py\nType: code\nDirectory: api/src/task/analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/analytics/create_analytics_db_csvs.py\nSize: 3.77 KB\nLast Modified: 2025-02-14T17:08:26.455Z"}
{"path":"frontend/src/components/search/ExportSearchResultsButton.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/ExportSearchResultsButton.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"import click\nimport sqlalchemy\nfrom pydantic import Field\n\nimport src.adapters.db as db\nimport src.adapters.db.flask_db as flask_db\nfrom src.db.models import metadata as api_metadata\nfrom src.task.ecs_background_task import ecs_background_task\nfrom src.task.task import Task\nfrom src.task.task_blueprint import task_blueprint\nfrom src.util import file_util\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\nTABLES_TO_EXTRACT = [\n    \"opportunity\",\n    \"opportunity_summary\",\n    \"current_opportunity_summary\",\n    \"lk_opportunity_category\",\n    \"lk_opportunity_status\",\n]\n\n\n@task_blueprint.cli.command(\n    \"create-analytics-db-csvs\",\n    help=\"Create extract CSVs of our database tables that analytics can use\",\n)\n@click.option(\"--tables-to-extract\", \"-t\", help=\"Tables to extract to a CSV file\", multiple=True)\n@flask_db.with_db_session()\n@ecs_background_task(task_name=\"create-analytics-db-csvs\")\ndef create_analytics_db_csvs(db_session: db.Session, tables_to_extract: list[str]) -> None:\n    logger.info(\"Create extract CSV file start\")\n\n    CreateAnalyticsDbCsvsTask(db_session, tables_to_extract).run()\n\n    logger.info(\"Create extract CSV file complete\")\n\n\nclass CreateAnalyticsDbCsvsConfig(PydanticBaseEnvConfig):\n    # API_ANALYTICS_DB_EXTRACTS_PATH\n    file_path: str = Field(alias=\"API_ANALYTICS_DB_EXTRACTS_PATH\")\n\n    # Override the schema for where the tables exist, only needed\n    # for testing right now\n    db_schema: str | None = Field(None, alias=\"API_ANALYTICS_DB_SCHEMA\")\n\n\nclass CreateAnalyticsDbCsvsTask(Task):\n\n    class Metrics(StrEnum):\n        TABLE_COUNT = \"table_count\"\n        ROW_COUNT = \"row_count\"\n\n    def __init__(\n        self,\n        db_session: db.Session,\n        tables_to_extract: list[str] | None = None,\n        config: CreateAnalyticsDbCsvsConfig | None = None,\n    ) -> None:\n        super().__init__(db_session)\n\n        if tables_to_extract is None or len(tables_to_extract) == 0:\n            tables_to_extract = TABLES_TO_EXTRACT\n\n        # We only want to process tables that were configured\n        self.tables: list[sqlalchemy.Table] = [\n            t for t in api_metadata.tables.values() if t.name in tables_to_extract\n        ]\n\n        if config is None:\n            config = CreateAnalyticsDbCsvsConfig()\n        self.config = config\n\n    def run_task(self) -> None:\n        for table in self.tables:\n            self.generate_csv(table)\n\n    def generate_csv(self, table: sqlalchemy.Table) -> None:\n        \"\"\"Generate the CSV file of a given table\"\"\"\n        output_path = file_util.join(self.config.file_path, f\"{table.name}.csv\")\n        log_extra = {\n            \"table_name\": table.name,\n            \"output_path\": output_path,\n        }\n        logger.info(\"Generating CSV extract for table\", extra=log_extra)\n\n        start_time = time.monotonic()\n\n        cursor = self.db_session.connection().connection.cursor()\n        schema = table.schema if self.config.db_schema is None else self.config.db_schema\n\n        with cursor.copy(\n            f\"COPY {schema}.{table.name} TO STDOUT with (DELIMITER ',', FORMAT CSV, HEADER TRUE, FORCE_QUOTE *, encoding 'utf-8')\"\n        ) as cursor_copy:\n            with file_util.open_stream(output_path, \"wb\") as outfile:\n                for data in cursor_copy:\n                    outfile.write(data)\n\n            row_count = cursor.rowcount\n\n        duration = round(time.monotonic() - start_time, 3)\n        self.increment(self.Metrics.TABLE_COUNT)\n        self.set_metrics({f\"{table.name}.time\": duration})\n        self.increment(self.Metrics.ROW_COUNT, row_count, prefix=table.name)\n\n        logger.info(\n            \"Generated CSV extract for table\",\n            extra=log_extra | {\"table_extract_duration_sec\": duration, \"row_count\": row_count},\n        )"}
{"path":"frontend/src/components/search/SearchAnalytics.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchAnalytics.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/task/ecs_background_task.py\nLanguage: py\nType: code\nDirectory: api/src/task\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/ecs_background_task.py\nSize: 5.09 KB\nLast Modified: 2025-02-14T17:08:26.455Z"}
{"path":"frontend/src/components/search/SearchBar.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchBar.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"import newrelic.agent\nimport requests\n\nfrom src.logging.flask_logger import add_extra_data_to_global_logs\n\nlogger = logging.getLogger(__name__)\n\nP = ParamSpec(\"P\")\nT = TypeVar(\"T\")\n\n\ndef ecs_background_task(task_name: str) -> Callable[[Callable[P, T]], Callable[P, T]]:\n    \"\"\"\n    Decorator for any ECS Task entrypoint function.\n\n    This encapsulates the setup required by all ECS tasks, making it easy to:\n    - add new shared initialization steps for logging\n    - write new ECS task code without thinking about the boilerplate\n\n    Usage:\n\n        TASK_NAME = \"my-cool-task\"\n\n        @task_blueprint.cli.command(TASK_NAME, help=\"For running my cool task\")\n        @ecs_background_task(TASK_NAME)\n        @flask_db.with_db_session()\n        def entrypoint(db_session: db.Session):\n            do_cool_stuff()\n\n    Parameters:\n      task_name (str): Name of the ECS task\n\n    IMPORTANT: Do not specify this decorator before the task command.\n               Click effectively rewrites your function to be a main function\n               and any decorators from before the \"task_blueprint.cli.command(...)\"\n               line are discarded.\n               See: https://click.palletsprojects.com/en/8.1.x/quickstart/#basic-concepts-creating-a-command\n    \"\"\"\n\n    def decorator(f: Callable[P, T]) -> Callable[P, T]:\n        @wraps(f)\n        def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:\n            # Wrap with New Relic instrumentation\n            application = newrelic.agent.register_application(timeout=10.0)\n            with newrelic.agent.BackgroundTask(application, name=task_name, group=\"Python/ECSTask\"):\n                # Wrap with our own logging (timing/general logs)\n                with _ecs_background_task_impl(task_name):\n                    # Finally actually run the task\n                    return f(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\n@contextlib.contextmanager\ndef _ecs_background_task_impl(task_name: str) -> Generator[None, None, None]:\n    # The actual implementation, see the docs on the\n    # decorator method above for details on usage\n    start = time.perf_counter()\n    _add_log_metadata(task_name)\n\n    logger.info(\"Starting ECS task %s\", task_name)\n\n    try:\n        yield\n    except Exception:\n        # We want to make certain that any exception will always\n        # be logged as an error\n        # logger.exception is just an alias for logger.error(<msg>, exc_info=True)\n        logger.exception(\"ECS task failed\", extra={\"status\": \"error\"})\n        raise\n\n    end = time.perf_counter()\n    duration = round((end - start), 3)\n    logger.info(\n        \"Completed ECS task %s\",\n        task_name,\n        extra={\"ecs_task_duration_sec\": duration, \"status\": \"success\"},\n    )\n\n\ndef _add_log_metadata(task_name: str) -> None:\n    # Note we set an \"aws.ecs.task_name\" as well pulled from ECS\n    # which may be different as that value is set based on our infra setup\n    # while this one is just based on whatever we passed the @ecs_background_task decorator\n    add_extra_data_to_global_logs({\"task_name\": task_name, \"task_uuid\": str(uuid.uuid4())})\n    add_extra_data_to_global_logs(_get_ecs_metadata())\n\n\ndef _get_ecs_metadata() -> dict:\n    \"\"\"\n    Retrieves ECS metadata from an AWS-provided metadata URI. This URI is injected to all ECS tasks by AWS as an envar.\n    See https://docs.aws.amazon.com/AmazonECS/latest/userguide/task-metadata-endpoint-v4-fargate.html for more.\n    \"\"\"\n    ecs_metadata_uri = os.environ.get(\"ECS_CONTAINER_METADATA_URI_V4\")\n\n    if os.environ.get(\"ENVIRONMENT\", \"local\") == \"local\" or ecs_metadata_uri is None:\n        logger.info(\n            \"ECS metadata not available for local environments. Run this task on ECS to see metadata.\"\n        )\n        return {}\n\n    task_metadata = requests.get(ecs_metadata_uri, timeout=1)  # 1sec timeout\n    logger.info(\"Retrieved task metadata from ECS\")\n    metadata_json = task_metadata.json()\n\n    ecs_task_name = metadata_json[\"Name\"]\n    ecs_task_id = metadata_json[\"Labels\"][\"com.amazonaws.ecs.task-arn\"].split(\"/\")[-1]\n    ecs_taskdef = \":\".join(\n        [\n            metadata_json[\"Labels\"][\"com.amazonaws.ecs.task-definition-family\"],\n            metadata_json[\"Labels\"][\"com.amazonaws.ecs.task-definition-version\"],\n        ]\n    )\n    cloudwatch_log_group = metadata_json[\"LogOptions\"][\"awslogs-group\"]\n    cloudwatch_log_stream = metadata_json[\"LogOptions\"][\"awslogs-stream\"]\n\n    # Step function only\n    sfn_execution_id = os.environ.get(\"SFN_EXECUTION_ID\")\n    sfn_id = sfn_execution_id.split(\":\")[-2] if sfn_execution_id is not None else None\n\n    return {\n        \"aws.ecs.task_name\": ecs_task_name,\n        \"aws.ecs.task_id\": ecs_task_id,\n        \"aws.ecs.task_definition\": ecs_taskdef,\n        # these will be added automatically by New Relic log ingester, but\n        # just to be sure and for non-log usages, explicitly declare them\n        \"aws.cloudwatch.log_group\": cloudwatch_log_group,\n        \"aws.cloudwatch.log_stream\": cloudwatch_log_stream,\n        \"aws.step_function.id\": sfn_id,\n    }"}
{"path":"frontend/src/components/search/SearchCallToAction.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchCallToAction.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/task/notifications/generate_notifications.py\nLanguage: py\nType: code\nDirectory: api/src/task/notifications\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/notifications/generate_notifications.py\nSize: 10.73 KB\nLast Modified: 2025-02-14T17:08:26.455Z"}
{"path":"frontend/src/components/search/SearchFilterAccordion/SearchFilterAccordion.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search/SearchFilterAccordion","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchFilterAccordion/SearchFilterAccordion.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"import botocore.client\nfrom pydantic import Field\nfrom sqlalchemy import select, update\n\nimport src.adapters.db as db\nimport src.adapters.db.flask_db as flask_db\nimport src.adapters.search as search\nimport src.adapters.search.flask_opensearch as flask_opensearch\nfrom src.adapters.aws.pinpoint_adapter import send_pinpoint_email_raw\nfrom src.db.models.opportunity_models import OpportunityChangeAudit\nfrom src.db.models.user_models import (\n    User,\n    UserNotificationLog,\n    UserSavedOpportunity,\n    UserSavedSearch,\n)\nfrom src.services.opportunities_v1.search_opportunities import search_opportunities_id\nfrom src.task.ecs_background_task import ecs_background_task\nfrom src.task.task import Task\nfrom src.task.task_blueprint import task_blueprint\nfrom src.util import datetime_util\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass GenerateNotificationsConfig(PydanticBaseEnvConfig):\n    app_id: str = Field(alias=\"PINPOINT_APP_ID\")\n\n\n@task_blueprint.cli.command(\n    \"generate-notifications\", help=\"Send notifications for opportunity and search changes\"\n)\n@ecs_background_task(\"generate-notifications\")\n@flask_opensearch.with_search_client()\n@flask_db.with_db_session()\ndef run_notification_task(db_session: db.Session, search_client: search.SearchClient) -> None:\n    \"\"\"Run the daily notification task\"\"\"\n    task = NotificationTask(db_session, search_client)\n    task.run()\n\n\nclass NotificationConstants:\n    OPPORTUNITY_UPDATES = \"opportunity_updates\"\n    SEARCH_UPDATES = \"search_updates\"\n\n\n@dataclass\nclass NotificationContainer:\n    \"\"\"Container for collecting notifications for a single user\"\"\"\n\n    saved_opportunities: list[UserSavedOpportunity] = field(default_factory=list)\n    saved_searches: list[UserSavedSearch] = field(default_factory=list)\n\n\nclass NotificationTask(Task):\n    \"\"\"Task that runs daily to collect and send notifications to users about changes\"\"\"\n\n    # TODO: Confirm with team if we want to use these metrics\n    class Metrics(StrEnum):\n        USERS_NOTIFIED = \"users_notified\"\n        OPPORTUNITIES_TRACKED = \"opportunities_tracked\"\n        SEARCHES_TRACKED = \"searches_tracked\"\n        NOTIFICATIONS_SENT = \"notifications_sent\"\n\n    def __init__(\n        self,\n        db_session: db.Session,\n        search_client: search.SearchClient,\n        pinpoint_client: botocore.client.BaseClient | None = None,\n        pinpoint_app_id: str | None = None,\n    ) -> None:\n        super().__init__(db_session)\n        self.config = GenerateNotificationsConfig()\n\n        self.user_notification_map: dict[uuid.UUID, NotificationContainer] = {}\n        self.search_client = search_client\n        self.pinpoint_client = pinpoint_client\n        self.app_id = pinpoint_app_id\n\n    def run_task(self) -> None:\n        \"\"\"Main task logic to collect and send notifications\"\"\"\n        self._collect_opportunity_notifications()\n        self._collect_search_notifications()\n        self._send_notifications()\n\n    def _collect_opportunity_notifications(self) -> None:\n        \"\"\"Collect notifications for changed opportunities that users are tracking\"\"\"\n        stmt = (\n            select(UserSavedOpportunity)\n            .join(\n                OpportunityChangeAudit,\n                OpportunityChangeAudit.opportunity_id == UserSavedOpportunity.opportunity_id,\n            )\n            .where(OpportunityChangeAudit.updated_at > UserSavedOpportunity.last_notified_at)\n            .distinct()\n        )\n\n        results = self.db_session.execute(stmt)\n\n        for row in results.scalars():\n            user_id = row.user_id\n            if user_id not in self.user_notification_map:\n                self.user_notification_map[user_id] = NotificationContainer()\n            self.user_notification_map[user_id].saved_opportunities.append(row)\n\n        logger.info(\n            \"Collected opportunity notifications\",\n            extra={\n                \"user_count\": len(self.user_notification_map),\n                \"total_notifications\": sum(\n                    len(container.saved_opportunities)\n                    for container in self.user_notification_map.values()\n                ),\n            },\n        )\n\n    def _collect_search_notifications(self) -> None:\n        \"\"\"Collect notifications for changed saved searches\"\"\"\n        # Get all saved searches that haven't been checked since last notification\n        stmt = select(UserSavedSearch).where(\n            UserSavedSearch.last_notified_at < datetime_util.utcnow()\n        )\n        saved_searches = self.db_session.execute(stmt).scalars()\n\n        # Group searches by query to minimize search index calls\n        query_map: dict[str, list[UserSavedSearch]] = {}\n        for saved_search in saved_searches:\n            # Remove pagination parameters before using as key\n            search_query = _strip_pagination_params(saved_search.search_query)\n            query_key = str(search_query)\n\n            if query_key not in query_map:\n                query_map[query_key] = []\n            query_map[query_key].append(saved_search)\n\n        # For each unique query, check if results have changed\n        for searches in query_map.values():\n            current_results: list[int] = search_opportunities_id(\n                self.search_client, searches[0].search_query\n            )\n\n            for saved_search in searches:\n                previous_results = set(saved_search.searched_opportunity_ids)\n                if set(current_results) != previous_results:\n                    user_id = saved_search.user_id\n                    if user_id not in self.user_notification_map:\n                        self.user_notification_map[user_id] = NotificationContainer()\n                    self.user_notification_map[user_id].saved_searches.append(saved_search)\n\n                    # Update the saved search with new results\n                    saved_search.searched_opportunity_ids = list(current_results)\n\n        logger.info(\n            \"Collected search notifications\",\n            extra={\n                \"user_count\": len(self.user_notification_map),\n                \"total_searches\": sum(\n                    len(container.saved_searches)\n                    for container in self.user_notification_map.values()\n                ),\n            },\n        )\n\n    def _send_notifications(self) -> None:\n        \"\"\"Send collected notifications to users\"\"\"\n        for user_id, container in self.user_notification_map.items():\n            if not container.saved_opportunities and not container.saved_searches:\n                continue\n\n            user = self.db_session.execute(\n                select(User).where(User.user_id == user_id)\n            ).scalar_one_or_none()\n\n            if not user or not user.email:\n                logger.warning(\"No email found for user\", extra={\"user_id\": user_id})\n                continue\n\n            # Send email via Pinpoint\n            subject = \"Updates to Your Saved Opportunities\"\n            message = (\n                f\"You have updates to {len(container.saved_opportunities)} saved opportunities\"\n            )\n\n            logger.info(\n                \"Sending notification to user\",\n                extra={\n                    \"user_id\": user_id,\n                    \"opportunity_count\": len(container.saved_opportunities),\n                    \"search_count\": len(container.saved_searches),\n                },\n            )\n\n            notification_log = UserNotificationLog(\n                user_id=user_id,\n                notification_reason=NotificationConstants.OPPORTUNITY_UPDATES,\n                notification_sent=False,  # Default to False, update on success\n            )\n            self.db_session.add(notification_log)\n\n            try:\n                send_pinpoint_email_raw(\n                    to_address=user.email,\n                    subject=subject,\n                    message=message,\n                    pinpoint_client=self.pinpoint_client,\n                    app_id=self.config.app_id,\n                )\n                notification_log.notification_sent = True\n                logger.info(\n                    \"Successfully sent notification to user\",\n                    extra={\n                        \"user_id\": user_id,\n                        \"opportunity_count\": len(container.saved_opportunities),\n                        \"search_count\": len(container.saved_searches),\n                    },\n                )\n            except Exception:\n                # Notification log will be updated in the finally block\n                logger.exception(\n                    \"Failed to send notification email\",\n                    extra={\"user_id\": user_id, \"email\": user.email},\n                )\n\n            self.db_session.add(notification_log)\n\n            if container.saved_searches:\n                search_notification_log = UserNotificationLog(\n                    user_id=user_id,\n                    notification_reason=NotificationConstants.SEARCH_UPDATES,\n                    notification_sent=False,  # Default to False, update if email was successful\n                )\n                self.db_session.add(search_notification_log)\n                if notification_log.notification_sent:\n                    search_notification_log.notification_sent = True\n\n            # Update last_notified_at for all opportunities we just notified about\n            opportunity_ids = [\n                saved_opp.opportunity_id for saved_opp in container.saved_opportunities\n            ]\n            self.db_session.execute(\n                update(UserSavedOpportunity)\n                .where(\n                    UserSavedOpportunity.user_id == user_id,\n                    UserSavedOpportunity.opportunity_id.in_(opportunity_ids),\n                )\n                .values(last_notified_at=datetime_util.utcnow())\n            )\n\n            # Update last_notified_at for all searches we just notified about\n            if container.saved_searches:\n                search_ids = [\n                    saved_search.saved_search_id for saved_search in container.saved_searches\n                ]\n                self.db_session.execute(\n                    update(UserSavedSearch)\n                    .where(UserSavedSearch.saved_search_id.in_(search_ids))\n                    .values(last_notified_at=datetime_util.utcnow())\n                )\n\n            self.increment(self.Metrics.OPPORTUNITIES_TRACKED, len(container.saved_opportunities))\n            self.increment(self.Metrics.SEARCHES_TRACKED, len(container.saved_searches))\n            self.increment(self.Metrics.NOTIFICATIONS_SENT)\n            self.increment(self.Metrics.USERS_NOTIFIED)\n\n\ndef _strip_pagination_params(search_query: dict) -> dict:\n    \"\"\"Remove pagination parameters from a search query\"\"\"\n    search_query = search_query.copy()\n    search_query.pop(\"pagination\", None)\n    return search_query"}
{"path":"frontend/src/components/search/SearchFilterAccordion/SearchFilterCheckbox.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search/SearchFilterAccordion","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchFilterAccordion/SearchFilterCheckbox.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/task/opportunities/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/task/opportunities\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/opportunities/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.455Z"}
{"path":"frontend/src/components/search/SearchFilterAccordion/SearchFilterOptions.ts","language":"typescript","type":"code","directory":"frontend/src/components/search/SearchFilterAccordion","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchFilterAccordion/SearchFilterOptions.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":""}
{"path":"frontend/src/components/search/SearchFilterAccordion/SearchFilterSection/SearchFilterSection.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search/SearchFilterAccordion/SearchFilterSection","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchFilterAccordion/SearchFilterSection/SearchFilterSection.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/task/opportunities/export_opportunity_data_task.py\nLanguage: py\nType: code\nDirectory: api/src/task/opportunities\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/opportunities/export_opportunity_data_task.py\nSize: 5.39 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/components/search/SearchFilterAccordion/SearchFilterSection/SectionLinkCount.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search/SearchFilterAccordion/SearchFilterSection","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchFilterAccordion/SearchFilterSection/SectionLinkCount.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"from pydantic import Field\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import noload, selectinload\n\nimport src.adapters.db as db\nimport src.adapters.db.flask_db as flask_db\nimport src.util.file_util as file_util\nfrom src.api.opportunities_v1.opportunity_schemas import OpportunityV1Schema\nfrom src.constants.lookup_constants import ExtractType\nfrom src.db.models.extract_models import ExtractMetadata\nfrom src.db.models.opportunity_models import CurrentOpportunitySummary, Opportunity\nfrom src.services.opportunities_v1.opportunity_to_csv import opportunities_to_csv\nfrom src.task.ecs_background_task import ecs_background_task\nfrom src.task.task import Task\nfrom src.task.task_blueprint import task_blueprint\nfrom src.util.datetime_util import get_now_us_eastern_datetime\nfrom src.util.env_config import PydanticBaseEnvConfig\n\nlogger = logging.getLogger(__name__)\n\n\n@task_blueprint.cli.command(\n    \"export-opportunity-data\",\n    help=\"Generate JSON and CSV files containing an export of all opportunity data\",\n)\n@flask_db.with_db_session()\n@ecs_background_task(task_name=\"export-opportunity-data\")\ndef export_opportunity_data(db_session: db.Session) -> None:\n    ExportOpportunityDataTask(db_session).run()\n\n\nclass ExportOpportunityDataConfig(PydanticBaseEnvConfig):\n    file_path: str = Field(..., alias=\"PUBLIC_FILES_OPPORTUNITY_DATA_EXTRACTS_PATH\")\n\n\nclass ExportOpportunityDataTask(Task):\n    class Metrics(StrEnum):\n        RECORDS_EXPORTED = \"records_exported\"\n\n    def __init__(\n        self,\n        db_session: db.Session,\n        config: ExportOpportunityDataConfig | None = None,\n    ) -> None:\n        super().__init__(db_session)\n\n        if config is None:\n            config = ExportOpportunityDataConfig()\n        self.config = config\n\n        self.current_timestamp = get_now_us_eastern_datetime().strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n        self.json_file = os.path.join(\n            config.file_path, f\"opportunity_data-{self.current_timestamp}.json\"\n        )\n        self.csv_file = os.path.join(\n            config.file_path, f\"opportunity_data-{self.current_timestamp}.csv\"\n        )\n\n        self.set_metrics({\"csv_file\": self.csv_file, \"json_file\": self.json_file})\n\n    def run_task(self) -> None:\n        # Load records\n        schema = OpportunityV1Schema()\n\n        opportunities = []\n        for opp_batch in self.fetch_opportunities():\n            for record in opp_batch:\n                self.increment(self.Metrics.RECORDS_EXPORTED)\n                opportunities.append(schema.dump(record))\n\n        # Format data\n        data_to_export: dict = {\n            \"metadata\": {\"file_generated_at\": self.current_timestamp},\n            \"opportunities\": opportunities,\n        }\n\n        # Export data\n        self.export_data_to_json(data_to_export)\n        self.export_opportunities_to_csv(opportunities)\n\n        # Export data and create metadata entries\n        json_size = self.export_data_to_json(data_to_export)\n        csv_size = self.export_opportunities_to_csv(opportunities)\n\n        # Create metadata entries\n        json_metadata = ExtractMetadata(\n            extract_type=ExtractType.OPPORTUNITIES_JSON,\n            file_name=f\"opportunity_data-{self.current_timestamp}.json\",\n            file_path=self.json_file,\n            file_size_bytes=json_size,\n        )\n\n        csv_metadata = ExtractMetadata(\n            extract_type=ExtractType.OPPORTUNITIES_CSV,\n            file_name=f\"opportunity_data-{self.current_timestamp}.csv\",\n            file_path=self.csv_file,\n            file_size_bytes=csv_size,\n        )\n\n        self.db_session.add(json_metadata)\n        self.db_session.add(csv_metadata)\n        self.db_session.commit()\n\n    def fetch_opportunities(self) -> Iterator[Sequence[Opportunity]]:\n        \"\"\"\n        Fetch the opportunities in batches. The iterator returned\n        will give you each individual batch to be processed.\n\n        Fetches all opportunities where:\n            * is_draft = False\n            * current_opportunity_summary is not None\n        \"\"\"\n        return (\n            self.db_session.execute(\n                select(Opportunity)\n                .join(CurrentOpportunitySummary)\n                .where(\n                    Opportunity.is_draft.is_(False),\n                    CurrentOpportunitySummary.opportunity_status.isnot(None),\n                )\n                .options(selectinload(\"*\"), noload(Opportunity.all_opportunity_summaries))\n                .execution_options(yield_per=5000)\n            )\n            .scalars()\n            .partitions()\n        )\n\n    def export_data_to_json(self, data_to_export: dict) -> int:\n        # create the json file\n        logger.info(\n            \"Creating Opportunity JSON extract\", extra={\"json_extract_path\": self.json_file}\n        )\n        json_object = json.dumps(data_to_export, indent=4)\n        with file_util.open_stream(self.json_file, \"w\") as outfile:\n            outfile.write(json_object)\n\n        return len(json_object.encode(\"utf-8\"))\n\n    def export_opportunities_to_csv(self, opportunities: Sequence[dict]) -> int:\n        # create the csv file\n        logger.info(\"Creating Opportunity CSV extract\", extra={\"csv_extract_path\": self.csv_file})\n\n        with file_util.open_stream(self.csv_file, \"w\") as outfile:\n            opportunities_to_csv(opportunities, outfile)\n\n        csv_size = file_util.get_file_length_bytes(self.csv_file)\n        return csv_size"}
{"path":"frontend/src/components/search/SearchFilterAccordion/SearchFilterSection/SectionLinkLabel.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search/SearchFilterAccordion/SearchFilterSection","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchFilterAccordion/SearchFilterSection/SectionLinkLabel.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/task/opportunities/set_current_opportunities_task.py\nLanguage: py\nType: code\nDirectory: api/src/task/opportunities\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/opportunities/set_current_opportunities_task.py\nSize: 11.19 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/components/search/SearchFilterAccordion/SearchFilterToggleAll.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search/SearchFilterAccordion","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchFilterAccordion/SearchFilterToggleAll.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"from sqlalchemy import select\nfrom sqlalchemy.orm import selectinload\n\nimport src.adapters.db as db\nimport src.adapters.db.flask_db as flask_db\nfrom src.constants.lookup_constants import OpportunityStatus\nfrom src.db.models.opportunity_models import (\n    CurrentOpportunitySummary,\n    Opportunity,\n    OpportunitySummary,\n)\nfrom src.task.task import Task\nfrom src.task.task_blueprint import task_blueprint\nfrom src.util.datetime_util import get_now_us_eastern_date\n\nlogger = logging.getLogger(__name__)\n\n\n@task_blueprint.cli.command(\n    \"set-current-opportunities\",\n    help=\"For each opportunity in the database set/update the current opportunity record\",\n)\n@flask_db.with_db_session()\ndef set_current_opportunities(db_session: db.Session) -> None:\n    SetCurrentOpportunitiesTask(db_session).run()\n\n\nclass SetCurrentOpportunitiesTask(Task):\n    def __init__(self, db_session: db.Session, current_date: date | None = None) -> None:\n        super().__init__(db_session)\n        if current_date is None:\n            current_date = get_now_us_eastern_date()\n        self.current_date = current_date\n\n    class Metrics(StrEnum):\n        OPPORTUNITY_COUNT = \"opportunity_count\"\n\n        UNMODIFIED_OPPORTUNITY_COUNT = \"unmodified_opportunity_count\"\n        MODIFIED_OPPORTUNITY_COUNT = \"modified_opportunity_count\"\n\n        DELETED_CURRENT_OPPORTUNITY_COUNT = \"deleted_current_opportunity_count\"\n        NEW_CURRENT_OPPORTUNITY_COUNT = \"new_current_opportunity_count\"\n        UPDATED_CURRENT_OPPORTUNITY_COUNT = \"updated_current_opportunity_count\"\n\n        NONE_OPPORTUNITY_STATUS_COUNT = \"none_opportunity_status_count\"\n        POSTED_OPPORTUNITY_STATUS_COUNT = \"posted_opportunity_status_count\"\n        FORECASTED_OPPORTUNITY_STATUS_COUNT = \"forecasted_opportunity_status_count\"\n        CLOSED_OPPORTUNITY_STATUS_COUNT = \"closed_opportunity_status_count\"\n        ARCHIVED_OPPORTUNITY_STATUS_COUNT = \"archived_opportunity_status_count\"\n\n    def run_task(self) -> None:\n        with self.db_session.begin():\n            self._process_opportunities()\n\n    def _process_opportunities(self) -> None:\n        # This selectinload significantly imrproves performance as it tells SQLAlchemy\n        # to fetch all summaries+current opportunity summaries rather than lazy loading\n        # them as needed.\n        opportunities = self.db_session.scalars(\n            select(Opportunity)\n            .options(\n                selectinload(Opportunity.all_opportunity_summaries),\n                selectinload(Opportunity.current_opportunity_summary),\n                # yield_per makes it so the query loads records 5000 at a time into memory\n                # rather than everything all at once.\n                # https://docs.sqlalchemy.org/en/20/orm/queryguide/api.html#fetching-large-result-sets-with-yield-per\n            )\n            .execution_options(yield_per=5000)\n        )\n\n        for opportunity in opportunities:\n            self._process_opportunity(opportunity)\n\n    def _process_opportunity(self, opportunity: Opportunity) -> None:\n        self.increment(self.Metrics.OPPORTUNITY_COUNT)\n\n        log_extra = {\n            \"opportunity_id\": opportunity.opportunity_id,\n            \"is_draft\": opportunity.is_draft,\n            \"existing_opportunity_status\": opportunity.opportunity_status,\n        }\n        log_extra |= get_log_extra_for_summary(\n            (\n                opportunity.current_opportunity_summary.opportunity_summary\n                if opportunity.current_opportunity_summary\n                else None\n            ),\n            \"existing\",\n        )\n        logger.info(\"Processing opportunity %s\", opportunity.opportunity_id, extra=log_extra)\n\n        # Determine what the current opportunity summary + status should be\n        current_summary, status = self.determine_current_and_status(opportunity)\n\n        # Count the number of opportunity statuses we calculated (None included)\n        self.increment(f\"{str(status).lower()}_opportunity_status_count\")\n\n        # Check whether we actually found any change.\n        # No need to update records in the DB that aren't changing\n        if is_opportunity_changed(opportunity, current_summary, status):\n            self.increment(self.Metrics.MODIFIED_OPPORTUNITY_COUNT)\n            log_extra |= {\"updated_opportunity_status\": status}\n            log_extra |= get_log_extra_for_summary(current_summary, \"updated\")\n\n            logger.info(\n                \"Opportunity %s requires an update\", opportunity.opportunity_id, extra=log_extra\n            )\n        else:\n            self.increment(self.Metrics.UNMODIFIED_OPPORTUNITY_COUNT)\n            logger.info(\n                \"Opportunity %s does not require an update for its current summary or status\",\n                opportunity.opportunity_id,\n                extra=log_extra,\n            )\n            return\n\n        if current_summary is None:\n            # We determined the opportunity should not have a current and need to delete it\n            if opportunity.current_opportunity_summary is not None:\n                self.db_session.delete(opportunity.current_opportunity_summary)\n                self.increment(self.Metrics.DELETED_CURRENT_OPPORTUNITY_COUNT)\n\n            # Whether or not we needed to delete a record, or it was already null, we can\n            # safely return here as there isn't anything else to update\n            return\n\n        # If the current opportunity summary doesn't already exist, create it first\n        if opportunity.current_opportunity_summary is None:\n            opportunity.current_opportunity_summary = CurrentOpportunitySummary(\n                opportunity=opportunity\n            )\n            self.increment(self.Metrics.NEW_CURRENT_OPPORTUNITY_COUNT)\n        else:\n            self.increment(self.Metrics.UPDATED_CURRENT_OPPORTUNITY_COUNT)\n\n        # In either case, update the summary + status\n        opportunity.current_opportunity_summary.opportunity_summary = current_summary\n        opportunity.current_opportunity_summary.opportunity_status = cast(OpportunityStatus, status)\n\n    def determine_current_and_status(\n        self, opportunity: Opportunity\n    ) -> Tuple[OpportunitySummary | None, OpportunityStatus | None]:\n        # Determine latest forecasted and non-forecasted opportunity summaries\n        latest_forecasted_summary: OpportunitySummary | None = None\n        latest_non_forecasted_summary: OpportunitySummary | None = None\n\n        # If the opportunity is a draft, we don't want to create a status\n        if opportunity.is_draft:\n            return None, None\n\n        # Latest is based entirely off of the revision number, the latest\n        # will always have a null revision number, and because of how the\n        # data is structured that we import, we'll only ever have a single\n        # null value for forecast and non-forecast respectively\n        for summary in opportunity.all_opportunity_summaries:\n            if summary.is_forecast and summary.revision_number is None:\n                latest_forecasted_summary = summary\n            elif not summary.is_forecast and summary.revision_number is None:\n                latest_non_forecasted_summary = summary\n\n        # We need to make sure the latest can actually be publicly displayed\n        # Note that if it cannot, we do not want to use an earlier revision\n        # even if that revision doesn't have the same issue. Only the latest\n        # revisions of forecast/non-forecast records are ever an option\n        if (\n            latest_forecasted_summary is not None\n            and not latest_forecasted_summary.can_summary_be_public(self.current_date)\n        ):\n            latest_forecasted_summary = None\n\n        if (\n            latest_non_forecasted_summary is not None\n            and not latest_non_forecasted_summary.can_summary_be_public(self.current_date)\n        ):\n            latest_non_forecasted_summary = None\n\n        if latest_forecasted_summary is None and latest_non_forecasted_summary is None:\n            return None, None\n\n        # A non-forecast always takes precedence over a forecast\n        if latest_non_forecasted_summary is not None:\n            return latest_non_forecasted_summary, self.determine_opportunity_status(\n                latest_non_forecasted_summary\n            )\n\n        # Otherwise we'll use the forecast\n        return latest_forecasted_summary, self.determine_opportunity_status(\n            cast(OpportunitySummary, latest_forecasted_summary)\n        )\n\n    def determine_opportunity_status(\n        self, opportunity_summary: OpportunitySummary\n    ) -> OpportunityStatus:\n        # Any past archive date, it should be archived\n        if (\n            opportunity_summary.archive_date is not None\n            and opportunity_summary.archive_date < self.current_date\n        ):\n            return OpportunityStatus.ARCHIVED\n\n        # Any past close date, should be closed\n        if (\n            opportunity_summary.close_date is not None\n            and opportunity_summary.close_date < self.current_date\n        ):\n            return OpportunityStatus.CLOSED\n\n        # Otherwise the status is based on whether it is a forecast\n        # note that we know it is after the post date as that was checked\n        # before calling this method\n        if opportunity_summary.is_forecast:\n            return OpportunityStatus.FORECASTED\n\n        return OpportunityStatus.POSTED\n\n\ndef is_opportunity_changed(\n    opportunity: Opportunity,\n    current_summary: OpportunitySummary | None,\n    status: OpportunityStatus | None,\n) -> bool:\n    # This is a utility method to help us know whether an opportunity will be changed\n    # during this iteration of the process.\n\n    if opportunity.current_opportunity_summary is None:\n        # There is no current, and we still don't think there should be one\n        if current_summary is None:\n            return False\n\n        # There is no current, but we want to add one\n        return True\n\n    # We plan to remove the current summary\n    if current_summary is None:\n        return True\n\n    # The specific current opportunity summary is changing\n    if (\n        opportunity.current_opportunity_summary.opportunity_summary_id\n        != current_summary.opportunity_summary_id\n    ):\n        return True\n\n    return opportunity.current_opportunity_summary.opportunity_status != status\n\n\ndef get_log_extra_for_summary(summary: OpportunitySummary | None, prefix: str) -> dict[str, Any]:\n    return {\n        f\"{prefix}_opportunity_summary_id\": summary.opportunity_summary_id if summary else None,\n        f\"{prefix}_opportunity_summary_revision_number\": (\n            summary.revision_number if summary else None\n        ),\n        f\"{prefix}_opportunity_summary_is_forecast\": summary.is_forecast if summary else None,\n        f\"{prefix}_opportunity_summary_post_date\": summary.post_date if summary else None,\n        f\"{prefix}_opportunity_summary_close_date\": summary.close_date if summary else None,\n        f\"{prefix}_opportunity_summary_archive_date\": summary.archive_date if summary else None,\n        f\"{prefix}_opportunity_summary_is_deleted\": summary.is_deleted if summary else None,\n        f\"{prefix}_opportunity_summary_created_at\": summary.created_at if summary else None,\n        f\"{prefix}_opportunity_summary_updated_at\": summary.updated_at if summary else None,\n    }"}
{"path":"frontend/src/components/search/SearchFilters.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchFilters.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/task/subtask.py\nLanguage: py\nType: code\nDirectory: api/src/task\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/subtask.py\nSize: 2.05 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/components/search/SearchOpportunityStatus.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchOpportunityStatus.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"import src.adapters.db as db\nfrom src.task.task import Task\n\nlogger = logging.getLogger(__name__)\n\n\nclass SubTask(abc.ABC, metaclass=abc.ABCMeta):\n    \"\"\"\n    A SubTask is a class that defines a set of behavior\n    that can be seen as a subset of a Task.\n\n    This object has access to the same internal metrics\n    and reporting attributes as its Task, but can be defined\n    as a separate class which can help with organizing large\n    complex tasks that can't be easily broken down.\n    \"\"\"\n\n    def __init__(self, task: Task):\n        self.task = task\n\n    def run(self) -> None:\n        try:\n            logger.info(\"Starting subtask %s\", self.cls_name())\n            start = time.perf_counter()\n\n            # Run the actual subtask\n            self.run_subtask()\n\n            # Calculate and set a duration\n            end = time.perf_counter()\n            duration = round((end - start), 3)\n            self.set_metrics({f\"{self.cls_name()}_subtask_duration_sec\": duration})\n\n            logger.info(\"Completed subtask %s in %s seconds\", self.cls_name(), duration)\n\n        except Exception:\n            logger.exception(\"Failed to run subtask %s\", self.cls_name())\n            raise\n\n    def set_metrics(self, metrics: dict[str, Any]) -> None:\n        # Passthrough method to the task set_metrics function\n        self.task.set_metrics(metrics)\n\n    def increment(self, name: str, value: int = 1, prefix: str | None = None) -> None:\n        # Passthrough method to the task increment function\n        self.task.increment(name, value, prefix)\n\n    def cls_name(self) -> str:\n        return self.__class__.__name__\n\n    @abc.abstractmethod\n    def run_subtask(self) -> None:\n        \"\"\"Override to define the subtask logic\"\"\"\n        pass\n\n    @property\n    def db_session(self) -> db.Session:\n        # Property to make it so the subtask can reference the db_session\n        # as if it were the task itself\n        return self.task.db_session\n\n    @property\n    def metrics(self) -> dict[str, Any]:\n        return self.task.metrics"}
{"path":"frontend/src/components/search/SearchPagination.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchPagination.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/task/task.py\nLanguage: py\nType: code\nDirectory: api/src/task\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/task.py\nSize: 3.10 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/components/search/SearchResults.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchResults.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"import src.adapters.db as db\nfrom src.db.models.task_models import JobLog, JobStatus\n\nlogger = logging.getLogger(__name__)\n\n\nclass Task(abc.ABC, metaclass=abc.ABCMeta):\n    \"\"\"\n    Abstract base class representing an arbitrary\n    task that works with the database.\n\n    This approach handles a few basic patterns including:\n    - Simple metric aggregation & logging\n    - Timing metrics\n    - High-level error handling\n    \"\"\"\n\n    class Metrics(StrEnum):\n        # Derived classes will implement their own\n        # Metrics class with metrics\n        pass\n\n    def __init__(self, db_session: db.Session) -> None:\n        self.db_session = db_session\n        self.metrics: dict[str, Any] = {}\n        self.job: JobLog | None = None\n\n    def run(self) -> None:\n        job_succeeded = True\n\n        try:\n            # Create initial job record\n            self.job = JobLog(job_type=self.cls_name(), job_status=JobStatus.STARTED)\n            self.db_session.add(self.job)\n            self.db_session.commit()\n\n            # Initialize the metrics\n            self.initialize_metrics()\n\n            # Run the actual task\n            self.run_task()\n\n            logger.info(\"Starting %s\", self.cls_name())\n            start = time.perf_counter()\n\n            # Calculate and set a duration\n            end = time.perf_counter()\n            duration = round((end - start), 3)\n            self.set_metrics({\"task_duration_sec\": duration})\n\n            logger.info(\"Completed %s in %s seconds\", self.cls_name(), duration, extra=self.metrics)\n        except Exception:\n            job_succeeded = False\n            raise\n        finally:\n            job_status = JobStatus.COMPLETED if job_succeeded else JobStatus.FAILED\n\n            # Rollback if the session is not active due to error above\n            if not self.db_session.is_active:\n                self.db_session.rollback()\n\n            self.update_job(job_status, metrics=self.metrics)\n\n    def initialize_metrics(self) -> None:\n        zero_metrics_dict: dict[str, Any] = {metric: 0 for metric in self.Metrics}\n        self.set_metrics(zero_metrics_dict)\n\n    def set_metrics(self, metrics: dict[str, Any]) -> None:\n        self.metrics.update(**metrics)\n\n    def increment(self, name: str, value: int = 1, prefix: str | None = None) -> None:\n        if name not in self.metrics:\n            self.metrics[name] = 0\n\n        self.metrics[name] += value\n\n        if prefix is not None:\n            # Rather than re-implement the above, just re-use the function without a prefix\n            self.increment(f\"{prefix}.{name}\", value, prefix=None)\n\n    def cls_name(self) -> str:\n        return self.__class__.__name__\n\n    def update_job(self, job_status: JobStatus, metrics: dict[str, Any] | None = None) -> None:\n        if self.job is None:\n            raise ValueError(\"Job is not initialized\")\n\n        self.job.job_status = job_status\n        self.job.metrics = self.metrics\n        self.db_session.commit()\n\n    @abc.abstractmethod\n    def run_task(self) -> None:\n        \"\"\"Override to define the task logic\"\"\"\n        pass"}
{"path":"frontend/src/components/search/SearchResultsHeader.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchResultsHeader.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/task/task_blueprint.py\nLanguage: py\nType: code\nDirectory: api/src/task\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/task/task_blueprint.py\nSize: 0.12 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/components/search/SearchResultsList.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchResultsList.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"task_blueprint = APIBlueprint(\"task\", __name__, enable_openapi=False, cli_group=\"task\")"}
{"path":"frontend/src/components/search/SearchResultsListItem.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchResultsListItem.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/util/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/components/search/SearchSortBy.tsx","language":"typescript","type":"code","directory":"frontend/src/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/search/SearchSortBy.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":""}
{"path":"frontend/src/components/subscribe/SubscriptionForm.tsx","language":"typescript","type":"code","directory":"frontend/src/components/subscribe","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/subscribe/SubscriptionForm.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/util/datetime_util.py\nLanguage: py\nType: code\nDirectory: api/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/datetime_util.py\nSize: 1.91 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/components/subscribe/SubscriptionSubmitButton.tsx","language":"typescript","type":"code","directory":"frontend/src/components/subscribe","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/subscribe/SubscriptionSubmitButton.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"import pytz\n\n\ndef utcnow() -> datetime:\n    \"\"\"Current time in UTC tagged with timezone info marking it as UTC, unlike datetime.utcnow().\n\n    See https://docs.python.org/3/library/datetime.html#datetime.datetime.utcnow\n    \"\"\"\n    return datetime.now(timezone.utc)\n\n\ndef adjust_timezone(timestamp: datetime, timezone_str: str) -> datetime:\n    \"\"\"\n    Utility method for converting a datetime object\n    between different timezones. The string passed in\n    can be anything recognized by the pytz library\n\n    Details on how to find all the potential timezone\n    names can be found in http://pytz.sourceforge.net/#helpers\n    but a few that are likely useful include:\n    * UTC\n    * US/Eastern\n    * US/Central\n    * US/Mountain\n    * US/Pacific\n    \"\"\"\n    new_timezone = pytz.timezone(timezone_str)\n    return timestamp.astimezone(new_timezone)\n\n\ndef make_timezone_aware(timestamp: datetime, timezone_str: str) -> datetime:\n    new_timezone = zoneinfo.ZoneInfo(timezone_str)\n    return timestamp.replace(tzinfo=new_timezone)\n\n\ndef get_now_us_eastern_datetime() -> datetime:\n    \"\"\"\n    Return the current time in the eastern time zone. DST is handled based on the local time.\n    For information on handling Daylight Savings Time, refer to this documentation on now() vs. utcnow():\n    http://pytz.sourceforge.net/#problems-with-localtime\n    \"\"\"\n\n    # Note that this uses Eastern time (not UTC)\n    tz = pytz.timezone(\"America/New_York\")\n    return datetime.now(tz)\n\n\ndef get_now_us_eastern_date() -> date:\n    # We get the datetime and truncate it to the date portion\n    # as there aren't any direct date methods that take in a timezone\n    return get_now_us_eastern_datetime().date()\n\n\ndef datetime_str_to_date(datetime_str: Optional[str]) -> Optional[date]:\n    if not datetime_str:\n        return None\n    return datetime.fromisoformat(datetime_str).date()"}
{"path":"frontend/src/components/user/OpportunitySaveUserControl.tsx","language":"typescript","type":"code","directory":"frontend/src/components/user","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/user/OpportunitySaveUserControl.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/util/deploy_metadata.py\nLanguage: py\nType: code\nDirectory: api/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/deploy_metadata.py\nSize: 2.46 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/components/user/SessionCheck.tsx","language":"typescript","type":"code","directory":"frontend/src/components/user","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/user/SessionCheck.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"from pydantic_settings import SettingsConfigDict\n\nimport src.util.datetime_util as datetime_util\nfrom src.util.env_config import PydanticBaseEnvConfig\n\n# We expect release notes to be formatted as:\n# YYYY-MM-DD-#\n# However we don't always put leading zeroes, so all of the following\n# would be valid release versions:\n# 2024.11.27-1\n# 2024.11.5-1\n# 2024.4.30-1\nRELEASE_NOTE_REGEX = re.compile(\n    r\"\"\"\n    ^[0-9]{4}         # Exactly 4 leading digits\n    (?:\\.[0-9]{1,2})  # Period followed by 1-2 digits\n    (?:\\.[0-9]{1,2})  # Period followed by 1-2 digits\n    (?:\\-[0-9]{1,2})$ # Ends with a dash and 1-2 digits\n    \"\"\",\n    re.ASCII | re.VERBOSE,\n)\n\n\nclass DeployMetadataConfig(PydanticBaseEnvConfig):\n    model_config = SettingsConfigDict(extra=\"allow\")\n\n    # We don't want these values being None to break\n    # any of our system, so allow them to be None\n    deploy_github_ref: str | None = None  # DEPLOY_GITHUB_REF\n    deploy_github_sha: str | None = None  # DEPLOY_GITHUB_SHA\n    deploy_timestamp: datetime | None = None  # DEPLOY_TIMESTAMP\n    deploy_whoami: str | None = None  # DEPLOY_WHOAMI\n\n    def model_post_init(self, _context: typing.Any) -> None:\n        \"\"\"Run after __init__ sets above values from env vars\"\"\"\n\n        if self.deploy_github_ref and RELEASE_NOTE_REGEX.match(self.deploy_github_ref):\n            self.release_notes = (\n                f\"https://github.com/HHS/simpler-grants-gov/releases/tag/{self.deploy_github_ref}\"\n            )\n        else:\n            self.release_notes = \"https://github.com/HHS/simpler-grants-gov/releases\"\n\n        if self.deploy_github_sha:\n            self.deploy_commit = (\n                f\"https://github.com/HHS/simpler-grants-gov/commit/{self.deploy_github_sha}\"\n            )\n        else:\n            self.deploy_commit = \"https://github.com/HHS/simpler-grants-gov\"\n\n        if self.deploy_timestamp:\n            self.deploy_datetime_est = datetime_util.adjust_timezone(\n                self.deploy_timestamp, \"US/Eastern\"\n            )\n        else:\n            # Just put when the API started up as a fallback\n            self.deploy_datetime_est = datetime_util.get_now_us_eastern_datetime()\n\n\n_deploy_metadata_config: DeployMetadataConfig | None = None\n\n\ndef get_deploy_metadata_config() -> DeployMetadataConfig:\n    global _deploy_metadata_config\n    if _deploy_metadata_config is None:\n        _deploy_metadata_config = DeployMetadataConfig()\n\n    return _deploy_metadata_config"}
{"path":"frontend/src/components/user/UserControl.tsx","language":"typescript","type":"code","directory":"frontend/src/components/user","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/components/user/UserControl.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/util/dict_util.py\nLanguage: py\nType: code\nDirectory: api/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/dict_util.py\nSize: 0.97 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/constants/breadcrumbs.ts","language":"typescript","type":"code","directory":"frontend/src/constants","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/constants/breadcrumbs.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"def flatten_dict(in_dict: Any, separator: str = \".\", prefix: str = \"\") -> dict:\n    \"\"\"\n    Takes a set of nested dictionaries and flattens it\n\n    For example::\n\n    {\n        \"a\": {\n            \"b\": {\n                \"c\": \"value_c\"\n            },\n            \"d\": \"value_d\"\n        },\n        \"e\": \"value_e\"\n    }\n\n    Would become::\n\n    {\n        \"a.b.c\": \"value_c\",\n        \"a.d\": \"value_d\",\n        \"e\": \"value_e\"\n    }\n    \"\"\"\n\n    if isinstance(in_dict, dict):\n        return_dict = {}\n        # Iterate over each item in the dictionary\n        for kk, vv in in_dict.items():\n            # Flatten each item in the dictionary\n            for k, v in flatten_dict(vv, separator, str(kk)).items():\n                # Update the path\n                new_key = prefix + separator + str(k) if prefix else str(k)\n                return_dict[new_key] = v\n\n        return return_dict\n\n    # value isn't a dictionary, so no more recursion\n    return {prefix: in_dict}"}
{"path":"frontend/src/constants/defaultFeatureFlags.ts","language":"typescript","type":"code","directory":"frontend/src/constants","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/constants/defaultFeatureFlags.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/util/env_config.py\nLanguage: py\nType: code\nDirectory: api/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/env_config.py\nSize: 0.32 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/constants/environments.ts","language":"typescript","type":"code","directory":"frontend/src/constants","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/constants/environments.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"from pydantic_settings import BaseSettings, SettingsConfigDict\n\nimport src\n\nenv_file = os.path.join(\n    os.path.dirname(os.path.dirname(src.__file__)),\n    \"config\",\n    \"%s.env\" % os.getenv(\"ENVIRONMENT\", \"local\"),\n)\n\n\nclass PydanticBaseEnvConfig(BaseSettings):\n    model_config = SettingsConfigDict(env_file=env_file)"}
{"path":"frontend/src/constants/nofoPdfs.ts","language":"typescript","type":"code","directory":"frontend/src/constants","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/constants/nofoPdfs.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/util/file_util.py\nLanguage: py\nType: code\nDirectory: api/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/file_util.py\nSize: 5.26 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/constants/routes.ts","language":"typescript","type":"code","directory":"frontend/src/constants","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/constants/routes.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"import botocore.client\nimport smart_open\nfrom botocore.config import Config\n\nfrom src.adapters.aws import S3Config, get_boto_session, get_s3_client\n\n##################################\n# Path parsing utils\n##################################\n\n\ndef is_s3_path(path: str | Path) -> bool:\n    return str(path).startswith(\"s3://\")\n\n\ndef split_s3_url(path: str | Path) -> tuple[str, str]:\n    parts = urlparse(str(path))\n    bucket_name = parts.netloc\n    prefix = parts.path.lstrip(\"/\")\n    return bucket_name, prefix\n\n\ndef get_s3_bucket(path: str | Path) -> str:\n    return split_s3_url(path)[0]\n\n\ndef get_s3_file_key(path: str | Path) -> str:\n    return split_s3_url(path)[1]\n\n\ndef get_file_name(path: str) -> str:\n    return os.path.basename(path)\n\n\ndef join(*parts: str) -> str:\n    return os.path.join(*parts)\n\n\n##################################\n#  File operations\n##################################\n\n\ndef open_stream(path: str | Path, mode: str = \"r\", encoding: str | None = None) -> Any:\n    if is_s3_path(path):\n        s3_client = get_s3_client()\n\n        so_config = Config(\n            max_pool_connections=10,\n            connect_timeout=60,\n            read_timeout=60,\n            retries={\"max_attempts\": 10},\n        )\n        so_transport_params = {\"client_kwargs\": {\"config\": so_config}, \"client\": s3_client}\n\n        return smart_open.open(path, mode, transport_params=so_transport_params, encoding=encoding)\n    else:\n        return smart_open.open(path, mode, encoding=encoding)\n\n\ndef pre_sign_file_location(file_path: str) -> str:\n    s3_config = S3Config()\n    s3_client = get_s3_client(s3_config, get_boto_session())\n    bucket, key = split_s3_url(file_path)\n    pre_sign_file_loc = s3_client.generate_presigned_url(\n        \"get_object\",\n        Params={\"Bucket\": bucket, \"Key\": key},\n        ExpiresIn=s3_config.presigned_s3_duration,\n    )\n    if s3_config.s3_endpoint_url:\n        # Only relevant when local, due to docker path issues\n        pre_sign_file_loc = pre_sign_file_loc.replace(\n            s3_config.s3_endpoint_url, \"http://localhost:4566\"\n        )\n\n    return pre_sign_file_loc\n\n\ndef get_file_length_bytes(path: str) -> int:\n    if is_s3_path(path):\n        s3_client = (\n            get_s3_client()\n        )  # from our aws utils - handles some of the weird localstack stuff\n\n        bucket, key = split_s3_url(path)\n        file_metadata = s3_client.head_object(Bucket=bucket, Key=key)\n        return file_metadata[\"ContentLength\"]\n\n    file_stats = os.stat(path)\n    return file_stats.st_size\n\n\ndef copy_file(source_path: str | Path, destination_path: str | Path) -> None:\n    is_source_s3 = is_s3_path(source_path)\n    is_dest_s3 = is_s3_path(destination_path)\n\n    # This isn't a download or upload method\n    # Don't allow \"copying\" between mismatched locations\n    if is_source_s3 != is_dest_s3:\n        raise Exception(\"Cannot download/upload between disk and S3 using this method\")\n\n    if is_source_s3:\n        s3_client = get_s3_client()\n\n        source_bucket, source_path = split_s3_url(source_path)\n        dest_bucket, dest_path = split_s3_url(destination_path)\n\n        s3_client.copy({\"Bucket\": source_bucket, \"Key\": source_path}, dest_bucket, dest_path)\n    else:\n        os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n        shutil.copy2(source_path, destination_path)\n\n\ndef delete_file(path: str | Path) -> None:\n    \"\"\"Delete a file from s3 or local disk\"\"\"\n    if is_s3_path(path):\n        bucket, s3_path = split_s3_url(path)\n\n        s3_client = get_s3_client()\n        s3_client.delete_object(Bucket=bucket, Key=s3_path)\n    else:\n        os.remove(path)\n\n\ndef move_file(source_path: str | Path, destination_path: str | Path) -> None:\n    is_source_s3 = is_s3_path(source_path)\n    is_dest_s3 = is_s3_path(destination_path)\n\n    # This isn't a download or upload method\n    # Don't allow \"copying\" between mismatched locations\n    if is_source_s3 != is_dest_s3:\n        raise Exception(\"Cannot download/upload between disk and S3 using this method\")\n\n    if is_source_s3:\n        copy_file(source_path, destination_path)\n        delete_file(source_path)\n\n    else:\n        os.renames(source_path, destination_path)\n\n\ndef file_exists(path: str | Path) -> bool:\n    \"\"\"Get whether a file exists or not\"\"\"\n    if is_s3_path(path):\n        s3_client = get_s3_client()\n\n        bucket, key = split_s3_url(path)\n\n        try:\n            s3_client.head_object(Bucket=bucket, Key=key)\n            return True\n        except botocore.exceptions.ClientError:\n            return False\n\n    # Local file system\n    return Path(path).exists()\n\n\ndef read_file(path: str | Path, mode: str = \"r\", encoding: str | None = None) -> str:\n    \"\"\"Simple function for just getting all of the contents of a file\"\"\"\n    with open_stream(path, mode, encoding) as input_file:\n        return input_file.read()\n\n\ndef convert_public_s3_to_cdn_url(file_path: str, cdn_url: str, s3_config: S3Config) -> str:\n    \"\"\"\n    Convert an S3 URL to a CDN URL\n\n    Example:\n        s3://bucket-name/path/to/file.txt -> https://cdn.example.com/path/to/file.txt\n    \"\"\"\n    if not is_s3_path(file_path):\n        raise ValueError(f\"Expected s3:// path, got: {file_path}\")\n\n    return file_path.replace(s3_config.public_files_bucket_path, cdn_url)"}
{"path":"frontend/src/constants/search.ts","language":"typescript","type":"code","directory":"frontend/src/constants","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/constants/search.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/util/local.py\nLanguage: py\nType: code\nDirectory: api/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/local.py\nSize: 0.96 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/errors.ts","language":"typescript","type":"code","directory":"frontend/src","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/errors.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"from dotenv import load_dotenv\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_local_env_vars(env_file: str = \"local.env\") -> None:\n    \"\"\"\n    Load environment variables from the local.env so\n    that they can be fetched with `os.getenv()` or with\n    other utils that pull env vars.\n\n    https://pypi.org/project/python-dotenv/\n\n    NOTE: any existing env vars will not be overriden by this\n    \"\"\"\n    environment = os.getenv(\"ENVIRONMENT\", None)\n\n    # If the environment is explicitly local or undefined\n    # we'll use the dotenv file, otherwise we'll skip\n    # Should never run if not local development\n    if environment is None or environment == \"local\":\n        load_dotenv(env_file)\n\n\ndef error_if_not_local() -> None:\n    if (env := os.getenv(\"ENVIRONMENT\")) != \"local\":\n        logger.error(\"Environment %s is not local - cannot run operation\", env)\n        raise Exception(\"Local-only process called when environment was set to non-local\")"}
{"path":"frontend/src/hoc/withFeatureFlag.tsx","language":"typescript","type":"code","directory":"frontend/src/hoc","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/hoc/withFeatureFlag.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/util/string_utils.py\nLanguage: py\nType: code\nDirectory: api/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/util/string_utils.py\nSize: 0.46 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/hooks/useFeatureFlags.ts","language":"typescript","type":"code","directory":"frontend/src/hooks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/hooks/useFeatureFlags.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"def join_list(joining_list: Optional[list], join_txt: str = \"\\n\") -> str:\n    \"\"\"\n    Utility to join a list.\n\n    Functionally equivalent to:\n    \"\" if joining_list is None else \"\\n\".join(joining_list)\n    \"\"\"\n    if not joining_list:\n        return \"\"\n\n    return join_txt.join(joining_list)\n\n\ndef is_valid_uuid(value: str) -> bool:\n    try:\n        uuid.UUID(value)\n        return True\n    except ValueError:\n        return False"}
{"path":"frontend/src/hooks/usePrevious.ts","language":"typescript","type":"code","directory":"frontend/src/hooks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/hooks/usePrevious.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/validation/__init__.py\nLanguage: py\nType: code\nDirectory: api/src/validation\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/validation/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/hooks/useSearchParamUpdater.ts","language":"typescript","type":"code","directory":"frontend/src/hooks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/hooks/useSearchParamUpdater.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":""}
{"path":"frontend/src/i18n/config.ts","language":"typescript","type":"code","directory":"frontend/src/i18n","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/i18n/config.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/src/validation/validation_constants.py\nLanguage: py\nType: code\nDirectory: api/src/validation\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/src/validation/validation_constants.py\nSize: 0.75 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/i18n/getMessagesWithFallbacks.ts","language":"typescript","type":"code","directory":"frontend/src/i18n","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/i18n/getMessagesWithFallbacks.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"class ValidationErrorType(StrEnum):\n    \"\"\"\n    Error type codes which clients\n    need to be aware of in order\n    to display proper messaging to users.\n\n    *** WARNING ***\n    Do not adjust these values unless you\n    are certain that any and all users\n    are aware of the change, safer to add a new one\n    \"\"\"\n\n    REQUIRED = \"required\"\n    NOT_NULL = \"not_null\"\n    UNKNOWN = \"unknown\"\n    INVALID = \"invalid\"\n\n    FORMAT = \"format\"\n    INVALID_CHOICE = \"invalid_choice\"\n    SPECIAL_NUMERIC = \"special_numeric\"\n\n    MIN_LENGTH = \"min_length\"\n    MAX_LENGTH = \"max_length\"\n    MIN_OR_MAX_LENGTH = \"min_or_max_length\"\n    EQUALS = \"equals\"\n\n    MIN_VALUE = \"min_value\"\n    MAX_VALUE = \"max_value\"\n    MIN_OR_MAX_VALUE = \"min_or_max_value\""}
{"path":"frontend/src/i18n/messages/en/index.ts","language":"typescript","type":"code","directory":"frontend/src/i18n/messages/en","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/i18n/messages/en/index.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/tests/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/i18n/messages/es/index.ts","language":"typescript","type":"code","directory":"frontend/src/i18n/messages/es","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/i18n/messages/es/index.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":""}
{"path":"frontend/src/i18n/request.ts","language":"typescript","type":"code","directory":"frontend/src/i18n","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/i18n/request.ts","size":324715,"lastModified":"2025-02-14T17:08:31.133Z","content":"File: api/tests/conftest.py\nLanguage: py\nType: code\nDirectory: api/tests\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/conftest.py\nSize: 14.07 KB\nLast Modified: 2025-02-14T17:08:26.456Z"}
{"path":"frontend/src/middleware.ts","language":"typescript","type":"code","directory":"frontend/src","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/middleware.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"import _pytest.monkeypatch\nimport boto3\nimport flask.testing\nimport moto\nimport pytest\nfrom apiflask import APIFlask\nfrom cryptography.hazmat.primitives import serialization\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom sqlalchemy import text\n\nimport src.adapters.db as db\nimport src.app as app_entry\nimport src.auth.login_gov_jwt_auth as login_gov_jwt_auth\nimport tests.src.db.models.factories as factories\nfrom src.adapters import search\nfrom src.adapters.aws import S3Config\nfrom src.adapters.oauth.login_gov.mock_login_gov_oauth_client import MockLoginGovOauthClient\nfrom src.auth.api_jwt_auth import create_jwt_for_user\nfrom src.constants.schema import Schemas\nfrom src.db import models\nfrom src.db.models.foreign import metadata as foreign_metadata\nfrom src.db.models.lookup.sync_lookup_values import sync_lookup_values\nfrom src.db.models.opportunity_models import Opportunity\nfrom src.db.models.staging import metadata as staging_metadata\nfrom src.util.local import load_local_env_vars\nfrom tests.lib import db_testing\nfrom tests.lib.auth_test_utils import mock_oauth_endpoint\n\nlogger = logging.getLogger(__name__)\n\n\n@pytest.fixture\ndef user(enable_factory_create, db_session):\n    return factories.UserFactory.create()\n\n\n@pytest.fixture\ndef user_auth_token(user, db_session):\n    token, _ = create_jwt_for_user(user, db_session)\n    db_session.commit()\n    return token\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef env_vars():\n    \"\"\"\n    Default environment variables for tests to be\n    based on the local.env file. These get set once\n    before all tests run. As \"session\" is the highest\n    scope, this will run before any other explicit fixtures\n    in a test.\n\n    See: https://docs.pytest.org/en/6.2.x/fixture.html#autouse-order\n\n    To set a different environment variable for a test,\n    use the monkeypatch fixture, for example:\n\n    ```py\n    def test_example(monkeypatch):\n        monkeypatch.setenv(\"LOG_LEVEL\", \"debug\")\n    ```\n\n    Several monkeypatch fixtures exists below for different\n    scope levels.\n    \"\"\"\n    load_local_env_vars()\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef set_logging_defaults(monkeypatch_session):\n    # Some loggers are noisy/buggy in our tests, so adjust them\n    monkeypatch_session.setenv(\"LOG_LEVEL_OVERRIDES\", \"newrelic.core.agent=ERROR\")\n\n\n####################\n# Test DB session\n####################\n\n\n# From https://github.com/pytest-dev/pytest/issues/363\n@pytest.fixture(scope=\"session\")\ndef monkeypatch_session():\n    \"\"\"\n    Create a monkeypatch instance that can be used to\n    monkeypatch global environment, objects, and attributes\n    for the duration the test session.\n    \"\"\"\n    mpatch = _pytest.monkeypatch.MonkeyPatch()\n    yield mpatch\n    mpatch.undo()\n\n\n# From https://github.com/pytest-dev/pytest/issues/363\n@pytest.fixture(scope=\"class\")\ndef monkeypatch_class():\n    \"\"\"\n    Create a monkeypatch instance that can be used to\n    monkeypatch global environment, objects, and attributes\n    for the duration of a test class.\n    \"\"\"\n    mpatch = _pytest.monkeypatch.MonkeyPatch()\n    yield mpatch\n    mpatch.undo()\n\n\n# From https://github.com/pytest-dev/pytest/issues/363\n@pytest.fixture(scope=\"module\")\ndef monkeypatch_module():\n    mpatch = _pytest.monkeypatch.MonkeyPatch()\n    yield mpatch\n    mpatch.undo()\n\n\n@pytest.fixture(scope=\"session\")\ndef db_client(monkeypatch_session, db_schema_prefix) -> db.DBClient:\n    \"\"\"\n    Creates an isolated database for the test session.\n\n    Creates a new empty PostgreSQL schema, creates all tables in the new schema\n    using SQLAlchemy, then returns a db.DBClient instance that can be used to\n    get connections or sessions to this database schema. The schema is dropped\n    after the test suite session completes.\n    \"\"\"\n\n    with db_testing.create_isolated_db(monkeypatch_session, db_schema_prefix) as db_client:\n        with db_client.get_connection() as conn, conn.begin():\n            models.metadata.create_all(bind=conn)\n            staging_metadata.create_all(bind=conn)\n            foreign_metadata.create_all(bind=conn)\n\n        sync_lookup_values(db_client)\n        yield db_client\n\n\n@pytest.fixture\ndef db_session(db_client: db.DBClient) -> db.Session:\n    \"\"\"\n    Returns a database session connected to the schema used for the test session.\n    \"\"\"\n    with db_client.get_session() as session:\n        yield session\n\n\n@pytest.fixture\ndef enable_factory_create(monkeypatch, db_session, mock_s3_bucket) -> db.Session:\n    \"\"\"\n    Allows the create method of factories to be called. By default, the create\n    throws an exception to prevent accidental creation of database objects for tests\n    that do not need persistence. This fixture only allows the create method to be\n    called for the current test. Each test that needs to call Factory.create should pull in\n    this fixture.\n    \"\"\"\n    monkeypatch.setattr(factories, \"_db_session\", db_session)\n    return db_session\n\n\n@pytest.fixture(scope=\"session\")\ndef db_schema_prefix():\n    return f\"test_{uuid.uuid4().int}_\"\n\n\n@pytest.fixture(scope=\"session\")\ndef test_api_schema(db_schema_prefix):\n    return f\"{db_schema_prefix}{Schemas.API}\"\n\n\n@pytest.fixture(scope=\"session\")\ndef test_staging_schema(db_schema_prefix):\n    return f\"{db_schema_prefix}{Schemas.STAGING}\"\n\n\n@pytest.fixture(scope=\"session\")\ndef test_foreign_schema(db_schema_prefix):\n    return f\"{db_schema_prefix}{Schemas.LEGACY}\"\n\n\n####################\n# Opensearch Fixtures\n####################\n\n\n@pytest.fixture(scope=\"session\")\ndef search_client() -> search.SearchClient:\n    client = search.SearchClient()\n    try:\n        yield client\n    finally:\n        # Just in case a test setup an index\n        # in a way that didn't clean it up, delete\n        # all indexes at the end of a run that start with test\n        client.delete_index(\"test-*\")\n\n\n@pytest.fixture(scope=\"session\")\ndef opportunity_index(search_client):\n    # create a random index name just to make sure it won't ever conflict\n    # with an actual one, similar to how we create schemas for database tests\n    index_name = f\"test-opportunity-index-{uuid.uuid4().int}\"\n\n    search_client.create_index(index_name)\n\n    try:\n        yield index_name\n    finally:\n        # Try to clean up the index at the end\n        # Use a prefix which will delete the above (if it exists)\n        # and any that might not have been cleaned up due to issues\n        # in prior runs\n        search_client.delete_index(\"test-opportunity-index-*\")\n\n\n@pytest.fixture(scope=\"session\")\ndef opportunity_index_alias(search_client, monkeypatch_session):\n    # Note we don't actually create anything, this is just a random name\n    alias = f\"test-opportunity-index-alias-{uuid.uuid4().int}\"\n    monkeypatch_session.setenv(\"OPPORTUNITY_SEARCH_INDEX_ALIAS\", alias)\n    return alias\n\n\n@pytest.fixture(scope=\"class\")\ndef opportunity_search_index_class(search_client, monkeypatch):\n    # Note we don't actually create anything, this is just a random name\n    alias = f\"test-opportunity-index-alias-{uuid.uuid4().int}\"\n    monkeypatch.setenv(\"OPPORTUNITY_SEARCH_INDEX_ALIAS\", alias)\n    return alias\n\n\ndef _generate_rsa_key_pair():\n    # Rather than define a private/public key, generate one for the tests\n    key = rsa.generate_private_key(public_exponent=65537, key_size=2048)\n\n    private_key = key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.TraditionalOpenSSL,\n        encryption_algorithm=serialization.NoEncryption(),\n    )\n\n    public_key = key.public_key().public_bytes(\n        encoding=serialization.Encoding.PEM, format=serialization.PublicFormat.SubjectPublicKeyInfo\n    )\n\n    return private_key, public_key\n\n\n@pytest.fixture(scope=\"session\")\ndef rsa_key_pair():\n    return _generate_rsa_key_pair()\n\n\n@pytest.fixture(scope=\"session\")\ndef private_rsa_key(rsa_key_pair):\n    return rsa_key_pair[0]\n\n\n@pytest.fixture(scope=\"session\")\ndef public_rsa_key(rsa_key_pair):\n    return rsa_key_pair[1]\n\n\n@pytest.fixture(scope=\"session\")\ndef other_rsa_key_pair():\n    return _generate_rsa_key_pair()\n\n\n@pytest.fixture(scope=\"session\")\ndef mock_oauth_client():\n    return MockLoginGovOauthClient()\n\n\n@pytest.fixture(scope=\"session\")\ndef setup_login_gov_auth(monkeypatch_session, public_rsa_key):\n    \"\"\"Setup login.gov JWK endpoint to be stubbed out\"\"\"\n\n    def override_method(config):\n        config.public_key_map = {\"test-key-id\": public_rsa_key}\n\n    monkeypatch_session.setattr(login_gov_jwt_auth, \"_refresh_keys\", override_method)\n\n\n####################\n# Test App & Client\n####################\n\n\n# Make app session scoped so the database connection pool is only created once\n# for the test session. This speeds up the tests.\n@pytest.fixture(scope=\"session\")\ndef app(\n    db_client,\n    opportunity_index_alias,\n    monkeypatch_session,\n    private_rsa_key,\n    mock_oauth_client,\n    setup_login_gov_auth,\n) -> APIFlask:\n    # Override the OAuth endpoint path before creating the app which loads the config at startup\n    monkeypatch_session.setenv(\n        \"LOGIN_GOV_AUTH_ENDPOINT\", \"http://localhost:8080/test-endpoint/oauth-authorize\"\n    )\n    app = app_entry.create_app()\n\n    # Add endpoints and mocks for handling the external OAuth logic\n    mock_oauth_endpoint(app, monkeypatch_session, private_rsa_key, mock_oauth_client)\n\n    return app\n\n\n@pytest.fixture\ndef client(app: flask.Flask) -> flask.testing.FlaskClient:\n    return app.test_client()\n\n\n@pytest.fixture\ndef cli_runner(app: flask.Flask) -> flask.testing.CliRunner:\n    return app.test_cli_runner()\n\n\n@pytest.fixture\ndef all_api_auth_tokens(monkeypatch):\n    all_auth_tokens = [\"abcd1234\", \"wxyz7890\", \"lmno56\"]\n    monkeypatch.setenv(\"API_AUTH_TOKEN\", \",\".join(all_auth_tokens))\n    return all_auth_tokens\n\n\n@pytest.fixture\ndef api_auth_token(monkeypatch, all_api_auth_tokens):\n    auth_token = all_api_auth_tokens[0]\n    return auth_token\n\n\n####################\n# AWS Mock Fixtures\n####################\n\n\n@pytest.fixture\ndef reset_aws_env_vars(monkeypatch):\n    # Reset the env vars so you can't accidentally connect\n    # to a real AWS account if you were doing some local testing\n    monkeypatch.setenv(\"AWS_ACCESS_KEY_ID\", \"testing\")\n    monkeypatch.setenv(\"AWS_SECRET_ACCESS_KEY\", \"testing\")\n    monkeypatch.setenv(\"AWS_SECURITY_TOKEN\", \"testing\")\n    monkeypatch.setenv(\"AWS_SESSION_TOKEN\", \"testing\")\n    monkeypatch.setenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n    monkeypatch.delenv(\"S3_ENDPOINT_URL\")\n\n\n@pytest.fixture\ndef mock_s3(reset_aws_env_vars):\n    # https://docs.getmoto.org/en/stable/docs/configuration/index.html#whitelist-services\n    with moto.mock_aws(config={\"core\": {\"service_whitelist\": [\"s3\"]}}):\n        yield boto3.resource(\"s3\")\n\n\n@pytest.fixture\ndef mock_s3_bucket_resource(mock_s3):\n    bucket = mock_s3.Bucket(\"local-mock-public-bucket\")\n    bucket.create()\n    yield bucket\n\n\n@pytest.fixture\ndef mock_s3_bucket(mock_s3_bucket_resource):\n    yield mock_s3_bucket_resource.name\n\n\n@pytest.fixture\ndef other_mock_s3_bucket_resource(mock_s3):\n    # This second bucket exists for tests where we want there to be multiple buckets\n    # and/or test behavior when moving files between buckets.\n    bucket = mock_s3.Bucket(\"local-mock-draft-bucket\")\n    bucket.create()\n    yield bucket\n\n\n@pytest.fixture\ndef other_mock_s3_bucket(other_mock_s3_bucket_resource):\n    yield other_mock_s3_bucket_resource.name\n\n\n@pytest.fixture\ndef s3_config(mock_s3_bucket, other_mock_s3_bucket):\n    return S3Config(\n        PUBLIC_FILES_BUCKET=f\"s3://{mock_s3_bucket}\",\n        DRAFT_FILES_BUCKET=f\"s3://{other_mock_s3_bucket}\",\n    )\n\n\n####################\n# Class-based testing\n####################\n\n\nclass BaseTestClass:\n    \"\"\"\n    A base class to derive a test class from. This lets\n    us have a set of fixtures with a scope greater than\n    an individual test, but that need to be more granular than\n    session scoping.\n\n    Useful for avoiding repetition in setup of tests which\n    can be clearer or provide better performance.\n\n    See: https://docs.pytest.org/en/7.1.x/how-to/fixtures.html#fixture-scopes\n\n    For example:\n\n    class TestExampleClass(BaseTestClass):\n\n        @pytest.fixture(scope=\"class\")\n        def setup_data(db_session):\n            # note that the db_session here would be the one created in this class\n            # as it will pull from the class scope instead\n\n            examples = ExampleFactory.create_batch(size=100)\n    \"\"\"\n\n    @pytest.fixture(scope=\"class\")\n    def db_session(self, db_client, monkeypatch_class):\n        # Note this shadows the db_session fixture for tests in this class\n        with db_client.get_session() as db_session:\n            yield db_session\n\n    @pytest.fixture(scope=\"class\")\n    def enable_factory_create(self, monkeypatch_class, db_session):\n        \"\"\"\n        Allows the create method of factories to be called. By default, the create\n            throws an exception to prevent accidental creation of database objects for tests\n            that do not need persistence. This fixture only allows the create method to be\n            called for the current class of tests. Each test that needs to call Factory.create should pull in\n            this fixture.\n        \"\"\"\n        monkeypatch_class.setattr(factories, \"_db_session\", db_session)\n\n    @pytest.fixture(scope=\"class\")\n    def truncate_opportunities(self, db_session):\n        \"\"\"\n        Use this fixture when you want to truncate the opportunity table\n        and handle deleting all related records.\n\n        As this is at the class scope, this will only run once for a given\n        class implementation.\n        \"\"\"\n\n        opportunities = db_session.query(Opportunity).all()\n        for opp in opportunities:\n            db_session.delete(opp)\n\n        # Force the deletes to the DB\n        db_session.commit()\n\n    @pytest.fixture(scope=\"class\")\n    def truncate_staging_tables(self, db_session, test_staging_schema):\n        for table in staging_metadata.tables.values():\n            db_session.execute(text(f\"TRUNCATE TABLE {test_staging_schema}.{table.name}\"))\n\n        db_session.commit()\n\n    @pytest.fixture(scope=\"class\")\n    def truncate_foreign_tables(self, db_session, test_foreign_schema):\n        for table in foreign_metadata.tables.values():\n            db_session.execute(text(f\"TRUNCATE TABLE {test_foreign_schema}.{table.name}\"))\n\n        db_session.commit()"}
{"path":"frontend/src/services/auth/UserProvider.tsx","language":"typescript","type":"code","directory":"frontend/src/services/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/auth/UserProvider.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/lib/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/lib\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/__init__.py\nSize: 0.07 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/services/auth/session.ts","language":"typescript","type":"code","directory":"frontend/src/services/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/auth/session.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"pytest.register_assert_rewrite(\"tests.lib.assertions\")"}
{"path":"frontend/src/services/auth/sessionUtils.ts","language":"typescript","type":"code","directory":"frontend/src/services/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/auth/sessionUtils.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/lib/assertions.py\nLanguage: py\nType: code\nDirectory: api/tests/lib\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/assertions.py\nSize: 0.24 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/services/auth/types.tsx","language":"typescript","type":"code","directory":"frontend/src/services/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/auth/types.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.134Z","content":""}
{"path":"frontend/src/services/auth/useUser.tsx","language":"typescript","type":"code","directory":"frontend/src/services/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/auth/useUser.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/lib/auth_test_utils.py\nLanguage: py\nType: code\nDirectory: api/tests/lib\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/auth_test_utils.py\nSize: 4.02 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/services/featureFlags/FeatureFlagManager.ts","language":"typescript","type":"code","directory":"frontend/src/services/featureFlags","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/featureFlags/FeatureFlagManager.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"import urllib\nimport uuid\nfrom datetime import datetime, timedelta\n\nimport flask\nimport jwt\n\nfrom src.adapters.oauth.oauth_client_models import OauthTokenResponse\nfrom src.auth.login_gov_jwt_auth import get_config\n\n\ndef create_jwt(\n    user_id: str,\n    private_key: str | bytes,\n    email: str = \"fake_mail@mail.com\",\n    nonce: str = \"abc123\",\n    expires_at: datetime | None = None,\n    issued_at: datetime | None = None,\n    not_before: datetime | None = None,\n    # Note that these values need to match what we set\n    # in conftest.py::setup_login_gov_auth\n    issuer: str | None = None,\n    audience: str | None = None,\n):\n    \"\"\"Create a JWT in roughly the format login.gov will give us\"\"\"\n\n    # Default datetime values are set to clearly not be an issue\n    if expires_at is None:\n        expires_at = datetime.now() + timedelta(days=365)\n    if issued_at is None:\n        issued_at = datetime.now() - timedelta(days=365)\n    if not_before is None:\n        not_before = datetime.now() - timedelta(days=365)\n    if issuer is None:\n        issuer = get_config().login_gov_endpoint\n    if audience is None:\n        audience = get_config().client_id\n\n    payload = {\n        \"sub\": user_id,\n        \"iss\": issuer,\n        \"aud\": audience,\n        \"email\": email,\n        \"nonce\": nonce,\n        # The jwt encode function automatically turns these datetime\n        # objects into a UTC timestamp integer\n        \"exp\": expires_at,\n        \"iat\": issued_at,\n        \"nbf\": not_before,\n        # These values aren't checked by anything at the moment\n        # but are a part of the token from login.gov\n        \"jti\": \"abc123\",\n        \"at_hash\": \"abc123\",\n        \"c_hash\": \"abc123\",\n        \"acr\": \"urn:acr.login.gov:auth-only\",\n    }\n\n    return jwt.encode(payload, private_key, algorithm=\"RS256\", headers={\"kid\": \"test-key-id\"})\n\n\ndef oauth_param_override():\n    \"\"\"Override endpoint called in the mock authorize endpoint setup below.\n\n    To override you can do the following in your test:\n\n        def override():\n            return {\"error\": \"access_denied\"}\n\n        monkeypatch.setattr(\"tests.lib.auth_test_utils.oauth_param_override\", override)\n    \"\"\"\n    return {}\n\n\ndef mock_oauth_endpoint(app, monkeypatch, private_key, mock_oauth_client):\n    \"\"\"Add mock oauth endpoints to the application\n\n    For the initial authorize endpoint, we create an endpoint on the app itself\n    which redirects back to the configured redirect_uri and also sets up the\n    mock_oauth_client to have a successful response when calling it later for a token.\n    \"\"\"\n\n    @app.get(\"/test-endpoint/oauth-authorize\")\n    def oauth_authorize():\n        # This endpoint represents a mocked version of\n        # https://developers.login.gov/oidc/authorization/\n        # and needs to return the state value as well as a code.\n        query_args = flask.request.args\n\n        params = {\"state\": query_args.get(\"state\"), \"code\": str(uuid.uuid4())}\n        params.update(oauth_param_override())\n\n        # Add a dummy response we'll later get if the token endpoint is called\n        id_token = create_jwt(\n            user_id=query_args.get(\"state\"),  # Re-use the state as the user ID\n            private_key=private_key,\n            nonce=query_args.get(\"nonce\"),\n        )\n        mocked_token_response = OauthTokenResponse(\n            id_token=id_token, access_token=\"fake_token\", token_type=\"Bearer\", expires_in=300\n        )\n        mock_oauth_client.add_token_response(params[\"code\"], mocked_token_response)\n\n        encoded_params = urllib.parse.urlencode(params)\n\n        redirect_uri = f\"{query_args['redirect_uri']}?{encoded_params}\"\n\n        return flask.redirect(redirect_uri)\n\n    # Override our callback endpoint to use this mocked client instead of the real one\n    def override_get_client():\n        \"\"\"Override the login_gov client we use in unit tests to be the mock version\"\"\"\n        return mock_oauth_client\n\n    monkeypatch.setattr(\n        \"src.services.users.login_gov_callback_handler.get_login_gov_client\", override_get_client\n    )"}
{"path":"frontend/src/services/featureFlags/featureFlagHelpers.ts","language":"typescript","type":"code","directory":"frontend/src/services/featureFlags","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/featureFlags/featureFlagHelpers.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/lib/db_testing.py\nLanguage: py\nType: code\nDirectory: api/tests/lib\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/db_testing.py\nSize: 1.88 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/services/fetch/endpointConfigs.ts","language":"typescript","type":"code","directory":"frontend/src/services/fetch","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/fetch/endpointConfigs.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"import contextlib\nimport logging\n\nfrom sqlalchemy import text\n\nimport src.adapters.db as db\nfrom src.adapters.db.clients.postgres_config import get_db_config\n\nlogger = logging.getLogger(__name__)\n\n\n@contextlib.contextmanager\ndef create_isolated_db(monkeypatch, db_schema_prefix) -> db.DBClient:\n    \"\"\"\n    Creates a temporary PostgreSQL schema and creates a database engine\n    that connects to that schema. Drops the schema after the context manager\n    exits.\n    \"\"\"\n\n    # To improve test performance, don't check the database connection\n    # when initializing the DB client.\n    monkeypatch.setenv(\"DB_CHECK_CONNECTION_ON_INIT\", \"False\")\n    # We set the prefix override here so when the API client creates a DB config\n    # it also has the appropriate prefix value for mapping\n    monkeypatch.setenv(\"SCHEMA_PREFIX_OVERRIDE\", db_schema_prefix)\n\n    db_config = db.PostgresDBConfig(schema_prefix_override=db_schema_prefix)\n    db_client = db.PostgresDBClient(db_config)\n    test_schemas = db_config.get_schema_translate_map().values()\n\n    with db_client.get_connection() as conn:\n        for schema in test_schemas:\n            _create_schema(conn, schema)\n\n        try:\n            yield db_client\n\n        finally:\n            for schema in test_schemas:\n                _drop_schema(conn, schema)\n\n\ndef _create_schema(conn: db.Connection, schema_name: str):\n    \"\"\"Create a database schema.\"\"\"\n    db_test_user = get_db_config().username\n\n    with conn.begin():\n        conn.execute(\n            text(f\"CREATE SCHEMA IF NOT EXISTS {schema_name} AUTHORIZATION {db_test_user};\")\n        )\n    logger.info(\"create schema %s\", schema_name)\n\n\ndef _drop_schema(conn: db.Connection, schema_name: str):\n    \"\"\"Drop a database schema.\"\"\"\n    with conn.begin():\n        conn.execute(text(f\"DROP SCHEMA {schema_name} CASCADE;\"))\n    logger.info(\"drop schema %s\", schema_name)"}
{"path":"frontend/src/services/fetch/fetcherHelpers.ts","language":"typescript","type":"code","directory":"frontend/src/services/fetch","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/fetch/fetcherHelpers.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/lib/seed_agencies.py\nLanguage: py\nType: code\nDirectory: api/tests/lib\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/seed_agencies.py\nSize: 12.52 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/services/fetch/fetchers/clientSearchResultsDownloadFetcher.ts","language":"typescript","type":"code","directory":"frontend/src/services/fetch/fetchers","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/fetch/fetchers/clientSearchResultsDownloadFetcher.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"import src.adapters.db as db\nimport tests.src.db.models.factories as factories\nfrom src.db.models.agency_models import Agency\n\nlogger = logging.getLogger(__name__)\n\n\n# Agencies we want to create locally - note that these are a subset of popular\n# agencies from production\nAGENCIES_TO_CREATE = [\n    {\"agency_id\": 3, \"agency_code\": \"DOC\", \"agency_name\": \"Department of Commerce\"},\n    {\"agency_id\": 53, \"agency_code\": \"DOD\", \"agency_name\": \"Department of Defense\"},\n    {\"agency_id\": 140, \"agency_code\": \"DOE\", \"agency_name\": \"Department of Energy\"},\n    {\"agency_id\": 156, \"agency_code\": \"DOI\", \"agency_name\": \"Department of the Interior\"},\n    {\"agency_id\": 504, \"agency_code\": \"EPA\", \"agency_name\": \"Environmental Protection Agency\"},\n    {\n        \"agency_id\": 674,\n        \"agency_code\": \"HHS\",\n        \"agency_name\": \"Department of Health and Human Services\",\n    },\n    {\"agency_id\": 864, \"agency_code\": \"NSF\", \"agency_name\": \"National Science Foundation\"},\n    {\"agency_id\": 1, \"agency_code\": \"USAID\", \"agency_name\": \"Agency for International Development\"},\n    {\"agency_id\": 983, \"agency_code\": \"USDA\", \"agency_name\": \"Department of Agriculture\"},\n    {\"agency_id\": 1105, \"agency_code\": \"USDOJ\", \"agency_name\": \"Department of Justice\"},\n    {\n        \"agency_id\": 4,\n        \"agency_code\": \"DOC-EDA\",\n        \"agency_name\": \"Economic Development Administration\",\n        \"top_level_agency_id\": 3,\n    },\n    {\n        \"agency_id\": 34,\n        \"agency_code\": \"DOC-DOCNOAAERA\",\n        \"agency_name\": \"DOC NOAA - ERA Production\",\n        \"top_level_agency_id\": 3,\n    },\n    {\n        \"agency_id\": 40,\n        \"agency_code\": \"DOC-NIST\",\n        \"agency_name\": \"National Institute of Standards and Technology\",\n        \"top_level_agency_id\": 3,\n    },\n    {\n        \"agency_id\": 46,\n        \"agency_code\": \"DOD-AFOSR\",\n        \"agency_name\": \"Air Force Office of Scientific Research\",\n        \"top_level_agency_id\": 53,\n    },\n    {\n        \"agency_id\": 54,\n        \"agency_code\": \"DOD-AFRL\",\n        \"agency_name\": \"Air Force -- Research Lab\",\n        \"top_level_agency_id\": 53,\n    },\n    {\n        \"agency_id\": 55,\n        \"agency_code\": \"DOD-AMC\",\n        \"agency_name\": \"Dept of the Army -- Materiel Command\",\n        \"top_level_agency_id\": 53,\n    },\n    {\n        \"agency_id\": 73,\n        \"agency_code\": \"DOD-AMRAA\",\n        \"agency_name\": \"Dept. of the Army -- USAMRAA\",\n        \"top_level_agency_id\": 53,\n    },\n    {\n        \"agency_id\": 74,\n        \"agency_code\": \"DOD-COE\",\n        \"agency_name\": \"Dept. of the Army  --  Corps of Engineers\",\n        \"top_level_agency_id\": 53,\n    },\n    {\n        \"agency_id\": 75,\n        \"agency_code\": \"DOD-COE-ERDC\",\n        \"agency_name\": \"Engineer Research and Development Center\",\n        \"top_level_agency_id\": 53,\n    },\n    {\n        \"agency_id\": 81,\n        \"agency_code\": \"DOD-COE-FW\",\n        \"agency_name\": \"Fort Worth District\",\n        \"top_level_agency_id\": 53,\n    },\n    {\n        \"agency_id\": 89,\n        \"agency_code\": \"DOD-DARPA-MTO\",\n        \"agency_name\": \"DARPA - Microsystems Technology Office \",\n        \"top_level_agency_id\": 53,\n    },\n    {\n        \"agency_id\": 97,\n        \"agency_code\": \"DOD-DARPA-DSO\",\n        \"agency_name\": \"DARPA - Defense Sciences Office\",\n        \"top_level_agency_id\": 53,\n    },\n    {\n        \"agency_id\": 110,\n        \"agency_code\": \"DOD-ONR\",\n        \"agency_name\": \"Office of Naval Research\",\n        \"top_level_agency_id\": 53,\n    },\n    {\n        \"agency_id\": 113,\n        \"agency_code\": \"DOD-ONR-FAC\",\n        \"agency_name\": \"NAVAL FACILITIES ENGINEERING COMMAND\",\n        \"top_level_agency_id\": 53,\n    },\n    {\n        \"agency_id\": 138,\n        \"agency_code\": \"DOE-GFO\",\n        \"agency_name\": \"Golden Field Office\",\n        \"top_level_agency_id\": 140,\n    },\n    {\n        \"agency_id\": 139,\n        \"agency_code\": \"DOE-NETL\",\n        \"agency_name\": \"National Energy Technology Laboratory\",\n        \"top_level_agency_id\": 140,\n    },\n    {\n        \"agency_id\": 144,\n        \"agency_code\": \"DOI-BOR\",\n        \"agency_name\": \"Bureau of Reclamation\",\n        \"top_level_agency_id\": 156,\n    },\n    {\n        \"agency_id\": 150,\n        \"agency_code\": \"DOE-ARPAE\",\n        \"agency_name\": \"Advanced Research Projects Agency Energy \",\n        \"top_level_agency_id\": 140,\n    },\n    {\n        \"agency_id\": 158,\n        \"agency_code\": \"DOI-BLM\",\n        \"agency_name\": \"Bureau of Land Management\",\n        \"top_level_agency_id\": 156,\n    },\n    {\n        \"agency_id\": 167,\n        \"agency_code\": \"DOI-BOR-LC\",\n        \"agency_name\": \"Bureau of Reclamation - Lower Colorado Region\",\n        \"top_level_agency_id\": 156,\n    },\n    {\n        \"agency_id\": 169,\n        \"agency_code\": \"DOI-BIA\",\n        \"agency_name\": \"Bureau of Indian Affairs\",\n        \"top_level_agency_id\": 156,\n    },\n    {\n        \"agency_id\": 170,\n        \"agency_code\": \"DOI-BOR-MP\",\n        \"agency_name\": \"Bureau of Reclamation - Mid-Pacific Region\",\n        \"top_level_agency_id\": 156,\n    },\n    {\n        \"agency_id\": 172,\n        \"agency_code\": \"DOI-BOEM\",\n        \"agency_name\": \"Bureau of Ocean Energy Management\",\n        \"top_level_agency_id\": 156,\n    },\n    {\n        \"agency_id\": 174,\n        \"agency_code\": \"DOI-FWS\",\n        \"agency_name\": \"Fish and Wildlife Service\",\n        \"top_level_agency_id\": 156,\n    },\n    {\n        \"agency_id\": 191,\n        \"agency_code\": \"DOI-NPS\",\n        \"agency_name\": \"National Park Service\",\n        \"top_level_agency_id\": 156,\n    },\n    {\n        \"agency_id\": 209,\n        \"agency_code\": \"DOI-USGS1\",\n        \"agency_name\": \"Geological Survey\",\n        \"top_level_agency_id\": 156,\n    },\n    {\n        \"agency_id\": 657,\n        \"agency_code\": \"HHS-ACF\",\n        \"agency_name\": \"Administration for Children and Families\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 675,\n        \"agency_code\": \"HHS-AHRQ\",\n        \"agency_name\": \"Agency for Health Care Research and Quality\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 677,\n        \"agency_code\": \"HHS-ACF-OPRE\",\n        \"agency_name\": \"Administration for Children and Families - OPRE\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 679,\n        \"agency_code\": \"HHS-ACF-FYSB\",\n        \"agency_name\": \"Administration for Children & Families - ACYF/FYSB\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 681,\n        \"agency_code\": \"HHS-ACL\",\n        \"agency_name\": \"Administration for Community Living\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 684,\n        \"agency_code\": \"HHS-ACF-OHS\",\n        \"agency_name\": \"Administration for Children and Families - OHS\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 690,\n        \"agency_code\": \"HHS-AOA\",\n        \"agency_name\": \"Administration on Aging\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 691,\n        \"agency_code\": \"HHS-CDC\",\n        \"agency_name\": \"Centers for Disease Control and Prevention\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 692,\n        \"agency_code\": \"HHS-CDC-HHSCDCERA\",\n        \"agency_name\": \"Centers for Disease Control and Prevention - ERA\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 693,\n        \"agency_code\": \"HHS-CDC-NCCDPHP\",\n        \"agency_name\": \"Centers for Disease Control - NCCDPHP\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 696,\n        \"agency_code\": \"HHS-CDC-CGH\",\n        \"agency_name\": \"Centers for Disease Control - CGH\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 707,\n        \"agency_code\": \"HHS-CMS\",\n        \"agency_name\": \"Centers for Medicare & Medicaid Services\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 708,\n        \"agency_code\": \"HHS-FDA\",\n        \"agency_name\": \"Food and Drug Administration\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 711,\n        \"agency_code\": \"HHS-CDC-NCHHSTP\",\n        \"agency_name\": \"Centers for Disease Control - NCHHSTP\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 725,\n        \"agency_code\": \"HHS-OPHS\",\n        \"agency_name\": \"Office of the Assistant Secretary for Health\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 726,\n        \"agency_code\": \"HHS-HRSA\",\n        \"agency_name\": \"Health Resources and Services Administration\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 727,\n        \"agency_code\": \"HHS-NIH11\",\n        \"agency_name\": \"National Institutes of Health\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 728,\n        \"agency_code\": \"HHS-IHS\",\n        \"agency_name\": \"Indian Health Service\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 742,\n        \"agency_code\": \"HHS-SAMHS\",\n        \"agency_name\": \"Substance Abuse and Mental Health Services Admin\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 758,\n        \"agency_code\": \"HHS-SAMHS-SAMHSA\",\n        \"agency_name\": \"Substance Abuse and Mental Health Services Adminis\",\n        \"top_level_agency_id\": 674,\n    },\n    {\n        \"agency_id\": 915,\n        \"agency_code\": \"USAID-ETH\",\n        \"agency_name\": \"Ethiopia USAID-Addis Ababa \",\n        \"top_level_agency_id\": 1,\n    },\n    {\n        \"agency_id\": 969,\n        \"agency_code\": \"USAID-SAF\",\n        \"agency_name\": \"South Africa USAID-Pretoria\",\n        \"top_level_agency_id\": 1,\n    },\n    {\n        \"agency_id\": 980,\n        \"agency_code\": \"USDA-AMS\",\n        \"agency_name\": \"Agricultural Marketing Service\",\n        \"top_level_agency_id\": 983,\n    },\n    {\n        \"agency_id\": 981,\n        \"agency_code\": \"USDA-CSREE\",\n        \"agency_name\": \"CSREES\",\n        \"top_level_agency_id\": 983,\n    },\n    {\n        \"agency_id\": 997,\n        \"agency_code\": \"USDA-FAS\",\n        \"agency_name\": \"Foreign Agricultural Service\",\n        \"top_level_agency_id\": 983,\n    },\n    {\n        \"agency_id\": 1014,\n        \"agency_code\": \"USDA-FNS1\",\n        \"agency_name\": \"Food and Nutrition Service\",\n        \"top_level_agency_id\": 983,\n    },\n    {\n        \"agency_id\": 1015,\n        \"agency_code\": \"USDA-NRCS\",\n        \"agency_name\": \"Natural Resources Conservation Service\",\n        \"top_level_agency_id\": 983,\n    },\n    {\n        \"agency_id\": 1016,\n        \"agency_code\": \"USDA-FS\",\n        \"agency_name\": \"Forest Service\",\n        \"top_level_agency_id\": 983,\n    },\n    {\n        \"agency_id\": 1022,\n        \"agency_code\": \"USDA-NIFA\",\n        \"agency_name\": \"National Institute of Food and Agriculture\",\n        \"top_level_agency_id\": 983,\n    },\n    {\n        \"agency_id\": 1085,\n        \"agency_code\": \"USDA-RBCS\",\n        \"agency_name\": \"Rural Business-Cooperative Service \",\n        \"top_level_agency_id\": 983,\n    },\n    {\n        \"agency_id\": 1102,\n        \"agency_code\": \"USDOJ-BOP-NIC\",\n        \"agency_name\": \"National Institute of Corrections\",\n        \"top_level_agency_id\": 1105,\n    },\n    {\n        \"agency_id\": 1104,\n        \"agency_code\": \"USDOJ-OJP-BJA\",\n        \"agency_name\": \"Bureau of Justice Assistance\",\n        \"top_level_agency_id\": 1105,\n    },\n    {\n        \"agency_id\": 1109,\n        \"agency_code\": \"USDA-RUS\",\n        \"agency_name\": \"Rural Utilities Service\",\n        \"top_level_agency_id\": 983,\n    },\n    {\n        \"agency_id\": 1111,\n        \"agency_code\": \"USDOJ-OJP-BJS\",\n        \"agency_name\": \"Bureau of Justice Statistics\",\n        \"top_level_agency_id\": 1105,\n    },\n    {\n        \"agency_id\": 1113,\n        \"agency_code\": \"USDOJ-OJP-NIJ\",\n        \"agency_name\": \"National Institute of Justice\",\n        \"top_level_agency_id\": 1105,\n    },\n    {\n        \"agency_id\": 1114,\n        \"agency_code\": \"USDOJ-OJP-OJJDP\",\n        \"agency_name\": \"Office of Juvenile Justice Delinquency Prevention \",\n        \"top_level_agency_id\": 1105,\n    },\n    {\n        \"agency_id\": 1121,\n        \"agency_code\": \"USDOJ-OJP-OVW\",\n        \"agency_name\": \"Office on Violence Against Women\",\n        \"top_level_agency_id\": 1105,\n    },\n    {\n        \"agency_id\": 1122,\n        \"agency_code\": \"USDOJ-OJP-OVC\",\n        \"agency_name\": \"Office for Victims of Crime\",\n        \"top_level_agency_id\": 1105,\n    },\n]\n\n\ndef _build_agencies(db_session: db.Session) -> None:\n    # Create a static set of agencies, only if they don't already exist\n    agencies = db_session.query(Agency).all()\n    agency_codes = set([a.agency_code for a in agencies])\n\n    for agency_to_create in AGENCIES_TO_CREATE:\n        if agency_to_create[\"agency_code\"] in agency_codes:\n            continue\n\n        logger.info(\"Creating agency %s in agency table\", agency_to_create[\"agency_code\"])\n        factories.AgencyFactory.create(**agency_to_create)"}
{"path":"frontend/src/services/fetch/fetchers/clientUserFetcher.ts","language":"typescript","type":"code","directory":"frontend/src/services/fetch/fetchers","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/fetch/fetchers/clientUserFetcher.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/lib/seed_local_db.py\nLanguage: py\nType: code\nDirectory: api/tests/lib\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/seed_local_db.py\nSize: 2.92 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/services/fetch/fetchers/fetchers.ts","language":"typescript","type":"code","directory":"frontend/src/services/fetch/fetchers","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/fetch/fetchers/fetchers.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"import click\n\nimport src.adapters.db as db\nimport src.logging\nimport src.util.datetime_util as datetime_util\nimport tests.src.db.models.factories as factories\nfrom src.adapters.db import PostgresDBClient\nfrom src.util.local import error_if_not_local\nfrom tests.lib.seed_agencies import _build_agencies\n\nlogger = logging.getLogger(__name__)\n\n\ndef _build_opportunities(db_session: db.Session, iterations: int) -> None:\n    # Just create a variety of opportunities for local testing\n    # we can eventually look into creating more specific scenarios\n    for i in range(iterations):\n        logger.info(f\"Creating opportunity batch number {i}\")\n        # Create a few opportunities in various scenarios\n        factories.OpportunityFactory.create_batch(size=5, is_forecasted_summary=True)\n        factories.OpportunityFactory.create_batch(\n            size=5, is_posted_summary=True, has_attachments=True\n        )\n        factories.OpportunityFactory.create_batch(size=5, is_closed_summary=True)\n        factories.OpportunityFactory.create_batch(size=5, is_archived_non_forecast_summary=True)\n        factories.OpportunityFactory.create_batch(size=5, is_archived_forecast_summary=True)\n        factories.OpportunityFactory.create_batch(size=5, no_current_summary=True)\n        factories.OpportunityFactory.create_batch(\n            size=2, is_posted_summary=True, has_long_descriptions=True\n        )\n\n        # generate a few opportunities with mostly null values\n        all_null_opportunities = factories.OpportunityFactory.create_batch(\n            size=5, all_fields_null=True\n        )\n        for all_null_opportunity in all_null_opportunities:\n            summary = factories.OpportunitySummaryFactory.create(\n                # We  set post_date to something so that running the set-current-opportunities logic\n                # won't get rid of it for having a null post date\n                all_fields_null=True,\n                opportunity=all_null_opportunity,\n                post_date=datetime_util.get_now_us_eastern_date(),\n            )\n            factories.CurrentOpportunitySummaryFactory.create(\n                opportunity=all_null_opportunity, opportunity_summary=summary\n            )\n\n    logger.info(\"Finished creating opportunities\")\n\n\n@click.command()\n@click.option(\n    \"--iterations\",\n    default=1,\n    help=\"Number of sets of opportunities to create, note that several are created per iteration\",\n)\ndef seed_local_db(iterations: int) -> None:\n    with src.logging.init(\"seed_local_db\"):\n        logger.info(\"Running seed script for local DB\")\n        error_if_not_local()\n\n        db_client = PostgresDBClient()\n\n        with db_client.get_session() as db_session:\n            factories._db_session = db_session\n\n            _build_opportunities(db_session, iterations)\n            # Need to commit to force any updates made\n            # after factories created objects\n            db_session.commit()\n\n            _build_agencies(db_session)"}
{"path":"frontend/src/services/fetch/fetchers/opportunityFetcher.ts","language":"typescript","type":"code","directory":"frontend/src/services/fetch/fetchers","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/fetch/fetchers/opportunityFetcher.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/lib/seed_local_legacy_tables.py\nLanguage: py\nType: code\nDirectory: api/tests/lib\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/lib/seed_local_legacy_tables.py\nSize: 2.91 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/services/fetch/fetchers/savedOpportunityFetcher.ts","language":"typescript","type":"code","directory":"frontend/src/services/fetch/fetchers","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/fetch/fetchers/savedOpportunityFetcher.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"import datetime\nimport logging\n\nimport factory\nimport sqlalchemy\n\nimport src.adapters.db\nimport src.logging\nimport tests.src.db.models.factories as factories\nfrom src.db.models import foreign\n\nlogger = logging.getLogger(__name__)\n\n\ndef seed_local_source_tables() -> None:\n    with src.logging.init(\"seed_local_source_tables\"):\n        logger.info(\"populating source tables with mock data\")\n\n        db_client = src.adapters.db.PostgresDBClient()\n\n        with db_client.get_session() as db_session:\n            factories._db_session = db_session\n            update_and_append_data(db_session)\n\n        logger.info(\"populating source tables done\")\n\n\ndef update_and_append_data(db_session):\n    max_opportunity_id = get_max_opportunity_id(db_session)\n    if max_opportunity_id:\n        update_existing_data(db_session, max_opportunity_id)\n    append_new_data(db_session, max_opportunity_id)\n    db_session.commit()\n\n\ndef update_existing_data(db_session, max_opportunity_id):\n    \"\"\"Update a subset of existing opportunities in the source tables.\"\"\"\n\n    # Every record with id ending in 001:\n    update_ids = set(range(1, max_opportunity_id, 100))\n    # Plus the 20 most recently created records:\n    update_ids |= set(range(max_opportunity_id - 19, max_opportunity_id + 1))\n\n    logger.info(\"updating rows %r\" % sorted(update_ids))\n\n    for opportunity_id in update_ids:\n        factory.random.reseed_random(opportunity_id + max_opportunity_id)\n        factories.ForeignTopportunityFactory.reset_sequence(opportunity_id, force=True)\n\n        updated_opportunity = factories.ForeignTopportunityFactory.build()._dict()\n        updated_opportunity.pop(\"created_date\")\n        updated_opportunity[\"last_upd_date\"] = datetime.datetime.now()\n\n        db_session.execute(\n            sqlalchemy.update(foreign.opportunity.Topportunity)\n            .where(foreign.opportunity.Topportunity.opportunity_id == opportunity_id)\n            .values(**updated_opportunity)\n        )\n\n\ndef append_new_data(db_session, max_opportunity_id):\n    count = 5000 if max_opportunity_id == 0 else 500\n    generate_batch(db_session, max_opportunity_id + 1, count)\n\n\ndef generate_batch(db_session, start_id, count):\n    \"\"\"Generate a reproducible batch of opportunities in the source tables.\"\"\"\n    logger.info(\"appending batch of %i rows starting with id %i\" % (count, start_id))\n\n    factory.random.reseed_random(start_id)\n    factories.ForeignTopportunityFactory.reset_sequence(start_id, force=True)\n\n    factories.ForeignTopportunityFactory.create_batch(size=count)\n\n\ndef get_max_opportunity_id(db_session):\n    max_opportunity_id = db_session.query(\n        sqlalchemy.func.max(foreign.opportunity.Topportunity.opportunity_id)\n    ).scalar()\n    logger.info(\"max(opportunity_id) = %r\" % max_opportunity_id)\n    if max_opportunity_id is None:\n        return 0\n    return int(max_opportunity_id)\n\n\nif __name__ == \"__main__\":\n    seed_local_source_tables()"}
{"path":"frontend/src/services/fetch/fetchers/searchFetcher.ts","language":"typescript","type":"code","directory":"frontend/src/services/fetch/fetchers","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/fetch/fetchers/searchFetcher.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/services/fetch/fetchers/userFetcher.ts","language":"typescript","type":"code","directory":"frontend/src/services/fetch/fetchers","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/fetch/fetchers/userFetcher.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":""}
{"path":"frontend/src/services/search/SearchFilterManager.ts","language":"typescript","type":"code","directory":"frontend/src/services/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/services/search/SearchFilterManager.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/styles/_loading.scss","language":"unknown","type":"code","directory":"frontend/src/styles","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/styles/_loading.scss","size":0,"lastModified":"2025-02-14T17:08:31.134Z","content":""}
{"path":"frontend/src/styles/_uswds-theme-custom-styles.scss","language":"unknown","type":"code","directory":"frontend/src/styles","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/styles/_uswds-theme-custom-styles.scss","size":0,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/aws/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/aws\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/aws/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/styles/_uswds-theme.scss","language":"unknown","type":"code","directory":"frontend/src/styles","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/styles/_uswds-theme.scss","size":0,"lastModified":"2025-02-14T17:08:31.134Z","content":""}
{"path":"frontend/src/styles/styles.scss","language":"unknown","type":"code","directory":"frontend/src/styles","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/styles/styles.scss","size":0,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/aws/test_pinpoint_adapter.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/aws\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/aws/test_pinpoint_adapter.py\nSize: 2.60 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/types/18n.d.ts","language":"typescript","type":"code","directory":"frontend/src/types","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/types/18n.d.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"def test_send_pinpoint_email_raw_with_mock():\n    \"\"\"Just a quick sanity check that the pinpoint mocking has a behavior we can use for other tests\"\"\"\n    resp1 = send_pinpoint_email_raw(\n        to_address=\"fake_mail1@fake.com\",\n        subject=\"email subject\",\n        message=\"this is an email\",\n        app_id=\"fake_app_id1\",\n    )\n    resp2 = send_pinpoint_email_raw(\n        to_address=\"fake_mail2@fake.com\",\n        subject=\"different subject\",\n        message=\"different email\",\n        app_id=\"fake_app_id2\",\n    )\n    resp3 = send_pinpoint_email_raw(\n        to_address=\"fake_mail3@fake.com\",\n        subject=\"another subject\",\n        message=\"another email\",\n        app_id=\"fake_app_id2\",\n    )\n    mock_responses = _get_mock_responses()\n    assert len(mock_responses) == 3\n\n    req_resp1 = mock_responses[0]\n    assert resp1 == req_resp1[1]\n    req1 = req_resp1[0]\n    assert req1[\"ApplicationId\"] == \"fake_app_id1\"\n    assert req1[\"MessageRequest\"][\"Addresses\"] == {\"fake_mail1@fake.com\": {\"ChannelType\": \"EMAIL\"}}\n    assert (\n        req1[\"MessageRequest\"][\"MessageConfiguration\"][\"EmailMessage\"][\"SimpleEmail\"][\"Subject\"][\n            \"Data\"\n        ]\n        == \"email subject\"\n    )\n    assert (\n        req1[\"MessageRequest\"][\"MessageConfiguration\"][\"EmailMessage\"][\"SimpleEmail\"][\"HtmlPart\"][\n            \"Data\"\n        ]\n        == \"this is an email\"\n    )\n\n    req_resp2 = mock_responses[1]\n    assert resp2 == req_resp2[1]\n    req2 = req_resp2[0]\n    assert req2[\"ApplicationId\"] == \"fake_app_id2\"\n    assert req2[\"MessageRequest\"][\"Addresses\"] == {\"fake_mail2@fake.com\": {\"ChannelType\": \"EMAIL\"}}\n    assert (\n        req2[\"MessageRequest\"][\"MessageConfiguration\"][\"EmailMessage\"][\"SimpleEmail\"][\"Subject\"][\n            \"Data\"\n        ]\n        == \"different subject\"\n    )\n    assert (\n        req2[\"MessageRequest\"][\"MessageConfiguration\"][\"EmailMessage\"][\"SimpleEmail\"][\"HtmlPart\"][\n            \"Data\"\n        ]\n        == \"different email\"\n    )\n\n    req_resp3 = mock_responses[2]\n    assert resp3 == req_resp3[1]\n    req3 = req_resp3[0]\n    assert req3[\"ApplicationId\"] == \"fake_app_id2\"\n    assert req3[\"MessageRequest\"][\"Addresses\"] == {\"fake_mail3@fake.com\": {\"ChannelType\": \"EMAIL\"}}\n    assert (\n        req3[\"MessageRequest\"][\"MessageConfiguration\"][\"EmailMessage\"][\"SimpleEmail\"][\"Subject\"][\n            \"Data\"\n        ]\n        == \"another subject\"\n    )\n    assert (\n        req3[\"MessageRequest\"][\"MessageConfiguration\"][\"EmailMessage\"][\"SimpleEmail\"][\"HtmlPart\"][\n            \"Data\"\n        ]\n        == \"another email\"\n    )"}
{"path":"frontend/src/types/apiResponseTypes.ts","language":"typescript","type":"code","directory":"frontend/src/types","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/types/apiResponseTypes.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/db/clients/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/db/clients\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/db/clients/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/types/generalTypes.ts","language":"typescript","type":"code","directory":"frontend/src/types","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/types/generalTypes.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":""}
{"path":"frontend/src/types/intl.ts","language":"typescript","type":"code","directory":"frontend/src/types","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/types/intl.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/db/clients/test_postgres_client.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/db/clients\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/db/clients/test_postgres_client.py\nSize: 1.35 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/types/opportunity/opportunityResponseTypes.ts","language":"typescript","type":"code","directory":"frontend/src/types/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/types/opportunity/opportunityResponseTypes.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"import pytest\n\nfrom src.adapters.db.clients.postgres_client import get_connection_parameters, verify_ssl\nfrom src.adapters.db.clients.postgres_config import get_db_config\n\n\n@dataclass\nclass DummyPgConn:\n    ssl_in_use: bool\n\n\nclass DummyConnectionInfo:\n    def __init__(self, ssl_in_use):\n        self.pgconn = DummyPgConn(ssl_in_use)\n\n\ndef test_verify_ssl(caplog):\n    caplog.set_level(logging.INFO)  # noqa: B1\n\n    conn_info = DummyConnectionInfo(True)\n    verify_ssl(conn_info)\n\n    assert caplog.messages == [\"database connection is using SSL\"]\n    assert caplog.records[0].levelname == \"INFO\"\n\n\ndef test_verify_ssl_not_in_use(caplog):\n    caplog.set_level(logging.INFO)  # noqa: B1\n\n    conn_info = DummyConnectionInfo(False)\n    verify_ssl(conn_info)\n\n    assert caplog.messages == [\"database connection is not using SSL\"]\n    assert caplog.records[0].levelname == \"WARNING\"\n\n\ndef test_get_connection_parameters(monkeypatch: pytest.MonkeyPatch):\n    monkeypatch.delenv(\"DB_SSL_MODE\")\n    db_config = get_db_config()\n    conn_params = get_connection_parameters(db_config)\n\n    assert conn_params == dict(\n        host=db_config.host,\n        dbname=db_config.name,\n        user=db_config.username,\n        password=db_config.password,\n        port=db_config.port,\n        connect_timeout=10,\n        sslmode=\"require\",\n    )"}
{"path":"frontend/src/types/saved-opportunity/savedOpportunityResponseTypes.ts","language":"typescript","type":"code","directory":"frontend/src/types/saved-opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/types/saved-opportunity/savedOpportunityResponseTypes.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/db/test_db.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/db\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/db/test_db.py\nSize: 0.65 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/types/search/searchRequestTypes.ts","language":"typescript","type":"code","directory":"frontend/src/types/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/types/search/searchRequestTypes.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"import src.adapters.db as db\n\n\ndef test_db_connection(db_client):\n    db_client = db.PostgresDBClient()\n    with db_client.get_connection() as conn:\n        assert conn.scalar(text(\"SELECT 1\")) == 1\n\n\ndef test_check_db_connection(caplog, monkeypatch: pytest.MonkeyPatch):\n    monkeypatch.setenv(\"DB_CHECK_CONNECTION_ON_INIT\", \"True\")\n    db.PostgresDBClient()\n    assert \"database connection is not using SSL\" in caplog.messages\n\n\ndef test_get_session():\n    db_client = db.PostgresDBClient()\n    with db_client.get_session() as session:\n        with session.begin():\n            assert session.scalar(text(\"SELECT 1\")) == 1"}
{"path":"frontend/src/types/search/searchResponseTypes.ts","language":"typescript","type":"code","directory":"frontend/src/types/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/types/search/searchResponseTypes.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/db/test_flask_db.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/db\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/db/test_flask_db.py\nSize: 1.70 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/types/uiTypes.ts","language":"typescript","type":"code","directory":"frontend/src/types","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/types/uiTypes.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"import src.adapters.db as db\nimport src.adapters.db.flask_db as flask_db\n\n\n# Define an isolated example Flask app fixture specific to this test module\n# to avoid dependencies on any project-specific fixtures in conftest.py\n@pytest.fixture\ndef example_app() -> Flask:\n    app = Flask(__name__)\n    db_client = db.PostgresDBClient()\n    flask_db.register_db_client(db_client, app)\n    return app\n\n\ndef test_get_db(example_app: Flask):\n    @example_app.route(\"/hello\")\n    def hello():\n        with flask_db.get_db(current_app).get_connection() as conn:\n            return {\"data\": conn.scalar(text(\"SELECT 'hello, world'\"))}\n\n    response = example_app.test_client().get(\"/hello\")\n    assert response.get_json() == {\"data\": \"hello, world\"}\n\n\ndef test_with_db_session(example_app: Flask):\n    @example_app.route(\"/hello\")\n    @flask_db.with_db_session()\n    def hello(db_session: db.Session):\n        with db_session.begin():\n            return {\"data\": db_session.scalar(text(\"SELECT 'hello, world'\"))}\n\n    response = example_app.test_client().get(\"/hello\")\n    assert response.get_json() == {\"data\": \"hello, world\"}\n\n\ndef test_with_db_session_not_default_name(example_app: Flask):\n    db_client = db.PostgresDBClient()\n    flask_db.register_db_client(db_client, example_app, client_name=\"something_else\")\n\n    @example_app.route(\"/hello\")\n    @flask_db.with_db_session(client_name=\"something_else\")\n    def hello(db_session: db.Session):\n        with db_session.begin():\n            return {\"data\": db_session.scalar(text(\"SELECT 'hello, world'\"))}\n\n    response = example_app.test_client().get(\"/hello\")\n    assert response.get_json() == {\"data\": \"hello, world\"}"}
{"path":"frontend/src/utils/authUtil.ts","language":"typescript","type":"code","directory":"frontend/src/utils","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/utils/authUtil.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/db/type_decorators/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/db/type_decorators\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/db/type_decorators/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/utils/dateUtil.ts","language":"typescript","type":"code","directory":"frontend/src/utils","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/utils/dateUtil.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":""}
{"path":"frontend/src/utils/generalUtils.ts","language":"typescript","type":"code","directory":"frontend/src/utils","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/utils/generalUtils.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/db/type_decorators/test_postgres_type_decorators.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/db/type_decorators\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/db/type_decorators/test_postgres_type_decorators.py\nSize: 2.14 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/utils/getRoutes.ts","language":"typescript","type":"code","directory":"frontend/src/utils","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/utils/getRoutes.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"import pytest\nfrom sqlalchemy import text\n\nfrom src.adapters.db.type_decorators.postgres_type_decorators import LookupColumn\nfrom src.constants.lookup_constants import OpportunityCategory\nfrom src.db.models.lookup_models import LkOpportunityCategory\nfrom src.db.models.opportunity_models import Opportunity\nfrom tests.src.db.models.factories import OpportunityFactory\n\n\n@pytest.mark.parametrize(\n    \"category,db_value\",\n    [(OpportunityCategory.CONTINUATION, 3), (OpportunityCategory.EARMARK, 4), (None, None)],\n)\ndef test_lookup_column_conversion(\n    db_session, enable_factory_create, category, db_value, test_api_schema\n):\n    # Verify column works with factories\n    opportunity = OpportunityFactory.create(category=category)\n    assert opportunity.category == category\n\n    # Verify fetching from the DB works\n    db_session.expire_all()\n\n    opportunity_db = (\n        db_session.query(Opportunity)\n        .where(Opportunity.opportunity_id == opportunity.opportunity_id)\n        .first()\n    )\n    assert opportunity_db.category == category\n\n    # Verify what we stored in the DB is the integer\n    raw_db_value = db_session.execute(\n        text(\n            f\"select opportunity_category_id from {test_api_schema}.{Opportunity.get_table_name()} where opportunity_id={opportunity.opportunity_id}\"  # nosec\n        )\n    ).scalar()\n    assert raw_db_value == db_value\n\n\ndef test_lookup_column_bind_type_invalid():\n    lookup_column = LookupColumn(LkOpportunityCategory)\n    with pytest.raises(Exception, match=\"Cannot convert value of type\"):\n        lookup_column.process_bind_param(\"hello\", None)\n\n    class TestEnum(StrEnum):\n        DISCRETIONARY = \"D\"\n\n    # Verify that just because an enum looks similar, if it's a different\n    # type it will also error\n    with pytest.raises(Exception, match=\"Cannot convert value of type\"):\n        lookup_column.process_bind_param(TestEnum.DISCRETIONARY, None)\n\n\ndef test_lookup_column_process_result_type_invalid():\n    lookup_column = LookupColumn(LkOpportunityCategory)\n    with pytest.raises(Exception, match=\"Cannot process value from DB of type\"):\n        lookup_column.process_result_value(\"hello\", None)"}
{"path":"frontend/src/utils/opportunity/isSummary.ts","language":"typescript","type":"code","directory":"frontend/src/utils/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/utils/opportunity/isSummary.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/oauth/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/oauth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/oauth/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/utils/search/convertSearchParamsToProperTypes.ts","language":"typescript","type":"code","directory":"frontend/src/utils/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/utils/search/convertSearchParamsToProperTypes.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":""}
{"path":"frontend/src/utils/search/generateAgencyNameLookup.ts","language":"typescript","type":"code","directory":"frontend/src/utils/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/utils/search/generateAgencyNameLookup.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/oauth/login_gov/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/oauth/login_gov\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/oauth/login_gov/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/utils/testing/FeatureFlagTestUtils.ts","language":"typescript","type":"code","directory":"frontend/src/utils/testing","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/utils/testing/FeatureFlagTestUtils.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":""}
{"path":"frontend/src/utils/testing/commonTestUtils.ts","language":"typescript","type":"code","directory":"frontend/src/utils/testing","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/utils/testing/commonTestUtils.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/oauth/login_gov/test_login_gov_oauth_client.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/oauth/login_gov\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/oauth/login_gov/test_login_gov_oauth_client.py\nSize: 1.79 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/src/utils/testing/intlMocks.ts","language":"typescript","type":"code","directory":"frontend/src/utils/testing","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/utils/testing/intlMocks.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"import requests\n\nfrom src.adapters.oauth.login_gov.login_gov_oauth_client import LoginGovOauthClient\nfrom src.adapters.oauth.oauth_client_models import OauthTokenRequest\nfrom src.auth.login_gov_jwt_auth import LoginGovConfig\n\n\ndef mock_response(monkeypatch, mocked_response: dict):\n    def mock_post(*args, **kwargs):\n        response = requests.Response()\n        # _content is fetched by the text method which we use when deserializing\n        response._content = bytes(json.dumps(mocked_response), \"utf-8\")\n        return response\n\n    monkeypatch.setattr(\"requests.Session.request\", mock_post)\n\n\ndef test_get_token(monkeypatch):\n\n    mock_response(\n        monkeypatch,\n        {\"id_token\": \"abc123\", \"access_token\": \"xyz456\", \"token_type\": \"Bearer\", \"expires_in\": 300},\n    )\n\n    client = LoginGovOauthClient(LoginGovConfig())\n    resp = client.get_token(OauthTokenRequest(code=\"abc123\", client_assertion=\"fake_token\"))\n\n    assert resp.id_token == \"abc123\"\n    assert resp.access_token == \"xyz456\"\n    assert resp.token_type == \"Bearer\"\n    assert resp.expires_in == 300\n    assert resp.error is None\n    assert resp.error_description is None\n    assert resp.is_error_response() is False\n\n\ndef test_get_token_error(monkeypatch):\n    mock_response(\n        monkeypatch,\n        {\"error\": \"invalid_request\", \"error_description\": \"missing required parameter grant_type\"},\n    )\n\n    client = LoginGovOauthClient(LoginGovConfig())\n    resp = client.get_token(OauthTokenRequest(code=\"abc123\", client_assertion=\"fake_token\"))\n\n    assert resp.id_token == \"\"\n    assert resp.access_token == \"\"\n    assert resp.token_type == \"\"\n    assert resp.expires_in == 0\n    assert resp.error == \"invalid_request\"\n    assert resp.error_description == \"missing required parameter grant_type\"\n    assert resp.is_error_response() is True"}
{"path":"frontend/src/utils/testing/jsdomNodeEnvironment.ts","language":"typescript","type":"code","directory":"frontend/src/utils/testing","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/utils/testing/jsdomNodeEnvironment.ts","size":324715,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/search/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/search/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.457Z"}
{"path":"frontend/stories/components/BetaAlert.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/BetaAlert.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.134Z","content":""}
{"path":"frontend/stories/components/Breadcrumbs.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/Breadcrumbs.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/search/test_opensearch_client.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/search/test_opensearch_client.py\nSize: 9.29 KB\nLast Modified: 2025-02-14T17:08:26.458Z"}
{"path":"frontend/stories/components/ContentDisplayToggle.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/ContentDisplayToggle.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.134Z","content":"import opensearchpy\nimport pytest\n\nfrom src.adapters.search import get_opensearch_config\nfrom src.adapters.search.opensearch_client import _get_connection_parameters\n\n########################################################################\n# These tests are primarily looking to validate\n# that our wrappers around the OpenSearch client\n# are being used correctly / account for error cases correctly.\n#\n# We are not validating all the intricacies of OpenSearch itself.\n########################################################################\n\n\n@pytest.fixture\ndef generic_index(search_client):\n    # This is very similar to the opportunity_index fixture, but\n    # is reused per unit test rather than a global value\n    index_name = f\"test-index-{uuid.uuid4().int}\"\n\n    search_client.create_index(index_name)\n\n    try:\n        yield index_name\n    finally:\n        # Try to clean up the index at the end\n        search_client.delete_index(index_name)\n\n\ndef test_create_and_delete_index_duplicate(search_client):\n    index_name = f\"test-index-{uuid.uuid4().int}\"\n\n    search_client.create_index(index_name)\n    with pytest.raises(Exception, match=\"already exists\"):\n        search_client.create_index(index_name)\n\n    # Cleanup the index\n    search_client.delete_index(index_name)\n    with pytest.raises(Exception, match=\"no such index\"):\n        search_client.delete_index(index_name)\n\n\ndef test_bulk_upsert(search_client, generic_index):\n    records = [\n        {\"id\": 1, \"title\": \"Green Eggs & Ham\", \"notes\": \"why are the eggs green?\"},\n        {\"id\": 2, \"title\": \"The Cat in the Hat\", \"notes\": \"silly cat wears a hat\"},\n        {\"id\": 3, \"title\": \"One Fish, Two Fish, Red Fish, Blue Fish\", \"notes\": \"fish\"},\n    ]\n\n    search_client.bulk_upsert(generic_index, records, primary_key_field=\"id\")\n    # Verify the records are in the index\n    for record in records:\n        assert search_client._client.get(generic_index, record[\"id\"])[\"_source\"] == record\n\n    # Can update + add more\n    records = [\n        {\"id\": 1, \"title\": \"Green Eggs & Ham\", \"notes\": \"Sam, eat the eggs\"},\n        {\"id\": 2, \"title\": \"The Cat in the Hat\", \"notes\": \"watch the movie\"},\n        {\"id\": 3, \"title\": \"One Fish, Two Fish, Red Fish, Blue Fish\", \"notes\": \"colors & numbers\"},\n        {\"id\": 4, \"title\": \"How the Grinch Stole Christmas\", \"notes\": \"who\"},\n    ]\n    search_client.bulk_upsert(generic_index, records, primary_key_field=\"id\")\n\n    for record in records:\n        assert search_client._client.get(generic_index, record[\"id\"])[\"_source\"] == record\n\n\ndef test_bulk_delete(search_client, generic_index):\n    records = [\n        {\"id\": 1, \"title\": \"Green Eggs & Ham\", \"notes\": \"why are the eggs green?\"},\n        {\"id\": 2, \"title\": \"The Cat in the Hat\", \"notes\": \"silly cat wears a hat\"},\n        {\"id\": 3, \"title\": \"One Fish, Two Fish, Red Fish, Blue Fish\", \"notes\": \"fish\"},\n    ]\n\n    search_client.bulk_upsert(generic_index, records, primary_key_field=\"id\")\n\n    search_client.bulk_delete(generic_index, [1])\n\n    resp = search_client.search(generic_index, {}, include_scores=False)\n    assert resp.records == records[1:]\n\n    search_client.bulk_delete(generic_index, [2, 3])\n    resp = search_client.search(generic_index, {}, include_scores=False)\n    assert resp.records == []\n\n\ndef test_swap_alias_index(search_client, generic_index):\n    alias_name = f\"tmp-alias-{uuid.uuid4().int}\"\n\n    # Populate the generic index, we won't immediately use this one\n    records = [\n        {\"id\": 1, \"data\": \"abc123\"},\n        {\"id\": 2, \"data\": \"def456\"},\n        {\"id\": 3, \"data\": \"xyz789\"},\n    ]\n    search_client.bulk_upsert(generic_index, records, primary_key_field=\"id\")\n\n    # Create a different index that we'll attach to the alias first.\n    tmp_index = f\"test-tmp-index-{uuid.uuid4().int}\"\n    search_client.create_index(tmp_index)\n    # Add a few records\n    tmp_index_records = [\n        {\"id\": 1, \"data\": \"abc123\"},\n        {\"id\": 2, \"data\": \"xyz789\"},\n    ]\n    search_client.bulk_upsert(tmp_index, tmp_index_records, primary_key_field=\"id\")\n\n    # Set the alias\n    search_client.swap_alias_index(tmp_index, alias_name)\n\n    # Can search by this alias and get records from the tmp index\n    resp = search_client.search(alias_name, {}, include_scores=False)\n    assert resp.records == tmp_index_records\n\n    # Swap the index to the generic one + delete the tmp one\n    search_client.swap_alias_index(generic_index, alias_name)\n    search_client.cleanup_old_indices(\"test-tmp-index\", [generic_index])\n\n    resp = search_client.search(alias_name, {}, include_scores=False)\n    assert resp.records == records\n\n    # Verify the tmp one was deleted\n    assert search_client._client.indices.exists(tmp_index) is False\n\n\ndef test_index_or_alias_exists(search_client, generic_index):\n    # Create a few aliased indexes\n    index_a = f\"test-index-a-{uuid.uuid4().int}\"\n    index_b = f\"test-index-b-{uuid.uuid4().int}\"\n    index_c = f\"test-index-c-{uuid.uuid4().int}\"\n\n    search_client.create_index(index_a)\n    search_client.create_index(index_b)\n    search_client.create_index(index_c)\n\n    alias_index_a = f\"test-alias-a-{uuid.uuid4().int}\"\n    alias_index_b = f\"test-alias-b-{uuid.uuid4().int}\"\n    alias_index_c = f\"test-alias-c-{uuid.uuid4().int}\"\n\n    search_client.swap_alias_index(index_a, alias_index_a)\n    search_client.swap_alias_index(index_b, alias_index_b)\n    search_client.swap_alias_index(index_c, alias_index_c)\n\n    # Checking the indexes directly - we expect the index method to return true\n    # and the alias method to not\n    assert search_client.index_exists(index_a) is True\n    assert search_client.index_exists(index_b) is True\n    assert search_client.index_exists(index_c) is True\n\n    assert search_client.alias_exists(index_a) is False\n    assert search_client.alias_exists(index_b) is False\n    assert search_client.alias_exists(index_c) is False\n\n    # We just created these aliases, they should exist\n    assert search_client.index_exists(alias_index_a) is True\n    assert search_client.index_exists(alias_index_b) is True\n    assert search_client.index_exists(alias_index_c) is True\n\n    assert search_client.alias_exists(alias_index_a) is True\n    assert search_client.alias_exists(alias_index_b) is True\n    assert search_client.alias_exists(alias_index_c) is True\n\n    # Other random things won't be found for either case\n    assert search_client.index_exists(\"test-index-a\") is False\n    assert search_client.index_exists(\"asdasdasd\") is False\n    assert search_client.index_exists(alias_index_a + \"-other\") is False\n\n    assert search_client.alias_exists(\"test-index-a\") is False\n    assert search_client.alias_exists(\"asdasdasd\") is False\n    assert search_client.alias_exists(alias_index_a + \"-other\") is False\n\n\ndef test_scroll(search_client, generic_index):\n    records = [\n        {\"id\": 1, \"title\": \"Green Eggs & Ham\", \"notes\": \"why are the eggs green?\"},\n        {\"id\": 2, \"title\": \"The Cat in the Hat\", \"notes\": \"silly cat wears a hat\"},\n        {\"id\": 3, \"title\": \"One Fish, Two Fish, Red Fish, Blue Fish\", \"notes\": \"fish\"},\n        {\"id\": 4, \"title\": \"Fox in Socks\", \"notes\": \"why he wearing socks?\"},\n        {\"id\": 5, \"title\": \"The Lorax\", \"notes\": \"trees\"},\n        {\"id\": 6, \"title\": \"Oh, the Places You'll Go\", \"notes\": \"graduation gift\"},\n        {\"id\": 7, \"title\": \"Hop on Pop\", \"notes\": \"Let him sleep\"},\n        {\"id\": 8, \"title\": \"How the Grinch Stole Christmas\", \"notes\": \"who\"},\n    ]\n\n    search_client.bulk_upsert(generic_index, records, primary_key_field=\"id\")\n\n    results = []\n\n    for response in search_client.scroll(generic_index, {\"size\": 3}):\n        assert response.total_records == 8\n        results.append(response)\n\n    assert len(results) == 3\n    assert len(results[0].records) == 3\n    assert len(results[1].records) == 3\n    assert len(results[2].records) == 2\n\n\ndef test_get_connection_parameters():\n    # Just validating this builds as expected for local mode\n    config = get_opensearch_config()\n    params = _get_connection_parameters(config)\n\n    # Mostly validating defaults get used\n    assert params == {\n        \"hosts\": [{\"host\": config.search_endpoint, \"port\": 9200}],\n        \"http_compress\": True,\n        \"use_ssl\": False,\n        \"verify_certs\": False,\n        \"connection_class\": opensearchpy.RequestsHttpConnection,\n        \"pool_maxsize\": 10,\n    }\n\n\ndef test_cleanup_old_indices(search_client):\n    index_name_1 = f\"test-index-{uuid.uuid4().int}\"  # old index\n    index_name_2 = f\"test-index-{uuid.uuid4().int}\"  # old index\n    index_name_3 = f\"partial-refresh-index-{uuid.uuid4().int}\"  # old index\n    index_name_4 = f\"test-index-{uuid.uuid4().int}\"  # new index\n\n    search_client.create_index(index_name_1)\n    search_client.create_index(index_name_2)\n    search_client.create_index(index_name_3)\n    search_client.create_index(index_name_4)\n\n    # check all indexes were created\n    assert search_client.index_exists(index_name_1) is True\n    assert search_client.index_exists(index_name_2) is True\n    assert search_client.index_exists(index_name_3) is True\n    assert search_client.index_exists(index_name_4) is True\n\n    # expect old index with same prefix to be deleted and others to remain\n    search_client.cleanup_old_indices(\"test-index\", [index_name_4])\n\n    assert search_client.index_exists(index_name_1) is False\n    assert search_client.index_exists(index_name_2) is False\n    assert search_client.index_exists(index_name_3) is True\n    assert search_client.index_exists(index_name_4) is True"}
{"path":"frontend/stories/components/ContentLayout.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/ContentLayout.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.134Z","content":"File: api/tests/src/adapters/search/test_opensearch_query_builder.py\nLanguage: py\nType: code\nDirectory: api/tests/src/adapters/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/adapters/search/test_opensearch_query_builder.py\nSize: 24.03 KB\nLast Modified: 2025-02-14T17:08:26.458Z"}
{"path":"frontend/stories/components/Footer.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/Footer.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.134Z","content":"import pytest\n\nfrom src.adapters.search.opensearch_query_builder import SearchQueryBuilder\nfrom src.pagination.pagination_models import SortDirection\nfrom tests.conftest import BaseTestClass\n\nWAY_OF_KINGS = {\n    \"id\": 1,\n    \"title\": \"The Way of Kings\",\n    \"author\": \"Brandon Sanderson\",\n    \"in_stock\": True,\n    \"page_count\": 1007,\n    \"publication_date\": \"2010-08-31\",\n}\nWORDS_OF_RADIANCE = {\n    \"id\": 2,\n    \"title\": \"Words of Radiance\",\n    \"author\": \"Brandon Sanderson\",\n    \"in_stock\": False,\n    \"page_count\": 1087,\n    \"publication_date\": \"2014-03-04\",\n}\nOATHBRINGER = {\n    \"id\": 3,\n    \"title\": \"Oathbringer\",\n    \"author\": \"Brandon Sanderson\",\n    \"in_stock\": True,\n    \"page_count\": 1248,\n    \"publication_date\": \"2017-11-14\",\n}\nRHYTHM_OF_WAR = {\n    \"id\": 4,\n    \"title\": \"Rhythm of War\",\n    \"author\": \"Brandon Sanderson\",\n    \"in_stock\": False,\n    \"page_count\": 1232,\n    \"publication_date\": \"2020-11-17\",\n}\nGAME_OF_THRONES = {\n    \"id\": 5,\n    \"title\": \"A Game of Thrones\",\n    \"author\": \"George R.R. Martin\",\n    \"in_stock\": True,\n    \"page_count\": 694,\n    \"publication_date\": \"1996-08-01\",\n}\nCLASH_OF_KINGS = {\n    \"id\": 6,\n    \"title\": \"A Clash of Kings\",\n    \"author\": \"George R.R. Martin\",\n    \"in_stock\": True,\n    \"page_count\": 768,\n    \"publication_date\": \"1998-11-16\",\n}\nSTORM_OF_SWORDS = {\n    \"id\": 7,\n    \"title\": \"A Storm of Swords\",\n    \"author\": \"George R.R. Martin\",\n    \"in_stock\": True,\n    \"page_count\": 973,\n    \"publication_date\": \"2000-08-08\",\n}\nFEAST_FOR_CROWS = {\n    \"id\": 8,\n    \"title\": \"A Feast for Crows\",\n    \"author\": \"George R.R. Martin\",\n    \"in_stock\": True,\n    \"page_count\": 753,\n    \"publication_date\": \"2005-10-17\",\n}\nDANCE_WITH_DRAGONS = {\n    \"id\": 9,\n    \"title\": \"A Dance with Dragons\",\n    \"author\": \"George R.R. Martin\",\n    \"in_stock\": False,\n    \"page_count\": 1056,\n    \"publication_date\": \"2011-07-12\",\n}\nFELLOWSHIP_OF_THE_RING = {\n    \"id\": 10,\n    \"title\": \"The Fellowship of the Ring\",\n    \"author\": \"J R.R. Tolkien\",\n    \"in_stock\": True,\n    \"page_count\": 423,\n    \"publication_date\": \"1954-07-29\",\n}\nTWO_TOWERS = {\n    \"id\": 11,\n    \"title\": \"The Two Towers\",\n    \"author\": \"J R.R. Tolkien\",\n    \"in_stock\": True,\n    \"page_count\": 352,\n    \"publication_date\": \"1954-11-11\",\n}\nRETURN_OF_THE_KING = {\n    \"id\": 12,\n    \"title\": \"The Return of the King\",\n    \"author\": \"J R.R. Tolkien\",\n    \"in_stock\": True,\n    \"page_count\": 416,\n    \"publication_date\": \"1955-10-20\",\n}\n\nFULL_DATA = [\n    WAY_OF_KINGS,\n    WORDS_OF_RADIANCE,\n    OATHBRINGER,\n    RHYTHM_OF_WAR,\n    GAME_OF_THRONES,\n    CLASH_OF_KINGS,\n    STORM_OF_SWORDS,\n    FEAST_FOR_CROWS,\n    DANCE_WITH_DRAGONS,\n    FELLOWSHIP_OF_THE_RING,\n    TWO_TOWERS,\n    RETURN_OF_THE_KING,\n]\n\n\ndef validate_valid_request(\n    search_client, index, request, expected_results, expected_aggregations=None\n):\n    json_value = request.build()\n    try:\n        resp = search_client.search(index, json_value, include_scores=False)\n\n    except Exception:\n        # If it errors while making the query, catch the exception just to give a message that makes it a bit clearer\n        pytest.fail(\n            f\"Request generated was invalid and caused an error in search client: {json_value}\"\n        )\n\n    assert (\n        resp.records == expected_results\n    ), f\"{[record['title'] for record in resp.records]} != {[expected['title'] for expected in expected_results]}\"\n\n    if expected_aggregations is not None:\n        assert resp.aggregations == expected_aggregations\n\n\nclass TestOpenSearchQueryBuilder(BaseTestClass):\n    @pytest.fixture(scope=\"class\")\n    def search_index(self, search_client):\n        index_name = f\"test-search-index-{uuid.uuid4().int}\"\n\n        search_client.create_index(index_name)\n\n        try:\n            yield index_name\n        finally:\n            # Try to clean up the index at the end\n            search_client.delete_index(index_name)\n\n    @pytest.fixture(scope=\"class\", autouse=True)\n    def seed_data(self, search_client, search_index):\n        search_client.bulk_upsert(search_index, FULL_DATA, primary_key_field=\"id\")\n\n    def test_query_builder_empty(self, search_client, search_index):\n        builder = SearchQueryBuilder()\n\n        assert builder.build() == {\n            \"size\": 25,\n            \"from\": 0,\n            \"track_scores\": True,\n            \"track_total_hits\": True,\n        }\n\n        validate_valid_request(search_client, search_index, builder, FULL_DATA)\n\n    @pytest.mark.parametrize(\n        \"page_size,page_number,sort_values,expected_sort,expected_results\",\n        [\n            ### ID Sorting\n            (25, 1, [(\"id\", SortDirection.ASCENDING)], [{\"id\": {\"order\": \"asc\"}}], FULL_DATA),\n            (3, 1, [(\"id\", SortDirection.ASCENDING)], [{\"id\": {\"order\": \"asc\"}}], FULL_DATA[:3]),\n            (\n                15,\n                1,\n                [(\"id\", SortDirection.DESCENDING)],\n                [{\"id\": {\"order\": \"desc\"}}],\n                FULL_DATA[::-1],\n            ),\n            (\n                5,\n                2,\n                [(\"id\", SortDirection.DESCENDING)],\n                [{\"id\": {\"order\": \"desc\"}}],\n                FULL_DATA[-6:-11:-1],\n            ),\n            (10, 100, [(\"id\", SortDirection.DESCENDING)], [{\"id\": {\"order\": \"desc\"}}], []),\n            ### Title sorting\n            (\n                2,\n                1,\n                [(\"title.keyword\", SortDirection.ASCENDING)],\n                [{\"title.keyword\": {\"order\": \"asc\"}}],\n                [CLASH_OF_KINGS, DANCE_WITH_DRAGONS],\n            ),\n            (\n                3,\n                4,\n                [(\"title.keyword\", SortDirection.DESCENDING)],\n                [{\"title.keyword\": {\"order\": \"desc\"}}],\n                [FEAST_FOR_CROWS, DANCE_WITH_DRAGONS, CLASH_OF_KINGS],\n            ),\n            (\n                10,\n                2,\n                [(\"title.keyword\", SortDirection.ASCENDING)],\n                [{\"title.keyword\": {\"order\": \"asc\"}}],\n                [WAY_OF_KINGS, WORDS_OF_RADIANCE],\n            ),\n            ### Page Count\n            (\n                3,\n                1,\n                [(\"page_count\", SortDirection.ASCENDING)],\n                [{\"page_count\": {\"order\": \"asc\"}}],\n                [TWO_TOWERS, RETURN_OF_THE_KING, FELLOWSHIP_OF_THE_RING],\n            ),\n            (\n                4,\n                2,\n                [(\"page_count\", SortDirection.DESCENDING)],\n                [{\"page_count\": {\"order\": \"desc\"}}],\n                [WAY_OF_KINGS, STORM_OF_SWORDS, CLASH_OF_KINGS, FEAST_FOR_CROWS],\n            ),\n            ### Multi-sorts\n            # Author ascending (Primary) + Page count descending (Secondary)\n            (\n                5,\n                1,\n                [\n                    (\"author.keyword\", SortDirection.ASCENDING),\n                    (\"page_count\", SortDirection.DESCENDING),\n                ],\n                [{\"author.keyword\": {\"order\": \"asc\"}}, {\"page_count\": {\"order\": \"desc\"}}],\n                [OATHBRINGER, RHYTHM_OF_WAR, WORDS_OF_RADIANCE, WAY_OF_KINGS, DANCE_WITH_DRAGONS],\n            ),\n            # Author descending (Primary) + ID descending (Secondary)\n            (\n                4,\n                1,\n                [(\"author.keyword\", SortDirection.DESCENDING), (\"id\", SortDirection.DESCENDING)],\n                [{\"author.keyword\": {\"order\": \"desc\"}}, {\"id\": {\"order\": \"desc\"}}],\n                [RETURN_OF_THE_KING, TWO_TOWERS, FELLOWSHIP_OF_THE_RING, DANCE_WITH_DRAGONS],\n            ),\n        ],\n    )\n    def test_query_builder_pagination_and_sorting(\n        self,\n        search_client,\n        search_index,\n        page_size,\n        page_number,\n        sort_values,\n        expected_sort,\n        expected_results,\n    ):\n        builder = (\n            SearchQueryBuilder()\n            .pagination(page_size=page_size, page_number=page_number)\n            .sort_by(sort_values)\n        )\n\n        assert builder.build() == {\n            \"size\": page_size,\n            \"from\": page_size * (page_number - 1),\n            \"track_scores\": True,\n            \"track_total_hits\": True,\n            \"sort\": expected_sort,\n        }\n\n        validate_valid_request(search_client, search_index, builder, expected_results)\n\n    # Note that by having parametrize twice, it will run every one of the specific tests with the different\n    # sort by parameter to show that they behave the same\n    @pytest.mark.parametrize(\"sort_by\", [[], [(\"relevancy\", SortDirection.DESCENDING)]])\n    @pytest.mark.parametrize(\n        \"filters,expected_results\",\n        [\n            ### Author\n            (\n                [(\"author.keyword\", [\"Brandon Sanderson\"])],\n                [WAY_OF_KINGS, WORDS_OF_RADIANCE, OATHBRINGER, RHYTHM_OF_WAR],\n            ),\n            (\n                [(\"author.keyword\", [\"George R.R. Martin\", \"Mark Twain\"])],\n                [\n                    GAME_OF_THRONES,\n                    CLASH_OF_KINGS,\n                    STORM_OF_SWORDS,\n                    FEAST_FOR_CROWS,\n                    DANCE_WITH_DRAGONS,\n                ],\n            ),\n            (\n                [(\"author.keyword\", [\"J R.R. Tolkien\"])],\n                [FELLOWSHIP_OF_THE_RING, TWO_TOWERS, RETURN_OF_THE_KING],\n            ),\n            (\n                [(\"author.keyword\", [\"Brandon Sanderson\", \"J R.R. Tolkien\"])],\n                [\n                    WAY_OF_KINGS,\n                    WORDS_OF_RADIANCE,\n                    OATHBRINGER,\n                    RHYTHM_OF_WAR,\n                    FELLOWSHIP_OF_THE_RING,\n                    TWO_TOWERS,\n                    RETURN_OF_THE_KING,\n                ],\n            ),\n            (\n                [(\"author.keyword\", [\"Brandon Sanderson\", \"George R.R. Martin\", \"J R.R. Tolkien\"])],\n                FULL_DATA,\n            ),\n            ([(\"author.keyword\", [\"Mark Twain\"])], []),\n            ### in stock\n            ([(\"in_stock\", [False])], [WORDS_OF_RADIANCE, RHYTHM_OF_WAR, DANCE_WITH_DRAGONS]),\n            ([(\"in_stock\", [True, False])], FULL_DATA),\n            ### page count\n            ([(\"page_count\", [1007, 694, 352])], [WAY_OF_KINGS, GAME_OF_THRONES, TWO_TOWERS]),\n            ([(\"page_count\", [1, 2, 3])], []),\n            ### Multi-filter\n            # Author + In Stock\n            (\n                [(\"author.keyword\", [\"Brandon Sanderson\"]), (\"in_stock\", [True])],\n                [WAY_OF_KINGS, OATHBRINGER],\n            ),\n            (\n                [\n                    (\"author.keyword\", [\"George R.R. Martin\", \"J R.R. Tolkien\"]),\n                    (\"in_stock\", [False]),\n                ],\n                [DANCE_WITH_DRAGONS],\n            ),\n            # Author + Title\n            (\n                [\n                    (\"author.keyword\", [\"Brandon Sanderson\", \"J R.R. Tolkien\", \"Mark Twain\"]),\n                    (\n                        \"title.keyword\",\n                        [\"A Game of Thrones\", \"The Way of Kings\", \"The Fellowship of the Ring\"],\n                    ),\n                ],\n                [WAY_OF_KINGS, FELLOWSHIP_OF_THE_RING],\n            ),\n            (\n                [\n                    (\"author.keyword\", [\"George R.R. Martin\", \"J R.R. Tolkien\"]),\n                    (\n                        \"title.keyword\",\n                        [\"A Game of Thrones\", \"The Way of Kings\", \"The Fellowship of the Ring\"],\n                    ),\n                ],\n                [GAME_OF_THRONES, FELLOWSHIP_OF_THE_RING],\n            ),\n        ],\n    )\n    def test_query_builder_filter_terms(\n        self, search_client, search_index, filters, expected_results, sort_by\n    ):\n        builder = SearchQueryBuilder().sort_by(sort_by)\n\n        expected_terms = []\n        for filter in filters:\n            builder.filter_terms(filter[0], filter[1])\n\n            expected_terms.append({\"terms\": {filter[0]: filter[1]}})\n\n        expected_query = {\n            \"size\": 25,\n            \"from\": 0,\n            \"track_scores\": True,\n            \"track_total_hits\": True,\n            \"query\": {\"bool\": {\"filter\": expected_terms}},\n        }\n\n        if len(sort_by) > 0:\n            expected_query[\"sort\"] = [{\"_score\": {\"order\": \"desc\"}}]\n\n        assert builder.build() == expected_query\n\n        validate_valid_request(search_client, search_index, builder, expected_results)\n\n    @pytest.mark.parametrize(\n        \"start_date,end_date,expected_results\",\n        [\n            # Date range that will include all results\n            # Absolute\n            (date(1900, 1, 1), date(2050, 1, 1), FULL_DATA),\n            # Relative\n            (-45656, 9131, FULL_DATA),\n            # Start only date range that will get all results\n            # Absolute\n            (date(1950, 1, 1), None, FULL_DATA),\n            # Relative\n            (-45656, None, FULL_DATA),\n            # End only date range that will get all results\n            # Absolute\n            (None, date(2025, 1, 1), FULL_DATA),\n            # Relative\n            (None, 9131, FULL_DATA),\n            # Range that filters to just oldest\n            (\n                date(1950, 1, 1),\n                date(1960, 1, 1),\n                [FELLOWSHIP_OF_THE_RING, TWO_TOWERS, RETURN_OF_THE_KING],\n            ),\n            # Unbounded range for oldest few\n            (\n                None,\n                date(1990, 1, 1),\n                [FELLOWSHIP_OF_THE_RING, TWO_TOWERS, RETURN_OF_THE_KING],\n            ),\n            # Unbounded range for newest few\n            (date(2011, 8, 1), None, [WORDS_OF_RADIANCE, OATHBRINGER, RHYTHM_OF_WAR]),\n            # Selecting a few in the middle\n            (\n                date(2005, 1, 1),\n                date(2014, 1, 1),\n                [WAY_OF_KINGS, FEAST_FOR_CROWS, DANCE_WITH_DRAGONS],\n            ),\n            # Exact date\n            # Absolute\n            (date(1954, 7, 29), date(1954, 7, 29), [FELLOWSHIP_OF_THE_RING]),\n            # Relative\n            (-25747, -25747, []),\n            # None fetched in range\n            # Absolute\n            (date(1981, 1, 1), date(1989, 1, 1), []),\n            # Relative\n            (-16093, -13171, []),\n        ],\n    )\n    def test_query_builder_filter_date_range(\n        self,\n        search_client,\n        search_index,\n        start_date,\n        end_date,\n        expected_results,\n    ):\n        builder = (\n            SearchQueryBuilder()\n            .sort_by([])\n            .filter_date_range(\"publication_date\", start_date, end_date)\n        )\n\n        expected_ranges = {}\n        if start_date is not None:\n            expected_ranges[\"gte\"] = (\n                f\"now{start_date:+}d\" if isinstance(start_date, int) else start_date.isoformat()\n            )\n        if end_date is not None:\n            expected_ranges[\"lte\"] = (\n                f\"now{end_date:+}d\" if isinstance(end_date, int) else end_date.isoformat()\n            )\n\n        expected_query = {\n            \"size\": 25,\n            \"from\": 0,\n            \"track_scores\": True,\n            \"track_total_hits\": True,\n            \"query\": {\"bool\": {\"filter\": [{\"range\": {\"publication_date\": expected_ranges}}]}},\n        }\n\n        assert builder.build() == expected_query\n\n        validate_valid_request(search_client, search_index, builder, expected_results)\n\n    @pytest.mark.parametrize(\n        \"min_value,max_value,expected_results\",\n        [\n            # All fetched\n            (1, 2000, FULL_DATA),\n            # None fetched\n            (2000, 3000, []),\n            # \"Short\" books\n            (300, 700, [GAME_OF_THRONES, FELLOWSHIP_OF_THE_RING, TWO_TOWERS, RETURN_OF_THE_KING]),\n            # Unbounded short\n            (None, 416, [TWO_TOWERS, RETURN_OF_THE_KING]),\n            # Unbounded long\n            (1050, None, [WORDS_OF_RADIANCE, OATHBRINGER, RHYTHM_OF_WAR, DANCE_WITH_DRAGONS]),\n            # Middle length\n            (\n                500,\n                1010,\n                [WAY_OF_KINGS, GAME_OF_THRONES, CLASH_OF_KINGS, STORM_OF_SWORDS, FEAST_FOR_CROWS],\n            ),\n        ],\n    )\n    def test_query_builder_filter_int_range(\n        self, search_client, search_index, min_value, max_value, expected_results\n    ):\n        builder = (\n            SearchQueryBuilder().sort_by([]).filter_int_range(\"page_count\", min_value, max_value)\n        )\n\n        expected_ranges = {}\n        if min_value is not None:\n            expected_ranges[\"gte\"] = min_value\n        if max_value is not None:\n            expected_ranges[\"lte\"] = max_value\n\n        expected_query = {\n            \"size\": 25,\n            \"from\": 0,\n            \"track_scores\": True,\n            \"track_total_hits\": True,\n            \"query\": {\"bool\": {\"filter\": [{\"range\": {\"page_count\": expected_ranges}}]}},\n        }\n\n        assert builder.build() == expected_query\n\n        validate_valid_request(search_client, search_index, builder, expected_results)\n\n    def test_multiple_ranges(self, search_client, search_index):\n        # Sanity test that we can specify multiple ranges (in this case, a date + int range)\n        # in the same query\n        builder = (\n            SearchQueryBuilder()\n            .sort_by([])\n            .filter_int_range(\"page_count\", 600, 1100)\n            .filter_date_range(\"publication_date\", date(2000, 1, 1), date(2013, 1, 1))\n        )\n\n        expected_results = [WAY_OF_KINGS, STORM_OF_SWORDS, FEAST_FOR_CROWS, DANCE_WITH_DRAGONS]\n        validate_valid_request(\n            search_client, search_index, builder, expected_results=expected_results\n        )\n\n    def test_filter_int_range_both_none(self):\n        with pytest.raises(ValueError, match=\"Cannot use int range filter\"):\n            SearchQueryBuilder().filter_int_range(\"test_field\", None, None)\n\n    def test_filter_date_range_all_none(self):\n        with pytest.raises(ValueError, match=\"Cannot use date range filter\"):\n            SearchQueryBuilder().filter_date_range(\"test_field\", None, None)\n\n    @pytest.mark.parametrize(\n        \"query,fields,expected_results,expected_aggregations\",\n        [\n            (\n                \"king\",\n                [\"title\"],\n                [WAY_OF_KINGS, CLASH_OF_KINGS, RETURN_OF_THE_KING],\n                {\n                    \"author\": {\n                        \"Brandon Sanderson\": 1,\n                        \"George R.R. Martin\": 1,\n                        \"J R.R. Tolkien\": 1,\n                    },\n                    \"in_stock\": {0: 0, 1: 3},\n                },\n            ),\n            (\n                \"R.R.\",\n                [\"author\"],\n                [\n                    GAME_OF_THRONES,\n                    CLASH_OF_KINGS,\n                    STORM_OF_SWORDS,\n                    FEAST_FOR_CROWS,\n                    DANCE_WITH_DRAGONS,\n                    FELLOWSHIP_OF_THE_RING,\n                    TWO_TOWERS,\n                    RETURN_OF_THE_KING,\n                ],\n                {\n                    \"author\": {\n                        \"Brandon Sanderson\": 0,\n                        \"George R.R. Martin\": 5,\n                        \"J R.R. Tolkien\": 3,\n                    },\n                    \"in_stock\": {0: 1, 1: 7},\n                },\n            ),\n            (\n                \"Martin (Crows | Storm)\",\n                [\"title\", \"author\"],\n                [STORM_OF_SWORDS, FEAST_FOR_CROWS],\n                {\n                    \"author\": {\n                        \"Brandon Sanderson\": 0,\n                        \"George R.R. Martin\": 2,\n                        \"J R.R. Tolkien\": 0,\n                    },\n                    \"in_stock\": {0: 0, 1: 2},\n                },\n            ),\n            (\n                \"(Sanderson + (Words | King)) | Tolkien | Crow\",\n                [\"title\", \"author\"],\n                [\n                    WAY_OF_KINGS,\n                    WORDS_OF_RADIANCE,\n                    FEAST_FOR_CROWS,\n                    FELLOWSHIP_OF_THE_RING,\n                    TWO_TOWERS,\n                    RETURN_OF_THE_KING,\n                ],\n                {\n                    \"author\": {\n                        \"Brandon Sanderson\": 2,\n                        \"George R.R. Martin\": 1,\n                        \"J R.R. Tolkien\": 3,\n                    },\n                    \"in_stock\": {0: 1, 1: 5},\n                },\n            ),\n            (\n                \"-R.R. -Oathbringer\",\n                [\"title\", \"author\"],\n                [WAY_OF_KINGS, WORDS_OF_RADIANCE, RHYTHM_OF_WAR],\n                {\n                    \"author\": {\n                        \"Brandon Sanderson\": 3,\n                        \"George R.R. Martin\": 0,\n                        \"J R.R. Tolkien\": 0,\n                    },\n                    \"in_stock\": {0: 2, 1: 1},\n                },\n            ),\n            (\n                \"Brandon | George | J\",\n                [\"title\", \"author\"],\n                FULL_DATA,\n                {\n                    \"author\": {\n                        \"Brandon Sanderson\": 4,\n                        \"George R.R. Martin\": 5,\n                        \"J R.R. Tolkien\": 3,\n                    },\n                    \"in_stock\": {0: 3, 1: 9},\n                },\n            ),\n            (\n                \"how to make a pizza\",\n                [\"title\", \"author\"],\n                [],\n                {\n                    \"author\": {\n                        \"Brandon Sanderson\": 0,\n                        \"George R.R. Martin\": 0,\n                        \"J R.R. Tolkien\": 0,\n                    },\n                    \"in_stock\": {0: 0, 1: 0},\n                },\n            ),\n        ],\n    )\n    def test_query_builder_simple_query_and_aggregations(\n        self, search_client, search_index, query, fields, expected_results, expected_aggregations\n    ):\n        # Add a sort by ID ascending to make it so any relevancy from this is ignored, just testing that values returned\n        builder = SearchQueryBuilder().sort_by([(\"id\", SortDirection.ASCENDING)])\n\n        builder.simple_query(query, fields, \"AND\")\n\n        # Statically add the same aggregated fields every time\n        builder.aggregation_terms(\"author\", \"author.keyword\", minimum_count=0).aggregation_terms(\n            \"in_stock\", \"in_stock\", minimum_count=0\n        )\n\n        assert builder.build() == {\n            \"size\": 25,\n            \"from\": 0,\n            \"track_scores\": True,\n            \"track_total_hits\": True,\n            \"query\": {\n                \"bool\": {\n                    \"must\": [\n                        {\n                            \"simple_query_string\": {\n                                \"query\": query,\n                                \"fields\": fields,\n                                \"default_operator\": \"AND\",\n                            }\n                        }\n                    ]\n                }\n            },\n            \"sort\": [{\"id\": {\"order\": \"asc\"}}],\n            \"aggs\": {\n                \"author\": {\"terms\": {\"field\": \"author.keyword\", \"size\": 25, \"min_doc_count\": 0}},\n                \"in_stock\": {\"terms\": {\"field\": \"in_stock\", \"size\": 25, \"min_doc_count\": 0}},\n            },\n        }\n\n        validate_valid_request(\n            search_client, search_index, builder, expected_results, expected_aggregations\n        )\n\n    @pytest.mark.parametrize(\n        \"query,fields,query_operator,expected_results\",\n        [\n            # AND\n            (\n                \"king Tolkien\",\n                [\"title\", \"author\"],\n                \"AND\",\n                [RETURN_OF_THE_KING],\n            ),\n            # OR\n            (\n                \"king Tolkien\",\n                [\"title\", \"author\"],\n                \"OR\",\n                [\n                    RETURN_OF_THE_KING,\n                    WAY_OF_KINGS,\n                    CLASH_OF_KINGS,\n                    FELLOWSHIP_OF_THE_RING,\n                    TWO_TOWERS,\n                ],\n            ),\n        ],\n    )\n    def test_query_builder_query_operator(\n        self, search_client, search_index, query, fields, query_operator, expected_results\n    ):\n        builder = SearchQueryBuilder()\n        builder.simple_query(query, fields, query_operator)\n\n        resp = builder.build()\n\n        assert resp[\"query\"] == {\n            \"bool\": {\n                \"must\": [\n                    {\n                        \"simple_query_string\": {\n                            \"query\": query,\n                            \"fields\": fields,\n                            \"default_operator\": query_operator,\n                        }\n                    }\n                ]\n            }\n        }\n\n        validate_valid_request(search_client, search_index, builder, expected_results)"}
{"path":"frontend/stories/components/FullWidthAlert.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/FullWidthAlert.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.458Z"}
{"path":"frontend/stories/components/GoalContent.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/GoalContent.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":""}
{"path":"frontend/stories/components/GrantsIdentifier.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/GrantsIdentifier.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/agencies_v1/test_agencies_routes.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/agencies_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/agencies_v1/test_agencies_routes.py\nSize: 4.17 KB\nLast Modified: 2025-02-14T17:08:26.458Z"}
{"path":"frontend/stories/components/Header.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/Header.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"from src.db.models.agency_models import Agency\nfrom tests.conftest import BaseTestClass\nfrom tests.src.db.models.factories import AgencyFactory\n\n\nclass TestAgenciesRoutes(BaseTestClass):\n    @pytest.fixture(autouse=True)\n    def cleanup_agencies(self, db_session):\n        yield\n\n        # Use no_autoflush to prevent premature flushes\n        with db_session.no_autoflush:\n            # Fetch all agencies\n            agencies = db_session.query(Agency).all()\n            # Delete each agency\n            for agency in agencies:\n                db_session.delete(agency)\n\n        db_session.commit()\n\n    def test_agencies_get_default_dates(\n        self, client, api_auth_token, enable_factory_create, db_session\n    ):\n        # These should return in the default date range\n        AgencyFactory.create_batch(4)\n\n        # These should be excluded\n        AgencyFactory.create_batch(3, is_test_agency=True)\n\n        payload = {\n            \"filters\": {},\n            \"pagination\": {\n                \"page_size\": 10,\n                \"page_offset\": 1,\n                \"sort_order\": [\n                    {\"order_by\": \"created_at\", \"sort_direction\": \"descending\"},\n                ],\n            },\n        }\n        response = client.post(\"/v1/agencies\", headers={\"X-Auth\": api_auth_token}, json=payload)\n        assert response.status_code == 200\n        data = response.json[\"data\"]\n        assert len(data) == 4\n\n    def test_agencies_get_with_sub_agencies(\n        self,\n        client,\n        api_auth_token,\n        enable_factory_create,\n        db_session,\n    ):\n        # Create top-level agencies\n        hhs = AgencyFactory.create(agency_name=\"HHS\")\n        dod = AgencyFactory.create(agency_name=\"DOD\")\n\n        # Create sub-agencies\n        AgencyFactory.create(agency_name=\"HHS-AOA\", top_level_agency=hhs)\n        AgencyFactory.create(agency_name=\"HHS-CDC\", top_level_agency=hhs)\n        AgencyFactory.create(agency_name=\"DOD-ARMY\", top_level_agency=dod)\n        AgencyFactory.create(agency_name=\"DOD-NAVY\", top_level_agency=dod)\n\n        payload = {\n            \"filters\": {},\n            \"pagination\": {\n                \"page_size\": 10,\n                \"page_offset\": 1,\n                \"sort_order\": [\n                    {\"order_by\": \"created_at\", \"sort_direction\": \"descending\"},\n                ],\n            },\n        }\n\n        response = client.post(\"/v1/agencies\", headers={\"X-Auth\": api_auth_token}, json=payload)\n        assert response.status_code == 200\n        data = response.json[\"data\"]\n\n        # Verify the relationships\n        for agency in data:\n            if \"-\" in agency[\"agency_name\"]:\n                top_level_name = agency[\"agency_name\"].split(\"-\")[0]\n                assert agency[\"top_level_agency\"][\"agency_name\"] == top_level_name\n\n    def test_agencies_sorting(self, client, api_auth_token, enable_factory_create, db_session):\n        # Create agencies\n        AgencyFactory.create(agency_name=\"HHS\", agency_code=\"DOI\")\n        AgencyFactory.create(agency_name=\"DOD\", agency_code=\"HHS-ACL\")\n\n        # Test default sort order\n        payload = {\n            \"filters\": {},\n            \"pagination\": {\n                \"page_size\": 10,\n                \"page_offset\": 1,\n            },\n        }\n\n        response = client.post(\"/v1/agencies\", headers={\"X-Auth\": api_auth_token}, json=payload)\n        assert response.status_code == 200\n        data = response.json[\"data\"]\n\n        # assert defaults to sorting by agency_code asc\n        assert len(data) == 2\n        assert data[0][\"agency_code\"] < data[1][\"agency_code\"]\n\n        # Test multi-sort\n        AgencyFactory.create(agency_name=\"DOD\", agency_code=\"HHS-ACF\")\n\n        payload[\"pagination\"][\"sort_order\"] = [\n            {\"order_by\": \"agency_name\", \"sort_direction\": \"descending\"},\n            {\"order_by\": \"agency_code\", \"sort_direction\": \"descending\"},\n        ]\n\n        response = client.post(\"/v1/agencies\", headers={\"X-Auth\": api_auth_token}, json=payload)\n        assert response.status_code == 200\n        data = response.json[\"data\"]\n\n        # assert order by agency_name desc then agency_code desc\n        assert data[0][\"agency_name\"] > data[1][\"agency_name\"]\n        assert data[1][\"agency_code\"] > data[2][\"agency_code\"]"}
{"path":"frontend/stories/components/Hero.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/Hero.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/extracts_v1/test_extract_schema.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/extracts_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/extracts_v1/test_extract_schema.py\nSize: 3.54 KB\nLast Modified: 2025-02-14T17:08:26.458Z"}
{"path":"frontend/stories/components/Layout.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/Layout.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"import pytest\nfrom marshmallow import ValidationError\n\nfrom src.api.extracts_v1.extract_schema import (\n    ExtractMetadataListResponseSchema,\n    ExtractMetadataRequestSchema,\n    ExtractMetadataResponseSchema,\n)\nfrom src.db.models.extract_models import ExtractMetadata\n\n\n@pytest.fixture\ndef sample_extract_metadata():\n    return ExtractMetadata(\n        extract_metadata_id=1,\n        extract_type=\"opportunities_csv\",\n        file_name=\"test_extract.csv\",\n        file_path=\"/test/path/test_extract.csv\",\n        file_size_bytes=2048,\n    )\n\n\ndef test_request_schema_validation():\n    schema = ExtractMetadataRequestSchema()\n\n    # Test valid data\n    valid_data = {\n        \"filters\": {\n            \"extract_type\": \"opportunities_csv\",\n            \"created_at\": {\n                \"start_date\": \"2023-10-01\",\n                \"end_date\": \"2023-10-07\",\n            },\n        },\n        \"pagination\": {\n            \"order_by\": \"created_at\",\n            \"page_offset\": 1,\n            \"page_size\": 25,\n            \"sort_direction\": \"ascending\",\n        },\n    }\n    result = schema.load(valid_data)\n    assert result[\"filters\"][\"extract_type\"] == \"opportunities_csv\"\n    assert result[\"filters\"][\"created_at\"][\"start_date\"] == date(2023, 10, 1)\n    assert result[\"filters\"][\"created_at\"][\"end_date\"] == date(2023, 10, 7)\n\n    # Test invalid extract_type\n    invalid_data = {\"extract_type\": \"invalid_type\", \"start_date\": \"2023-10-01\"}\n    with pytest.raises(ValidationError):\n        schema.load(invalid_data)\n\n\ndef test_response_schema_single(sample_extract_metadata):\n    schema = ExtractMetadataResponseSchema()\n\n    sample_extract_metadata.download_path = \"http://www.example.com\"\n    extract_metadata = schema.dump(sample_extract_metadata)\n\n    assert extract_metadata[\"download_path\"] == \"http://www.example.com\"\n\n    assert extract_metadata[\"extract_metadata_id\"] == 1\n    assert extract_metadata[\"extract_type\"] == \"opportunities_csv\"\n    assert extract_metadata[\"download_path\"] == \"http://www.example.com\"\n    assert extract_metadata[\"file_size_bytes\"] == 2048\n\n\ndef test_response_schema_list(sample_extract_metadata):\n    schema = ExtractMetadataListResponseSchema()\n\n    # Create a list of two metadata records\n    metadata_list = {\n        \"data\": [\n            sample_extract_metadata,\n            ExtractMetadata(\n                extract_metadata_id=2,\n                extract_type=\"opportunities_json\",\n                file_name=\"test_extract2.xml\",\n                file_path=\"/test/path/test_extract2.xml\",\n                file_size_bytes=1024,\n            ),\n        ]\n    }\n\n    result = schema.dump(metadata_list)\n\n    assert len(result[\"data\"]) == 2\n    assert result[\"data\"][0][\"extract_metadata_id\"] == 1\n    assert result[\"data\"][0][\"extract_type\"] == \"opportunities_csv\"\n    assert result[\"data\"][1][\"extract_metadata_id\"] == 2\n    assert result[\"data\"][1][\"extract_type\"] == \"opportunities_json\"\n\n\ndef test_request_schema_null_values():\n    schema = ExtractMetadataRequestSchema()\n\n    # Test with some null values\n    data = {\n        \"filters\": {\n            \"extract_type\": None,\n            \"created_at\": {\"start_date\": \"2023-10-01\", \"end_date\": None},\n        },\n        \"pagination\": {\n            \"order_by\": \"created_at\",\n            \"page_offset\": 1,\n            \"page_size\": 25,\n            \"sort_direction\": \"ascending\",\n        },\n    }\n\n    result = schema.load(data)\n    assert result[\"filters\"][\"extract_type\"] is None\n    assert result[\"filters\"][\"created_at\"][\"start_date\"] == date(2023, 10, 1)\n    assert result[\"filters\"][\"created_at\"][\"end_date\"] is None"}
{"path":"frontend/stories/components/ProcessContent.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/ProcessContent.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/extracts_v1/test_extracts_routes.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/extracts_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/extracts_v1/test_extracts_routes.py\nSize: 6.92 KB\nLast Modified: 2025-02-14T17:08:26.458Z"}
{"path":"frontend/stories/components/ReaserchImpact.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/ReaserchImpact.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"import boto3\nimport pytest\nimport requests\n\nimport src.util.datetime_util as datetime_util\nfrom src.constants.lookup_constants import ExtractType\nfrom src.db.models.extract_models import ExtractMetadata\nfrom tests.src.db.models.factories import ExtractMetadataFactory\n\n\n@pytest.fixture(autouse=True)\ndef clear_extracts(db_session):\n    db_session.query(ExtractMetadata).delete()\n    db_session.commit()\n    yield\n\n\ndef test_extract_metadata_get_default_dates(\n    client, api_auth_token, enable_factory_create, db_session\n):\n    \"\"\"Test that default date range (last 7 days) is applied when no dates provided\"\"\"\n\n    # These should return in the default date range\n    ExtractMetadataFactory.create_batch(2, extract_type=ExtractType.OPPORTUNITIES_JSON)\n\n    # This should not return because it's outside the default date range\n    ExtractMetadataFactory(\n        created_at=datetime.now() - timedelta(days=15),\n    )\n\n    payload = {\n        \"filters\": {\"extract_type\": \"opportunities_json\"},\n        \"pagination\": {\n            \"page\": 1,\n            \"page_size\": 10,\n            \"page_offset\": 1,\n            \"order_by\": \"created_at\",\n            \"sort_direction\": \"descending\",\n        },\n    }\n    response = client.post(\"/v1/extracts\", headers={\"X-Auth\": api_auth_token}, json=payload)\n    assert response.status_code == 200\n    data = response.json[\"data\"]\n    assert len(data) == 2\n    assert data[0][\"extract_type\"] == \"opportunities_json\"\n\n\ndef test_extract_metadata_get_with_custom_dates(\n    client, api_auth_token, enable_factory_create, db_session, mock_s3_bucket\n):\n    \"\"\"Test with explicitly provided date range\"\"\"\n    ExtractMetadataFactory.create_batch(\n        2,\n        created_at=datetime.now() - timedelta(days=10),\n        file_path=f\"s3://{mock_s3_bucket}/path/to/file.txt\",\n    )\n\n    payload = {\n        \"filters\": {\n            \"created_at\": {\n                \"start_date\": (datetime.now() - timedelta(days=15)).date().isoformat(),\n                \"end_date\": datetime.now().date().isoformat(),\n            }\n        },\n        \"pagination\": {\n            \"page\": 1,\n            \"page_size\": 10,\n            \"page_offset\": 1,\n            \"order_by\": \"created_at\",\n            \"sort_direction\": \"descending\",\n        },\n    }\n\n    response = client.post(\"/v1/extracts\", headers={\"X-Auth\": api_auth_token}, json=payload)\n\n    assert response.status_code == 200\n    data = response.json[\"data\"]\n    assert len(data) == 2\n    assert \"/path/to/file.txt\" in data[0][\"download_path\"]\n\n\ndef test_extract_metadata_get_with_type_filter(\n    client, api_auth_token, enable_factory_create, db_session\n):\n    \"\"\"Test filtering by extract_type\"\"\"\n    ExtractMetadataFactory(extract_type=ExtractType.OPPORTUNITIES_JSON)\n    ExtractMetadataFactory(extract_type=ExtractType.OPPORTUNITIES_CSV)\n\n    payload = {\n        \"filters\": {\"extract_type\": \"opportunities_json\"},\n        \"pagination\": {\n            \"page\": 1,\n            \"page_size\": 10,\n            \"page_offset\": 1,\n            \"order_by\": \"created_at\",\n            \"sort_direction\": \"descending\",\n        },\n    }\n\n    response = client.post(\"/v1/extracts\", headers={\"X-Auth\": api_auth_token}, json=payload)\n\n    assert response.status_code == 200\n    data = response.json[\"data\"]\n    assert len(data) == 1\n    assert data[0][\"extract_type\"] == \"opportunities_json\"\n\n\ndef test_extract_metadata_get_pagination(client, api_auth_token, enable_factory_create, db_session):\n    \"\"\"Test pagination of results\"\"\"\n    ExtractMetadataFactory.create_batch(2)\n\n    payload = {\n        \"pagination\": {\n            \"page\": 1,\n            \"page_size\": 1,\n            \"page_offset\": 1,\n            \"order_by\": \"created_at\",\n            \"sort_direction\": \"descending\",\n        },\n    }\n\n    response = client.post(\"/v1/extracts\", headers={\"X-Auth\": api_auth_token}, json=payload)\n\n    assert response.status_code == 200\n    data = response.json[\"data\"]\n    assert len(data) == 1\n\n    # Test second page\n    payload[\"pagination\"][\"page\"] = 2\n    response = client.post(\"/v1/extracts\", headers={\"X-Auth\": api_auth_token}, json=payload)\n    assert response.status_code == 200\n    data = response.json[\"data\"]\n    assert len(data) == 1\n\n\ndef test_extract_metadata_get_pagination_info(\n    client, api_auth_token, enable_factory_create, db_session\n):\n    \"\"\"Test pagination information in response\"\"\"\n    # Create 5 extracts to test pagination\n    ExtractMetadataFactory.create_batch(\n        5, extract_type=ExtractType.OPPORTUNITIES_JSON, created_at=datetime_util.utcnow()\n    )\n\n    # Request 2 items per page\n    payload = {\n        \"pagination\": {\n            \"page_size\": 2,\n            \"page_offset\": 1,\n            \"order_by\": \"created_at\",\n            \"sort_direction\": \"descending\",\n        },\n    }\n\n    response = client.post(\"/v1/extracts\", headers={\"X-Auth\": api_auth_token}, json=payload)\n\n    assert response.status_code == 200\n\n    # Verify data length matches page_size\n    data = response.json[\"data\"]\n    assert len(data) == 2\n\n    # Verify pagination info\n    pagination = response.json[\"pagination_info\"]\n    assert pagination[\"total_records\"] == 5\n    assert pagination[\"total_pages\"] == 3\n    assert pagination[\"page_size\"] == 2\n    # Test last page\n    payload[\"pagination\"][\"page_offset\"] = 3\n    response = client.post(\"/v1/extracts\", headers={\"X-Auth\": api_auth_token}, json=payload)\n\n    assert response.status_code == 200\n    data = response.json[\"data\"]\n    pagination = response.json[\"pagination_info\"]\n\n    assert len(data) == 1  # Last page should have 1 item\n\n\ndef test_extract_metadata_presigned_url(\n    client, api_auth_token, enable_factory_create, db_session, mock_s3_bucket\n):\n    \"\"\"Test that pre-signed URLs are generated correctly and can be used to download files\"\"\"\n\n    # Create test extract with known file content\n    test_content = b\"test file content\"\n    test_file_path = f\"s3://{mock_s3_bucket}/test/file.csv\"\n\n    # Create extract metadata\n    ExtractMetadataFactory(\n        extract_type=ExtractType.OPPORTUNITIES_CSV,\n        file_path=test_file_path,\n        file_size_bytes=len(test_content),\n    )\n\n    # Upload test file to mock S3\n    s3_client = boto3.client(\"s3\")\n    s3_client.put_object(Bucket=mock_s3_bucket, Key=\"test/file.csv\", Body=test_content)\n\n    # Request extract metadata\n    payload = {\n        \"filters\": {\"extract_type\": \"opportunities_csv\"},\n        \"pagination\": {\n            \"page_size\": 10,\n            \"page_offset\": 1,\n            \"order_by\": \"created_at\",\n            \"sort_direction\": \"descending\",\n        },\n    }\n\n    response = client.post(\"/v1/extracts\", headers={\"X-Auth\": api_auth_token}, json=payload)\n\n    assert response.status_code == 200\n    data = response.json[\"data\"]\n    assert len(data) == 1\n\n    # Verify pre-signed URL format\n    download_url = data[0][\"download_path\"]\n\n    # Try downloading the file using the pre-signed URL\n    download_response = requests.get(download_url)  # nosec\n    assert download_response.status_code == 200\n    assert download_response.content == test_content"}
{"path":"frontend/stories/components/ReaserchIntro.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/ReaserchIntro.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/opportunities_v1/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/opportunities_v1/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.458Z"}
{"path":"frontend/stories/components/ReaserchThemes.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/ReaserchThemes.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":""}
{"path":"frontend/stories/components/ResearchArchetypes.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/ResearchArchetypes.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/opportunities_v1/conftest.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/opportunities_v1/conftest.py\nSize: 10.47 KB\nLast Modified: 2025-02-14T17:08:26.458Z"}
{"path":"frontend/stories/components/ResearchMethodology.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/ResearchMethodology.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"from src.constants.lookup_constants import (\n    ApplicantType,\n    FundingCategory,\n    FundingInstrument,\n    OpportunityStatus,\n)\nfrom src.db.models.opportunity_models import (\n    Opportunity,\n    OpportunityAssistanceListing,\n    OpportunityAttachment,\n    OpportunitySummary,\n)\n\n\n@pytest.fixture\ndef truncate_opportunities(db_session):\n    # Note that we can't just do db_session.query(Opportunity).delete() as the cascade deletes won't work automatically:\n    # https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-queryguide-update-delete-caveats\n    # but if we do it individually they will\n    opportunities = db_session.query(Opportunity).all()\n    for opp in opportunities:\n        db_session.delete(opp)\n\n    # Force the deletes to the DB\n    db_session.commit()\n\n\ndef get_search_request(\n    page_offset: int = 1,\n    page_size: int = 25,\n    sort_order: list[dict] | None = None,\n    query: str | None = None,\n    experimental: dict | None = None,\n    funding_instrument_one_of: list[FundingInstrument] | None = None,\n    funding_category_one_of: list[FundingCategory] | None = None,\n    applicant_type_one_of: list[ApplicantType] | None = None,\n    opportunity_status_one_of: list[OpportunityStatus] | None = None,\n    agency_one_of: list[str] | None = None,\n    assistance_listing_one_of: list[str] = None,\n    is_cost_sharing_one_of: list[bool | str] | None = None,\n    expected_number_of_awards: dict | None = None,\n    award_floor: dict | None = None,\n    award_ceiling: dict | None = None,\n    estimated_total_program_funding: dict | None = None,\n    post_date: dict | None = None,\n    close_date: dict | None = None,\n    format: str | None = None,\n):\n    if sort_order is None:\n        sort_order = [{\"order_by\": \"opportunity_id\", \"sort_direction\": \"ascending\"}]\n\n    req = {\n        \"pagination\": {\n            \"page_offset\": page_offset,\n            \"page_size\": page_size,\n            \"sort_order\": sort_order,\n        }\n    }\n\n    filters = {}\n\n    if funding_instrument_one_of is not None:\n        filters[\"funding_instrument\"] = {\"one_of\": funding_instrument_one_of}\n\n    if funding_category_one_of is not None:\n        filters[\"funding_category\"] = {\"one_of\": funding_category_one_of}\n\n    if applicant_type_one_of is not None:\n        filters[\"applicant_type\"] = {\"one_of\": applicant_type_one_of}\n\n    if opportunity_status_one_of is not None:\n        filters[\"opportunity_status\"] = {\"one_of\": opportunity_status_one_of}\n\n    if agency_one_of is not None:\n        filters[\"agency\"] = {\"one_of\": agency_one_of}\n\n    if assistance_listing_one_of is not None:\n        filters[\"assistance_listing_number\"] = {\"one_of\": assistance_listing_one_of}\n\n    if is_cost_sharing_one_of is not None:\n        filters[\"is_cost_sharing\"] = {\"one_of\": is_cost_sharing_one_of}\n\n    if expected_number_of_awards is not None:\n        filters[\"expected_number_of_awards\"] = expected_number_of_awards\n\n    if award_floor is not None:\n        filters[\"award_floor\"] = award_floor\n\n    if award_ceiling is not None:\n        filters[\"award_ceiling\"] = award_ceiling\n\n    if estimated_total_program_funding is not None:\n        filters[\"estimated_total_program_funding\"] = estimated_total_program_funding\n\n    if post_date is not None:\n        filters[\"post_date\"] = post_date\n\n    if close_date is not None:\n        filters[\"close_date\"] = close_date\n\n    if len(filters) > 0:\n        req[\"filters\"] = filters\n\n    if query is not None:\n        req[\"query\"] = query\n\n    if format is not None:\n        req[\"format\"] = format\n\n    if experimental is not None:\n        req[\"experimental\"] = experimental\n\n    return req\n\n\n#####################################\n# Validation utils\n#####################################\n\n\ndef validate_opportunity(db_opportunity: Opportunity, resp_opportunity: dict):\n    assert db_opportunity.opportunity_id == resp_opportunity[\"opportunity_id\"]\n    assert db_opportunity.opportunity_number == resp_opportunity[\"opportunity_number\"]\n    assert db_opportunity.opportunity_title == resp_opportunity[\"opportunity_title\"]\n    assert db_opportunity.agency_code == resp_opportunity[\"agency_code\"]\n    assert db_opportunity.agency_name == resp_opportunity[\"agency_name\"]\n    assert db_opportunity.category == resp_opportunity[\"category\"]\n    assert db_opportunity.category_explanation == resp_opportunity[\"category_explanation\"]\n\n    validate_opportunity_summary(db_opportunity.summary, resp_opportunity[\"summary\"])\n    validate_assistance_listings(\n        db_opportunity.opportunity_assistance_listings,\n        resp_opportunity[\"opportunity_assistance_listings\"],\n    )\n\n    assert db_opportunity.opportunity_status == resp_opportunity[\"opportunity_status\"]\n\n\ndef validate_opportunity_with_attachments(db_opportunity: Opportunity, resp_opportunity: dict):\n    validate_opportunity(db_opportunity, resp_opportunity)\n    validate_opportunity_attachments(\n        db_opportunity.opportunity_attachments,\n        resp_opportunity[\"attachments\"],\n    )\n\n\ndef validate_opportunity_attachments(\n    db_attachments: list[OpportunityAttachment], resp_attachments: list[dict]\n):\n    db_attachments.sort(key=lambda a: (a.file_size_bytes, a.file_name))\n    resp_attachments.sort(key=lambda a: (a[\"file_size_bytes\"], a[\"file_name\"]))\n\n    assert len(db_attachments) == len(resp_attachments)\n    for db_attachment, resp_attachment in zip(db_attachments, resp_attachments, strict=True):\n        assert db_attachment.mime_type == resp_attachment[\"mime_type\"]\n        assert db_attachment.file_name == resp_attachment[\"file_name\"]\n        assert db_attachment.file_description == resp_attachment[\"file_description\"]\n        assert db_attachment.file_size_bytes == resp_attachment[\"file_size_bytes\"]\n        assert db_attachment.created_at.strftime(\"%Y-%m-%dT%H:%M:%S.%f%:z\") == str(\n            resp_attachment[\"created_at\"]\n        )\n        assert db_attachment.updated_at.strftime(\"%Y-%m-%dT%H:%M:%S.%f%:z\") == str(\n            resp_attachment[\"updated_at\"]\n        )\n\n\ndef validate_opportunity_summary(db_summary: OpportunitySummary, resp_summary: dict):\n    if db_summary is None:\n        assert resp_summary is None\n        return\n\n    assert db_summary.summary_description == resp_summary[\"summary_description\"]\n    assert db_summary.is_cost_sharing == resp_summary[\"is_cost_sharing\"]\n    assert db_summary.is_forecast == resp_summary[\"is_forecast\"]\n    assert str(db_summary.close_date) == str(resp_summary[\"close_date\"])\n    assert db_summary.close_date_description == resp_summary[\"close_date_description\"]\n    assert str(db_summary.post_date) == str(resp_summary[\"post_date\"])\n    assert str(db_summary.archive_date) == str(resp_summary[\"archive_date\"])\n    assert db_summary.expected_number_of_awards == resp_summary[\"expected_number_of_awards\"]\n    assert (\n        db_summary.estimated_total_program_funding\n        == resp_summary[\"estimated_total_program_funding\"]\n    )\n    assert db_summary.award_floor == resp_summary[\"award_floor\"]\n    assert db_summary.award_ceiling == resp_summary[\"award_ceiling\"]\n    assert db_summary.additional_info_url == resp_summary[\"additional_info_url\"]\n    assert (\n        db_summary.additional_info_url_description\n        == resp_summary[\"additional_info_url_description\"]\n    )\n\n    assert str(db_summary.forecasted_post_date) == str(resp_summary[\"forecasted_post_date\"])\n    assert str(db_summary.forecasted_close_date) == str(resp_summary[\"forecasted_close_date\"])\n    assert (\n        db_summary.forecasted_close_date_description\n        == resp_summary[\"forecasted_close_date_description\"]\n    )\n    assert str(db_summary.forecasted_award_date) == str(resp_summary[\"forecasted_award_date\"])\n    assert str(db_summary.forecasted_project_start_date) == str(\n        resp_summary[\"forecasted_project_start_date\"]\n    )\n    assert db_summary.fiscal_year == resp_summary[\"fiscal_year\"]\n\n    assert db_summary.funding_category_description == resp_summary[\"funding_category_description\"]\n    assert (\n        db_summary.applicant_eligibility_description\n        == resp_summary[\"applicant_eligibility_description\"]\n    )\n\n    assert \"agency_phone_number\" not in resp_summary\n    assert db_summary.agency_contact_description == resp_summary[\"agency_contact_description\"]\n    assert db_summary.agency_email_address == resp_summary[\"agency_email_address\"]\n    assert (\n        db_summary.agency_email_address_description\n        == resp_summary[\"agency_email_address_description\"]\n    )\n\n    assert set(db_summary.funding_instruments) == set(resp_summary[\"funding_instruments\"])\n    assert set(db_summary.funding_categories) == set(resp_summary[\"funding_categories\"])\n    assert set(db_summary.applicant_types) == set(resp_summary[\"applicant_types\"])\n\n\ndef validate_assistance_listings(\n    db_assistance_listings: list[OpportunityAssistanceListing], resp_listings: list[dict]\n) -> None:\n    # In order to compare this list, sort them both the same and compare from there\n    db_assistance_listings.sort(key=lambda a: (a.assistance_listing_number, a.program_title))\n    resp_listings.sort(key=lambda a: (a[\"assistance_listing_number\"], a[\"program_title\"]))\n\n    assert len(db_assistance_listings) == len(resp_listings)\n    for db_assistance_listing, resp_listing in zip(\n        db_assistance_listings, resp_listings, strict=True\n    ):\n        assert (\n            db_assistance_listing.assistance_listing_number\n            == resp_listing[\"assistance_listing_number\"]\n        )\n        assert db_assistance_listing.program_title == resp_listing[\"program_title\"]\n\n\ndef validate_search_pagination(\n    search_response: dict,\n    search_request: dict,\n    expected_total_pages: int,\n    expected_total_records: int,\n    expected_response_record_count: int,\n):\n    pagination_info = search_response[\"pagination_info\"]\n    assert pagination_info[\"page_offset\"] == search_request[\"pagination\"][\"page_offset\"]\n    assert pagination_info[\"page_size\"] == search_request[\"pagination\"][\"page_size\"]\n    assert pagination_info[\"order_by\"] == search_request[\"pagination\"][\"order_by\"]\n    assert pagination_info[\"sort_direction\"] == search_request[\"pagination\"][\"sort_direction\"]\n\n    assert pagination_info[\"total_pages\"] == expected_total_pages\n    assert pagination_info[\"total_records\"] == expected_total_records\n\n    searched_opportunities = search_response[\"data\"]\n    assert len(searched_opportunities) == expected_response_record_count\n\n    # Verify data is sorted as expected\n    reverse = pagination_info[\"sort_direction\"] == \"descending\"\n    resorted_opportunities = sorted(\n        searched_opportunities, key=lambda u: u[pagination_info[\"order_by\"]], reverse=reverse\n    )\n    assert resorted_opportunities == searched_opportunities"}
{"path":"frontend/stories/components/SaveButton.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/SaveButton.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/opportunities_v1/test_opportunity_auth.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/opportunities_v1/test_opportunity_auth.py\nSize: 0.70 KB\nLast Modified: 2025-02-14T17:08:26.458Z"}
{"path":"frontend/stories/components/opportunity/OpportunityDocuments.stories.tsx","language":"typescript","type":"code","directory":"frontend/stories/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/stories/components/opportunity/OpportunityDocuments.stories.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"from tests.src.api.opportunities_v1.conftest import get_search_request\n\n\n@pytest.mark.parametrize(\n    \"method,url,body\",\n    [\n        (\"POST\", \"/v1/opportunities/search\", get_search_request()),\n        (\"GET\", \"/v1/opportunities/1\", None),\n    ],\n)\ndef test_opportunity_unauthorized_401(client, api_auth_token, method, url, body):\n    # open is just the generic method that post/get/etc. call under the hood\n    response = client.open(url, method=method, json=body, headers={\"X-Auth\": \"incorrect token\"})\n\n    assert response.status_code == 401\n    assert (\n        response.get_json()[\"message\"]\n        == \"The server could not verify that you are authorized to access the URL requested\"\n    )"}
{"path":"frontend/tests/__mocks__/focus-trap-react.tsx","language":"typescript","type":"code","directory":"frontend/tests/__mocks__","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/__mocks__/focus-trap-react.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/opportunities_v1/test_opportunity_route_get.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/opportunities_v1/test_opportunity_route_get.py\nSize: 6.51 KB\nLast Modified: 2025-02-14T17:08:26.458Z"}
{"path":"frontend/tests/api/auth/callback/route.test.ts","language":"typescript","type":"code","directory":"frontend/tests/api/auth/callback","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/api/auth/callback/route.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.135Z","content":"from tests.src.api.opportunities_v1.conftest import (\n    validate_opportunity,\n    validate_opportunity_with_attachments,\n)\nfrom tests.src.db.models.factories import (\n    AgencyFactory,\n    CurrentOpportunitySummaryFactory,\n    OpportunityAttachmentFactory,\n    OpportunityFactory,\n    OpportunitySummaryFactory,\n)\n\n#####################################\n# GET opportunity tests\n#####################################\n\n\n@pytest.mark.parametrize(\n    \"opportunity_params,opportunity_summary_params\",\n    [\n        ({}, {}),\n        # Only an opportunity exists, no other connected records\n        (\n            {\n                \"opportunity_assistance_listings\": [],\n            },\n            None,\n        ),\n        # Summary exists, but none of the list values set\n        (\n            {},\n            {\n                \"link_funding_instruments\": [],\n                \"link_funding_categories\": [],\n                \"link_applicant_types\": [],\n            },\n        ),\n        # All possible values set to null/empty\n        # Note this uses traits on the factories to handle setting everything\n        ({\"all_fields_null\": True}, {\"all_fields_null\": True}),\n    ],\n)\ndef test_get_opportunity_200(\n    client, api_auth_token, enable_factory_create, opportunity_params, opportunity_summary_params\n):\n    # Split the setup of the opportunity from the opportunity summary to simplify the factory usage a bit\n    db_opportunity = OpportunityFactory.create(\n        **opportunity_params, current_opportunity_summary=None\n    )  # We'll set the current opportunity below\n    if opportunity_summary_params is not None:\n        db_opportunity_summary = OpportunitySummaryFactory.create(\n            **opportunity_summary_params, opportunity=db_opportunity\n        )\n        CurrentOpportunitySummaryFactory.create(\n            opportunity=db_opportunity, opportunity_summary=db_opportunity_summary\n        )\n\n    resp = client.get(\n        f\"/v1/opportunities/{db_opportunity.opportunity_id}\", headers={\"X-Auth\": api_auth_token}\n    )\n    assert resp.status_code == 200\n    response_data = resp.get_json()[\"data\"]\n\n    validate_opportunity(db_opportunity, response_data)\n\n\ndef test_get_opportunity_with_attachment_200(\n    client, api_auth_token, enable_factory_create, db_session\n):\n    # Create an opportunity with an attachment\n    opportunity = OpportunityFactory.create(has_attachments=True)\n    db_session.commit()\n\n    # Make the GET request\n    resp = client.get(\n        f\"/v1/opportunities/{opportunity.opportunity_id}\", headers={\"X-Auth\": api_auth_token}\n    )\n\n    # Check the response\n    assert resp.status_code == 200\n    response_data = resp.get_json()[\"data\"]\n\n    # Validate the opportunity data\n    assert len(response_data[\"attachments\"]) > 0\n    validate_opportunity_with_attachments(opportunity, response_data)\n\n\ndef test_get_opportunity_with_agency_200(client, api_auth_token, enable_factory_create):\n    parent_agency = AgencyFactory.create(agency_code=\"EXAMPLEAGENCYXYZ\")\n    child_agency = AgencyFactory.create(\n        agency_code=\"EXAMPLEAGENCYXYZ-12345678\", top_level_agency=parent_agency\n    )\n\n    opportunity = OpportunityFactory.create(agency_code=child_agency.agency_code)\n\n    resp = client.get(\n        f\"/v1/opportunities/{opportunity.opportunity_id}\", headers={\"X-Auth\": api_auth_token}\n    )\n\n    assert resp.status_code == 200\n    response_data = resp.get_json()[\"data\"]\n\n    assert response_data[\"agency_code\"] == child_agency.agency_code\n    assert response_data[\"agency_name\"] == child_agency.agency_name\n    assert response_data[\"top_level_agency_name\"] == parent_agency.agency_name\n\n\ndef test_get_opportunity_s3_endpoint_url_200(\n    client, api_auth_token, enable_factory_create, db_session, mock_s3_bucket, monkeypatch_session\n):\n    monkeypatch_session.delenv(\"CDN_URL\")\n    # Create an opportunity with a specific attachment\n    opportunity = OpportunityFactory.create(opportunity_attachments=[])\n    object_name = \"test_file_1.txt\"\n    file_loc = f\"s3://{mock_s3_bucket}/{object_name}\"\n    OpportunityAttachmentFactory.create(\n        file_location=file_loc, opportunity=opportunity, file_contents=\"Hello, world\"\n    )\n\n    # Make the GET request\n    resp = client.get(\n        f\"/v1/opportunities/{opportunity.opportunity_id}\", headers={\"X-Auth\": api_auth_token}\n    )\n\n    # Check the response\n    assert resp.status_code == 200\n    response_data = resp.get_json()[\"data\"]\n    presigned_url = response_data[\"attachments\"][0][\"download_path\"]\n\n    # Validate pre-signed url\n    response = requests.get(presigned_url, timeout=5)\n    assert response.status_code == 200\n    assert response.text == \"Hello, world\"\n\n\ndef test_get_opportunity_404_not_found(client, api_auth_token, truncate_opportunities):\n    resp = client.get(\"/v1/opportunities/1\", headers={\"X-Auth\": api_auth_token})\n    assert resp.status_code == 404\n    assert resp.get_json()[\"message\"] == \"Could not find Opportunity with ID 1\"\n\n\ndef test_get_opportunity_404_not_found_is_draft(client, api_auth_token, enable_factory_create):\n    # The endpoint won't return drafts, so this'll be a 404 despite existing\n    opportunity = OpportunityFactory.create(is_draft=True)\n\n    resp = client.get(\n        f\"/v1/opportunities/{opportunity.opportunity_id}\", headers={\"X-Auth\": api_auth_token}\n    )\n    assert resp.status_code == 404\n    assert (\n        resp.get_json()[\"message\"]\n        == f\"Could not find Opportunity with ID {opportunity.opportunity_id}\"\n    )\n\n\ndef test_get_opportunity_returns_cdn_urls(\n    client, api_auth_token, monkeypatch_session, enable_factory_create, db_session, mock_s3_bucket\n):\n    monkeypatch_session.setenv(\"CDN_URL\", \"https://cdn.example.com\")\n    \"\"\"Test that S3 file locations are converted to CDN URLs in the response\"\"\"\n    # Create an opportunity with a specific attachment\n    opportunity = OpportunityFactory.create(opportunity_attachments=[])\n\n    object_name = \"test_file_1.txt\"\n    file_loc = f\"s3://{mock_s3_bucket}/{object_name}\"\n    OpportunityAttachmentFactory.create(\n        file_location=file_loc, opportunity=opportunity, file_contents=\"Hello, world\"\n    )\n\n    # Make the GET request\n    resp = client.get(\n        f\"/v1/opportunities/{opportunity.opportunity_id}\", headers={\"X-Auth\": api_auth_token}\n    )\n\n    # Check the response\n    assert resp.status_code == 200\n    response_data = resp.get_json()[\"data\"]\n\n    # Verify attachment URL is a CDN URL\n    assert len(response_data[\"attachments\"]) == 1\n    attachment = response_data[\"attachments\"][0]\n\n    assert attachment[\"download_path\"].startswith(\"https://cdn.\")\n    assert \"s3://\" not in attachment[\"download_path\"]"}
{"path":"frontend/tests/api/auth/login/route.test.ts","language":"typescript","type":"code","directory":"frontend/tests/api/auth/login","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/api/auth/login/route.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/opportunities_v1/test_opportunity_route_search.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/opportunities_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/opportunities_v1/test_opportunity_route_search.py\nSize: 57.69 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/api/auth/logout/route.test.ts","language":"typescript","type":"code","directory":"frontend/tests/api/auth/logout","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/api/auth/logout/route.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.135Z","content":"import pytest\n\nfrom src.api.opportunities_v1.opportunity_schemas import OpportunityV1Schema\nfrom src.constants.lookup_constants import (\n    ApplicantType,\n    FundingCategory,\n    FundingInstrument,\n    OpportunityStatus,\n)\nfrom src.db.models.opportunity_models import Opportunity\nfrom src.pagination.pagination_models import SortDirection\nfrom src.util.dict_util import flatten_dict\nfrom tests.conftest import BaseTestClass\nfrom tests.src.api.opportunities_v1.conftest import get_search_request\nfrom tests.src.db.models.factories import (\n    CurrentOpportunitySummaryFactory,\n    OpportunityAssistanceListingFactory,\n    OpportunityFactory,\n    OpportunitySummaryFactory,\n)\n\n\ndef validate_search_response(\n    search_response,\n    expected_results: list[Opportunity],\n    expected_status_code: int = 200,\n    is_csv_response: bool = False,\n):\n    assert search_response.status_code == expected_status_code\n\n    expected_ids = [exp.opportunity_id for exp in expected_results]\n\n    if is_csv_response:\n        reader = csv.DictReader(search_response.text.split(\"\\n\"))\n        opportunities = [record for record in reader]\n    else:\n        response_json = search_response.get_json()\n        opportunities = response_json[\"data\"]\n\n    response_ids = [int(opp[\"opportunity_id\"]) for opp in opportunities]\n\n    for opp in opportunities:\n        if \"summary\" in opp:\n            assert \"agency_phone_number\" not in opp[\"summary\"]\n\n    assert (\n        response_ids == expected_ids\n    ), f\"Actual opportunities:\\n {'\\n'.join([opp['opportunity_title'] for opp in opportunities])}\"\n\n\ndef call_search_and_validate(client, api_auth_token, search_request, expected_results):\n    resp = client.post(\n        \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n    )\n    validate_search_response(resp, expected_results)\n\n    search_request[\"format\"] = \"csv\"\n    resp = client.post(\n        \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n    )\n    validate_search_response(resp, expected_results, is_csv_response=True)\n\n\ndef build_opp(\n    opportunity_title: str,\n    opportunity_number: str,\n    agency: str,\n    summary_description: str,\n    opportunity_status: OpportunityStatus,\n    assistance_listings: list,\n    applicant_types: list,\n    funding_instruments: list,\n    funding_categories: list,\n    post_date: date,\n    close_date: date | None,\n    is_cost_sharing: bool,\n    expected_number_of_awards: int | None,\n    award_floor: int | None,\n    award_ceiling: int | None,\n    estimated_total_program_funding: int | None,\n    agency_phone_number: str | None = \"123-456-7890\",\n) -> Opportunity:\n    opportunity = OpportunityFactory.build(\n        opportunity_title=opportunity_title,\n        opportunity_number=opportunity_number,\n        agency_code=agency,\n        opportunity_assistance_listings=[],\n        current_opportunity_summary=None,\n    )\n\n    for assistance_listing in assistance_listings:\n        opportunity.opportunity_assistance_listings.append(\n            OpportunityAssistanceListingFactory.build(\n                opportunity=opportunity,\n                assistance_listing_number=assistance_listing[0],\n                program_title=assistance_listing[1],\n            )\n        )\n\n    opportunity_summary = OpportunitySummaryFactory.build(\n        opportunity=opportunity,\n        summary_description=summary_description,\n        applicant_types=applicant_types,\n        funding_instruments=funding_instruments,\n        funding_categories=funding_categories,\n        post_date=post_date,\n        close_date=close_date,\n        is_cost_sharing=is_cost_sharing,\n        expected_number_of_awards=expected_number_of_awards,\n        award_floor=award_floor,\n        award_ceiling=award_ceiling,\n        estimated_total_program_funding=estimated_total_program_funding,\n        agency_phone_number=agency_phone_number,\n    )\n\n    opportunity.current_opportunity_summary = CurrentOpportunitySummaryFactory.build(\n        opportunity_status=opportunity_status,\n        opportunity_summary=opportunity_summary,\n        opportunity=opportunity,\n    )\n\n    return opportunity\n\n\n##########################################\n# Opportunity scenarios for tests\n#\n# These try to mimic real opportunities\n##########################################\n\nEDUCATION_AL = (\"43.008\", \"Office of Stem Engagement (OSTEM)\")\nSPACE_AL = (\"43.012\", \"Space Technology\")\nAERONAUTICS_AL = (\"43.002\", \"Aeronautics\")\nLOC_AL = (\"42.011\", \"Library of Congress Grants\")\nAMERICAN_AL = (\"19.441\", \"ECA - American Spaces\")\nECONOMIC_AL = (\"11.307\", \"Economic Adjustment Assistance\")\nMANUFACTURING_AL = (\"11.611\", \"Manufacturing Extension Partnership\")\n\nNASA_SPACE_FELLOWSHIP = build_opp(\n    opportunity_title=\"National Space Grant College and Fellowship Program FY 2020 - 2024\",\n    opportunity_number=\"NNH123ZYX\",\n    agency=\"NASA\",\n    summary_description=\"This Cooperative Agreement Notice is a multi-year award that aims to contribute to NASA's mission\",\n    opportunity_status=OpportunityStatus.POSTED,\n    assistance_listings=[EDUCATION_AL],\n    applicant_types=[ApplicantType.OTHER],\n    funding_instruments=[FundingInstrument.COOPERATIVE_AGREEMENT],\n    funding_categories=[FundingCategory.EDUCATION],\n    post_date=date(2020, 3, 1),\n    close_date=date(2027, 6, 1),\n    is_cost_sharing=True,\n    expected_number_of_awards=3,\n    award_floor=50_000,\n    award_ceiling=5_000_000,\n    estimated_total_program_funding=15_000_000,\n    agency_phone_number=\"123-456-7890\",\n)\n\nNASA_INNOVATIONS = build_opp(\n    opportunity_title=\"Early Stage Innovations\",\n    opportunity_number=\"NNH24-TR0N\",\n    agency=\"NASA\",\n    summary_description=\"The program within STMD seeks proposals from accredited U.S. universities to develop unique, disruptive, or transformational space technologies.\",\n    opportunity_status=OpportunityStatus.FORECASTED,\n    assistance_listings=[SPACE_AL],\n    applicant_types=[ApplicantType.OTHER],\n    funding_instruments=[FundingInstrument.GRANT],\n    funding_categories=[FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT],\n    post_date=date(2019, 3, 1),\n    close_date=None,\n    is_cost_sharing=False,\n    expected_number_of_awards=1,\n    award_floor=5000,\n    award_ceiling=5000,\n    estimated_total_program_funding=5000,\n)\n\nNASA_SUPERSONIC = build_opp(\n    opportunity_title=\"Commercial Supersonic Technology (CST) Project\",\n    opportunity_number=\"NNH24-CST\",\n    agency=\"NASA\",\n    summary_description=\"Commercial Supersonic Technology seeks proposals for a fuel injector design concept and fabrication for testing at NASA Glenn Research Center\",\n    opportunity_status=OpportunityStatus.CLOSED,\n    assistance_listings=[AERONAUTICS_AL],\n    applicant_types=[ApplicantType.UNRESTRICTED],\n    funding_instruments=[FundingInstrument.GRANT],\n    funding_categories=[FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT],\n    post_date=date(2021, 3, 1),\n    close_date=date(2030, 6, 1),\n    is_cost_sharing=True,\n    expected_number_of_awards=9,\n    award_floor=10_000,\n    award_ceiling=50_000,\n    estimated_total_program_funding=None,\n)\n\nNASA_K12_DIVERSITY = build_opp(\n    opportunity_title=\"Space Grant K-12 Inclusiveness and Diversity in STEM\",\n    opportunity_number=\"NNH22ZHA\",\n    agency=\"NASA\",\n    summary_description=\"Expands the reach of individual Consortia to collaborate regionally on efforts that directly support middle and high school student participation in hands-on, NASA-aligned STEM activities\",\n    opportunity_status=OpportunityStatus.ARCHIVED,\n    assistance_listings=[EDUCATION_AL],\n    applicant_types=[ApplicantType.OTHER],\n    funding_instruments=[FundingInstrument.COOPERATIVE_AGREEMENT],\n    funding_categories=[FundingCategory.EDUCATION],\n    post_date=date(2025, 3, 1),\n    close_date=date(2018, 6, 1),\n    is_cost_sharing=False,\n    expected_number_of_awards=None,\n    award_floor=None,\n    award_ceiling=None,\n    estimated_total_program_funding=None,\n)\n\nLOC_TEACHING = build_opp(\n    opportunity_title=\"Teaching with Primary Sources - New Awards for FY25-FY27\",\n    opportunity_number=\"012ADV345\",\n    agency=\"LOC\",\n    summary_description=\"Builds student literacy, critical thinking skills, content knowledge and ability to conduct original research.\",\n    opportunity_status=OpportunityStatus.POSTED,\n    assistance_listings=[EDUCATION_AL],\n    applicant_types=[\n        ApplicantType.STATE_GOVERNMENTS,\n        ApplicantType.COUNTY_GOVERNMENTS,\n        ApplicantType.INDEPENDENT_SCHOOL_DISTRICTS,\n        ApplicantType.CITY_OR_TOWNSHIP_GOVERNMENTS,\n        ApplicantType.SPECIAL_DISTRICT_GOVERNMENTS,\n    ],\n    funding_instruments=[FundingInstrument.COOPERATIVE_AGREEMENT],\n    funding_categories=[FundingCategory.EDUCATION],\n    post_date=date(2031, 3, 1),\n    close_date=date(2010, 6, 1),\n    is_cost_sharing=True,\n    expected_number_of_awards=100,\n    award_floor=500,\n    award_ceiling=1_000,\n    estimated_total_program_funding=10_000,\n)\n\nLOC_HIGHER_EDUCATION = build_opp(\n    opportunity_title=\"Of the People: Widening the Path: CCDI â€“ Higher Education\",\n    opportunity_number=\"012ADV346\",\n    agency=\"LOC\",\n    summary_description=\"The Library of Congress will expand the connections between the Library and diverse communities and strengthen the use of Library of Congress digital collections and digital tools\",\n    opportunity_status=OpportunityStatus.FORECASTED,\n    assistance_listings=[LOC_AL],\n    applicant_types=[\n        ApplicantType.PRIVATE_INSTITUTIONS_OF_HIGHER_EDUCATION,\n        ApplicantType.PUBLIC_AND_STATE_INSTITUTIONS_OF_HIGHER_EDUCATION,\n    ],\n    funding_instruments=[FundingInstrument.GRANT],\n    funding_categories=[FundingCategory.OTHER],\n    post_date=date(2026, 3, 1),\n    close_date=None,\n    is_cost_sharing=False,\n    expected_number_of_awards=1,\n    award_floor=None,\n    award_ceiling=None,\n    estimated_total_program_funding=15_000_000,\n)\n\nDOS_DIGITAL_LITERACY = build_opp(\n    opportunity_title=\"American Spaces Digital Literacy and Training Program\",\n    opportunity_number=\"SFOP0001234\",\n    agency=\"DOS-ECA\",\n    summary_description=\"An open competition to administer a new award in the field of digital and media literacy and countering disinformation\",\n    opportunity_status=OpportunityStatus.CLOSED,\n    assistance_listings=[AMERICAN_AL],\n    applicant_types=[\n        ApplicantType.OTHER,\n        ApplicantType.NONPROFITS_NON_HIGHER_EDUCATION_WITH_501C3,\n        ApplicantType.PRIVATE_INSTITUTIONS_OF_HIGHER_EDUCATION,\n        ApplicantType.PUBLIC_AND_STATE_INSTITUTIONS_OF_HIGHER_EDUCATION,\n    ],\n    funding_instruments=[FundingInstrument.COOPERATIVE_AGREEMENT],\n    funding_categories=[FundingCategory.OTHER],\n    post_date=date(2028, 3, 1),\n    close_date=date(2023, 6, 1),\n    is_cost_sharing=True,\n    expected_number_of_awards=2,\n    award_floor=5,\n    award_ceiling=10,\n    estimated_total_program_funding=15,\n)\n\nDOC_SPACE_COAST = build_opp(\n    opportunity_title=\"Space Coast RIC\",\n    opportunity_number=\"SFOP0009876\",\n    agency=\"DOC-EDA\",\n    summary_description=\"diversification of Florida's Space Coast region\",\n    opportunity_status=OpportunityStatus.ARCHIVED,\n    assistance_listings=[ECONOMIC_AL],\n    applicant_types=[\n        ApplicantType.CITY_OR_TOWNSHIP_GOVERNMENTS,\n        ApplicantType.COUNTY_GOVERNMENTS,\n        ApplicantType.STATE_GOVERNMENTS,\n    ],\n    funding_instruments=[FundingInstrument.COOPERATIVE_AGREEMENT, FundingInstrument.GRANT],\n    funding_categories=[FundingCategory.OTHER, FundingCategory.REGIONAL_DEVELOPMENT],\n    post_date=date(2017, 3, 1),\n    close_date=date(2019, 6, 1),\n    is_cost_sharing=False,\n    expected_number_of_awards=1000,\n    award_floor=1,\n    award_ceiling=2,\n    estimated_total_program_funding=2000,\n)\n\nDOC_MANUFACTURING = build_opp(\n    opportunity_title=\"Advanced Manufacturing Jobs and Innovation Accelerator Challenge\",\n    opportunity_number=\"JIAC1234AM\",\n    agency=\"DOC-EDA\",\n    summary_description=\"foster job creation, increase public and private investments, and enhance economic prosperity\",\n    opportunity_status=OpportunityStatus.POSTED,\n    assistance_listings=[ECONOMIC_AL, MANUFACTURING_AL],\n    applicant_types=[ApplicantType.OTHER],\n    funding_instruments=[FundingInstrument.COOPERATIVE_AGREEMENT, FundingInstrument.GRANT],\n    funding_categories=[\n        FundingCategory.EMPLOYMENT_LABOR_AND_TRAINING,\n        FundingCategory.ENERGY,\n        FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT,\n    ],\n    post_date=date(2013, 3, 1),\n    close_date=date(2035, 6, 1),\n    is_cost_sharing=True,\n    expected_number_of_awards=25,\n    award_floor=50_000_000,\n    award_ceiling=5_000_000_000,\n    estimated_total_program_funding=15_000_000_000,\n)\n\nOPPORTUNITIES = [\n    NASA_SPACE_FELLOWSHIP,\n    NASA_INNOVATIONS,\n    NASA_SUPERSONIC,\n    NASA_K12_DIVERSITY,\n    LOC_TEACHING,\n    LOC_HIGHER_EDUCATION,\n    DOS_DIGITAL_LITERACY,\n    DOC_SPACE_COAST,\n    DOC_MANUFACTURING,\n]\n\n\ndef search_scenario_id_fnc(val):\n    if isinstance(val, dict):\n        return str(flatten_dict(val, separator=\"|\"))\n\n\nclass TestOpportunityRouteSearch(BaseTestClass):\n    @pytest.fixture(scope=\"class\", autouse=True)\n    def setup_search_data(self, opportunity_index, opportunity_index_alias, search_client):\n        # Load into the search index\n        schema = OpportunityV1Schema()\n        json_records = [schema.dump(opportunity) for opportunity in OPPORTUNITIES]\n        search_client.bulk_upsert(opportunity_index, json_records, \"opportunity_id\")\n\n        # Swap the search index alias\n        search_client.swap_alias_index(opportunity_index, opportunity_index_alias)\n\n    @pytest.mark.parametrize(\n        \"search_request,expected_results\",\n        [\n            # Opportunity ID\n            (\n                get_search_request(\n                    page_size=25,\n                    page_offset=1,\n                    sort_order=[\n                        {\"order_by\": \"opportunity_id\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                ),\n                OPPORTUNITIES,\n            ),\n            (\n                get_search_request(\n                    page_size=3,\n                    page_offset=2,\n                    sort_order=[\n                        {\"order_by\": \"opportunity_id\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                ),\n                OPPORTUNITIES[3:6],\n            ),\n            (\n                get_search_request(\n                    page_size=25,\n                    page_offset=1,\n                    sort_order=[\n                        {\"order_by\": \"opportunity_id\", \"sort_direction\": SortDirection.DESCENDING}\n                    ],\n                ),\n                OPPORTUNITIES[::-1],\n            ),\n            # Opportunity Number\n            (\n                get_search_request(\n                    page_size=3,\n                    page_offset=1,\n                    sort_order=[\n                        {\n                            \"order_by\": \"opportunity_number\",\n                            \"sort_direction\": SortDirection.ASCENDING,\n                        }\n                    ],\n                ),\n                [LOC_TEACHING, LOC_HIGHER_EDUCATION, DOC_MANUFACTURING],\n            ),\n            (\n                get_search_request(\n                    page_size=2,\n                    page_offset=3,\n                    sort_order=[\n                        {\n                            \"order_by\": \"opportunity_number\",\n                            \"sort_direction\": SortDirection.DESCENDING,\n                        }\n                    ],\n                ),\n                [NASA_K12_DIVERSITY, NASA_SPACE_FELLOWSHIP],\n            ),\n            # Opportunity Title\n            (\n                get_search_request(\n                    page_size=4,\n                    page_offset=2,\n                    sort_order=[\n                        {\"order_by\": \"opportunity_title\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                ),\n                [NASA_SPACE_FELLOWSHIP, LOC_HIGHER_EDUCATION, DOC_SPACE_COAST, NASA_K12_DIVERSITY],\n            ),\n            (\n                get_search_request(\n                    page_size=5,\n                    page_offset=1,\n                    sort_order=[\n                        {\n                            \"order_by\": \"opportunity_title\",\n                            \"sort_direction\": SortDirection.DESCENDING,\n                        }\n                    ],\n                ),\n                [\n                    LOC_TEACHING,\n                    NASA_K12_DIVERSITY,\n                    DOC_SPACE_COAST,\n                    LOC_HIGHER_EDUCATION,\n                    NASA_SPACE_FELLOWSHIP,\n                ],\n            ),\n            # Post Date\n            (\n                get_search_request(\n                    page_size=2,\n                    page_offset=1,\n                    sort_order=[\n                        {\"order_by\": \"post_date\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                ),\n                [DOC_MANUFACTURING, DOC_SPACE_COAST],\n            ),\n            (\n                get_search_request(\n                    page_size=3,\n                    page_offset=1,\n                    sort_order=[\n                        {\"order_by\": \"post_date\", \"sort_direction\": SortDirection.DESCENDING}\n                    ],\n                ),\n                [LOC_TEACHING, DOS_DIGITAL_LITERACY, LOC_HIGHER_EDUCATION],\n            ),\n            (\n                get_search_request(\n                    page_size=3,\n                    page_offset=12,\n                    sort_order=[\n                        {\"order_by\": \"post_date\", \"sort_direction\": SortDirection.DESCENDING}\n                    ],\n                ),\n                [],\n            ),\n            # Relevancy has a secondary sort of post date so should be identical.\n            (\n                get_search_request(\n                    page_size=2,\n                    page_offset=1,\n                    sort_order=[\n                        {\"order_by\": \"relevancy\", \"sort_direction\": SortDirection.ASCENDING},\n                        {\"order_by\": \"post_date\", \"sort_direction\": SortDirection.ASCENDING},\n                    ],\n                ),\n                [DOC_MANUFACTURING, DOC_SPACE_COAST],\n            ),\n            (\n                get_search_request(\n                    page_size=3,\n                    page_offset=1,\n                    sort_order=[\n                        {\"order_by\": \"relevancy\", \"sort_direction\": SortDirection.DESCENDING},\n                        {\"order_by\": \"post_date\", \"sort_direction\": SortDirection.DESCENDING},\n                    ],\n                ),\n                [LOC_TEACHING, DOS_DIGITAL_LITERACY, LOC_HIGHER_EDUCATION],\n            ),\n            (\n                get_search_request(\n                    page_size=3,\n                    page_offset=12,\n                    sort_order=[\n                        {\"order_by\": \"relevancy\", \"sort_direction\": SortDirection.DESCENDING},\n                        {\"order_by\": \"post_date\", \"sort_direction\": SortDirection.DESCENDING},\n                    ],\n                ),\n                [],\n            ),\n            # Close Date (note several have null values which always go to the end)\n            (\n                get_search_request(\n                    page_size=4,\n                    page_offset=1,\n                    sort_order=[\n                        {\"order_by\": \"close_date\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                ),\n                [LOC_TEACHING, NASA_K12_DIVERSITY, DOC_SPACE_COAST, DOS_DIGITAL_LITERACY],\n            ),\n            (\n                get_search_request(\n                    page_size=3,\n                    page_offset=1,\n                    sort_order=[\n                        {\"order_by\": \"close_date\", \"sort_direction\": SortDirection.DESCENDING}\n                    ],\n                ),\n                [DOC_MANUFACTURING, NASA_SUPERSONIC, NASA_SPACE_FELLOWSHIP],\n            ),\n            # close date - but check the end of the list to find the null values\n            (\n                get_search_request(\n                    page_size=5,\n                    page_offset=2,\n                    sort_order=[\n                        {\"order_by\": \"close_date\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                ),\n                [NASA_SUPERSONIC, DOC_MANUFACTURING, NASA_INNOVATIONS, LOC_HIGHER_EDUCATION],\n            ),\n            # Agency\n            (\n                get_search_request(\n                    page_size=5,\n                    page_offset=1,\n                    sort_order=[\n                        {\"order_by\": \"agency_code\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                ),\n                [\n                    DOC_SPACE_COAST,\n                    DOC_MANUFACTURING,\n                    DOS_DIGITAL_LITERACY,\n                    LOC_TEACHING,\n                    LOC_HIGHER_EDUCATION,\n                ],\n            ),\n            (\n                get_search_request(\n                    page_size=3,\n                    page_offset=1,\n                    sort_order=[\n                        {\"order_by\": \"agency_code\", \"sort_direction\": SortDirection.DESCENDING}\n                    ],\n                ),\n                [NASA_SPACE_FELLOWSHIP, NASA_INNOVATIONS, NASA_SUPERSONIC],\n            ),\n        ],\n        ids=search_scenario_id_fnc,\n    )\n    def test_sorting_and_pagination_200(\n        self, client, api_auth_token, search_request, expected_results\n    ):\n        call_search_and_validate(client, api_auth_token, search_request, expected_results)\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            get_search_request(page_size=0),\n            get_search_request(page_size=-1),\n            get_search_request(page_size=5001),\n        ],\n        ids=search_scenario_id_fnc,\n    )\n    def test_page_size_422(self, client, api_auth_token, search_request):\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n        assert resp.status_code == 422\n\n        json = resp.get_json()\n        error = json[\"errors\"][0]\n        assert json[\"message\"] == \"Validation error\"\n        assert (\n            error[\"message\"] == \"Must be greater than or equal to 1 and less than or equal to 5000.\"\n        )\n\n    @pytest.mark.parametrize(\n        \"search_request, expected_results\",\n        [\n            # Agency\n            (get_search_request(agency_one_of=[\"not an agency\"]), []),\n            (\n                get_search_request(agency_one_of=[\"NASA\"]),\n                [NASA_SPACE_FELLOWSHIP, NASA_INNOVATIONS, NASA_SUPERSONIC, NASA_K12_DIVERSITY],\n            ),\n            (get_search_request(agency_one_of=[\"LOC\"]), [LOC_TEACHING, LOC_HIGHER_EDUCATION]),\n            (get_search_request(agency_one_of=[\"DOS-ECA\"]), [DOS_DIGITAL_LITERACY]),\n            (get_search_request(agency_one_of=[\"DOC-EDA\"]), [DOC_SPACE_COAST, DOC_MANUFACTURING]),\n            (\n                get_search_request(\n                    agency_one_of=[\"DOC-EDA\", \"NASA\", \"LOC\", \"DOS-ECA\", \"something else\"]\n                ),\n                OPPORTUNITIES,\n            ),\n            # Opportunity Status\n            (\n                get_search_request(opportunity_status_one_of=[OpportunityStatus.POSTED]),\n                [NASA_SPACE_FELLOWSHIP, LOC_TEACHING, DOC_MANUFACTURING],\n            ),\n            (\n                get_search_request(opportunity_status_one_of=[OpportunityStatus.FORECASTED]),\n                [NASA_INNOVATIONS, LOC_HIGHER_EDUCATION],\n            ),\n            (\n                get_search_request(opportunity_status_one_of=[OpportunityStatus.CLOSED]),\n                [NASA_SUPERSONIC, DOS_DIGITAL_LITERACY],\n            ),\n            (\n                get_search_request(opportunity_status_one_of=[OpportunityStatus.ARCHIVED]),\n                [NASA_K12_DIVERSITY, DOC_SPACE_COAST],\n            ),\n            (\n                get_search_request(\n                    opportunity_status_one_of=[\n                        OpportunityStatus.POSTED,\n                        OpportunityStatus.FORECASTED,\n                    ]\n                ),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_INNOVATIONS,\n                    LOC_TEACHING,\n                    LOC_HIGHER_EDUCATION,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(\n                    opportunity_status_one_of=[\n                        OpportunityStatus.POSTED,\n                        OpportunityStatus.FORECASTED,\n                        OpportunityStatus.CLOSED,\n                        OpportunityStatus.ARCHIVED,\n                    ]\n                ),\n                OPPORTUNITIES,\n            ),\n            # Funding Instrument\n            (\n                get_search_request(\n                    funding_instrument_one_of=[FundingInstrument.COOPERATIVE_AGREEMENT]\n                ),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_K12_DIVERSITY,\n                    LOC_TEACHING,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_SPACE_COAST,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(funding_instrument_one_of=[FundingInstrument.GRANT]),\n                [\n                    NASA_INNOVATIONS,\n                    NASA_SUPERSONIC,\n                    LOC_HIGHER_EDUCATION,\n                    DOC_SPACE_COAST,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(\n                    funding_instrument_one_of=[FundingInstrument.PROCUREMENT_CONTRACT]\n                ),\n                [],\n            ),\n            (get_search_request(funding_instrument_one_of=[FundingInstrument.OTHER]), []),\n            (\n                get_search_request(\n                    funding_instrument_one_of=[\n                        FundingInstrument.COOPERATIVE_AGREEMENT,\n                        FundingInstrument.GRANT,\n                    ]\n                ),\n                OPPORTUNITIES,\n            ),\n            # Funding Category\n            (\n                get_search_request(funding_category_one_of=[FundingCategory.EDUCATION]),\n                [NASA_SPACE_FELLOWSHIP, NASA_K12_DIVERSITY, LOC_TEACHING],\n            ),\n            (\n                get_search_request(\n                    funding_category_one_of=[\n                        FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT\n                    ]\n                ),\n                [NASA_INNOVATIONS, NASA_SUPERSONIC, DOC_MANUFACTURING],\n            ),\n            (\n                get_search_request(funding_category_one_of=[FundingCategory.OTHER]),\n                [LOC_HIGHER_EDUCATION, DOS_DIGITAL_LITERACY, DOC_SPACE_COAST],\n            ),\n            (\n                get_search_request(funding_category_one_of=[FundingCategory.REGIONAL_DEVELOPMENT]),\n                [DOC_SPACE_COAST],\n            ),\n            (\n                get_search_request(\n                    funding_category_one_of=[FundingCategory.EMPLOYMENT_LABOR_AND_TRAINING]\n                ),\n                [DOC_MANUFACTURING],\n            ),\n            (\n                get_search_request(funding_category_one_of=[FundingCategory.ENERGY]),\n                [DOC_MANUFACTURING],\n            ),\n            (get_search_request(funding_category_one_of=[FundingCategory.HOUSING]), []),\n            (\n                get_search_request(\n                    funding_category_one_of=[\n                        FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT,\n                        FundingCategory.REGIONAL_DEVELOPMENT,\n                    ]\n                ),\n                [NASA_INNOVATIONS, NASA_SUPERSONIC, DOC_SPACE_COAST, DOC_MANUFACTURING],\n            ),\n            # Applicant Type\n            (\n                get_search_request(applicant_type_one_of=[ApplicantType.OTHER]),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_INNOVATIONS,\n                    NASA_K12_DIVERSITY,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(applicant_type_one_of=[ApplicantType.UNRESTRICTED]),\n                [NASA_SUPERSONIC],\n            ),\n            (\n                get_search_request(applicant_type_one_of=[ApplicantType.STATE_GOVERNMENTS]),\n                [LOC_TEACHING, DOC_SPACE_COAST],\n            ),\n            (\n                get_search_request(applicant_type_one_of=[ApplicantType.COUNTY_GOVERNMENTS]),\n                [LOC_TEACHING, DOC_SPACE_COAST],\n            ),\n            (\n                get_search_request(\n                    applicant_type_one_of=[\n                        ApplicantType.PUBLIC_AND_STATE_INSTITUTIONS_OF_HIGHER_EDUCATION\n                    ]\n                ),\n                [LOC_HIGHER_EDUCATION, DOS_DIGITAL_LITERACY],\n            ),\n            (get_search_request(applicant_type_one_of=[ApplicantType.INDIVIDUALS]), []),\n            (\n                get_search_request(\n                    applicant_type_one_of=[\n                        ApplicantType.STATE_GOVERNMENTS,\n                        ApplicantType.UNRESTRICTED,\n                    ]\n                ),\n                [NASA_SUPERSONIC, LOC_TEACHING, DOC_SPACE_COAST],\n            ),\n            # Mix\n            (\n                get_search_request(\n                    agency_one_of=[\"NASA\"], applicant_type_one_of=[ApplicantType.OTHER]\n                ),\n                [NASA_SPACE_FELLOWSHIP, NASA_INNOVATIONS, NASA_K12_DIVERSITY],\n            ),\n            (\n                get_search_request(\n                    funding_instrument_one_of=[\n                        FundingInstrument.GRANT,\n                        FundingInstrument.PROCUREMENT_CONTRACT,\n                    ],\n                    funding_category_one_of=[\n                        FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT\n                    ],\n                ),\n                [NASA_INNOVATIONS, NASA_SUPERSONIC, DOC_MANUFACTURING],\n            ),\n            (\n                get_search_request(\n                    opportunity_status_one_of=[OpportunityStatus.POSTED],\n                    applicant_type_one_of=[ApplicantType.OTHER],\n                ),\n                [NASA_SPACE_FELLOWSHIP, DOC_MANUFACTURING],\n            ),\n        ],\n        ids=search_scenario_id_fnc,\n    )\n    def test_search_filters_200(self, client, api_auth_token, search_request, expected_results):\n        call_search_and_validate(client, api_auth_token, search_request, expected_results)\n\n    @pytest.mark.parametrize(\n        \"search_request, expected_results\",\n        [\n            # Post date\n            (\n                get_search_request(\n                    post_date={\"start_date\": \"1970-01-01\", \"end_date\": \"2050-01-01\"}\n                ),\n                OPPORTUNITIES,\n            ),\n            (\n                get_search_request(\n                    post_date={\"start_date_relative\": -20111, \"end_date_relative\": 9131}\n                ),\n                OPPORTUNITIES,\n            ),\n            (\n                get_search_request(\n                    post_date={\"start_date\": \"1999-01-01\", \"end_date\": \"2000-01-01\"}\n                ),\n                [],\n            ),\n            (\n                get_search_request(\n                    post_date={\"start_date\": \"2015-01-01\", \"end_date\": \"2018-01-01\"}\n                ),\n                [DOC_SPACE_COAST],\n            ),\n            (\n                get_search_request(\n                    post_date={\"start_date\": \"2019-06-01\", \"end_date\": \"2024-01-01\"}\n                ),\n                [NASA_SPACE_FELLOWSHIP, NASA_SUPERSONIC],\n            ),\n            (\n                get_search_request(\n                    post_date={\"start_date_relative\": -2063, \"end_date_relative\": -389}\n                ),\n                [NASA_SPACE_FELLOWSHIP, NASA_SUPERSONIC],\n            ),\n            (get_search_request(post_date={\"end_date\": \"2016-01-01\"}), [DOC_MANUFACTURING]),\n            (get_search_request(post_date={\"end_date_relative\": -3310}), [DOC_MANUFACTURING]),\n            # Close date\n            (\n                get_search_request(\n                    close_date={\"start_date\": \"1970-01-01\", \"end_date\": \"2050-01-01\"}\n                ),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_SUPERSONIC,\n                    NASA_K12_DIVERSITY,\n                    LOC_TEACHING,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_SPACE_COAST,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(\n                    close_date={\"start_date_relative\": -20111, \"end_date_relative\": 9131}\n                ),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_SUPERSONIC,\n                    NASA_K12_DIVERSITY,\n                    LOC_TEACHING,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_SPACE_COAST,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(close_date={\"start_date\": \"2019-01-01\"}),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_SUPERSONIC,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_SPACE_COAST,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(close_date={\"start_date_relative\": -2214}),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_SUPERSONIC,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_SPACE_COAST,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(close_date={\"end_date\": \"2019-01-01\"}),\n                [NASA_K12_DIVERSITY, LOC_TEACHING],\n            ),\n            (\n                get_search_request(close_date={\"end_date_relative\": -2214}),\n                [NASA_K12_DIVERSITY, LOC_TEACHING],\n            ),\n            (\n                get_search_request(\n                    close_date={\"start_date\": \"2015-01-01\", \"end_date\": \"2019-12-01\"}\n                ),\n                [NASA_K12_DIVERSITY, DOC_SPACE_COAST],\n            ),\n            (\n                get_search_request(\n                    close_date={\"start_date_relative\": -3675, \"end_date_relative\": -1880}\n                ),\n                [NASA_K12_DIVERSITY, DOC_SPACE_COAST],\n            ),\n        ],\n    )\n    def test_search_filters_date_200(\n        self, client, api_auth_token, search_request, expected_results\n    ):\n        call_search_and_validate(client, api_auth_token, search_request, expected_results)\n\n    @pytest.mark.parametrize(\n        \"search_request, expected_results\",\n        [\n            # Is cost sharing\n            (get_search_request(is_cost_sharing_one_of=[True, False]), OPPORTUNITIES),\n            (get_search_request(is_cost_sharing_one_of=[\"1\", \"0\"]), OPPORTUNITIES),\n            (\n                get_search_request(is_cost_sharing_one_of=[\"t\"]),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_SUPERSONIC,\n                    LOC_TEACHING,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(is_cost_sharing_one_of=[\"on\"]),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_SUPERSONIC,\n                    LOC_TEACHING,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(is_cost_sharing_one_of=[\"false\"]),\n                [NASA_INNOVATIONS, NASA_K12_DIVERSITY, LOC_HIGHER_EDUCATION, DOC_SPACE_COAST],\n            ),\n            (\n                get_search_request(is_cost_sharing_one_of=[\"no\"]),\n                [NASA_INNOVATIONS, NASA_K12_DIVERSITY, LOC_HIGHER_EDUCATION, DOC_SPACE_COAST],\n            ),\n        ],\n    )\n    def test_search_bool_filters_200(\n        self, client, api_auth_token, search_request, expected_results\n    ):\n        call_search_and_validate(client, api_auth_token, search_request, expected_results)\n\n    @pytest.mark.parametrize(\n        \"search_request, expected_results\",\n        [\n            # Expected Number of Awards\n            (\n                get_search_request(expected_number_of_awards={\"min\": 0, \"max\": 1000}),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_INNOVATIONS,\n                    NASA_SUPERSONIC,\n                    LOC_TEACHING,\n                    LOC_HIGHER_EDUCATION,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_SPACE_COAST,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(expected_number_of_awards={\"min\": 5, \"max\": 10}),\n                [NASA_SUPERSONIC],\n            ),\n            (\n                get_search_request(expected_number_of_awards={\"min\": 12}),\n                [LOC_TEACHING, DOC_SPACE_COAST, DOC_MANUFACTURING],\n            ),\n            (\n                get_search_request(expected_number_of_awards={\"min\": 7}),\n                [NASA_SUPERSONIC, LOC_TEACHING, DOC_SPACE_COAST, DOC_MANUFACTURING],\n            ),\n            # Award Floor\n            (\n                get_search_request(award_floor={\"min\": 0, \"max\": 10_000_000_000}),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_INNOVATIONS,\n                    NASA_SUPERSONIC,\n                    LOC_TEACHING,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_SPACE_COAST,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(award_floor={\"min\": 1, \"max\": 5_000}),\n                [\n                    NASA_INNOVATIONS,\n                    LOC_TEACHING,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_SPACE_COAST,\n                ],\n            ),\n            (\n                get_search_request(award_floor={\"min\": 5_000, \"max\": 10_000}),\n                [\n                    NASA_INNOVATIONS,\n                    NASA_SUPERSONIC,\n                ],\n            ),\n            # Award Ceiling\n            (\n                get_search_request(award_ceiling={\"min\": 0, \"max\": 10_000_000_000}),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_INNOVATIONS,\n                    NASA_SUPERSONIC,\n                    LOC_TEACHING,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_SPACE_COAST,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(award_ceiling={\"min\": 5_000, \"max\": 50_000}),\n                [\n                    NASA_INNOVATIONS,\n                    NASA_SUPERSONIC,\n                ],\n            ),\n            (\n                get_search_request(award_ceiling={\"min\": 50_000}),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_SUPERSONIC,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            # Estimated Total Program Funding\n            (\n                get_search_request(\n                    estimated_total_program_funding={\"min\": 0, \"max\": 100_000_000_000}\n                ),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_INNOVATIONS,\n                    LOC_TEACHING,\n                    LOC_HIGHER_EDUCATION,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_SPACE_COAST,\n                    DOC_MANUFACTURING,\n                ],\n            ),\n            (\n                get_search_request(estimated_total_program_funding={\"min\": 0, \"max\": 5_000}),\n                [\n                    NASA_INNOVATIONS,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_SPACE_COAST,\n                ],\n            ),\n            # Mix\n            (\n                get_search_request(\n                    expected_number_of_awards={\"min\": 0},\n                    award_floor={\"max\": 10_000},\n                    award_ceiling={\"max\": 10_000_000},\n                    estimated_total_program_funding={\"min\": 10_000},\n                ),\n                [LOC_TEACHING],\n            ),\n            (\n                get_search_request(\n                    expected_number_of_awards={\"max\": 10},\n                    award_floor={\"min\": 1_000, \"max\": 10_000},\n                    award_ceiling={\"max\": 10_000_000},\n                ),\n                [NASA_INNOVATIONS, NASA_SUPERSONIC],\n            ),\n            (\n                get_search_request(\n                    expected_number_of_awards={\"min\": 1, \"max\": 2},\n                    award_floor={\"min\": 0, \"max\": 1000},\n                    award_ceiling={\"min\": 10000, \"max\": 10000000},\n                    estimated_total_program_funding={\"min\": 123456, \"max\": 345678},\n                ),\n                [],\n            ),\n        ],\n    )\n    def test_search_int_filters_200(self, client, api_auth_token, search_request, expected_results):\n        call_search_and_validate(client, api_auth_token, search_request, expected_results)\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            # Post Date\n            (get_search_request(post_date={\"start_date\": \"I am not a date\"})),\n            (get_search_request(post_date={\"start_date\": \"123-456-789\"})),\n            (get_search_request(post_date={\"start_date\": \"5\"})),\n            (get_search_request(post_date={\"start_date\": 5})),\n            (get_search_request(post_date={\"end_date\": \"I am not a date\"})),\n            (get_search_request(post_date={\"end_date\": \"123-456-789\"})),\n            (get_search_request(post_date={\"end_date\": \"5\"})),\n            (get_search_request(post_date={\"end_date\": 5})),\n            # Close Date\n            (get_search_request(close_date={\"start_date\": \"I am not a date\"})),\n            (get_search_request(close_date={\"start_date\": \"123-456-789\"})),\n            (get_search_request(close_date={\"start_date\": \"5\"})),\n            (get_search_request(close_date={\"start_date\": 5})),\n            (get_search_request(close_date={\"end_date\": \"I am not a date\"})),\n            (get_search_request(close_date={\"end_date\": \"123-456-789\"})),\n            (get_search_request(close_date={\"end_date\": \"5\"})),\n            (get_search_request(close_date={\"end_date\": 5})),\n        ],\n    )\n    def test_search_validate_date_filters_format_422(self, client, api_auth_token, search_request):\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n        assert resp.status_code == 422\n\n        json = resp.get_json()\n        error = json[\"errors\"][0]\n        assert json[\"message\"] == \"Validation error\"\n        assert error[\"message\"] == \"Not a valid date.\"\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            # Post Date\n            (get_search_request(post_date={\"start_date\": None, \"end_date\": None})),\n            (\n                get_search_request(\n                    post_date={\"start_date_relative\": None, \"end_date_relative\": None}\n                )\n            ),\n            (get_search_request(post_date={\"start_date\": None})),\n            (get_search_request(post_date={\"start_date_relative\": None})),\n            (get_search_request(post_date={\"end_date\": None})),\n            (get_search_request(post_date={\"end_date_relative\": None})),\n            (get_search_request(post_date={})),\n            # Close Date\n            (get_search_request(close_date={\"start_date\": None, \"end_date\": None})),\n            (\n                get_search_request(\n                    close_date={\"start_date_relative\": None, \"end_date_relative\": None}\n                )\n            ),\n            (get_search_request(close_date={\"start_date\": None})),\n            (get_search_request(close_date={\"start_date_relative\": None})),\n            (get_search_request(close_date={\"end_date\": None})),\n            (get_search_request(close_date={\"end_date_relative\": None})),\n            (get_search_request(close_date={})),\n        ],\n    )\n    def test_search_validate_date_filters_nullability_422(\n        self, client, api_auth_token, search_request\n    ):\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n        assert resp.status_code == 422\n\n        json = resp.get_json()\n        error = json[\"errors\"][0]\n        assert json[\"message\"] == \"Validation error\"\n        assert (\n            error[\"message\"]\n            == \"At least one of start_date/start_date_relative or end_date/end_date_relative must be provided.\"\n        )\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            # Post Date\n            (get_search_request(post_date={\"start_date_relative\": \"I am not a relative date\"})),\n            (get_search_request(post_date={\"start_date_relative\": \"2015-01-01\"})),\n            (get_search_request(post_date={\"end_date_relative\": \"I am not a relative date\"})),\n            (get_search_request(post_date={\"end_date_relative\": \"2015-01-01\"})),\n            # Close Date\n            (get_search_request(close_date={\"start_date_relative\": \"I am not a relative date\"})),\n            (get_search_request(close_date={\"start_date_relative\": \"2015-01-01\"})),\n            (get_search_request(close_date={\"end_date_relative\": \"I am not a relative date\"})),\n            (get_search_request(close_date={\"end_date_relative\": \"2015-01-01\"})),\n        ],\n    )\n    def test_search_validate_date_relative_filters_format_422(\n        self, client, api_auth_token, search_request\n    ):\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n        assert resp.status_code == 422\n\n        json = resp.get_json()\n        error = json[\"errors\"][0]\n        assert json[\"message\"] == \"Validation error\"\n        assert error[\"message\"] == \"Not a valid integer.\"\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            # post_date\n            (get_search_request(post_date={\"end_date_relative\": 1000001})),\n            (get_search_request(post_date={\"end_date_relative\": -1000001})),\n            # close_date\n            (get_search_request(close_date={\"end_date_relative\": 1000001})),\n            (get_search_request(close_date={\"end_date_relative\": -1000001})),\n        ],\n    )\n    def test_search_validate_date_relative_range_values_422(\n        self, client, api_auth_token, search_request\n    ):\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n\n        json = resp.get_json()\n        assert json[\"message\"] == \"Validation error\"\n        for error in json[\"errors\"]:\n            assert (\n                error[\"message\"]\n                == \"Must be greater than or equal to -1000000 and less than or equal to 1000000.\"\n            )\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            # Post Date\n            (get_search_request(post_date={\"start_date\": \"2015-01-01\", \"start_date_relative\": 15})),\n            # Close Date\n            (get_search_request(close_date={\"end_date_relative\": -4, \"end_date\": \"2015-01-01\"})),\n        ],\n    )\n    def test_search_validate_date_filters_mix_format_422(\n        self, client, api_auth_token, search_request\n    ):\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n        assert resp.status_code == 422\n\n        json = resp.get_json()\n        error = json[\"errors\"][0]\n        assert json[\"message\"] == \"Validation error\"\n        assert error[\"message\"] == \"Cannot have both absolute and relative start/end date.\"\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            get_search_request(assistance_listing_one_of=[\"12.345\", \"67.89\"]),\n            get_search_request(assistance_listing_one_of=[\"98.765\"]),\n            get_search_request(assistance_listing_one_of=[\"67.89\", \"54.24\", \"12.345\", \"86.753\"]),\n        ],\n    )\n    def test_search_validate_assistance_listing_filters_200(\n        self, client, api_auth_token, search_request\n    ):\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n        assert resp.status_code == 200\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            get_search_request(assistance_listing_one_of=[\"12.345\", \"675.89\"]),\n            get_search_request(assistance_listing_one_of=[\"hello\"]),\n            get_search_request(assistance_listing_one_of=[\"67.89\", \"54.2412\"]),\n            get_search_request(assistance_listing_one_of=[\"1.1\"]),\n            get_search_request(assistance_listing_one_of=[\"12.hello\"]),\n            get_search_request(assistance_listing_one_of=[\"fourfive.sixseveneight\"]),\n            get_search_request(assistance_listing_one_of=[\"11..11\"]),\n        ],\n    )\n    def test_search_validate_assistance_listing_filters_422(\n        self, client, api_auth_token, search_request\n    ):\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n        assert resp.status_code == 422\n\n        json = resp.get_json()\n        error = json[\"errors\"][0]\n        assert json[\"message\"] == \"Validation error\"\n        assert error[\"message\"] == \"String does not match expected pattern.\"\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            get_search_request(is_cost_sharing_one_of=[\"hello\"]),\n            get_search_request(is_cost_sharing_one_of=[True, \"definitely\"]),\n            get_search_request(is_cost_sharing_one_of=[5, 6]),\n            get_search_request(is_cost_sharing_one_of=[\"2024-01-01\"]),\n            get_search_request(is_cost_sharing_one_of=[{}]),\n        ],\n    )\n    def test_search_validate_is_cost_sharing_filters_422(\n        self, client, api_auth_token, search_request\n    ):\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n        assert resp.status_code == 422\n\n        json = resp.get_json()\n        error = json[\"errors\"][0]\n        assert json[\"message\"] == \"Validation error\"\n        assert error[\"message\"] == \"Not a valid boolean.\"\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            get_search_request(estimated_total_program_funding={\"min\": \"hello\", \"max\": \"345678\"}),\n            get_search_request(award_floor={\"min\": \"one\"}),\n            get_search_request(award_ceiling={\"min\": {}, \"max\": \"123e4f5\"}),\n        ],\n    )\n    def test_search_validate_award_values_422(self, client, api_auth_token, search_request):\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n        assert resp.status_code == 422\n\n        json = resp.get_json()\n        assert json[\"message\"] == \"Validation error\"\n        for error in json[\"errors\"]:\n            assert error[\"message\"] == \"Not a valid integer.\"\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            get_search_request(\n                expected_number_of_awards={\"min\": -1},\n                award_floor={\"max\": -2},\n                award_ceiling={\"max\": \"-10000000\"},\n                estimated_total_program_funding={\"min\": \"-123456\"},\n            ),\n            get_search_request(expected_number_of_awards={\"min\": -1, \"max\": 10000000}),\n            get_search_request(\n                estimated_total_program_funding={\"max\": \"-5\"}, award_floor={\"max\": \"-9\"}\n            ),\n        ],\n    )\n    def test_search_validate_award_values_negative_422(\n        self, client, api_auth_token, search_request\n    ):\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n\n        json = resp.get_json()\n        assert json[\"message\"] == \"Validation error\"\n        for error in json[\"errors\"]:\n            assert error[\"message\"] == \"Must be greater than or equal to 0.\"\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            # Both set to None\n            get_search_request(\n                expected_number_of_awards={\"min\": None, \"max\": None},\n                award_floor={\"min\": None, \"max\": None},\n                award_ceiling={\"min\": None, \"max\": None},\n                estimated_total_program_funding={\"min\": None, \"max\": None},\n            ),\n            # Min only set\n            get_search_request(\n                expected_number_of_awards={\"min\": None},\n                award_floor={\"min\": None},\n                award_ceiling={\"min\": None},\n                estimated_total_program_funding={\"min\": None},\n            ),\n            # Max only set\n            get_search_request(\n                expected_number_of_awards={\"max\": None},\n                award_floor={\"max\": None},\n                award_ceiling={\"max\": None},\n                estimated_total_program_funding={\"max\": None},\n            ),\n        ],\n    )\n    def test_search_validate_award_values_nullability_422(\n        self, client, api_auth_token, search_request\n    ):\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n\n        json = resp.get_json()\n        assert json[\"message\"] == \"Validation error\"\n        for error in json[\"errors\"]:\n            assert error[\"message\"] == \"At least one of min or max must be provided.\"\n\n    @pytest.mark.parametrize(\n        \"search_request, expected_results\",\n        [\n            # Note that the sorting is not relevancy for this as we intend to update the relevancy scores a bit\n            # and don't want to break this every time we adjust those.\n            (\n                get_search_request(\n                    sort_order=[\n                        {\"order_by\": \"opportunity_id\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                    query=\"space\",\n                ),\n                [\n                    NASA_SPACE_FELLOWSHIP,\n                    NASA_INNOVATIONS,\n                    NASA_K12_DIVERSITY,\n                    DOS_DIGITAL_LITERACY,\n                    DOC_SPACE_COAST,\n                ],\n            ),\n            (\n                get_search_request(\n                    sort_order=[\n                        {\"order_by\": \"opportunity_id\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                    query=\"43.008\",\n                ),\n                [NASA_SPACE_FELLOWSHIP, NASA_K12_DIVERSITY, LOC_TEACHING],\n            ),\n            (\n                get_search_request(\n                    sort_order=[\n                        {\"order_by\": \"opportunity_id\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                    query=\"012ADV*\",\n                ),\n                [LOC_TEACHING, LOC_HIGHER_EDUCATION],\n            ),\n            (\n                get_search_request(\n                    sort_order=[\n                        {\"order_by\": \"opportunity_id\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                    query=\"DOC*\",\n                ),\n                [DOC_SPACE_COAST, DOC_MANUFACTURING],\n            ),\n            (\n                get_search_request(\n                    sort_order=[\n                        {\"order_by\": \"opportunity_id\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                    query=\"Aeronautics\",\n                ),\n                [NASA_SUPERSONIC],\n            ),\n            (\n                get_search_request(\n                    sort_order=[\n                        {\"order_by\": \"opportunity_id\", \"sort_direction\": SortDirection.ASCENDING}\n                    ],\n                    query=\"literacy\",\n                ),\n                [LOC_TEACHING, DOS_DIGITAL_LITERACY],\n            ),\n        ],\n        ids=search_scenario_id_fnc,\n    )\n    def test_search_query_200(self, client, api_auth_token, search_request, expected_results):\n        # This test isn't looking to validate opensearch behavior, just that we've connected fields properly and\n        # results being returned are as expected.\n        call_search_and_validate(client, api_auth_token, search_request, expected_results)\n\n    def test_search_query_facets_200(self, client, api_auth_token):\n        search_response = client.post(\n            \"/v1/opportunities/search\",\n            json=get_search_request(),\n            headers={\"X-Auth\": api_auth_token},\n        )\n\n        assert search_response.status_code == 200\n        facet_counts = search_response.get_json()[\"facet_counts\"]\n        assert facet_counts.keys() == {\n            \"agency\",\n            \"applicant_type\",\n            \"funding_instrument\",\n            \"funding_category\",\n            \"opportunity_status\",\n        }\n\n    @pytest.mark.parametrize(\n        \"search_request\",\n        [\n            # default scoring rule\n            get_search_request(\n                query=\"literacy\",\n            ),\n            # agency scoring rule\n            get_search_request(\n                query=\"literacy\",\n                experimental={\"scoring_rule\": \"agency\"},\n            ),\n            # expanded scoring rule\n            get_search_request(\n                query=\"literacy\",\n                experimental={\"scoring_rule\": \"expanded\"},\n            ),\n        ],\n    )\n    def test_search_experimental_200(self, client, api_auth_token, search_request):\n        # We are only testing for 200 responses when adding the experimental field into the request body.\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n        assert resp.status_code == 200\n\n        search_request[\"format\"] = \"csv\"\n        resp = client.post(\n            \"/v1/opportunities/search\", json=search_request, headers={\"X-Auth\": api_auth_token}\n        )\n        assert resp.status_code == 200"}
{"path":"frontend/tests/api/auth/search/export/route.test.ts","language":"typescript","type":"code","directory":"frontend/tests/api/auth/search/export","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/api/auth/search/export/route.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/test_healthcheck.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/test_healthcheck.py\nSize: 1.23 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/api/auth/session/route.test.ts","language":"typescript","type":"code","directory":"frontend/tests/api/auth/session","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/api/auth/session/route.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.135Z","content":"import src.adapters.db as db\n\n\ndef test_get_healthcheck_200(client, monkeypatch):\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n\n    resp_json = response.get_json()\n    assert resp_json[\"message\"] == \"Service healthy\"\n\n    # Verify the release info is attached\n    assert resp_json[\"data\"][\"commit_sha\"] is not None\n    assert resp_json[\"data\"][\"commit_link\"].startswith(\n        \"https://github.com/HHS/simpler-grants-gov/commit/\"\n    )\n    assert resp_json[\"data\"][\"release_notes_link\"].startswith(\n        \"https://github.com/HHS/simpler-grants-gov/releases\"\n    )\n    assert datetime.fromisoformat(resp_json[\"data\"][\"last_deploy_time\"]) is not None\n    assert resp_json[\"data\"][\"deploy_whoami\"] == \"local-developer\"\n\n\ndef test_get_healthcheck_503_db_bad_state(client, monkeypatch):\n    # Make fetching the DB session fail\n    def err_method(*args):\n        raise Exception(\"Fake Error\")\n\n    # Mock db_session.Scalar to fail\n    monkeypatch.setattr(db.Session, \"scalar\", err_method)\n\n    response = client.get(\"/health\")\n    assert response.status_code == 503\n    assert response.get_json()[\"message\"] == \"Service Unavailable\"\n    assert response.get_json()[\"internal_request_id\"] is not None"}
{"path":"frontend/tests/api/user/saved-opportunities/[id]/route.test.ts","language":"typescript","type":"code","directory":"frontend/tests/api/user/saved-opportunities/[id]","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/api/user/saved-opportunities/[id]/route.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/test_index_route.py\nLanguage: py\nType: route\nDirectory: api/tests/src/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/test_index_route.py\nSize: 0.14 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/api/user/saved-opportunities/route.test.ts","language":"typescript","type":"code","directory":"frontend/tests/api/user/saved-opportunities","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/api/user/saved-opportunities/route.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.135Z","content":""}
{"path":"frontend/tests/artillery/processor.ts","language":"typescript","type":"code","directory":"frontend/tests/artillery","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/artillery/processor.ts","size":324715,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/test_route_error_format.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/test_route_error_format.py\nSize: 9.06 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/BetaAlert.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/BetaAlert.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"These tests aim to verify that the format and structure of the error\nresponses is consistent and functioning as intended.\n\"\"\"\n\nimport dataclasses\n\nimport pytest\nfrom apiflask import APIBlueprint\nfrom werkzeug.exceptions import BadRequest, Forbidden, NotFound, Unauthorized\nfrom werkzeug.http import HTTP_STATUS_CODES\n\nimport src.app as app_entry\nimport src.logging\nfrom src.api.response import ApiResponse, ValidationErrorDetail\nfrom src.api.route_utils import raise_flask_error\nfrom src.api.schemas.extension import Schema, fields\nfrom src.api.schemas.response_schema import AbstractResponseSchema, WarningMixinSchema\nfrom src.auth.api_key_auth import api_key_auth\nfrom src.util.dict_util import flatten_dict\nfrom tests.src.schemas.schema_validation_utils import (\n    FieldTestSchema,\n    get_expected_validation_errors,\n    get_invalid_field_test_schema_req,\n    get_valid_field_test_schema_req,\n)\n\nPATH = \"/test/\"\nVALID_UUID = \"1234a5b6-7c8d-90ef-1ab2-c3d45678e9f0\"\nFULL_PATH = PATH + VALID_UUID\n\n\ndef header(api_auth_token):\n    return {\"X-Auth\": api_auth_token}\n\n\nclass OutputData(Schema):\n    output_val = fields.String()\n\n\nclass OutputSchema(AbstractResponseSchema, WarningMixinSchema):\n    data = fields.Nested(OutputData())\n\n\ntest_blueprint = APIBlueprint(\"test\", __name__, tag=\"test\")\n\n\nclass OverridenClass:\n    \"\"\"\n    In order to arbitrarily change the implementation of\n    the test endpoint, create a simple function that tests\n    below can override by doing::\n\n        def override(self):\n            # if this method returns, it returns\n            # the response as a dictionary + a list\n            # of validation issues to attach to the response\n            return {\"output_val\": \"hello\"}, []\n\n        monkeypatch.setattr(OverridenClass, \"override_method\", override)\n    \"\"\"\n\n    def override_method(self):\n        return {\"output_val\": \"hello\"}, []\n\n\n@test_blueprint.patch(\"/test/<uuid:test_id>\")\n@test_blueprint.input(FieldTestSchema, arg_name=\"req\")\n@test_blueprint.output(OutputSchema)\n@test_blueprint.auth_required(api_key_auth)\ndef api_method(test_id, req):\n    resp, warnings = OverridenClass().override_method()\n    return ApiResponse(\"Test method run successfully\", data=resp, warnings=warnings)\n\n\n@pytest.fixture\ndef simple_app(monkeypatch):\n    def stub(app):\n        pass\n\n    # We want all the configurational setup for the app, but\n    # don't want the DB clients or blueprints to keep setup simpler\n    monkeypatch.setattr(app_entry, \"register_db_client\", stub)\n    monkeypatch.setattr(app_entry, \"register_blueprints\", stub)\n    monkeypatch.setattr(app_entry, \"setup_logging\", stub)\n\n    app = app_entry.create_app()\n\n    # To avoid re-initializing logging everytime we\n    # setup the app, we disabled it above and do it here\n    # in case you want it while running your tests\n    with src.logging.init(__package__):\n        yield app\n\n\n@pytest.fixture\ndef simple_client(simple_app):\n    simple_app.register_blueprint(test_blueprint)\n    return simple_app.test_client()\n\n\n@pytest.mark.parametrize(\n    \"exception\", [Exception, AttributeError, IndexError, NotImplementedError, ValueError]\n)\ndef test_exception(simple_client, api_auth_token, monkeypatch, exception):\n    def override(self):\n        raise exception(\"Exception message text\")\n\n    monkeypatch.setattr(OverridenClass, \"override_method\", override)\n\n    resp = simple_client.patch(\n        FULL_PATH, json=get_valid_field_test_schema_req(), headers=header(api_auth_token)\n    )\n\n    assert resp.status_code == 500\n    resp_json = resp.get_json()\n    assert resp_json[\"errors\"] == []\n    assert resp_json[\"message\"] == \"Internal Server Error\"\n\n\n@pytest.mark.parametrize(\"exception\", [Unauthorized, NotFound, Forbidden, BadRequest])\ndef test_werkzeug_exceptions(simple_client, api_auth_token, monkeypatch, exception):\n    def override(self):\n        raise exception(\"Exception message text\")\n\n    monkeypatch.setattr(OverridenClass, \"override_method\", override)\n\n    resp = simple_client.patch(\n        FULL_PATH, json=get_valid_field_test_schema_req(), headers=header(api_auth_token)\n    )\n\n    # Werkzeug errors use the proper status code, but\n    # any message is replaced with a generic one they have defined\n    assert resp.status_code == exception.code\n    resp_json = resp.get_json()\n    assert resp_json[\"data\"] == {}\n    assert resp_json[\"errors\"] == []\n    assert resp_json[\"message\"] == HTTP_STATUS_CODES[exception.code]\n\n\n@pytest.mark.parametrize(\n    \"error_code,message,detail,validation_issues\",\n    [\n        (422, \"message\", {\"field\": \"value\"}, []),\n        (\n            422,\n            \"message but different\",\n            None,\n            [\n                ValidationErrorDetail(\n                    type=\"example\", message=\"example message\", field=\"example_field\"\n                ),\n                ValidationErrorDetail(\n                    type=\"example2\", message=\"example message2\", field=\"example_field2\"\n                ),\n            ],\n        ),\n        (401, \"not allowed\", {\"field\": \"value\"}, []),\n        (403, \"bad request message\", None, []),\n    ],\n)\ndef test_flask_error(\n    simple_client, api_auth_token, monkeypatch, error_code, message, detail, validation_issues\n):\n    def override(self):\n        raise_flask_error(error_code, message, detail=detail, validation_issues=validation_issues)\n\n    monkeypatch.setattr(OverridenClass, \"override_method\", override)\n\n    resp = simple_client.patch(\n        FULL_PATH, json=get_valid_field_test_schema_req(), headers=header(api_auth_token)\n    )\n\n    assert resp.status_code == error_code\n    resp_json = resp.get_json()\n    assert resp_json[\"message\"] == message\n\n    if detail is None:\n        assert resp_json[\"data\"] == {}\n    else:\n        assert resp_json[\"data\"] == detail\n\n    if validation_issues:\n        errors = resp_json[\"errors\"]\n        assert len(validation_issues) == len(errors)\n\n        for validation_issue in validation_issues:\n            assert dataclasses.asdict(validation_issue) in errors\n    else:\n        assert resp_json[\"errors\"] == []\n\n\ndef test_invalid_path_param(simple_client, api_auth_token, monkeypatch):\n    resp = simple_client.patch(\n        PATH + \"not-a-uuid\", json=get_valid_field_test_schema_req(), headers=header(api_auth_token)\n    )\n\n    # This raises a Werkzeug NotFound so has those values\n    assert resp.status_code == 404\n    resp_json = resp.get_json()\n    assert resp_json[\"data\"] == {}\n    assert resp_json[\"errors\"] == []\n    assert resp_json[\"message\"] == \"Not Found\"\n\n\ndef test_auth_error(simple_client, monkeypatch):\n    resp = simple_client.patch(\n        FULL_PATH, json=get_valid_field_test_schema_req(), headers=header(\"not_valid_jwt\")\n    )\n\n    assert resp.status_code == 401\n    resp_json = resp.get_json()\n    assert resp_json[\"data\"] == {}\n    assert resp_json[\"errors\"] == []\n    assert (\n        resp_json[\"message\"]\n        == \"The server could not verify that you are authorized to access the URL requested\"\n    )\n\n\n@pytest.mark.parametrize(\n    \"issues\",\n    [\n        [],\n        [\n            ValidationErrorDetail(\n                type=\"required\", message=\"Field is required\", field=\"sub_obj.field_a\"\n            ),\n            ValidationErrorDetail(\n                type=\"format\", message=\"Invalid format for type string\", field=\"field_b\"\n            ),\n        ],\n        [ValidationErrorDetail(type=\"bad\", message=\"field is optional technically\")],\n    ],\n)\ndef test_added_validation_issues(simple_client, api_auth_token, monkeypatch, issues):\n    def override(self):\n        return {\"output_val\": \"hello with validation issues\"}, issues\n\n    monkeypatch.setattr(OverridenClass, \"override_method\", override)\n\n    resp = simple_client.patch(\n        FULL_PATH, json=get_valid_field_test_schema_req(), headers=header(api_auth_token)\n    )\n\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n    assert resp_json[\"data\"] == {\"output_val\": \"hello with validation issues\"}\n    assert resp_json[\"message\"] == \"Test method run successfully\"\n\n    warnings = resp_json[\"warnings\"]\n\n    assert len(issues) == len(warnings)\n    for issue in issues:\n        assert dataclasses.asdict(issue) in warnings\n\n\ndef test_marshmallow_validation(simple_client, api_auth_token, monkeypatch):\n    \"\"\"\n    Validate that Marshmallow errors get transformed properly\n    and attached in the expected format in an error response\n    \"\"\"\n\n    req = get_invalid_field_test_schema_req()\n    resp = simple_client.patch(FULL_PATH, json=req, headers=header(api_auth_token))\n\n    assert resp.status_code == 422\n    resp_json = resp.get_json()\n    assert resp_json[\"data\"] == {}\n    assert resp_json[\"message\"] == \"Validation error\"\n\n    resp_errors = resp_json[\"errors\"]\n\n    expected_errors = []\n    for field, errors in flatten_dict(get_expected_validation_errors()).items():\n        for error in errors:\n            expected_errors.append(\n                {\n                    \"type\": error.key,\n                    \"message\": error.message,\n                    \"field\": field.removesuffix(\"._schema\"),\n                }\n            )\n\n    assert len(expected_errors) == len(resp_errors)\n    for expected_error in expected_errors:\n        assert expected_error in resp_errors"}
{"path":"frontend/tests/components/Breadcrumbs.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/Breadcrumbs.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/users/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/ClientSideUrlUpdater.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ClientSideUrlUpdater.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":""}
{"path":"frontend/tests/components/ContentDisplayToggle.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ContentDisplayToggle.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/users/test_user_delete_saved_opportunity.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_delete_saved_opportunity.py\nSize: 2.67 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/ContentLayout.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ContentLayout.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"from src.auth.api_jwt_auth import create_jwt_for_user\nfrom src.db.models.user_models import UserSavedOpportunity\nfrom tests.src.db.models.factories import (\n    OpportunityFactory,\n    UserFactory,\n    UserSavedOpportunityFactory,\n)\n\n\n@pytest.fixture\ndef user(enable_factory_create, db_session):\n    user = UserFactory.create()\n    db_session.commit()\n    return user\n\n\n@pytest.fixture\ndef user_auth_token(user, db_session):\n    token, _ = create_jwt_for_user(user, db_session)\n    return token\n\n\n@pytest.fixture(autouse=True, scope=\"function\")\ndef clear_saved_opportunities(db_session):\n    db_session.query(UserSavedOpportunity).delete()\n    db_session.commit()\n    yield\n\n\ndef test_user_delete_saved_opportunity(\n    client, enable_factory_create, db_session, user, user_auth_token\n):\n    # Create and save an opportunity\n    opportunity = OpportunityFactory.create()\n    UserSavedOpportunityFactory.create(user=user, opportunity=opportunity)\n\n    # Delete the saved opportunity\n    response = client.delete(\n        f\"/v1/users/{user.user_id}/saved-opportunities/{opportunity.opportunity_id}\",\n        headers={\"X-SGG-Token\": user_auth_token},\n    )\n\n    assert response.status_code == 200\n    assert response.json[\"message\"] == \"Success\"\n\n    # Verify it was deleted\n    saved_count = db_session.query(UserSavedOpportunity).count()\n    assert saved_count == 0\n\n    # Delete the saved opportunity\n    response = client.delete(\n        f\"/v1/users/{user.user_id}/saved-opportunities/1234567890\",\n        headers={\"X-SGG-Token\": user_auth_token},\n    )\n\n    assert response.status_code == 404\n    assert response.json[\"message\"] == \"Saved opportunity not found\"\n\n\ndef test_user_delete_other_users_saved_opportunity(\n    client, enable_factory_create, db_session, user, user_auth_token\n):\n    \"\"\"Test that a user cannot delete another user's saved opportunity\"\"\"\n    # Create another user and save an opportunity for them\n    other_user = UserFactory.create()\n    opportunity = OpportunityFactory.create()\n    UserSavedOpportunityFactory.create(user=other_user, opportunity=opportunity)\n\n    # Try to delete the other user's saved opportunity\n    response = client.delete(\n        f\"/v1/users/{user.user_id}/saved-opportunities/{opportunity.opportunity_id}\",\n        headers={\"X-SGG-Token\": user_auth_token},\n    )\n\n    assert response.status_code == 404\n    assert response.json[\"message\"] == \"Saved opportunity not found\"\n\n    # Verify the saved opportunity still exists\n    saved_opportunity = db_session.query(UserSavedOpportunity).first()\n    assert saved_opportunity is not None\n    assert saved_opportunity.user_id == other_user.user_id\n    assert saved_opportunity.opportunity_id == opportunity.opportunity_id"}
{"path":"frontend/tests/components/Footer.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/Footer.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/users/test_user_delete_saved_search.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_delete_saved_search.py\nSize: 2.66 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/FullWidthAlert.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/FullWidthAlert.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"import pytest\n\nfrom src.db.models.user_models import UserSavedSearch, UserTokenSession\nfrom tests.src.db.models.factories import UserFactory, UserSavedSearchFactory\n\n\n@pytest.fixture\ndef saved_search(enable_factory_create, user, db_session):\n    search = UserSavedSearchFactory.create(\n        user=user, name=\"Test Search\", search_query={\"keywords\": \"python\"}\n    )\n    db_session.commit()\n    return search\n\n\n@pytest.fixture(autouse=True)\ndef clear_data(db_session):\n    db_session.query(UserSavedSearch).delete()\n    db_session.query(UserTokenSession).delete()\n    db_session.commit()\n    yield\n\n\ndef test_user_delete_saved_search_unauthorized_user(\n    client, enable_factory_create, db_session, user, user_auth_token, saved_search\n):\n    # Try to delete a search for a different user ID\n    different_user = UserFactory.create()\n\n    response = client.delete(\n        f\"/v1/users/{different_user.user_id}/saved-searches/{saved_search.saved_search_id}\",\n        headers={\"X-SGG-Token\": user_auth_token},\n    )\n\n    assert response.status_code == 401\n    assert response.json[\"message\"] == \"Unauthorized user\"\n\n    # Verify search was not deleted\n    saved_searches = db_session.query(UserSavedSearch).all()\n    assert len(saved_searches) == 1\n\n\ndef test_user_delete_saved_search_no_auth(\n    client, enable_factory_create, db_session, user, user_auth_token, saved_search\n):\n    # Try to delete a search without authentication\n    response = client.delete(\n        f\"/v1/users/{user.user_id}/saved-searches/{saved_search.saved_search_id}\",\n    )\n\n    assert response.status_code == 401\n    assert response.json[\"message\"] == \"Unable to process token\"\n\n    # Verify search was not deleted\n    saved_searches = db_session.query(UserSavedSearch).all()\n    assert len(saved_searches) == 1\n\n\ndef test_user_delete_saved_search_not_found(\n    client,\n    enable_factory_create,\n    db_session,\n    user,\n    user_auth_token,\n):\n    # Try to delete a non-existent search\n    response = client.delete(\n        f\"/v1/users/{user.user_id}/saved-searches/{uuid.uuid4()}\",\n        headers={\"X-SGG-Token\": user_auth_token},\n    )\n\n    assert response.status_code == 404\n    assert response.json[\"message\"] == \"Saved search not found\"\n\n\ndef test_user_delete_saved_search(client, db_session, user, user_auth_token, saved_search):\n    response = client.delete(\n        f\"/v1/users/{user.user_id}/saved-searches/{saved_search.saved_search_id}\",\n        headers={\"X-SGG-Token\": user_auth_token},\n    )\n\n    assert response.status_code == 200\n    assert response.json[\"message\"] == \"Success\"\n\n    # Verify the search was deleted\n    saved_searches = db_session.query(UserSavedSearch).all()\n    assert len(saved_searches) == 0"}
{"path":"frontend/tests/components/GoalContent.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/GoalContent.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/users/test_user_get_saved_searches.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_get_saved_searches.py\nSize: 4.76 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/GrantsIdentifier.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/GrantsIdentifier.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"import pytest\nfrom sqlalchemy import delete\n\nfrom src.constants.lookup_constants import FundingInstrument\nfrom src.db.models.user_models import UserSavedSearch, UserTokenSession\nfrom tests.src.db.models.factories import UserFactory, UserSavedSearchFactory\n\n\n@pytest.fixture\ndef saved_searches(user, db_session):\n    searches = [\n        UserSavedSearchFactory.create(\n            user=user,\n            name=\"Test Search 1\",\n            search_query={\n                \"query\": \"python\",\n                \"filters\": {\"funding_instrument\": {\"one_of\": [FundingInstrument.GRANT]}},\n            },\n            created_at=datetime(2024, 1, 1, tzinfo=timezone.utc),\n        ),\n        UserSavedSearchFactory.create(\n            user=user,\n            name=\"Test Search 2\",\n            search_query={\n                \"query\": \"python\",\n                \"filters\": {\n                    \"keywords\": \"java\",\n                    \"funding_instrument\": {\"one_of\": [FundingInstrument.COOPERATIVE_AGREEMENT]},\n                },\n            },\n            created_at=datetime(2024, 1, 2, tzinfo=timezone.utc),\n        ),\n        UserSavedSearchFactory.create(\n            user=user,\n            name=\"Test Search 2\",\n            search_query={\n                \"query\": \"python\",\n                \"filters\": {\n                    \"keywords\": \"java\",\n                    \"funding_instrument\": {\"one_of\": [FundingInstrument.PROCUREMENT_CONTRACT]},\n                },\n            },\n            created_at=datetime(2024, 1, 3, tzinfo=timezone.utc),\n        ),\n    ]\n    db_session.commit()\n    return searches\n\n\n@pytest.fixture(autouse=True)\ndef clear_data(db_session):\n    db_session.execute(delete(UserSavedSearch))\n    db_session.execute(delete(UserTokenSession))\n    db_session.commit()\n    yield\n\n\ndef test_user_get_saved_searches_unauthorized_user(\n    client, db_session, user, user_auth_token, saved_searches\n):\n    # Try to get searches for a different user ID\n    different_user = UserFactory.create()\n    db_session.commit()\n\n    response = client.post(\n        f\"/v1/users/{different_user.user_id}/saved-searches/list\",\n        headers={\"X-SGG-Token\": user_auth_token},\n        json={\n            \"pagination\": {\n                \"page_offset\": 1,\n                \"page_size\": 25,\n            }\n        },\n    )\n\n    assert response.status_code == 401\n    assert response.json[\"message\"] == \"Unauthorized user\"\n\n\ndef test_user_get_saved_searches_no_auth(client, db_session, user, saved_searches):\n    # Try to get searches without authentication\n    response = client.post(\n        f\"/v1/users/{user.user_id}/saved-searches/list\",\n        json={\n            \"pagination\": {\n                \"page_offset\": 1,\n                \"page_size\": 25,\n            }\n        },\n    )\n\n    assert response.status_code == 401\n    assert response.json[\"message\"] == \"Unable to process token\"\n\n\ndef test_user_get_saved_searches_empty(client, user, user_auth_token):\n    response = client.post(\n        f\"/v1/users/{user.user_id}/saved-searches/list\",\n        headers={\"X-SGG-Token\": user_auth_token},\n        json={\n            \"pagination\": {\n                \"page_offset\": 1,\n                \"page_size\": 25,\n            }\n        },\n    )\n\n    assert response.status_code == 200\n    assert response.json[\"message\"] == \"Success\"\n    assert response.json[\"data\"] == []\n\n\ndef test_user_get_saved_searches(client, user, user_auth_token, saved_searches):\n    response = client.post(\n        f\"/v1/users/{user.user_id}/saved-searches/list\",\n        headers={\"X-SGG-Token\": user_auth_token},\n        json={\n            \"pagination\": {\n                \"page_offset\": 1,\n                \"page_size\": 25,\n                \"sort_order\": [\n                    {\"order_by\": \"name\", \"sort_direction\": \"ascending\"},\n                    {\"order_by\": \"created_at\", \"sort_direction\": \"descending\"},\n                ],\n            }\n        },\n    )\n\n    assert response.status_code == 200\n    assert response.json[\"message\"] == \"Success\"\n\n    data = response.json[\"data\"]\n    assert len(data) == 3\n\n    # Verify the searches are returned in ascending order by name, then descending order by created_at\n    assert data[0][\"name\"] == \"Test Search 1\"\n    assert data[0][\"search_query\"][\"filters\"][\"funding_instrument\"][\"one_of\"] == [\"grant\"]\n    assert data[1][\"name\"] == \"Test Search 2\"\n    assert data[1][\"search_query\"][\"filters\"][\"funding_instrument\"][\"one_of\"] == [\n        \"procurement_contract\"\n    ]\n\n    assert data[2][\"name\"] == \"Test Search 2\"\n    assert data[2][\"search_query\"][\"filters\"][\"funding_instrument\"][\"one_of\"] == [\n        \"cooperative_agreement\"\n    ]\n\n    # Verify UUIDs are properly serialized\n    assert uuid.UUID(data[0][\"saved_search_id\"])\n    assert uuid.UUID(data[1][\"saved_search_id\"])\n    assert uuid.UUID(data[2][\"saved_search_id\"])"}
{"path":"frontend/tests/components/Header.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/Header.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/users/test_user_route_get.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_route_get.py\nSize: 0.98 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/Hero.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/Hero.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"from src.auth.api_jwt_auth import create_jwt_for_user\nfrom tests.src.db.models.factories import LinkExternalUserFactory\n\n################\n# GET user tests\n################\n\n\ndef test_get_user_200(enable_factory_create, client, db_session, api_auth_token):\n    external_user = LinkExternalUserFactory.create()\n    token, _ = create_jwt_for_user(external_user.user, db_session)\n    db_session.commit()\n\n    resp = client.get(f\"/v1/users/{external_user.user_id}\", headers={\"X-SGG-Token\": token})\n\n    assert resp.status_code == 200\n    assert resp.get_json()[\"data\"][\"user_id\"] == str(external_user.user_id)\n\n\ndef test_get_user_401(enable_factory_create, client, db_session, api_auth_token):\n    external_user = LinkExternalUserFactory.create()\n    token, _ = create_jwt_for_user(external_user.user, db_session)\n    db_session.commit()\n\n    random_uuid = str(uuid.uuid4())\n    resp = client.get(f\"/v1/users/{random_uuid}\", headers={\"X-SGG-Token\": token})\n\n    assert resp.status_code == 401"}
{"path":"frontend/tests/components/LoginButtonModal.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/LoginButtonModal.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/users/test_user_route_login.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_route_login.py\nSize: 16.88 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/ProcessAndResearchContent.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ProcessAndResearchContent.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"import pytest\n\nimport src.auth.login_gov_jwt_auth as login_gov_jwt_auth\nfrom src.adapters.oauth.oauth_client_models import OauthTokenResponse\nfrom src.api.route_utils import raise_flask_error\nfrom src.auth.api_jwt_auth import parse_jwt_for_user\nfrom src.db.models.user_models import LinkExternalUser, LoginGovState\nfrom src.util import datetime_util\nfrom tests.lib.auth_test_utils import create_jwt\nfrom tests.src.db.models.factories import LinkExternalUserFactory, LoginGovStateFactory\n\n##########################################\n# Full login flow tests\n##########################################\n\n\ndef test_user_login_flow_happy_path_302(client, db_session):\n    \"\"\"Happy path for a user logging in through the whole flow\"\"\"\n    login_gov_config = login_gov_jwt_auth.get_config()\n    resp = client.get(\"/v1/users/login\", follow_redirects=True)\n\n    # The final endpoint returns a 200\n    # and dumps the params it was called with\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n    assert resp_json[\"is_user_new\"] == \"1\"\n    assert resp_json[\"message\"] == \"success\"\n    assert resp_json[\"token\"] is not None\n\n    # Verify the token we generated works with our later parsing logic\n    user_token_session = parse_jwt_for_user(resp_json[\"token\"], db_session)\n    assert user_token_session.expires_at > datetime_util.utcnow()\n    assert user_token_session.is_valid is True\n\n    # History contains each redirect, we redirected 3 times\n    assert len(resp.history) == 3\n\n    first_redirect, second_redirect, third_redirect = resp.history\n\n    # Redirect to oauth\n    assert first_redirect.status_code == 302\n    first_redirect_url = urllib.parse.urlparse(first_redirect.headers[\"Location\"])\n    assert first_redirect_url.path == \"/test-endpoint/oauth-authorize\"\n\n    first_redirect_params = urllib.parse.parse_qs(first_redirect_url.query)\n    assert first_redirect_params[\"client_id\"][0] == login_gov_config.client_id\n    assert first_redirect_params[\"nonce\"][0] is not None\n    assert first_redirect_params[\"state\"][0] is not None\n    assert first_redirect_params[\"redirect_uri\"][0] == \"http://localhost/v1/users/login/callback\"\n    assert first_redirect_params[\"acr_values\"][0] == login_gov_config.acr_value\n    assert first_redirect_params[\"scope\"][0] == login_gov_config.scope\n    assert first_redirect_params[\"prompt\"][0] == \"select_account\"\n    assert first_redirect_params[\"response_type\"][0] == \"code\"\n\n    # Redirect back to our callback endpoint\n    assert second_redirect.status_code == 302\n    second_redirect_url = urllib.parse.urlparse(second_redirect.headers[\"Location\"])\n    assert second_redirect_url.path == \"/v1/users/login/callback\"\n\n    second_redirect_params = urllib.parse.parse_qs(second_redirect_url.query)\n    assert second_redirect_params[\"code\"][0] is not None\n    assert second_redirect_params[\"state\"][0] == first_redirect_params[\"state\"][0]\n\n    # Redirect to the final destination page\n    assert third_redirect.status_code == 302\n    third_redirect_url = urllib.parse.urlparse(third_redirect.headers[\"Location\"])\n    assert third_redirect_url.path == \"/v1/users/login/result\"\n\n    third_redirect_params = urllib.parse.parse_qs(third_redirect_url.query)\n    assert third_redirect_params[\"message\"][0] == \"success\"\n    assert third_redirect_params[\"is_user_new\"][0] == \"1\"\n    assert third_redirect_params[\"token\"][0] == resp_json[\"token\"]\n\n\ndef test_user_login_flow_error_in_login_302(client, monkeypatch):\n    \"\"\"Test that the redirect happens to the final endpoint directly if an error occurs\"\"\"\n\n    # Force the api to error by overriding a function call\n    def override(*args, **kwargs):\n        raise Exception(\"I am an error\")\n\n    monkeypatch.setattr(\"flask.url_for\", override)\n\n    resp = client.get(\"/v1/users/login\", follow_redirects=True)\n\n    # The final endpoint returns a 200\n    # and dumps the params it was called with\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n\n    assert resp_json[\"message\"] == \"error\"\n    assert resp_json[\"error_description\"] == \"internal error\"\n\n    # History contains each redirect, we redirected just once\n    assert len(resp.history) == 1\n    redirect = resp.history[0]\n\n    assert redirect.status_code == 302\n    redirect_url = urllib.parse.urlparse(redirect.headers[\"Location\"])\n    assert redirect_url.path == \"/v1/users/login/result\"\n\n\ndef test_user_login_flow_error_in_http_error_302(client, monkeypatch):\n    \"\"\"Test that the redirect happens to the final endpoint directly if an error occurs\n\n    Only difference from above test is that the error message gets passed through\n    for an HTTPError that we rose\n    \"\"\"\n\n    # Force the api to error by overriding a function call\n    def override(*args, **kwargs):\n        raise_flask_error(422, \"I am an error\")\n\n    monkeypatch.setattr(\"flask.url_for\", override)\n\n    resp = client.get(\"/v1/users/login\", follow_redirects=True)\n\n    # The final endpoint returns a 200\n    # and dumps the params it was called with\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n\n    assert resp_json[\"message\"] == \"error\"\n    assert resp_json[\"error_description\"] == \"I am an error\"\n\n    # History contains each redirect, we redirected just once\n    assert len(resp.history) == 1\n    redirect = resp.history[0]\n\n    assert redirect.status_code == 302\n    redirect_url = urllib.parse.urlparse(redirect.headers[\"Location\"])\n    assert redirect_url.path == \"/v1/users/login/result\"\n\n\ndef test_user_login_flow_error_in_http_error_internal_302(client, monkeypatch):\n    \"\"\"Test that the redirect happens to the final endpoint directly if an error occurs\n\n    Even if it is raised by raise_flask_error, if it is a 5xx error, we want to not display the message\n    \"\"\"\n\n    # Force the api to error by overriding a function call\n    def override(*args, **kwargs):\n        raise_flask_error(503, \"I am an internal error\")\n\n    monkeypatch.setattr(\"flask.url_for\", override)\n\n    resp = client.get(\"/v1/users/login\", follow_redirects=True)\n\n    # The final endpoint returns a 200\n    # and dumps the params it was called with\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n\n    assert resp_json[\"message\"] == \"error\"\n    assert resp_json[\"error_description\"] == \"internal error\"\n\n    # History contains each redirect, we redirected just once\n    assert len(resp.history) == 1\n    redirect = resp.history[0]\n\n    assert redirect.status_code == 302\n    redirect_url = urllib.parse.urlparse(redirect.headers[\"Location\"])\n    assert redirect_url.path == \"/v1/users/login/result\"\n\n\ndef test_user_login_flow_error_in_auth_response_302(client, monkeypatch):\n    \"\"\"Test behavior when we get a redirect back from login.gov with an error\"\"\"\n\n    def override():\n        return {\"error\": \"access_denied\", \"error_description\": \"user does not have access\"}\n\n    monkeypatch.setattr(\"tests.lib.auth_test_utils.oauth_param_override\", override)\n\n    resp = client.get(\"/v1/users/login\", follow_redirects=True)\n\n    # The final endpoint returns a 200 even when erroring as it is just a GET endpoint\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n\n    # Because it was a 5xx error, the errors are intentionally vague\n    assert resp_json[\"message\"] == \"error\"\n    assert resp_json[\"error_description\"] == \"internal error\"\n\n    # We still redirected through every endpoint\n    assert len(resp.history) == 3\n\n\n##########################################\n# Callback endpoint direct tests\n##########################################\n\n\ndef test_user_callback_new_user_302(\n    client, db_session, enable_factory_create, mock_oauth_client, private_rsa_key\n):\n    # Create state so the callback gets past the check\n    login_gov_state = LoginGovStateFactory.create()\n\n    code = str(uuid.uuid4())\n    id_token = create_jwt(\n        user_id=\"bob-xyz\",\n        nonce=str(login_gov_state.nonce),\n        private_key=private_rsa_key,\n    )\n    mock_oauth_client.add_token_response(\n        code,\n        OauthTokenResponse(\n            id_token=id_token, access_token=\"fake_token\", token_type=\"Bearer\", expires_in=300\n        ),\n    )\n\n    resp = client.get(\n        f\"/v1/users/login/callback?state={login_gov_state.login_gov_state_id}&code={code}\",\n        follow_redirects=True,\n    )\n\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n    assert resp_json[\"is_user_new\"] == \"1\"\n    assert resp_json[\"message\"] == \"success\"\n    assert resp_json[\"token\"] is not None\n\n    user_token_session = parse_jwt_for_user(resp_json[\"token\"], db_session)\n    assert user_token_session.expires_at > datetime_util.utcnow()\n    assert user_token_session.is_valid is True\n\n    # Make sure the external user record is created with expected IDs\n    external_user = (\n        db_session.query(LinkExternalUser)\n        .filter(\n            LinkExternalUser.user_id == user_token_session.user_id,\n            LinkExternalUser.external_user_id == \"bob-xyz\",\n        )\n        .one_or_none()\n    )\n    assert external_user is not None\n\n    # Make sure the login gov state was deleted\n    db_state = (\n        db_session.query(LoginGovState)\n        .filter(LoginGovState.login_gov_state_id == login_gov_state.login_gov_state_id)\n        .one_or_none()\n    )\n    assert db_state is None\n\n\ndef test_user_callback_existing_user_302(\n    client, db_session, enable_factory_create, mock_oauth_client, private_rsa_key\n):\n    # Create state so the callback gets past the check\n    login_gov_state = LoginGovStateFactory.create()\n\n    login_gov_id = str(uuid.uuid4())\n    external_user = LinkExternalUserFactory.create(\n        external_user_id=login_gov_id, email=\"some_old_email@mail.com\"\n    )\n\n    code = str(uuid.uuid4())\n    id_token = create_jwt(\n        user_id=login_gov_id,\n        nonce=str(login_gov_state.nonce),\n        private_key=private_rsa_key,\n    )\n    mock_oauth_client.add_token_response(\n        code,\n        OauthTokenResponse(\n            id_token=id_token, access_token=\"fake_token\", token_type=\"Bearer\", expires_in=300\n        ),\n    )\n\n    resp = client.get(\n        f\"/v1/users/login/callback?state={login_gov_state.login_gov_state_id}&code={code}\",\n        follow_redirects=True,\n    )\n\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n    assert resp_json[\"is_user_new\"] == \"0\"\n    assert resp_json[\"message\"] == \"success\"\n    assert resp_json[\"token\"] is not None\n\n    user_token_session = parse_jwt_for_user(resp_json[\"token\"], db_session)\n    assert user_token_session.expires_at > datetime_util.utcnow()\n    assert user_token_session.is_valid is True\n    assert user_token_session.user_id == external_user.user_id\n\n    # Make sure the login gov state was deleted\n    db_state = (\n        db_session.query(LoginGovState)\n        .filter(LoginGovState.login_gov_state_id == login_gov_state.login_gov_state_id)\n        .one_or_none()\n    )\n    assert db_state is None\n\n\ndef test_user_callback_unknown_state_302(client, monkeypatch):\n    \"\"\"Test behavior when we get a redirect back from login.gov with an unknown state value\"\"\"\n\n    # We can just call the callback directly with the state that doesn't exist\n    resp = client.get(\n        f\"/v1/users/login/callback?state={uuid.uuid4()}&code=xyz456\", follow_redirects=True\n    )\n\n    # The final endpoint returns a 200 even when erroring as it is just a GET endpoint\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n\n    assert resp_json[\"message\"] == \"error\"\n    assert resp_json[\"error_description\"] == \"OAuth state not found\"\n\n\ndef test_user_callback_invalid_state_302(client, monkeypatch):\n    \"\"\"Test behavior when we get a redirect back from login.gov with an invalid state value\"\"\"\n\n    # We can just call the callback directly with the state that isn't a uuid\n    resp = client.get(\"/v1/users/login/callback?state=abc123&code=xyz456\", follow_redirects=True)\n\n    # The final endpoint returns a 200 even when erroring as it is just a GET endpoint\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n\n    assert resp_json[\"message\"] == \"error\"\n    assert resp_json[\"error_description\"] == \"Invalid OAuth state value\"\n\n\ndef test_user_callback_error_in_token_302(client, enable_factory_create, caplog):\n    \"\"\"Test behavior when we call the callback endpoint, but the oauth token endpoint has nothing\"\"\"\n\n    # Create state so the callback gets past the check\n    login_gov_state = LoginGovStateFactory.create()\n\n    resp = client.get(\n        f\"/v1/users/login/callback?state={login_gov_state.login_gov_state_id}&code=xyz456\",\n        follow_redirects=True,\n    )\n\n    # The final endpoint returns a 200 even when erroring as it is just a GET endpoint\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n\n    assert resp_json[\"message\"] == \"error\"\n    assert resp_json[\"error_description\"] == \"internal error\"\n\n    # Verify it errored because of the response from token Oauth\n    assert (\n        \"Unexpected error occurred in login flow via raise_flask_error: default mock error description\"\n        in caplog.messages\n    )\n\n\n@pytest.mark.parametrize(\n    \"jwt_params,error_description\",\n    [\n        ({\"issuer\": \"not-the-right-issuer\"}, \"Unknown Issuer\"),\n        ({\"audience\": \"jeff\"}, \"Unknown Audience\"),\n        ({\"expires_at\": datetime_util.utcnow() - timedelta(days=1)}, \"Expired Token\"),\n        ({\"issued_at\": datetime_util.utcnow() + timedelta(days=1)}, \"Token not yet valid\"),\n        ({\"not_before\": datetime_util.utcnow() + timedelta(days=1)}, \"Token not yet valid\"),\n    ],\n)\ndef test_user_callback_token_fails_validation_302(\n    client,\n    db_session,\n    enable_factory_create,\n    mock_oauth_client,\n    private_rsa_key,\n    jwt_params,\n    error_description,\n):\n    # Create state so the callback gets past the check\n    login_gov_state = LoginGovStateFactory.create()\n\n    code = str(uuid.uuid4())\n    id_token = create_jwt(\n        user_id=str(uuid.uuid4()),\n        nonce=str(login_gov_state.nonce),\n        private_key=private_rsa_key,\n        **jwt_params,\n    )\n    mock_oauth_client.add_token_response(\n        code,\n        OauthTokenResponse(\n            id_token=id_token, access_token=\"fake_token\", token_type=\"Bearer\", expires_in=300\n        ),\n    )\n\n    resp = client.get(\n        f\"/v1/users/login/callback?state={login_gov_state.login_gov_state_id}&code={code}\",\n        follow_redirects=True,\n    )\n\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n    assert resp_json[\"message\"] == \"error\"\n    assert resp_json[\"error_description\"] == error_description\n\n    # Make sure the login gov state was deleted even though it errored\n    db_state = (\n        db_session.query(LoginGovState)\n        .filter(LoginGovState.login_gov_state_id == login_gov_state.login_gov_state_id)\n        .one_or_none()\n    )\n    assert db_state is None\n\n\ndef test_user_callback_token_fails_validation_bad_token_302(\n    client, db_session, enable_factory_create, mock_oauth_client, private_rsa_key\n):\n    # Create state so the callback gets past the check\n    login_gov_state = LoginGovStateFactory.create()\n\n    code = str(uuid.uuid4())\n\n    mock_oauth_client.add_token_response(\n        code,\n        OauthTokenResponse(\n            id_token=\"bad-token\", access_token=\"fake_token\", token_type=\"Bearer\", expires_in=300\n        ),\n    )\n\n    resp = client.get(\n        f\"/v1/users/login/callback?state={login_gov_state.login_gov_state_id}&code={code}\",\n        follow_redirects=True,\n    )\n\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n    assert resp_json[\"message\"] == \"error\"\n    assert resp_json[\"error_description\"] == \"Unable to parse token - invalid format\"\n\n    # Make sure the login gov state was deleted even though it errored\n    db_state = (\n        db_session.query(LoginGovState)\n        .filter(LoginGovState.login_gov_state_id == login_gov_state.login_gov_state_id)\n        .one_or_none()\n    )\n    assert db_state is None\n\n\ndef test_user_callback_token_fails_validation_no_valid_key_302(\n    client, db_session, enable_factory_create, mock_oauth_client, other_rsa_key_pair\n):\n    \"\"\"Create the token with a different key than we check against\"\"\"\n    # Create state so the callback gets past the check\n    login_gov_state = LoginGovStateFactory.create()\n\n    code = str(uuid.uuid4())\n    id_token = create_jwt(\n        user_id=str(uuid.uuid4()),\n        nonce=str(login_gov_state.nonce),\n        private_key=other_rsa_key_pair[0],\n    )\n    mock_oauth_client.add_token_response(\n        code,\n        OauthTokenResponse(\n            id_token=id_token, access_token=\"fake_token\", token_type=\"Bearer\", expires_in=300\n        ),\n    )\n\n    resp = client.get(\n        f\"/v1/users/login/callback?state={login_gov_state.login_gov_state_id}&code={code}\",\n        follow_redirects=True,\n    )\n\n    assert resp.status_code == 200\n    resp_json = resp.get_json()\n    assert resp_json[\"message\"] == \"error\"\n    assert resp_json[\"error_description\"] == \"Invalid Signature\"\n\n    # Make sure the login gov state was deleted even though it errored\n    db_state = (\n        db_session.query(LoginGovState)\n        .filter(LoginGovState.login_gov_state_id == login_gov_state.login_gov_state_id)\n        .one_or_none()\n    )\n    assert db_state is None"}
{"path":"frontend/tests/components/ProcessIntro.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ProcessIntro.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/users/test_user_route_token.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_route_token.py\nSize: 2.04 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/ProcessInvolved.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ProcessInvolved.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"from freezegun import freeze_time\n\nfrom src.auth.api_jwt_auth import create_jwt_for_user\nfrom tests.src.db.models.factories import UserFactory\n\n\n@freeze_time(\"2024-11-22 12:00:00\", tz_offset=0)\ndef test_post_user_route_token_refresh_200(\n    enable_factory_create, client, db_session, api_auth_token\n):\n    user = UserFactory.create()\n    token, user_token_session = create_jwt_for_user(user, db_session)\n    db_session.commit()\n\n    resp = client.post(\"v1/users/token/refresh\", headers={\"X-SGG-Token\": token})\n\n    db_session.refresh(user_token_session)\n\n    assert resp.status_code == 200\n    assert user_token_session.expires_at == datetime.fromisoformat(\"2024-11-22 12:30:00+00:00\")\n\n\ndef test_post_user_route_token_refresh_expired(\n    enable_factory_create, client, db_session, api_auth_token\n):\n    user = UserFactory.create()\n\n    token, session = create_jwt_for_user(user, db_session)\n    session.expires_at = datetime.fromisoformat(\"1980-01-01 12:00:00+00:00\")\n    db_session.commit()\n\n    resp = client.post(\"v1/users/token/refresh\", headers={\"X-SGG-Token\": token})\n\n    assert resp.status_code == 401\n    assert resp.get_json()[\"message\"] == \"Token expired\"\n\n\ndef test_post_user_route_token_logout_200(\n    enable_factory_create, client, db_session, api_auth_token\n):\n    user = UserFactory.create()\n    token, user_token_session = create_jwt_for_user(user, db_session)\n    db_session.commit()\n\n    resp = client.post(\"v1/users/token/logout\", headers={\"X-SGG-Token\": token})\n\n    db_session.refresh(user_token_session)\n\n    assert resp.status_code == 200\n    assert not user_token_session.is_valid\n\n\ndef test_post_user_route_token_logout_invalid(\n    enable_factory_create, client, db_session, api_auth_token\n):\n    user = UserFactory.create()\n\n    token, session = create_jwt_for_user(user, db_session)\n    session.is_valid = False\n    db_session.commit()\n\n    resp = client.post(\"v1/users/token/logout\", headers={\"X-SGG-Token\": token})\n\n    assert resp.status_code == 401\n    assert resp.get_json()[\"message\"] == \"Token is no longer valid\""}
{"path":"frontend/tests/components/ProcessNext.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ProcessNext.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/users/test_user_save_opportunity_post.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_save_opportunity_post.py\nSize: 2.94 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/ProcessProgress.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ProcessProgress.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"import pytest\n\nfrom src.db.models.user_models import UserSavedOpportunity\nfrom tests.src.db.models.factories import OpportunityFactory\n\n\n@pytest.fixture(autouse=True)\ndef clear_opportunities(db_session):\n    db_session.query(UserSavedOpportunity).delete()\n    db_session.commit()\n    yield\n\n\ndef test_user_save_opportunity_post_unauthorized_user(\n    client, db_session, user, user_auth_token, enable_factory_create\n):\n    # Create an opportunity\n    opportunity = OpportunityFactory.create()\n\n    # Try to save an opportunity for a different user ID\n    different_user_id = uuid.uuid4()\n    response = client.post(\n        f\"/v1/users/{different_user_id}/saved-opportunities\",\n        headers={\"X-SGG-Token\": user_auth_token},\n        json={\"opportunity_id\": opportunity.opportunity_id},\n    )\n\n    assert response.status_code == 401\n    assert response.json[\"message\"] == \"Unauthorized user\"\n\n    # Verify no opportunity was saved\n    saved_opportunities = db_session.query(UserSavedOpportunity).all()\n    assert len(saved_opportunities) == 0\n\n\ndef test_user_save_opportunity_post_no_auth(client, db_session, user, enable_factory_create):\n    # Create an opportunity\n    opportunity = OpportunityFactory.create()\n\n    # Try to save an opportunity without authentication\n    response = client.post(\n        f\"/v1/users/{user.user_id}/saved-opportunities\",\n        json={\"opportunity_id\": opportunity.opportunity_id},\n    )\n\n    assert response.status_code == 401\n    assert response.json[\"message\"] == \"Unable to process token\"\n\n    # Verify no opportunity was saved\n    saved_opportunities = db_session.query(UserSavedOpportunity).all()\n    assert len(saved_opportunities) == 0\n\n\ndef test_user_save_opportunity_post_invalid_request(\n    client, user, user_auth_token, enable_factory_create, db_session\n):\n    # Make request with missing opportunity_id\n    response = client.post(\n        f\"/v1/users/{user.user_id}/saved-opportunities\",\n        headers={\"X-SGG-Token\": user_auth_token},\n        json={},\n    )\n\n    assert response.status_code == 422  # Validation error\n\n    # Verify no opportunity was saved\n    saved_opportunities = db_session.query(UserSavedOpportunity).all()\n    assert len(saved_opportunities) == 0\n\n\ndef test_user_save_opportunity_post(\n    client, user, user_auth_token, enable_factory_create, db_session\n):\n    # Create an opportunity\n    opportunity = OpportunityFactory.create()\n\n    # Make the request to save an opportunity\n    response = client.post(\n        f\"/v1/users/{user.user_id}/saved-opportunities\",\n        headers={\"X-SGG-Token\": user_auth_token},\n        json={\"opportunity_id\": opportunity.opportunity_id},\n    )\n\n    assert response.status_code == 200\n    assert response.json[\"message\"] == \"Success\"\n\n    # Verify the opportunity was saved in the database\n    saved_opportunity = db_session.query(UserSavedOpportunity).one()\n    assert saved_opportunity.user_id == user.user_id\n    assert saved_opportunity.opportunity_id == opportunity.opportunity_id"}
{"path":"frontend/tests/components/ResearchArchetypes.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ResearchArchetypes.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"File: api/tests/src/api/users/test_user_save_search_post.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_save_search_post.py\nSize: 6.43 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/ResearchImpact.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ResearchImpact.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.135Z","content":"import pytest\n\nfrom src.api.opportunities_v1.opportunity_schemas import OpportunityV1Schema\nfrom src.constants.lookup_constants import (\n    ApplicantType,\n    FundingCategory,\n    FundingInstrument,\n    OpportunityStatus,\n)\nfrom src.db.models.user_models import UserSavedSearch\nfrom tests.src.api.opportunities_v1.conftest import get_search_request\nfrom tests.src.api.opportunities_v1.test_opportunity_route_search import build_opp\nfrom tests.src.db.models.factories import UserFactory\n\nSPORTS = build_opp(\n    opportunity_title=\"Research into Sports administrator industry\",\n    opportunity_number=\"EFG8532950\",\n    agency=\"USAID\",\n    summary_description=\"HHS-CDC is looking to further investigate this topic. Car matter style top quality generation effort. Computer purpose while consumer left.\",\n    opportunity_status=OpportunityStatus.FORECASTED,\n    assistance_listings=[(\"79.718\", \"Huff LLC\")],\n    applicant_types=[ApplicantType.OTHER],\n    funding_instruments=[FundingInstrument.GRANT],\n    funding_categories=[FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT],\n    post_date=date(2019, 12, 8),\n    close_date=date(2024, 12, 28),\n    is_cost_sharing=True,\n    expected_number_of_awards=1,\n    award_floor=402500,\n    award_ceiling=8050000,\n    estimated_total_program_funding=5000,\n)\nMEDICAL_LABORATORY = build_opp(\n    opportunity_title=\"Research into Medical laboratory scientific officer industry\",\n    opportunity_number=\"AO-44-EMC-878\",\n    agency=\"USAID\",\n    summary_description=\"HHS-CDC is looking to further investigate this topic. Car matter style top quality generation effort. Computer purpose while consumer left.\",\n    opportunity_status=OpportunityStatus.FORECASTED,\n    assistance_listings=[(\"43.012\", \"Brown LLC\")],\n    applicant_types=[ApplicantType.OTHER],\n    funding_instruments=[FundingInstrument.GRANT],\n    funding_categories=[FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT],\n    post_date=date(2025, 1, 25),  #\n    close_date=date(2025, 6, 4),  #\n    is_cost_sharing=True,  #\n    expected_number_of_awards=1,\n    award_floor=402500,\n    award_ceiling=8050000,\n    estimated_total_program_funding=5000,\n)\n\n\n@pytest.fixture(autouse=True, scope=\"function\")\ndef clear_saved_searches(db_session):\n    db_session.query(UserSavedSearch).delete()\n    db_session.commit()\n    yield\n\n\n@pytest.fixture\ndef opportunity_search_index_alias(search_client, monkeypatch):\n    # Note we don't actually create anything, this is just a random name\n    alias = f\"test-opportunity-search-index-alias-{uuid.uuid4().int}\"\n    monkeypatch.setenv(\"OPPORTUNITY_SEARCH_INDEX_ALIAS\", alias)\n    return alias\n\n\ndef test_user_save_search_post_unauthorized_user(client, db_session, user, user_auth_token):\n    # Try to save a search for a different user ID\n    different_user = UserFactory.create()\n\n    search_query = get_search_request(\n        funding_instrument_one_of=[FundingInstrument.GRANT],\n        agency_one_of=[\"LOC\"],\n    )\n\n    response = client.post(\n        f\"/v1/users/{different_user.user_id}/saved-searches\",\n        headers={\"X-SGG-Token\": user_auth_token},\n        json={\"name\": \"Test Search\", \"search_query\": search_query},\n    )\n\n    assert response.status_code == 401\n    assert response.json[\"message\"] == \"Unauthorized user\"\n\n    # Verify no search was saved\n    saved_searches = db_session.query(UserSavedSearch).all()\n    assert len(saved_searches) == 0\n\n\ndef test_user_save_search_post_no_auth(client, db_session, user):\n    search_query = get_search_request(\n        funding_instrument_one_of=[FundingInstrument.GRANT],\n        agency_one_of=[\"LOC\"],\n    )\n\n    # Try to save a search without authentication\n    response = client.post(\n        f\"/v1/users/{user.user_id}/saved-searches\",\n        json={\"name\": \"Test Search\", \"search_query\": search_query},\n    )\n\n    assert response.status_code == 401\n    assert response.json[\"message\"] == \"Unable to process token\"\n\n    # Verify no search was saved\n    saved_searches = db_session.query(UserSavedSearch).all()\n    assert len(saved_searches) == 0\n\n\ndef test_user_save_search_post_invalid_request(client, user, user_auth_token, db_session):\n    # Make request with missing required fields\n    response = client.post(\n        f\"/v1/users/{user.user_id}/saved-searches\",\n        headers={\"X-SGG-Token\": user_auth_token},\n        json={},\n    )\n\n    assert response.status_code == 422  # Validation error\n\n    # Verify no search was saved\n    saved_searches = db_session.query(UserSavedSearch).all()\n    assert len(saved_searches) == 0\n\n\ndef test_user_save_search_post(\n    client,\n    opportunity_index,\n    search_client,\n    user,\n    user_auth_token,\n    enable_factory_create,\n    db_session,\n    opportunity_search_index_alias,\n    monkeypatch,\n):\n    # Test data\n    search_name = \"Test Search\"\n    search_query = get_search_request(\n        funding_instrument_one_of=[FundingInstrument.GRANT],\n        agency_one_of=[\"USAID\"],\n    )\n\n    # Load into the search index\n    schema = OpportunityV1Schema()\n    json_records = [schema.dump(opp) for opp in [SPORTS, MEDICAL_LABORATORY]]\n    search_client.bulk_upsert(opportunity_index, json_records, \"opportunity_id\")\n\n    search_client.swap_alias_index(opportunity_index, opportunity_search_index_alias)\n\n    # Make the request to save a search\n    response = client.post(\n        f\"/v1/users/{user.user_id}/saved-searches\",\n        headers={\"X-SGG-Token\": user_auth_token},\n        json={\"name\": search_name, \"search_query\": search_query},\n    )\n\n    assert response.status_code == 200\n    assert response.json[\"message\"] == \"Success\"\n    # Verify the search was saved in the database\n    saved_search = db_session.query(UserSavedSearch).one()\n\n    assert saved_search.user_id == user.user_id\n    assert saved_search.name == search_name\n    assert saved_search.search_query == {\n        \"query_operator\": \"AND\",\n        \"format\": \"json\",\n        \"filters\": {\"agency\": {\"one_of\": [\"USAID\"]}, \"funding_instrument\": {\"one_of\": [\"grant\"]}},\n        \"pagination\": {\n            \"page_size\": 25,\n            \"page_offset\": 1,\n            \"sort_order\": [\n                {\n                    \"order_by\": \"opportunity_id\",\n                    \"sort_direction\": \"ascending\",\n                }\n            ],\n        },\n    }\n    # Verify pagination for the query was over-written. searched_opportunity_ids should be ordered by \"post_date\"\n    assert saved_search.searched_opportunity_ids == [\n        MEDICAL_LABORATORY.opportunity_id,\n        SPORTS.opportunity_id,\n    ]"}
{"path":"frontend/tests/components/ResearchIntro.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ResearchIntro.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/api/users/test_user_saved_opportunities_get.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_saved_opportunities_get.py\nSize: 2.45 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/ResearchMethodology.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ResearchMethodology.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"from src.auth.api_jwt_auth import create_jwt_for_user\nfrom src.db.models.user_models import UserSavedOpportunity\nfrom tests.src.db.models.factories import (\n    OpportunityFactory,\n    UserFactory,\n    UserSavedOpportunityFactory,\n)\n\n\n@pytest.fixture\ndef user(enable_factory_create, db_session):\n    return UserFactory.create()\n\n\n@pytest.fixture\ndef user_auth_token(user, db_session):\n    token, _ = create_jwt_for_user(user, db_session)\n    return token\n\n\n@pytest.fixture(autouse=True, scope=\"function\")\ndef clear_opportunities(db_session):\n    db_session.query(UserSavedOpportunity).delete()\n    db_session.commit()\n    yield\n\n\ndef test_user_get_saved_opportunities(\n    client, user, user_auth_token, enable_factory_create, db_session\n):\n    # Create an opportunity and save it for the user\n    opportunity = OpportunityFactory.create(opportunity_title=\"Test Opportunity\")\n    UserSavedOpportunityFactory.create(user=user, opportunity=opportunity)\n\n    # Make the request\n    response = client.get(\n        f\"/v1/users/{user.user_id}/saved-opportunities\", headers={\"X-SGG-Token\": user_auth_token}\n    )\n\n    assert response.status_code == 200\n    assert len(response.json[\"data\"]) == 1\n    assert response.json[\"data\"][0][\"opportunity_id\"] == opportunity.opportunity_id\n    assert response.json[\"data\"][0][\"opportunity_title\"] == opportunity.opportunity_title\n\n\ndef test_get_saved_opportunities_unauthorized_user(client, enable_factory_create, db_session, user):\n    \"\"\"Test that a user cannot view another user's saved opportunities\"\"\"\n    # Create a user and get their token\n    user = UserFactory.create()\n    token, _ = create_jwt_for_user(user, db_session)\n\n    # Create another user and save an opportunity for them\n    other_user = UserFactory.create()\n    opportunity = OpportunityFactory.create()\n    UserSavedOpportunityFactory.create(user=other_user, opportunity=opportunity)\n\n    # Try to get the other user's saved opportunities\n    response = client.get(\n        f\"/v1/users/{other_user.user_id}/saved-opportunities\", headers={\"X-SGG-Token\": token}\n    )\n\n    assert response.status_code == 401\n    assert response.json[\"message\"] == \"Unauthorized user\"\n\n    # Try with a non-existent user ID\n    different_user_id = \"123e4567-e89b-12d3-a456-426614174000\"\n    response = client.get(\n        f\"/v1/users/{different_user_id}/saved-opportunities\", headers={\"X-SGG-Token\": token}\n    )\n\n    assert response.status_code == 401\n    assert response.json[\"message\"] == \"Unauthorized user\""}
{"path":"frontend/tests/components/ResearchThemes.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ResearchThemes.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/api/users/test_user_update_saved_search.py\nLanguage: py\nType: code\nDirectory: api/tests/src/api/users\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/api/users/test_user_update_saved_search.py\nSize: 2.95 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/SaveButton.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/SaveButton.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"import pytest\n\nfrom src.db.models.user_models import UserSavedSearch, UserTokenSession\nfrom tests.src.db.models.factories import UserFactory, UserSavedSearchFactory\n\n\n@pytest.fixture\ndef saved_search(enable_factory_create, user, db_session):\n    search = UserSavedSearchFactory.create(\n        user=user, name=\"Save Search\", search_query={\"keywords\": \"python\"}\n    )\n    return search\n\n\n@pytest.fixture(autouse=True)\ndef clear_data(db_session):\n    db_session.query(UserSavedSearch).delete()\n    db_session.query(UserTokenSession).delete()\n    yield\n\n\ndef test_user_update_saved_search(client, db_session, user, user_auth_token, saved_search):\n    updated_name = \"Update Search\"\n    response = client.put(\n        f\"/v1/users/{user.user_id}/saved-searches/{saved_search.saved_search_id}\",\n        headers={\"X-SGG-Token\": user_auth_token},\n        json={\"name\": updated_name},\n    )\n\n    db_session.refresh(saved_search)\n\n    assert response.status_code == 200\n    assert response.json[\"message\"] == \"Success\"\n\n    # Verify search was updated\n    updated_saved_search = db_session.query(UserSavedSearch).first()\n\n    assert updated_saved_search.name == updated_name\n\n\ndef test_user_update_saved_search_not_found(\n    client,\n    enable_factory_create,\n    db_session,\n    user,\n    user_auth_token,\n):\n    # Try to update a non-existent search\n    response = client.put(\n        f\"/v1/users/{user.user_id}/saved-searches/{uuid.uuid4()}\",\n        headers={\"X-SGG-Token\": user_auth_token},\n        json={\"name\": \"Update Search\"},\n    )\n\n    assert response.status_code == 404\n    assert response.json[\"message\"] == \"Saved search not found\"\n\n\ndef test_user_update_saved_search_unauthorized(\n    client, enable_factory_create, db_session, user, user_auth_token, saved_search\n):\n    # Try to update a search with another user\n    unauthorized_user = UserFactory.create()\n    response = client.put(\n        f\"/v1/users/{unauthorized_user.user_id}/saved-searches/{saved_search.saved_search_id}\",\n        headers={\"X-SGG-Token\": user_auth_token},\n        json={\"name\": \"Update Search\"},\n    )\n\n    db_session.refresh(saved_search)\n\n    assert response.status_code == 401\n    assert response.json[\"message\"] == \"Unauthorized user\"\n\n    # Verify search was not updated\n    saved_searches = db_session.query(UserSavedSearch).first()\n    assert saved_searches.name == saved_search.name\n\n\ndef test_user_update_saved_search_no_auth(\n    client, enable_factory_create, db_session, user, user_auth_token, saved_search\n):\n    # Try to update a search without authentication\n    response = client.put(\n        f\"/v1/users/{user.user_id}/saved-searches/{saved_search.saved_search_id}\",\n        json={\"name\": \"Update Search\"},\n    )\n    db_session.refresh(saved_search)\n\n    assert response.status_code == 401\n    assert response.json[\"message\"] == \"Unable to process token\"\n\n    # Verify search was not updated\n    saved_searches = db_session.query(UserSavedSearch).first()\n    assert saved_searches.name == saved_search.name"}
{"path":"frontend/tests/components/ServerErrorAlert.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/ServerErrorAlert.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/auth/test_api_jwt_auth.py\nLanguage: py\nType: code\nDirectory: api/tests/src/auth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/auth/test_api_jwt_auth.py\nSize: 7.98 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/SessionCheck.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/SessionCheck.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"import jwt\nimport pytest\nfrom freezegun import freeze_time\n\nimport src.app as app_entry\nimport src.logging\nfrom src.auth.api_jwt_auth import (\n    ApiJwtConfig,\n    api_jwt_auth,\n    create_jwt_for_user,\n    parse_jwt_for_user,\n)\nfrom src.db.models.user_models import UserTokenSession\nfrom tests.src.db.models.factories import LinkExternalUserFactory, UserFactory\n\n\n@pytest.fixture\ndef jwt_config(private_rsa_key, public_rsa_key):\n    return ApiJwtConfig(\n        API_JWT_PRIVATE_KEY=private_rsa_key,\n        API_JWT_PUBLIC_KEY=public_rsa_key,\n    )\n\n\n@pytest.fixture(scope=\"module\")\ndef mini_app(monkeypatch_module):\n    def stub(app):\n        pass\n\n    \"\"\"Create a separate app that we can modify separate from the base one used by other tests\"\"\"\n    # We want all the configurational setup for the app, but\n    # don't want blueprints to keep setup simpler\n    monkeypatch_module.setattr(app_entry, \"register_blueprints\", stub)\n    monkeypatch_module.setattr(app_entry, \"setup_logging\", stub)\n    mini_app = app_entry.create_app()\n\n    @mini_app.get(\"/dummy_auth_endpoint\")\n    @mini_app.auth_required(api_jwt_auth)\n    def dummy_endpoint():\n        # For the tests that actually get past auth\n        # make sure the current user is set to the user session\n        assert api_jwt_auth.current_user is not None\n        assert isinstance(api_jwt_auth.current_user, UserTokenSession)\n\n        return {\"message\": \"ok\"}\n\n    # To avoid re-initializing logging everytime we\n    # setup the app, we disabled it above and do it here\n    # in case you want it while running your tests\n    with src.logging.init(__package__):\n        yield mini_app\n\n\n@freeze_time(\"2024-11-14 12:00:00\", tz_offset=0)\ndef test_create_jwt_for_user(enable_factory_create, db_session, jwt_config):\n    user = UserFactory.create()\n    linked_external_user = LinkExternalUserFactory.create(user=user)\n    token, token_session = create_jwt_for_user(user, db_session, jwt_config)\n    decoded_token = jwt.decode(\n        token, algorithms=[jwt_config.algorithm], options={\"verify_signature\": False}\n    )\n\n    # Verify the issued at timestamp is at the expected (now) timestamp\n    # note we have to convert it to a unix timestamp\n    assert decoded_token[\"iat\"] == timegm(\n        datetime.fromisoformat(\"2024-11-14 12:00:00+00:00\").utctimetuple()\n    )\n    assert decoded_token[\"user_id\"] == str(user.user_id)\n    assert decoded_token[\"email\"] is None\n    assert decoded_token[\"iss\"] == jwt_config.issuer\n    assert decoded_token[\"aud\"] == jwt_config.audience\n\n    token_with_email, _ = create_jwt_for_user(\n        user, db_session, jwt_config, email=linked_external_user.email\n    )\n    decoded_token_with_email = jwt.decode(\n        token_with_email, algorithms=[jwt_config.algorithm], options={\"verify_signature\": False}\n    )\n    assert decoded_token_with_email[\"email\"] == linked_external_user.email\n\n    # Verify that the sub_id returned can be used to fetch a UserTokenSession object\n    token_session = (\n        db_session.query(UserTokenSession)\n        .filter(UserTokenSession.token_id == decoded_token[\"sub\"])\n        .one_or_none()\n    )\n\n    assert token_session.user_id == user.user_id\n    assert token_session.is_valid is True\n    # Verify expires_at is set to 30 minutes after now by default\n    assert token_session.expires_at == datetime.fromisoformat(\"2024-11-14 12:30:00+00:00\")\n\n    # Basic testing that the JWT we create for a user can in turn be fetched and processed later\n    user_session = parse_jwt_for_user(token, db_session, jwt_config)\n    assert user_session.user_id == user.user_id\n\n\ndef test_api_jwt_auth_happy_path(mini_app, enable_factory_create, db_session):\n    user = UserFactory.create()\n    token, _ = create_jwt_for_user(user, db_session)\n    db_session.commit()  # need to commit here to push the session to the DB\n\n    resp = mini_app.test_client().get(\"/dummy_auth_endpoint\", headers={\"X-SGG-Token\": token})\n\n    assert resp.status_code == 200\n    assert resp.get_json()[\"message\"] == \"ok\"\n\n\ndef test_api_jwt_auth_expired_token(mini_app, enable_factory_create, db_session):\n    user = UserFactory.create()\n    token, session = create_jwt_for_user(user, db_session)\n    session.expires_at = datetime.fromisoformat(\"1980-01-01 12:00:00+00:00\")\n    db_session.commit()\n\n    resp = mini_app.test_client().get(\"/dummy_auth_endpoint\", headers={\"X-SGG-Token\": token})\n\n    assert resp.status_code == 401\n    assert resp.get_json()[\"message\"] == \"Token expired\"\n\n\ndef test_api_jwt_auth_invalid_token(mini_app, enable_factory_create, db_session):\n    user = UserFactory.create()\n    token, session = create_jwt_for_user(user, db_session)\n    session.is_valid = False\n    db_session.commit()\n\n    resp = mini_app.test_client().get(\"/dummy_auth_endpoint\", headers={\"X-SGG-Token\": token})\n\n    assert resp.status_code == 401\n    assert resp.get_json()[\"message\"] == \"Token is no longer valid\"\n\n\ndef test_api_jwt_auth_token_missing_in_db(mini_app, enable_factory_create, db_session):\n    user = UserFactory.create()\n    token, session = create_jwt_for_user(user, db_session)\n    db_session.expunge(session)  # Just drop it, never sending to the DB\n\n    resp = mini_app.test_client().get(\"/dummy_auth_endpoint\", headers={\"X-SGG-Token\": token})\n\n    assert resp.status_code == 401\n    assert resp.get_json()[\"message\"] == \"Token session does not exist\"\n\n\ndef test_api_jwt_auth_token_not_jwt(mini_app, enable_factory_create, db_session):\n    # Just call with a random set of characters\n    resp = mini_app.test_client().get(\"/dummy_auth_endpoint\", headers={\"X-SGG-Token\": \"abc123\"})\n    assert resp.status_code == 401\n    assert resp.get_json()[\"message\"] == \"Unable to process token\"\n\n\ndef test_api_jwt_auth_token_created_with_different_key(\n    mini_app, enable_factory_create, db_session, jwt_config\n):\n    # Note - jwt_config uses a key generated in the conftest within this directory\n    # while the config the app picks up grabs a key from our override.env file\n    user = UserFactory.create()\n    token, _ = create_jwt_for_user(user, db_session, jwt_config)\n    db_session.commit()\n\n    resp = mini_app.test_client().get(\"/dummy_auth_endpoint\", headers={\"X-SGG-Token\": token})\n\n    assert resp.status_code == 401\n    assert resp.get_json()[\"message\"] == \"Unable to process token\"\n\n\ndef test_api_jwt_auth_token_iat_future(mini_app, enable_factory_create, db_session):\n    # Set time to the 14th so the iat value will be then\n    with freeze_time(\"2024-11-14 12:00:00\", tz_offset=0):\n        user = UserFactory.create()\n        token, _ = create_jwt_for_user(user, db_session)\n        db_session.commit()\n\n    # Set time to the 12th when calling the API so the iat will be in the future now\n    with freeze_time(\"2024-11-12 12:00:00\", tz_offset=0):\n        resp = mini_app.test_client().get(\"/dummy_auth_endpoint\", headers={\"X-SGG-Token\": token})\n\n    assert resp.status_code == 401\n    assert resp.get_json()[\"message\"] == \"Token not yet valid\"\n\n\ndef test_api_jwt_auth_token_unknown_issuer(mini_app, enable_factory_create, db_session):\n    config = ApiJwtConfig(API_JWT_ISSUER=\"some-guy\")\n    user = UserFactory.create()\n    token, _ = create_jwt_for_user(user, db_session, config)\n    db_session.commit()\n\n    resp = mini_app.test_client().get(\"/dummy_auth_endpoint\", headers={\"X-SGG-Token\": token})\n    assert resp.status_code == 401\n    assert resp.get_json()[\"message\"] == \"Unknown Issuer\"\n\n\ndef test_api_jwt_auth_token_unknown_audience(mini_app, enable_factory_create, db_session):\n    config = ApiJwtConfig(API_JWT_AUDIENCE=\"someone-else\")\n    user = UserFactory.create()\n    token, _ = create_jwt_for_user(user, db_session, config)\n    db_session.commit()\n\n    resp = mini_app.test_client().get(\"/dummy_auth_endpoint\", headers={\"X-SGG-Token\": token})\n    assert resp.status_code == 401\n    assert resp.get_json()[\"message\"] == \"Unknown Audience\"\n\n\ndef test_api_jwt_auth_no_token(mini_app, enable_factory_create, db_session):\n    resp = mini_app.test_client().get(\"/dummy_auth_endpoint\", headers={})\n    assert resp.status_code == 401\n    assert resp.get_json()[\"message\"] == \"Unable to process token\""}
{"path":"frontend/tests/components/Spinner.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/Spinner.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/auth/test_api_key_auth.py\nLanguage: py\nType: code\nDirectory: api/tests/src/auth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/auth/test_api_key_auth.py\nSize: 2.12 KB\nLast Modified: 2025-02-14T17:08:26.459Z"}
{"path":"frontend/tests/components/USWDSIcon.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/USWDSIcon.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"import src.app as app_entry\nfrom src.auth.api_key_auth import api_key_auth, verify_token\n\n\ndef test_verify_token_success(app, api_auth_token):\n    # Passing it the configured auth token successfully returns a user\n    with app.test_request_context():  # So we can attach the user to the flask app\n        user = verify_token(api_auth_token)\n\n        assert user.username == \"auth_token_0\"\n        assert g.get(\"current_user\") == user\n\n\ndef test_verify_token_other_tokens(app, all_api_auth_tokens):\n    # Verify all auth tokens configured are valid and have their own usernames\n    with app.test_request_context():\n        for i, auth_token in enumerate(all_api_auth_tokens):\n            user = verify_token(auth_token)\n\n            assert user.username == f\"auth_token_{i}\"\n            assert g.get(\"current_user\") == user\n\n\ndef test_username_logging(caplog, all_api_auth_tokens):\n    # Create a quick endpoint to test that the username gets attached.\n    # We don't use an existing one to avoid breaking this test as we implement other endpoints\n    # We can't use the app from the tests as you can't make a new endpoint after\n    # any endpoint is called\n    app = app_entry.create_app()\n\n    @app.get(\"/dummy_auth_endpoint\")\n    @app.auth_required(api_key_auth)\n    def dummy_endpoint():\n        return \"ok\"\n\n    for i, api_auth_token in enumerate(all_api_auth_tokens):\n        app.test_client().get(\"/dummy_auth_endpoint\", headers={\"X-Auth\": api_auth_token})\n\n        # Check that the username is attached to the log record, we'll just grab the last one\n        assert caplog.records[-1].__dict__[\"auth.username\"] == f\"auth_token_{i}\"\n\n\ndef test_verify_token_invalid_token(api_auth_token):\n    # If you pass it the wrong token\n    with pytest.raises(HTTPError):\n        verify_token(\"not the right token\")\n\n\ndef test_verify_token_no_configuration(monkeypatch):\n    # Remove the API_AUTH_TOKEN env var if set in\n    # your local environment\n    monkeypatch.delenv(\"API_AUTH_TOKEN\", raising=False)\n    # If the auth token is not setup\n    with pytest.raises(HTTPError):\n        verify_token(\"any token\")"}
{"path":"frontend/tests/components/opportunity/OpportunityAwardGridRow.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/opportunity/OpportunityAwardGridRow.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/auth/test_login_gov_jwt_auth.py\nLanguage: py\nType: code\nDirectory: api/tests/src/auth\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/auth/test_login_gov_jwt_auth.py\nSize: 9.36 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/opportunity/OpportunityAwardInfo.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/opportunity/OpportunityAwardInfo.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"import freezegun\nimport jwt\nimport pytest\n\nimport src.auth.login_gov_jwt_auth as login_gov_jwt_auth\nfrom src.auth.login_gov_jwt_auth import JwtValidationError, LoginGovConfig, validate_token\n\nDEFAULT_CLIENT_ID = \"urn:gov:unit-test\"\nDEFAULT_ISSUER = \"http://localhost:3000\"\nDEFAULT_NONCE = \"abc123\"\n\n\n@pytest.fixture\ndef login_gov_config(public_rsa_key, private_rsa_key):\n    # Note this isn't session scoped so it gets remade\n    # for every test in the event of changes to it\n    return LoginGovConfig(\n        LOGIN_GOV_PUBLIC_KEY_MAP={\"test-key-id\": public_rsa_key},\n        LOGIN_GOV_JWK_ENDPOINT=\"not_used\",\n        LOGIN_GOV_ENDPOINT=DEFAULT_ISSUER,\n        LOGIN_GOV_CLIENT_ID=DEFAULT_CLIENT_ID,\n        LOGIN_GOV_CLIENT_ASSERTION_PRIVATE_KEY=private_rsa_key,\n    )\n\n\ndef create_jwt(\n    user_id: str,\n    email: str,\n    expires_at: datetime,\n    issued_at: datetime,\n    not_before: datetime,\n    private_key: str | bytes,\n    issuer: str = DEFAULT_ISSUER,\n    audience: str = DEFAULT_CLIENT_ID,\n    acr: str = \"urn:acr.login.gov:auth-only\",\n    nonce: str = DEFAULT_NONCE,\n    kid: str = \"test-key-id\",\n):\n    payload = {\n        \"sub\": user_id,\n        \"iss\": issuer,\n        \"acr\": acr,\n        \"aud\": audience,\n        \"email\": email,\n        \"nonce\": nonce,\n        # The jwt encode function automatically turns these datetime\n        # objects into a UTC timestamp integer\n        \"exp\": expires_at,\n        \"iat\": issued_at,\n        \"nbf\": not_before,\n        # These values aren't checked by anything at the moment\n        # but are a part of the token from login.gov\n        \"jti\": \"abc123\",\n        \"at_hash\": \"abc123\",\n        \"c_hash\": \"abc123\",\n    }\n\n    return jwt.encode(payload, private_key, algorithm=\"RS256\", headers={\"kid\": kid})\n\n\ndef test_validate_token_happy_path(login_gov_config, private_rsa_key):\n    user_id = \"12345678-abc\"\n    email = \"fake@mail.com\"\n\n    token = create_jwt(\n        user_id=user_id,\n        email=email,\n        private_key=private_rsa_key,\n        expires_at=datetime.now(tz=timezone.utc) + timedelta(days=30),\n        issued_at=datetime.now(tz=timezone.utc) - timedelta(days=1),\n        not_before=datetime.now(tz=timezone.utc) - timedelta(days=1),\n    )\n\n    login_gov_user = validate_token(token, nonce=DEFAULT_NONCE, config=login_gov_config)\n\n    assert login_gov_user.user_id == user_id\n    assert login_gov_user.email == email\n\n\ndef test_validate_token_expired(login_gov_config, private_rsa_key):\n    token = create_jwt(\n        user_id=\"abc123\",\n        email=\"mail@fake.com\",\n        private_key=private_rsa_key,\n        expires_at=datetime.now(tz=timezone.utc) - timedelta(days=1),\n        issued_at=datetime.now(tz=timezone.utc) - timedelta(days=30),\n        not_before=datetime.now(tz=timezone.utc) - timedelta(days=30),\n    )\n\n    with pytest.raises(JwtValidationError, match=\"Expired Token\"):\n        validate_token(token, nonce=DEFAULT_NONCE, config=login_gov_config)\n\n\ndef test_validate_token_issued_at_future(login_gov_config, private_rsa_key):\n    token = create_jwt(\n        user_id=\"abc123\",\n        email=\"mail@fake.com\",\n        private_key=private_rsa_key,\n        expires_at=datetime.now(tz=timezone.utc) + timedelta(days=1),\n        issued_at=datetime.now(tz=timezone.utc) + timedelta(days=1),\n        not_before=datetime.now(tz=timezone.utc) - timedelta(days=30),\n    )\n\n    with pytest.raises(JwtValidationError, match=\"Token not yet valid\"):\n        validate_token(token, nonce=DEFAULT_NONCE, config=login_gov_config)\n\n\ndef test_validate_token_not_before_future(login_gov_config, private_rsa_key):\n    token = create_jwt(\n        user_id=\"abc123\",\n        email=\"mail@fake.com\",\n        private_key=private_rsa_key,\n        expires_at=datetime.now(tz=timezone.utc) + timedelta(days=30),\n        issued_at=datetime.now(tz=timezone.utc) - timedelta(days=1),\n        not_before=datetime.now(tz=timezone.utc) + timedelta(days=1),\n    )\n\n    with pytest.raises(JwtValidationError, match=\"Token not yet valid\"):\n        validate_token(token, nonce=DEFAULT_NONCE, config=login_gov_config)\n\n\ndef test_validate_token_unknown_issuer(login_gov_config, private_rsa_key):\n    token = create_jwt(\n        user_id=\"abc123\",\n        email=\"mail@fake.com\",\n        issuer=\"fred\",\n        private_key=private_rsa_key,\n        expires_at=datetime.now(tz=timezone.utc) + timedelta(days=30),\n        issued_at=datetime.now(tz=timezone.utc) - timedelta(days=1),\n        not_before=datetime.now(tz=timezone.utc) - timedelta(days=1),\n    )\n\n    with pytest.raises(JwtValidationError, match=\"Unknown Issuer\"):\n        validate_token(token, nonce=DEFAULT_NONCE, config=login_gov_config)\n\n\ndef test_validate_token_unknown_audience(login_gov_config, private_rsa_key):\n    token = create_jwt(\n        user_id=\"abc123\",\n        email=\"mail@fake.com\",\n        audience=\"fred\",\n        private_key=private_rsa_key,\n        expires_at=datetime.now(tz=timezone.utc) + timedelta(days=30),\n        issued_at=datetime.now(tz=timezone.utc) - timedelta(days=1),\n        not_before=datetime.now(tz=timezone.utc) - timedelta(days=1),\n    )\n\n    with pytest.raises(JwtValidationError, match=\"Unknown Audience\"):\n        validate_token(token, nonce=DEFAULT_NONCE, config=login_gov_config)\n\n\ndef test_validate_token_invalid_signature(login_gov_config, other_rsa_key_pair, monkeypatch):\n    token = create_jwt(\n        user_id=\"abc123\",\n        email=\"mail@fake.com\",\n        private_key=other_rsa_key_pair[0],  # Create it with a different key\n        expires_at=datetime.now(tz=timezone.utc) + timedelta(days=30),\n        issued_at=datetime.now(tz=timezone.utc) - timedelta(days=1),\n        not_before=datetime.now(tz=timezone.utc) - timedelta(days=1),\n    )\n\n    # Need to override the refresh logic so it doesn't try to reach out to anything\n    # We don't need to set the keys to anything else here.\n    def override_method(config):\n        pass\n\n    monkeypatch.setattr(login_gov_jwt_auth, \"_refresh_keys\", override_method)\n\n    with pytest.raises(\n        JwtValidationError,\n        match=\"Invalid Signature\",\n    ):\n        validate_token(token, nonce=DEFAULT_NONCE, config=login_gov_config)\n\n\ndef test_validate_token_key_found_on_refresh(login_gov_config, other_rsa_key_pair, monkeypatch):\n    user_id = \"12345678-abcxyz\"\n    email = \"xfake@mail.com\"\n\n    token = create_jwt(\n        user_id=user_id,\n        email=email,\n        private_key=other_rsa_key_pair[0],  # Create it with a different key\n        expires_at=datetime.now(tz=timezone.utc) + timedelta(days=30),\n        issued_at=datetime.now(tz=timezone.utc) - timedelta(days=1),\n        not_before=datetime.now(tz=timezone.utc) - timedelta(days=1),\n        kid=\"a-different-key\",\n    )\n\n    def override_method(config):\n        config.public_key_map = {\"a-different-key\": other_rsa_key_pair[1]}\n\n    monkeypatch.setattr(login_gov_jwt_auth, \"_refresh_keys\", override_method)\n\n    login_gov_user = validate_token(token, nonce=DEFAULT_NONCE, config=login_gov_config)\n\n    assert login_gov_user.user_id == user_id\n    assert login_gov_user.email == email\n\n\ndef test_validate_token_kid_not_found(login_gov_config, other_rsa_key_pair, monkeypatch):\n    user_id = \"12345678-abc\"\n    email = \"fake@mail.com\"\n\n    token = create_jwt(\n        user_id=user_id,\n        email=email,\n        private_key=other_rsa_key_pair[0],  # Create it with a different key\n        expires_at=datetime.now(tz=timezone.utc) + timedelta(days=30),\n        issued_at=datetime.now(tz=timezone.utc) - timedelta(days=1),\n        not_before=datetime.now(tz=timezone.utc) - timedelta(days=1),\n        kid=\"a-different-key\",\n    )\n\n    # Override it so nothing is found\n    def override_method(config):\n        config.public_key_map = {}\n\n    monkeypatch.setattr(login_gov_jwt_auth, \"_refresh_keys\", override_method)\n\n    with pytest.raises(\n        JwtValidationError,\n        match=\"No public key could be found for token\",\n    ):\n        validate_token(token, nonce=DEFAULT_NONCE, config=login_gov_config)\n\n\n@freezegun.freeze_time(\"2024-11-14 12:00:00\", tz_offset=0)\ndef test_get_login_gov_client_assertion(login_gov_config, public_rsa_key):\n    client_assertion = login_gov_jwt_auth.get_login_gov_client_assertion(login_gov_config)\n\n    # Turn the jwt back into a dict\n    # Validate with the public key\n    decoded_jwt = jwt.decode(\n        client_assertion,\n        key=public_rsa_key,\n        algorithms=[\"RS256\"],\n        issuer=login_gov_config.client_id,\n        audience=login_gov_config.login_gov_token_endpoint,\n    )\n\n    assert decoded_jwt[\"iss\"] == login_gov_config.client_id\n    assert decoded_jwt[\"sub\"] == login_gov_config.client_id\n    assert decoded_jwt[\"aud\"] == login_gov_config.login_gov_token_endpoint\n    assert decoded_jwt[\"jti\"] is not None\n    # exp is 5 minutes from \"now\"\n    assert decoded_jwt[\"exp\"] == timegm(\n        datetime.fromisoformat(\"2024-11-14 12:05:00+00:00\").utctimetuple()\n    )\n\n\ndef test_validate_token_invalid_nonce(login_gov_config, private_rsa_key):\n    token = create_jwt(\n        user_id=\"abc123\",\n        email=\"mail@fake.com\",\n        nonce=\"something_else\",\n        private_key=private_rsa_key,\n        expires_at=datetime.now(tz=timezone.utc) + timedelta(days=30),\n        issued_at=datetime.now(tz=timezone.utc) - timedelta(days=1),\n        not_before=datetime.now(tz=timezone.utc) - timedelta(days=1),\n    )\n\n    with pytest.raises(JwtValidationError, match=\"Nonce does not match expected\"):\n        validate_token(token, nonce=DEFAULT_NONCE, config=login_gov_config)"}
{"path":"frontend/tests/components/opportunity/OpportunityCTA.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/opportunity/OpportunityCTA.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/opportunity/OpportunityDescription.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/opportunity/OpportunityDescription.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":""}
{"path":"frontend/tests/components/opportunity/OpportunityDocuments.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/opportunity/OpportunityDocuments.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/load/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/load\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/load/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/opportunity/OpportunityDownload.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/opportunity/OpportunityDownload.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":""}
{"path":"frontend/tests/components/opportunity/OpportunityHistory.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/opportunity/OpportunityHistory.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/load/test_load_oracle_data_task.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/load\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/load/test_load_oracle_data_task.py\nSize: 6.87 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/opportunity/OpportunityIntro.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/opportunity/OpportunityIntro.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"import datetime\n\nimport freezegun\nimport pytest\nimport sqlalchemy\n\nimport src.db.models.foreign\nimport src.db.models.staging\nfrom src.data_migration.load import load_oracle_data_task\nfrom tests.conftest import BaseTestClass\nfrom tests.src.db.models.factories import ForeignTopportunityFactory, StagingTopportunityFactory\n\n\ndef validate_copied_value(\n    source_table,\n    source_record,\n    destination_record,\n    is_delete: bool = False,\n    is_unmodified: bool = False,\n):\n    if is_delete:\n        assert destination_record.is_deleted is True\n        assert destination_record.transformed_at is None\n        assert destination_record.deleted_at is not None\n        return\n\n    mismatches = []\n\n    for column_name in source_table.c.keys():\n        source_value = getattr(source_record, column_name)\n        destination_value = getattr(destination_record, column_name)\n\n        if source_value != destination_value:\n            mismatches.append(f\"{column_name}: {source_value} != {destination_value}\")\n\n    if is_unmodified:\n        assert (\n            len(mismatches) > 0\n        ), \"Expected no update on destination record, but had some: \" + str(mismatches)\n\n    else:\n        assert (\n            len(mismatches) == 0\n        ), \"Expected updates on destination record, but did not match \" + str(mismatches)\n\n\nclass TestLoadOracleData(BaseTestClass):\n\n    @pytest.fixture(scope=\"class\")\n    def foreign_tables(self):\n        return {t.name: t for t in src.db.models.foreign.metadata.tables.values()}\n\n    @pytest.fixture(scope=\"class\")\n    def staging_tables(self):\n        return {t.name: t for t in src.db.models.staging.metadata.tables.values()}\n\n    def test_load_data(self, db_session, foreign_tables, staging_tables, enable_factory_create):\n        time1 = datetime.datetime(2024, 1, 20, 7, 15, 0)\n        time2 = datetime.datetime(2024, 1, 20, 7, 15, 1)\n        time3 = datetime.datetime(2024, 4, 10, 22, 0, 1)\n\n        source_table = foreign_tables[\"topportunity\"]\n        destination_table = staging_tables[\"topportunity\"]\n\n        db_session.execute(sqlalchemy.delete(source_table))\n        db_session.execute(sqlalchemy.delete(destination_table))\n\n        ## Source records\n        # inserts:\n        source_record1 = ForeignTopportunityFactory.create(\n            opportunity_id=1, oppnumber=\"A-1\", cfdas=[], last_upd_date=time3\n        )\n        source_record2 = ForeignTopportunityFactory.create(\n            opportunity_id=2, oppnumber=\"A-2\", cfdas=[], last_upd_date=time3\n        )\n        # unchanged:\n        source_record3 = ForeignTopportunityFactory.create(\n            opportunity_id=3, oppnumber=\"A-3\", cfdas=[], last_upd_date=time3\n        )\n        # update:\n        source_record4 = ForeignTopportunityFactory.create(\n            opportunity_id=4, oppnumber=\"A-4-update\", cfdas=[], last_upd_date=time3\n        )\n        source_record5 = ForeignTopportunityFactory.create(\n            opportunity_id=6, oppnumber=\"A-6-update\", cfdas=[], last_upd_date=time3\n        )\n\n        ## Destination records\n        # unchanged:\n        StagingTopportunityFactory.create(\n            opportunity_id=3, oppnumber=\"A-3\", cfdas=[], last_upd_date=time3\n        )\n        # update:\n        StagingTopportunityFactory.create(\n            opportunity_id=4, oppnumber=\"A-4\", cfdas=[], last_upd_date=time1\n        )\n        StagingTopportunityFactory.create(\n            opportunity_id=6, oppnumber=\"A-6\", cfdas=[], last_upd_date=None\n        )\n        # delete:\n        StagingTopportunityFactory.create(\n            opportunity_id=5, oppnumber=\"A-5\", cfdas=[], last_upd_date=time2\n        )\n\n        task = load_oracle_data_task.LoadOracleDataTask(\n            db_session, foreign_tables, staging_tables, [\"topportunity\"]\n        )\n        task.run()\n\n        # Force the data to be fetched from the DB and not a cache\n        # this prevents some weirdness with the value comparison we'll do\n        db_session.expire_all()\n\n        assert db_session.query(source_table).count() == 5\n        assert db_session.query(destination_table).count() == 6\n\n        destination_records = (\n            db_session.query(destination_table).order_by(destination_table.c.opportunity_id).all()\n        )\n\n        validate_copied_value(source_table, source_record1, destination_records[0])\n        validate_copied_value(source_table, source_record2, destination_records[1])\n        validate_copied_value(\n            source_table, source_record3, destination_records[2], is_unmodified=True\n        )\n        validate_copied_value(source_table, source_record4, destination_records[3])\n        validate_copied_value(source_table, None, destination_records[4], is_delete=True)\n        validate_copied_value(source_table, source_record5, destination_records[5])\n\n        assert task.metrics[\"count.delete.topportunity\"] == 1\n        assert task.metrics[\"count.insert.topportunity\"] == 2\n        assert task.metrics[\"count.update.topportunity\"] == 2\n        assert task.metrics[\"count.delete.total\"] == 1\n        assert task.metrics[\"count.insert.total\"] == 2\n        assert task.metrics[\"count.update.total\"] == 2\n\n    def test_raises_if_table_dicts_different(self, db_session, foreign_tables, staging_tables):\n        with pytest.raises(\n            ValueError, match=\"keys of foreign_tables and staging_tables must be equal\"\n        ):\n            load_oracle_data_task.LoadOracleDataTask(\n                db_session, foreign_tables, {}, [\"topportunity\"]\n            )\n\n    @freezegun.freeze_time()\n    def test_load_data_chunked(\n        self, db_session, foreign_tables, staging_tables, enable_factory_create\n    ):\n        time1 = datetime.datetime(2024, 1, 20, 7, 15, 0)\n\n        source_table = foreign_tables[\"topportunity\"]\n        destination_table = staging_tables[\"topportunity\"]\n\n        db_session.execute(sqlalchemy.delete(source_table))\n        db_session.execute(sqlalchemy.delete(destination_table))\n\n        source_records = ForeignTopportunityFactory.create_batch(\n            size=100, last_upd_date=time1, cfdas=[]\n        )\n\n        task = load_oracle_data_task.LoadOracleDataTask(\n            db_session, foreign_tables, staging_tables, [\"topportunity\"], insert_chunk_size=30\n        )\n        task.run()\n\n        assert db_session.query(source_table).count() == 100\n        assert db_session.query(destination_table).count() == 100\n\n        assert set(\n            db_session.scalars(sqlalchemy.select(destination_table.c.opportunity_id))\n        ) == set([record.opportunity_id for record in source_records])\n\n        assert task.metrics[\"count.delete.topportunity\"] == 0\n        assert task.metrics[\"count.insert.topportunity\"] == 100\n        assert task.metrics[\"count.insert.chunk.topportunity\"] == \"30,30,30,10\"\n        assert task.metrics[\"count.update.topportunity\"] == 0\n        assert task.metrics[\"count.delete.total\"] == 0\n        assert task.metrics[\"count.insert.total\"] == 100\n        assert task.metrics[\"count.update.total\"] == 0"}
{"path":"frontend/tests/components/opportunity/OpportunityLink.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/opportunity/OpportunityLink.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/load/test_sql.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/load\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/load/test_sql.py\nSize: 4.17 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/opportunity/OpportunityStatusWidget.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/opportunity","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/opportunity/OpportunityStatusWidget.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"import pytest\nimport sqlalchemy\n\nfrom src.data_migration.load import sql\n\n\n@pytest.fixture(scope=\"module\")\ndef sqlalchemy_metadata():\n    return sqlalchemy.MetaData()\n\n\n@pytest.fixture(scope=\"module\")\ndef source_table(sqlalchemy_metadata):\n    return sqlalchemy.Table(\n        \"test_source_table\",\n        sqlalchemy_metadata,\n        sqlalchemy.Column(\"id1\", sqlalchemy.Integer, primary_key=True),\n        sqlalchemy.Column(\"id2\", sqlalchemy.Integer, primary_key=True),\n        sqlalchemy.Column(\"x\", sqlalchemy.Text),\n        sqlalchemy.Column(\"last_upd_date\", sqlalchemy.TIMESTAMP),\n        sqlalchemy.Column(\"created_date\", sqlalchemy.TIMESTAMP),\n    )\n\n\n@pytest.fixture(scope=\"module\")\ndef destination_table(sqlalchemy_metadata):\n    return sqlalchemy.Table(\n        \"test_destination_table\",\n        sqlalchemy_metadata,\n        sqlalchemy.Column(\"id1\", sqlalchemy.Integer, primary_key=True),\n        sqlalchemy.Column(\"id2\", sqlalchemy.Integer, primary_key=True),\n        sqlalchemy.Column(\"x\", sqlalchemy.Text),\n        sqlalchemy.Column(\"is_deleted\", sqlalchemy.Boolean),\n        sqlalchemy.Column(\"last_upd_date\", sqlalchemy.TIMESTAMP),\n        sqlalchemy.Column(\"created_date\", sqlalchemy.TIMESTAMP),\n    )\n\n\ndef test_build_select_new_rows_sql(source_table, destination_table):\n    select = sql.build_select_new_rows_sql(source_table, destination_table)\n    assert str(select) == (\n        \"SELECT test_source_table.id1, test_source_table.id2 \\n\"\n        \"FROM test_source_table \\n\"\n        \"WHERE ((test_source_table.id1, test_source_table.id2) \"\n        \"NOT IN (\"\n        \"SELECT test_destination_table.id1, test_destination_table.id2 \\n\"\n        \"FROM test_destination_table)) \"\n        \"ORDER BY test_source_table.id1, test_source_table.id2\"\n    )\n\n\ndef test_build_select_updated_rows_sql(source_table, destination_table):\n    select = sql.build_select_updated_rows_sql(source_table, destination_table)\n    assert str(select) == (\n        \"SELECT test_destination_table.id1, test_destination_table.id2 \\n\"\n        \"FROM test_destination_table \"\n        \"JOIN test_source_table ON \"\n        \"(test_destination_table.id1, test_destination_table.id2) = \"\n        \"(test_source_table.id1, test_source_table.id2) \\n\"\n        \"WHERE coalesce(test_destination_table.last_upd_date, test_destination_table.created_date) < test_source_table.last_upd_date \"\n        \"ORDER BY test_source_table.id1, test_source_table.id2\"\n    )\n\n\ndef test_build_insert_select_sql(source_table, destination_table):\n    insert = sql.build_insert_select_sql(source_table, destination_table, [(1, 2), (3, 4), (5, 6)])\n    assert str(insert) == (\n        \"INSERT INTO test_destination_table (id1, id2, x, last_upd_date, created_date, is_deleted) \"\n        \"SELECT test_source_table.id1, test_source_table.id2, test_source_table.x, \"\n        \"test_source_table.last_upd_date, test_source_table.created_date, FALSE AS is_deleted \\n\"\n        \"FROM test_source_table \\n\"\n        \"WHERE (test_source_table.id1, test_source_table.id2) IN (__[POSTCOMPILE_param_1])\"\n    )\n\n\ndef test_build_update_sql(source_table, destination_table):\n    update = sql.build_update_sql(source_table, destination_table, [(1, 2), (3, 4), (5, 6)])\n    assert str(update) == (\n        \"UPDATE test_destination_table \"\n        \"SET id1=test_source_table.id1, id2=test_source_table.id2, x=test_source_table.x, \"\n        \"last_upd_date=test_source_table.last_upd_date, created_date=test_source_table.created_date \"\n        \"FROM test_source_table WHERE (test_destination_table.id1, test_destination_table.id2) = \"\n        \"(test_source_table.id1, test_source_table.id2) AND \"\n        \"(test_source_table.id1, test_source_table.id2) \"\n        \"IN (__[POSTCOMPILE_param_1])\"\n    )\n\n\ndef test_build_mark_deleted_sql(source_table, destination_table):\n    update = sql.build_mark_deleted_sql(source_table, destination_table)\n    assert str(update) == (\n        \"UPDATE test_destination_table \"\n        \"SET is_deleted=:is_deleted \"\n        \"WHERE test_destination_table.is_deleted = false \"\n        \"AND ((test_destination_table.id1, test_destination_table.id2) \"\n        \"NOT IN (SELECT test_source_table.id1, test_source_table.id2 \\n\"\n        \"FROM test_source_table))\"\n    )"}
{"path":"frontend/tests/components/search/ExportSearchResultsButton.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/ExportSearchResultsButton.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/test_setup_foreign_tables.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/test_setup_foreign_tables.py\nSize: 2.97 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/search/SearchAnalytics.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchAnalytics.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"import pytest\nimport sqlalchemy\n\nimport src.db.models.foreign\nfrom src.data_migration.setup_foreign_tables import build_sql\n\nEXPECTED_LOCAL_OPPORTUNITY_SQL = [\n    \"CREATE TABLE IF NOT EXISTS __[SCHEMA_legacy].topportunity (\",\n    \"opportunity_id BIGSERIAL NOT NULL,\",\n    \"oppnumber TEXT,\",\n    \"revision_number BIGINT,\",\n    \"opptitle TEXT,\",\n    \"owningagency TEXT,\",\n    \"publisheruid TEXT,\",\n    \"listed TEXT,\",\n    \"oppcategory TEXT,\",\n    \"initial_opportunity_id BIGINT,\",\n    \"modified_comments TEXT,\",\n    \"created_date TIMESTAMP WITH TIME ZONE,\",\n    \"last_upd_date TIMESTAMP WITH TIME ZONE,\",\n    \"creator_id TEXT,\",\n    \"last_upd_id TEXT,\",\n    \"flag_2006 TEXT,\",\n    \"category_explanation TEXT,\",\n    \"publisher_profile_id BIGINT,\",\n    \"is_draft TEXT,\",\n    \"PRIMARY KEY (opportunity_id)\",\n    \")\",\n]\n\nEXPECTED_NONLOCAL_OPPORTUNITY_SQL = [\n    \"CREATE FOREIGN TABLE IF NOT EXISTS __[SCHEMA_legacy].topportunity (\",\n    \"opportunity_id BIGINT OPTIONS (key 'true') NOT NULL,\",\n    \"oppnumber TEXT,\",\n    \"revision_number BIGINT,\",\n    \"opptitle TEXT,\",\n    \"owningagency TEXT,\",\n    \"publisheruid TEXT,\",\n    \"listed TEXT,\",\n    \"oppcategory TEXT,\",\n    \"initial_opportunity_id BIGINT,\",\n    \"modified_comments TEXT,\",\n    \"created_date TIMESTAMP WITH TIME ZONE,\",\n    \"last_upd_date TIMESTAMP WITH TIME ZONE,\",\n    \"creator_id TEXT,\",\n    \"last_upd_id TEXT,\",\n    \"flag_2006 TEXT,\",\n    \"category_explanation TEXT,\",\n    \"publisher_profile_id BIGINT,\",\n    \"is_draft TEXT\",\n    \") SERVER grants OPTIONS (schema 'EGRANTSADMIN', table 'TOPPORTUNITY', readonly 'true', prefetch '1000')\",\n]\n\n\nTEST_METADATA = sqlalchemy.MetaData()\nTEST_TABLE = sqlalchemy.Table(\n    \"test_table\",\n    TEST_METADATA,\n    sqlalchemy.Column(\"id\", sqlalchemy.Integer, nullable=False, primary_key=True),\n    sqlalchemy.Column(\"description\", sqlalchemy.Text),\n    schema=\"schema1\",\n)\nEXPECTED_LOCAL_TEST_SQL = [\n    \"CREATE TABLE IF NOT EXISTS __[SCHEMA_schema1].test_table (\",\n    \"id SERIAL NOT NULL,\",\n    \"description TEXT,\",\n    \"PRIMARY KEY (id)\",\n    \")\",\n]\nEXPECTED_NONLOCAL_TEST_SQL = [\n    \"CREATE FOREIGN TABLE IF NOT EXISTS __[SCHEMA_schema1].test_table (\",\n    \"id INTEGER OPTIONS (key 'true') NOT NULL,\",\n    \"description TEXT\",\n    \") SERVER grants OPTIONS (schema 'EGRANTSADMIN', table 'TEST_TABLE', readonly 'true', prefetch '1000')\",\n]\n\n\n@pytest.mark.parametrize(\n    \"table,is_local,expected_sql\",\n    [\n        (TEST_TABLE, True, EXPECTED_LOCAL_TEST_SQL),\n        (TEST_TABLE, False, EXPECTED_NONLOCAL_TEST_SQL),\n        (\n            src.db.models.foreign.metadata.tables[\"legacy.topportunity\"],\n            True,\n            EXPECTED_LOCAL_OPPORTUNITY_SQL,\n        ),\n        (\n            src.db.models.foreign.metadata.tables[\"legacy.topportunity\"],\n            False,\n            EXPECTED_NONLOCAL_OPPORTUNITY_SQL,\n        ),\n    ],\n)\ndef test_build_sql(table, is_local, expected_sql, test_foreign_schema):\n    sql = build_sql(table, is_local, test_foreign_schema)\n\n    assert re.split(r\"\\s*\\n\\s*\", sql) == expected_sql"}
{"path":"frontend/tests/components/search/SearchBar.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchBar.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/transformation/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/search/SearchFilterAccordion/SearchFilterAccordion.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search/SearchFilterAccordion","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchFilterAccordion/SearchFilterAccordion.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":""}
{"path":"frontend/tests/components/search/SearchFilterAccordion/SearchFilterCheckbox.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search/SearchFilterAccordion","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchFilterAccordion/SearchFilterCheckbox.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/transformation/conftest.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/conftest.py\nSize: 28.34 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/search/SearchFilterAccordion/SearchFilterSection/SearchFilterSection.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search/SearchFilterAccordion/SearchFilterSection","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchFilterAccordion/SearchFilterSection/SearchFilterSection.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"import pytest\n\nimport tests.src.db.models.factories as f\nfrom src.adapters.aws import S3Config\nfrom src.constants.lookup_constants import ApplicantType, FundingCategory, FundingInstrument\nfrom src.data_migration.transformation.transform_oracle_data_task import TransformOracleDataTask\nfrom src.db.models import staging\nfrom src.db.models.agency_models import Agency\nfrom src.db.models.opportunity_models import (\n    LinkOpportunitySummaryApplicantType,\n    LinkOpportunitySummaryFundingCategory,\n    LinkOpportunitySummaryFundingInstrument,\n    Opportunity,\n    OpportunityAssistanceListing,\n    OpportunityAttachment,\n    OpportunitySummary,\n)\nfrom src.services.opportunity_attachments import attachment_util\nfrom src.util import file_util\nfrom tests.conftest import BaseTestClass\n\n\nclass BaseTransformTestClass(BaseTestClass):\n    @pytest.fixture()\n    def transform_oracle_data_task(\n        self, db_session, enable_factory_create, truncate_opportunities\n    ) -> TransformOracleDataTask:\n        return TransformOracleDataTask(db_session)\n\n\ndef setup_opportunity(\n    create_existing: bool,\n    is_delete: bool = False,\n    is_already_processed: bool = False,\n    source_values: dict | None = None,\n    all_fields_null: bool = False,\n) -> staging.opportunity.Topportunity:\n    if source_values is None:\n        source_values = {}\n\n    source_opportunity = f.StagingTopportunityFactory.create(\n        **source_values,\n        is_deleted=is_delete,\n        already_transformed=is_already_processed,\n        all_fields_null=all_fields_null,\n        cfdas=[],\n    )\n\n    if create_existing:\n        f.OpportunityFactory.create(\n            opportunity_id=source_opportunity.opportunity_id,\n            opportunity_attachments=[],\n            # set created_at/updated_at to an earlier time so its clear\n            # when they were last updated\n            timestamps_in_past=True,\n        )\n\n    return source_opportunity\n\n\ndef setup_cfda(\n    create_existing: bool,\n    is_delete: bool = False,\n    is_already_processed: bool = False,\n    source_values: dict | None = None,\n    all_fields_null: bool = False,\n    opportunity: Opportunity | None = None,\n) -> staging.opportunity.TopportunityCfda:\n    if source_values is None:\n        source_values = {}\n\n    # If you don't provide an opportunity, you need to provide an ID\n    if opportunity is not None:\n        source_values[\"opportunity_id\"] = opportunity.opportunity_id\n\n    source_cfda = f.StagingTopportunityCfdaFactory.create(\n        **source_values,\n        opportunity=None,  # To override the factory trying to create something\n        is_deleted=is_delete,\n        already_transformed=is_already_processed,\n        all_fields_null=all_fields_null,\n    )\n\n    if create_existing:\n        f.OpportunityAssistanceListingFactory.create(\n            opportunity=opportunity,\n            opportunity_assistance_listing_id=source_cfda.opp_cfda_id,\n            # set created_at/updated_at to an earlier time so its clear\n            # when they were last updated\n            timestamps_in_past=True,\n        )\n\n    return source_cfda\n\n\ndef setup_synopsis_forecast(\n    is_forecast: bool,\n    revision_number: int | None,\n    create_existing: bool,\n    opportunity: Opportunity | None,\n    is_delete: bool = False,\n    is_already_processed: bool = False,\n    is_existing_current_opportunity_summary: bool = False,\n    source_values: dict | None = None,\n):\n    if source_values is None:\n        source_values = {}\n\n    if is_forecast:\n        if revision_number is None:\n            factory_cls = f.StagingTforecastFactory\n        else:\n            factory_cls = f.StagingTforecastHistFactory\n    else:\n        if revision_number is None:\n            factory_cls = f.StagingTsynopsisFactory\n        else:\n            factory_cls = f.StagingTsynopsisHistFactory\n\n    if revision_number is not None:\n        source_values[\"revision_number\"] = revision_number\n\n    if opportunity is not None:\n        source_values[\"opportunity_id\"] = opportunity.opportunity_id\n\n    source_summary = factory_cls.create(\n        **source_values,\n        opportunity=None,  # To override the factory trying to create something\n        is_deleted=is_delete,\n        already_transformed=is_already_processed,\n    )\n\n    if create_existing:\n        opportunity_summary = f.OpportunitySummaryFactory.create(\n            opportunity=opportunity, is_forecast=is_forecast, revision_number=revision_number\n        )\n        if is_existing_current_opportunity_summary:\n            f.CurrentOpportunitySummaryFactory.create(\n                opportunity=opportunity, opportunity_summary=opportunity_summary\n            )\n\n    return source_summary\n\n\ndef setup_applicant_type(\n    create_existing: bool,\n    opportunity_summary: OpportunitySummary,\n    legacy_lookup_value: str,\n    applicant_type: ApplicantType | None = None,\n    is_delete: bool = False,\n    is_already_processed: bool = False,\n    source_values: dict | None = None,\n):\n    if create_existing and is_delete is False and applicant_type is None:\n        raise Exception(\n            \"If create_existing is True, is_delete is False - must provide the properly converted / mapped value for applicant_type\"\n        )\n\n    if source_values is None:\n        source_values = {}\n\n    if opportunity_summary.is_forecast:\n        source_values[\"forecast\"] = None\n        if opportunity_summary.revision_number is None:\n            factory_cls = f.StagingTapplicanttypesForecastFactory\n        else:\n            factory_cls = f.StagingTapplicanttypesForecastHistFactory\n            source_values[\"revision_number\"] = opportunity_summary.revision_number\n    else:\n        source_values[\"synopsis\"] = None\n        if opportunity_summary.revision_number is None:\n            factory_cls = f.StagingTapplicanttypesSynopsisFactory\n        else:\n            factory_cls = f.StagingTapplicanttypesSynopsisHistFactory\n            source_values[\"revision_number\"] = opportunity_summary.revision_number\n\n    source_applicant_type = factory_cls.create(\n        **source_values,\n        opportunity_id=opportunity_summary.opportunity_id,\n        is_deleted=is_delete,\n        already_transformed=is_already_processed,\n        at_id=legacy_lookup_value,\n    )\n\n    if create_existing:\n        if opportunity_summary.is_forecast:\n            legacy_id = source_applicant_type.at_frcst_id\n        else:\n            legacy_id = source_applicant_type.at_syn_id\n\n        f.LinkOpportunitySummaryApplicantTypeFactory.create(\n            opportunity_summary=opportunity_summary,\n            legacy_applicant_type_id=legacy_id,\n            applicant_type=applicant_type,\n        )\n\n    return source_applicant_type\n\n\ndef setup_funding_instrument(\n    create_existing: bool,\n    opportunity_summary: OpportunitySummary,\n    legacy_lookup_value: str,\n    funding_instrument: FundingInstrument | None = None,\n    is_delete: bool = False,\n    is_already_processed: bool = False,\n    source_values: dict | None = None,\n):\n    if create_existing and is_delete is False and funding_instrument is None:\n        raise Exception(\n            \"If create_existing is True, is_delete is False - must provide the properly converted / mapped value for funding_instrument\"\n        )\n\n    if source_values is None:\n        source_values = {}\n\n    if opportunity_summary.is_forecast:\n        source_values[\"forecast\"] = None\n        if opportunity_summary.revision_number is None:\n            factory_cls = f.StagingTfundinstrForecastFactory\n        else:\n            factory_cls = f.StagingTfundinstrForecastHistFactory\n            source_values[\"revision_number\"] = opportunity_summary.revision_number\n    else:\n        source_values[\"synopsis\"] = None\n        if opportunity_summary.revision_number is None:\n            factory_cls = f.StagingTfundinstrSynopsisFactory\n        else:\n            factory_cls = f.StagingTfundinstrSynopsisHistFactory\n            source_values[\"revision_number\"] = opportunity_summary.revision_number\n\n    source_funding_instrument = factory_cls.create(\n        **source_values,\n        opportunity_id=opportunity_summary.opportunity_id,\n        is_deleted=is_delete,\n        already_transformed=is_already_processed,\n        fi_id=legacy_lookup_value,\n    )\n\n    if create_existing:\n        if opportunity_summary.is_forecast:\n            legacy_id = source_funding_instrument.fi_frcst_id\n        else:\n            legacy_id = source_funding_instrument.fi_syn_id\n\n        f.LinkOpportunitySummaryFundingInstrumentFactory.create(\n            opportunity_summary=opportunity_summary,\n            legacy_funding_instrument_id=legacy_id,\n            funding_instrument=funding_instrument,\n        )\n\n    return source_funding_instrument\n\n\ndef setup_funding_category(\n    create_existing: bool,\n    opportunity_summary: OpportunitySummary,\n    legacy_lookup_value: str,\n    funding_category: FundingCategory | None = None,\n    is_delete: bool = False,\n    is_already_processed: bool = False,\n    source_values: dict | None = None,\n):\n    if create_existing and is_delete is False and funding_category is None:\n        raise Exception(\n            \"If create_existing is True, is_delete is False - must provide the properly converted / mapped value for funding_category\"\n        )\n\n    if source_values is None:\n        source_values = {}\n\n    if opportunity_summary.is_forecast:\n        source_values[\"forecast\"] = None\n        if opportunity_summary.revision_number is None:\n            factory_cls = f.StagingTfundactcatForecastFactory\n        else:\n            factory_cls = f.StagingTfundactcatForecastHistFactory\n            source_values[\"revision_number\"] = opportunity_summary.revision_number\n    else:\n        source_values[\"synopsis\"] = None\n        if opportunity_summary.revision_number is None:\n            factory_cls = f.StagingTfundactcatSynopsisFactory\n        else:\n            factory_cls = f.StagingTfundactcatSynopsisHistFactory\n            source_values[\"revision_number\"] = opportunity_summary.revision_number\n\n    source_funding_category = factory_cls.create(\n        **source_values,\n        opportunity_id=opportunity_summary.opportunity_id,\n        is_deleted=is_delete,\n        already_transformed=is_already_processed,\n        fac_id=legacy_lookup_value,\n    )\n\n    if create_existing:\n        if opportunity_summary.is_forecast:\n            legacy_id = source_funding_category.fac_frcst_id\n        else:\n            legacy_id = source_funding_category.fac_syn_id\n\n        f.LinkOpportunitySummaryFundingCategoryFactory.create(\n            opportunity_summary=opportunity_summary,\n            legacy_funding_category_id=legacy_id,\n            funding_category=funding_category,\n        )\n\n    return source_funding_category\n\n\ndef setup_agency(\n    agency_code: str,\n    create_existing: bool,\n    is_already_processed: bool = False,\n    deleted_fields: set | None = None,\n    already_processed_fields: set | None = None,\n    source_values: dict | None = None,\n):\n    if source_values is None:\n        source_values = {}\n\n    tgroups = f.create_tgroups_agency(\n        agency_code,\n        is_already_processed=is_already_processed,\n        deleted_fields=deleted_fields,\n        already_processed_fields=already_processed_fields,\n        **source_values,\n    )\n\n    if create_existing:\n        f.AgencyFactory.create(agency_code=agency_code)\n\n    return tgroups\n\n\ndef setup_opportunity_attachment(\n    create_existing: bool,\n    opportunity: Opportunity,\n    config: S3Config,\n    is_delete: bool = False,\n    is_already_processed: bool = False,\n    source_values: dict | None = None,\n):\n    if source_values is None:\n        source_values = {}\n\n    synopsis_attachment = f.StagingTsynopsisAttachmentFactory.create(\n        opportunity=None,\n        opportunity_id=opportunity.opportunity_id,\n        is_deleted=is_delete,\n        already_transformed=is_already_processed,\n        **source_values,\n    )\n\n    if create_existing:\n        s3_path = attachment_util.get_s3_attachment_path(\n            synopsis_attachment.file_name, synopsis_attachment.syn_att_id, opportunity, config\n        )\n\n        with file_util.open_stream(s3_path, \"w\") as outfile:\n            outfile.write(f.fake.sentence(25))\n\n        f.OpportunityAttachmentFactory.create(\n            attachment_id=synopsis_attachment.syn_att_id,\n            opportunity=opportunity,\n            file_location=s3_path,\n        )\n\n    return synopsis_attachment\n\n\ndef validate_matching_fields(\n    source, destination, fields: list[Tuple[str, str]], expect_all_to_match: bool\n):\n    mismatched_fields = []\n\n    for source_field, destination_field in fields:\n        if isinstance(source, dict):\n            source_value = source.get(source_field)\n        else:\n            source_value = getattr(source, source_field)\n\n        destination_value = getattr(destination, destination_field)\n\n        # Some fields that we copy in are datetime typed (although behave as dates and we convert as such)\n        # If so, we need to make sure they're both dates for the purposes of comparison\n        if isinstance(source_value, datetime) and isinstance(destination_value, date):\n            source_value = source_value.date()\n\n        if source_value != destination_value:\n            mismatched_fields.append(\n                f\"{source_field}/{destination_field}: '{source_value}' != '{destination_value}'\"\n            )\n\n    # If a values weren't copied in an update\n    # then we should expect most things to not match,\n    # but randomness in the factories might cause some overlap\n    if expect_all_to_match:\n        assert (\n            len(mismatched_fields) == 0\n        ), f\"Expected all fields to match between {source.__class__} and {destination.__class__}, but found mismatched fields: {','.join(mismatched_fields)}\"\n    else:\n        assert (\n            len(mismatched_fields) != 0\n        ), f\"Did not expect all fields to match between {source.__class__} and {destination.__class__}, but they did which means an unexpected update occurred\"\n\n\ndef validate_opportunity(\n    db_session,\n    source_opportunity: staging.opportunity.Topportunity,\n    expect_in_db: bool = True,\n    expect_values_to_match: bool = True,\n):\n    opportunity = (\n        db_session.query(Opportunity)\n        .filter(Opportunity.opportunity_id == source_opportunity.opportunity_id)\n        .one_or_none()\n    )\n\n    if not expect_in_db:\n        assert opportunity is None\n        return\n\n    assert opportunity is not None\n    # For fields that we expect to match 1:1, verify that they match as expected\n    validate_matching_fields(\n        source_opportunity,\n        opportunity,\n        [\n            (\"oppnumber\", \"opportunity_number\"),\n            (\"opptitle\", \"opportunity_title\"),\n            (\"owningagency\", \"agency\"),\n            (\"category_explanation\", \"category_explanation\"),\n            (\"revision_number\", \"revision_number\"),\n            (\"modified_comments\", \"modified_comments\"),\n            (\"publisheruid\", \"publisher_user_id\"),\n            (\"publisher_profile_id\", \"publisher_profile_id\"),\n        ],\n        expect_values_to_match,\n    )\n\n    # Validation of fields that aren't copied exactly\n    if expect_values_to_match:\n        # Deliberately validating is_draft with a different calculation\n        if source_opportunity.is_draft == \"N\":\n            assert opportunity.is_draft is False\n        else:\n            assert opportunity.is_draft is True\n\n\ndef validate_assistance_listing(\n    db_session,\n    source_cfda: staging.opportunity.TopportunityCfda,\n    expect_in_db: bool = True,\n    expect_values_to_match: bool = True,\n):\n    assistance_listing = (\n        db_session.query(OpportunityAssistanceListing)\n        .filter(\n            OpportunityAssistanceListing.opportunity_assistance_listing_id\n            == source_cfda.opp_cfda_id\n        )\n        .one_or_none()\n    )\n\n    if not expect_in_db:\n        assert assistance_listing is None\n        return\n\n    assert assistance_listing is not None\n    # For fields that we expect to match 1:1, verify that they match as expected\n    validate_matching_fields(\n        source_cfda,\n        assistance_listing,\n        [\n            (\"cfdanumber\", \"assistance_listing_number\"),\n            (\"programtitle\", \"program_title\"),\n        ],\n        expect_values_to_match,\n    )\n\n\ndef get_summary_from_source(db_session, source_summary):\n    revision_number = None\n    is_forecast = source_summary.is_forecast\n    if isinstance(source_summary, (staging.synopsis.TsynopsisHist, staging.forecast.TforecastHist)):\n        revision_number = source_summary.revision_number\n\n    opportunity_summary = (\n        db_session.query(OpportunitySummary)\n        .filter(\n            OpportunitySummary.opportunity_id == source_summary.opportunity_id,\n            OpportunitySummary.revision_number == revision_number,\n            OpportunitySummary.is_forecast == is_forecast,\n            # Populate existing to force it to fetch updates from the DB\n        )\n        .execution_options(populate_existing=True)\n        .one_or_none()\n    )\n\n    return opportunity_summary\n\n\ndef validate_opportunity_summary(\n    db_session, source_summary, expect_in_db: bool = True, expect_values_to_match: bool = True\n):\n    opportunity_summary = get_summary_from_source(db_session, source_summary)\n\n    if not expect_in_db:\n        assert opportunity_summary is None\n        return\n\n    matching_fields = [\n        (\"version_nbr\", \"version_number\"),\n        (\"posting_date\", \"post_date\"),\n        (\"archive_date\", \"archive_date\"),\n        (\"fd_link_url\", \"additional_info_url\"),\n        (\"fd_link_desc\", \"additional_info_url_description\"),\n        (\"modification_comments\", \"modification_comments\"),\n        (\"oth_cat_fa_desc\", \"funding_category_description\"),\n        (\"applicant_elig_desc\", \"applicant_eligibility_description\"),\n        (\"ac_name\", \"agency_name\"),\n        (\"ac_email_addr\", \"agency_email_address\"),\n        (\"ac_email_desc\", \"agency_email_address_description\"),\n        (\"publisher_profile_id\", \"publisher_profile_id\"),\n        (\"publisheruid\", \"publisher_user_id\"),\n        (\"last_upd_id\", \"updated_by\"),\n        (\"creator_id\", \"created_by\"),\n    ]\n\n    if isinstance(source_summary, (staging.synopsis.Tsynopsis, staging.synopsis.TsynopsisHist)):\n        matching_fields.extend(\n            [\n                (\"syn_desc\", \"summary_description\"),\n                (\"a_sa_code\", \"agency_code\"),\n                (\"ac_phone_number\", \"agency_phone_number\"),\n                (\"agency_contact_desc\", \"agency_contact_description\"),\n                (\"response_date\", \"close_date\"),\n                (\"response_date_desc\", \"close_date_description\"),\n                (\"unarchive_date\", \"unarchive_date\"),\n            ]\n        )\n    else:  # Forecast+ForecastHist\n        matching_fields.extend(\n            [\n                (\"forecast_desc\", \"summary_description\"),\n                (\"agency_code\", \"agency_code\"),\n                (\"ac_phone\", \"agency_phone_number\"),\n                (\"est_synopsis_posting_date\", \"forecasted_post_date\"),\n                (\"est_appl_response_date\", \"forecasted_close_date\"),\n                (\"est_appl_response_date_desc\", \"forecasted_close_date_description\"),\n                (\"est_award_date\", \"forecasted_award_date\"),\n                (\"est_project_start_date\", \"forecasted_project_start_date\"),\n                (\"fiscal_year\", \"fiscal_year\"),\n            ]\n        )\n\n\ndef validate_summary_and_nested(\n    db_session,\n    source_summary,\n    expected_applicant_types: list[ApplicantType],\n    expected_funding_categories: list[FundingCategory],\n    expected_funding_instruments: list[FundingInstrument],\n    expect_in_db: bool = True,\n    expect_values_to_match: bool = True,\n):\n    validate_opportunity_summary(db_session, source_summary, expect_in_db, expect_values_to_match)\n\n    if not expect_in_db:\n        return\n\n    created_record = get_summary_from_source(db_session, source_summary)\n\n    assert set(created_record.applicant_types) == set(expected_applicant_types)\n    assert set(created_record.funding_categories) == set(expected_funding_categories)\n    assert set(created_record.funding_instruments) == set(expected_funding_instruments)\n\n\ndef validate_applicant_type(\n    db_session,\n    source_applicant_type,\n    expect_in_db: bool = True,\n    expected_applicant_type: ApplicantType | None = None,\n    was_processed: bool = True,\n    expect_values_to_match: bool = True,\n):\n    assert (source_applicant_type.transformed_at is not None) == was_processed\n\n    # In order to properly find the link table value, need to first determine\n    # the opportunity summary in a subquery\n    opportunity_summary_id = (\n        db_session.query(OpportunitySummary.opportunity_summary_id)\n        .filter(\n            OpportunitySummary.revision_number == source_applicant_type.revision_number,\n            OpportunitySummary.is_forecast == source_applicant_type.is_forecast,\n            OpportunitySummary.opportunity_id == source_applicant_type.opportunity_id,\n        )\n        .scalar()\n    )\n\n    link_applicant_type = (\n        db_session.query(LinkOpportunitySummaryApplicantType)\n        .filter(\n            LinkOpportunitySummaryApplicantType.legacy_applicant_type_id\n            == source_applicant_type.legacy_applicant_type_id,\n            LinkOpportunitySummaryApplicantType.opportunity_summary_id == opportunity_summary_id,\n        )\n        .one_or_none()\n    )\n\n    if not expect_in_db:\n        assert link_applicant_type is None\n        return\n\n    assert link_applicant_type is not None\n    assert link_applicant_type.applicant_type == expected_applicant_type\n\n    validate_matching_fields(\n        source_applicant_type,\n        link_applicant_type,\n        [(\"creator_id\", \"created_by\"), (\"last_upd_id\", \"updated_by\")],\n        expect_values_to_match,\n    )\n\n\ndef validate_funding_instrument(\n    db_session,\n    source_funding_instrument,\n    expect_in_db: bool = True,\n    expected_funding_instrument: FundingInstrument | None = None,\n    was_processed: bool = True,\n    expect_values_to_match: bool = True,\n):\n    assert (source_funding_instrument.transformed_at is not None) == was_processed\n\n    # In order to properly find the link table value, need to first determine\n    # the opportunity summary in a subquery\n    opportunity_summary_id = (\n        db_session.query(OpportunitySummary.opportunity_summary_id)\n        .filter(\n            OpportunitySummary.revision_number == source_funding_instrument.revision_number,\n            OpportunitySummary.is_forecast == source_funding_instrument.is_forecast,\n            OpportunitySummary.opportunity_id == source_funding_instrument.opportunity_id,\n        )\n        .scalar()\n    )\n\n    link_funding_instrument = (\n        db_session.query(LinkOpportunitySummaryFundingInstrument)\n        .filter(\n            LinkOpportunitySummaryFundingInstrument.legacy_funding_instrument_id\n            == source_funding_instrument.legacy_funding_instrument_id,\n            LinkOpportunitySummaryFundingInstrument.opportunity_summary_id\n            == opportunity_summary_id,\n        )\n        .one_or_none()\n    )\n\n    if not expect_in_db:\n        assert link_funding_instrument is None\n        return\n\n    assert link_funding_instrument is not None\n    assert link_funding_instrument.funding_instrument == expected_funding_instrument\n\n    validate_matching_fields(\n        source_funding_instrument,\n        link_funding_instrument,\n        [(\"creator_id\", \"created_by\"), (\"last_upd_id\", \"updated_by\")],\n        expect_values_to_match,\n    )\n\n\ndef validate_funding_category(\n    db_session,\n    source_funding_category,\n    expect_in_db: bool = True,\n    expected_funding_category: FundingCategory | None = None,\n    was_processed: bool = True,\n    expect_values_to_match: bool = True,\n):\n    assert (source_funding_category.transformed_at is not None) == was_processed\n\n    # In order to properly find the link table value, need to first determine\n    # the opportunity summary in a subquery\n    opportunity_summary_id = (\n        db_session.query(OpportunitySummary.opportunity_summary_id)\n        .filter(\n            OpportunitySummary.revision_number == source_funding_category.revision_number,\n            OpportunitySummary.is_forecast == source_funding_category.is_forecast,\n            OpportunitySummary.opportunity_id == source_funding_category.opportunity_id,\n        )\n        .scalar()\n    )\n\n    link_funding_category = (\n        db_session.query(LinkOpportunitySummaryFundingCategory)\n        .filter(\n            LinkOpportunitySummaryFundingCategory.legacy_funding_category_id\n            == source_funding_category.legacy_funding_category_id,\n            LinkOpportunitySummaryFundingCategory.opportunity_summary_id == opportunity_summary_id,\n        )\n        .one_or_none()\n    )\n\n    if not expect_in_db:\n        assert link_funding_category is None\n        return\n\n    assert link_funding_category is not None\n    assert link_funding_category.funding_category == expected_funding_category\n\n    validate_matching_fields(\n        source_funding_category,\n        link_funding_category,\n        [(\"creator_id\", \"created_by\"), (\"last_upd_id\", \"updated_by\")],\n        expect_values_to_match,\n    )\n\n\nAGENCY_FIELD_MAPPING = [\n    (\"AgencyName\", \"agency_name\"),\n    (\"AgencyCode\", \"sub_agency_code\"),\n    (\"AgencyCFDA\", \"assistance_listing_number\"),\n    (\"ldapGp\", \"ldap_group\"),\n    (\"description\", \"description\"),\n    (\"label\", \"label\"),\n]\n\nAGENCY_CONTACT_FIELD_MAPPING = [\n    (\"AgencyContactName\", \"contact_name\"),\n    (\"AgencyContactAddress1\", \"address_line_1\"),\n    (\"AgencyContactCity\", \"city\"),\n    (\"AgencyContactState\", \"state\"),\n    (\"AgencyContactZipCode\", \"zip_code\"),\n    (\"AgencyContactTelephone\", \"phone_number\"),\n    (\"AgencyContactEMail\", \"primary_email\"),\n]\n\n\ndef validate_agency(\n    db_session,\n    source_tgroups: list[staging.tgroups.Tgroups],\n    expect_in_db: bool = True,\n    expect_values_to_match: bool = True,\n    is_test_agency: bool = False,\n    non_matching_fields: set | None = None,\n    deleted_fields: set | None = None,\n):\n    agency_code = source_tgroups[0].get_agency_code()\n    agency = db_session.query(Agency).filter(Agency.agency_code == agency_code).one_or_none()\n\n    if not expect_in_db:\n        assert agency is None\n        return\n\n    assert agency is not None\n\n    # need to restructure the tgroups into a dict\n    tgroup_map = {tgroup.get_field_name(): tgroup.value for tgroup in source_tgroups}\n\n    if non_matching_fields is not None:\n        agency_field_mapping = [m for m in AGENCY_FIELD_MAPPING if m[0] not in non_matching_fields]\n    else:\n        agency_field_mapping = AGENCY_FIELD_MAPPING\n\n    if deleted_fields is not None:\n        agency_field_mapping = [m for m in agency_field_mapping if m[0] not in deleted_fields]\n\n        deleted_field_mapping = [m for m in AGENCY_FIELD_MAPPING if m[0] in deleted_fields]\n        for deleted_field in deleted_field_mapping:\n            assert getattr(agency, deleted_field[1]) is None\n\n    validate_matching_fields(tgroup_map, agency, agency_field_mapping, expect_values_to_match)\n    assert agency.is_test_agency == is_test_agency\n\n    if non_matching_fields is not None:\n        agency_contact_field_mapping = [\n            m for m in AGENCY_CONTACT_FIELD_MAPPING if m[0] not in non_matching_fields\n        ]\n    else:\n        agency_contact_field_mapping = AGENCY_CONTACT_FIELD_MAPPING\n\n    validate_matching_fields(\n        tgroup_map, agency.agency_contact_info, agency_contact_field_mapping, expect_values_to_match\n    )\n\n\ndef validate_opportunity_attachment(\n    db_session,\n    source_attachment,\n    expect_in_db: bool = True,\n    expect_values_to_match: bool = True,\n):\n\n    opportunity_attachment = (\n        db_session.query(OpportunityAttachment)\n        .filter(OpportunityAttachment.attachment_id == source_attachment.syn_att_id)\n        .one_or_none()\n    )\n\n    if not expect_in_db:\n        assert opportunity_attachment is None\n        return\n\n    assert opportunity_attachment is not None\n    validate_matching_fields(\n        source_attachment,\n        opportunity_attachment,\n        [\n            (\"syn_att_id\", \"attachment_id\"),\n            (\"opportunity_id\", \"opportunity_id\"),\n            (\"mime_type\", \"mime_type\"),\n            (\"file_name\", \"file_name\"),\n            (\"file_desc\", \"file_description\"),\n            (\"file_lob_size\", \"file_size_bytes\"),\n            (\"creator_id\", \"created_by\"),\n            (\"last_upd_id\", \"updated_by\"),\n            (\"syn_att_folder_id\", \"legacy_folder_id\"),\n        ],\n        expect_values_to_match,\n    )\n\n    # Validate the contents of the file and that the file exists on s3\n    with file_util.open_stream(opportunity_attachment.file_location) as s3_file:\n        contents = s3_file.read()\n\n        if expect_values_to_match:\n            assert contents.encode() == source_attachment.file_lob\n        else:\n            assert contents.encode() != source_attachment.file_lob"}
{"path":"frontend/tests/components/search/SearchFilterAccordion/SearchFilterSection/SectionLinkCount.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search/SearchFilterAccordion/SearchFilterSection","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchFilterAccordion/SearchFilterSection/SectionLinkCount.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/transformation/subtask/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/search/SearchFilterAccordion/SearchFilterSection/SectionLinkLabel.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search/SearchFilterAccordion/SearchFilterSection","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchFilterAccordion/SearchFilterSection/SectionLinkLabel.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":""}
{"path":"frontend/tests/components/search/SearchFilterAccordion/SearchFilterToggleAll.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search/SearchFilterAccordion","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchFilterAccordion/SearchFilterToggleAll.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/transformation/subtask/test_transform_agency.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_agency.py\nSize: 12.92 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/search/SearchFilters.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchFilters.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"import pytest\n\nimport src.data_migration.transformation.transform_constants as transform_constants\nfrom src.constants.lookup_constants import (\n    AgencyDownloadFileType,\n    AgencySubmissionNotificationSetting,\n)\nfrom src.data_migration.transformation.subtask.transform_agency import (\n    TgroupAgency,\n    TransformAgency,\n    TransformAgencyHierarchy,\n    apply_updates,\n    transform_agency_download_file_types,\n    transform_agency_notify,\n)\nfrom src.db.models.agency_models import Agency\nfrom tests.src.data_migration.transformation.conftest import (\n    BaseTransformTestClass,\n    setup_agency,\n    validate_agency,\n)\nfrom tests.src.db.models.factories import AgencyFactory\n\n\nclass TestTransformAgencyHierarchy(BaseTransformTestClass):\n    @pytest.fixture()\n    def transform_agency_hierarchy(self, transform_oracle_data_task):\n        return TransformAgencyHierarchy(transform_oracle_data_task)\n\n    def test_transform_records(self, db_session, transform_agency_hierarchy):\n        # Create agencies with varying top-level agency codes\n        [\n            AgencyFactory.create(agency_code=\"DHS\"),\n            AgencyFactory.create(agency_code=\"DHS-ICE\"),\n            AgencyFactory.create(agency_code=\"DHS--ICE\"),\n            AgencyFactory.create(agency_code=\"DHS-ICE-123\"),\n            AgencyFactory.create(agency_code=\"ABC-ICE\"),\n        ]\n\n        # Run the transformation\n        transform_agency_hierarchy.transform_records()\n\n        # Fetch the agencies again to verify the changes\n        agency1 = db_session.query(Agency).filter(Agency.agency_code == \"DHS\").one_or_none()\n        agency2 = db_session.query(Agency).filter(Agency.agency_code == \"DHS-ICE\").one_or_none()\n        agency3 = db_session.query(Agency).filter(Agency.agency_code == \"DHS-ICE-123\").one_or_none()\n        agency4 = db_session.query(Agency).filter(Agency.agency_code == \"ABC-ICE\").one_or_none()\n        agency5 = db_session.query(Agency).filter(Agency.agency_code == \"DHS--ICE\").one_or_none()\n\n        # Verify that the top-level agencies are set correctly\n        assert agency1.top_level_agency_id is None\n        assert agency2.top_level_agency_id == agency1.agency_id\n        assert agency3.top_level_agency_id == agency1.agency_id\n        assert agency4.top_level_agency_id is None\n        assert agency5.top_level_agency_id == agency1.agency_id\n\n    def test_get_top_level_agency_code(self, transform_agency_hierarchy):\n        assert transform_agency_hierarchy.get_top_level_agency_code(\"DHS-ICE\") == \"DHS\"\n        assert transform_agency_hierarchy.get_top_level_agency_code(\"DHS\") is None\n\n\nclass TestTransformAgency(BaseTransformTestClass):\n    @pytest.fixture()\n    def transform_agency(self, transform_oracle_data_task):\n        return TransformAgency(transform_oracle_data_task)\n\n    def test_process_agencies(self, db_session, transform_agency):\n        insert_agency1 = setup_agency(\"INSERT-AGENCY-1\", create_existing=False)\n        insert_agency2 = setup_agency(\"INSERT-AGENCY-2\", create_existing=False)\n        insert_agency3 = setup_agency(\"INSERT-AGENCY-3\", create_existing=False)\n        insert_agency4 = setup_agency(\n            \"INSERT-AGENCY-4\",\n            create_existing=False,\n            # None passed in here will make it not appear at all in the tgroups rows\n            source_values={\"ldapGp\": None, \"description\": None, \"label\": None},\n        )\n        insert_test_agency = setup_agency(\"GDIT\", create_existing=False)\n\n        # Already processed fields are ones that were handled on a prior run and won't be updated\n        # during this specific run\n        update_agency1 = setup_agency(\"UPDATE-AGENCY-1\", create_existing=True)\n        update_agency2 = setup_agency(\n            \"UPDATE-AGENCY-2\",\n            create_existing=True,\n            deleted_fields={\"AgencyContactEMail2\", \"ldapGp\", \"description\"},\n        )\n        update_agency3 = setup_agency(\n            \"UPDATE-AGENCY-3\",\n            create_existing=True,\n            already_processed_fields={\n                \"AgencyName\",\n                \"AgencyCFDA\",\n                \"description\",\n                \"AgencyContactName\",\n                \"AgencyContactAddress1\",\n            },\n        )\n        update_test_agency = setup_agency(\"SECSCAN\", create_existing=True)\n\n        already_processed1 = setup_agency(\n            \"ALREADY-PROCESSED-1\", create_existing=True, is_already_processed=True\n        )\n        already_processed2 = setup_agency(\n            \"ALREADY-PROCESSED-2\", create_existing=True, is_already_processed=True\n        )\n        already_processed3 = setup_agency(\n            \"ALREADY-PROCESSED-3\", create_existing=True, is_already_processed=True\n        )\n\n        insert_error = setup_agency(\n            \"INSERT-ERROR\", create_existing=False, source_values={\"AgencyName\": None}\n        )\n        update_error1 = setup_agency(\n            \"UPDATE-ERROR-1\", create_existing=True, source_values={\"AgencyDownload\": \"xyz\"}\n        )\n        update_error2 = setup_agency(\n            \"UPDATE-ERROR-2\", create_existing=True, source_values={\"UnknownField\": \"xyz\"}\n        )\n\n        transform_agency.run_subtask()\n\n        validate_agency(db_session, insert_agency1)\n        validate_agency(db_session, insert_agency2)\n        validate_agency(db_session, insert_agency3)\n        validate_agency(db_session, insert_agency4)\n        validate_agency(db_session, insert_test_agency, is_test_agency=True)\n\n        validate_agency(db_session, update_agency1)\n        validate_agency(db_session, update_agency2, deleted_fields={\"ldapGp\", \"description\"})\n        validate_agency(\n            db_session,\n            update_agency3,\n            non_matching_fields={\n                \"AgencyName\",\n                \"AgencyCFDA\",\n                \"description\",\n                \"AgencyContactName\",\n                \"AgencyContactAddress1\",\n            },\n        )\n        validate_agency(db_session, update_test_agency, is_test_agency=True)\n\n        validate_agency(db_session, already_processed1, expect_values_to_match=False)\n        validate_agency(db_session, already_processed2, expect_values_to_match=False)\n        validate_agency(db_session, already_processed3, expect_values_to_match=False)\n\n        validate_agency(db_session, insert_error, expect_in_db=False)\n        validate_agency(db_session, update_error1, expect_values_to_match=False)\n        validate_agency(db_session, update_error2, expect_values_to_match=False)\n\n        metrics = transform_agency.metrics\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 12\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 5\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_ERROR_COUNT] == 3\n\n        # Rerunning does mostly nothing, it will attempt to re-process the three that errored\n        # but otherwise won't find anything else\n        db_session.commit()  # commit to end any existing transactions as run_subtask starts a new one\n        transform_agency.run_subtask()\n\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 15\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 5\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_ERROR_COUNT] == 6\n\n    def test_process_tgroups_missing_fields_for_insert(self, db_session, transform_agency):\n        # Fields set to None don't get a tgroup record created\n        insert_that_will_fail = setup_agency(\n            \"ERROR-CASE-MISSING-FIELDS\",\n            create_existing=False,\n            source_values={\"AgencyName\": None, \"AgencyContactCity\": None},\n        )\n\n        with pytest.raises(\n            ValueError,\n            match=\"Cannot create agency ERROR-CASE-MISSING-FIELDS as required fields are missing\",\n        ):\n            transform_agency.process_tgroups(\n                TgroupAgency(\"ERROR-CASE-MISSING-FIELDS\", insert_that_will_fail, has_update=True),\n                None,\n            )\n\n        validate_agency(db_session, insert_that_will_fail, expect_in_db=False)\n\n    def test_process_tgroups_unknown_field(self, db_session, transform_agency):\n        insert_that_will_fail = setup_agency(\n            \"ERROR-CASE-UNKNOWN-FIELD\", create_existing=False, source_values={\"MysteryField\": \"X\"}\n        )\n\n        with pytest.raises(ValueError, match=\"Unknown tgroups agency field\"):\n            transform_agency.process_tgroups(\n                TgroupAgency(\"ERROR-CASE-UNKNOWN-FIELD\", insert_that_will_fail, has_update=True),\n                None,\n            )\n\n        validate_agency(db_session, insert_that_will_fail, expect_in_db=False)\n\n    def test_process_tgroups_disallowed_deleted_fields(self, db_session, transform_agency):\n        update_that_will_fail = setup_agency(\n            \"ERROR-CASE-DELETED-FIELD\", create_existing=True, deleted_fields={\"AgencyContactCity\"}\n        )\n\n        with pytest.raises(\n            ValueError,\n            match=\"Field AgencyContactCity in tgroups cannot be deleted as it is not nullable\",\n        ):\n            transform_agency.process_tgroups(\n                TgroupAgency(\"ERROR-CASE-DELETED-FIELD\", update_that_will_fail, has_update=True),\n                None,\n            )\n\n        validate_agency(db_session, update_that_will_fail, expect_values_to_match=False)\n\n    def test_process_tgroups_invalid_file_type(self, db_session, transform_agency):\n        insert_that_will_fail = setup_agency(\n            \"ERROR-CASE-BAD-DOWNLOAD\", create_existing=False, source_values={\"AgencyDownload\": \"X\"}\n        )\n\n        with pytest.raises(ValueError, match=\"Unrecognized agency download file type value\"):\n            transform_agency.process_tgroups(\n                TgroupAgency(\"ERROR-CASE-BAD-DOWNLOAD\", insert_that_will_fail, has_update=True),\n                None,\n            )\n\n        validate_agency(db_session, insert_that_will_fail, expect_in_db=False)\n\n    def test_process_tgroups_invalid_agency_notify(self, db_session, transform_agency):\n        insert_that_will_fail = setup_agency(\n            \"ERROR-CASE-BAD-NOTIFY\", create_existing=False, source_values={\"AgencyNotify\": \"4\"}\n        )\n\n        with pytest.raises(ValueError, match=\"Unrecognized agency notify setting value\"):\n            transform_agency.process_tgroups(\n                TgroupAgency(\"ERROR-CASE-BAD-NOTIFY\", insert_that_will_fail, has_update=True), None\n            )\n\n        validate_agency(db_session, insert_that_will_fail, expect_in_db=False)\n\n\n@pytest.mark.parametrize(\n    \"value,expected_value\",\n    [\n        (\"0\", set()),\n        (\"1\", {AgencyDownloadFileType.XML}),\n        (\"2\", {AgencyDownloadFileType.XML, AgencyDownloadFileType.PDF}),\n        (\"3\", {AgencyDownloadFileType.PDF}),\n    ],\n)\ndef test_transform_agency_download_file_types(value, expected_value):\n    assert transform_agency_download_file_types(value) == expected_value\n\n\n@pytest.mark.parametrize(\"value\", [\"A\", \"B\", \"NULL\", \"\", None])\ndef test_transform_agency_download_file_types_unexpected_values(value):\n    with pytest.raises(ValueError, match=\"Unrecognized agency download file type value\"):\n        transform_agency_download_file_types(value)\n\n\n@pytest.mark.parametrize(\n    \"value,expected_value\",\n    [\n        (\"1\", AgencySubmissionNotificationSetting.NEVER),\n        (\"2\", AgencySubmissionNotificationSetting.FIRST_APPLICATION_ONLY),\n        (\"3\", AgencySubmissionNotificationSetting.ALWAYS),\n    ],\n)\ndef test_transform_agency_notify(value, expected_value):\n    assert transform_agency_notify(value) == expected_value\n\n\n@pytest.mark.parametrize(\"value\", [\"A\", \"B\", \"NULL\", \"\", None])\ndef test_transform_agency_notify_unexpected_value(value):\n    with pytest.raises(ValueError, match=\"Unrecognized agency notify setting value\"):\n        transform_agency_notify(value)\n\n\n@pytest.mark.parametrize(\n    \"agency_created_at,agency_updated_at,created_at,updated_at,expect_created_at_to_change,expect_updated_at_to_change\",\n    [\n        (None, None, None, None, False, False),\n        (None, None, datetime.now(), datetime.now(), True, True),\n        (\n            datetime(2020, 1, 1),\n            datetime(2021, 1, 1),\n            datetime(2019, 12, 31),\n            datetime(2021, 1, 2),\n            False,\n            True,\n        ),\n        (\n            datetime(2020, 1, 1),\n            datetime(2021, 1, 1),\n            datetime(2020, 12, 31),\n            datetime(2020, 1, 1),\n            False,\n            False,\n        ),\n    ],\n)\ndef test_apply_updates_timestamps(\n    agency_created_at,\n    agency_updated_at,\n    created_at,\n    updated_at,\n    expect_created_at_to_change,\n    expect_updated_at_to_change,\n):\n    agency = AgencyFactory.build(created_at=agency_created_at, updated_at=agency_updated_at)\n\n    apply_updates(agency, {}, created_at, updated_at)\n\n    if expect_created_at_to_change:\n        assert agency.created_at == created_at\n    else:\n        assert agency.created_at == agency_created_at\n\n    if expect_updated_at_to_change:\n        assert agency.updated_at == updated_at\n    else:\n        assert agency.updated_at == agency_updated_at"}
{"path":"frontend/tests/components/search/SearchOpportunityStatus.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchOpportunityStatus.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/transformation/subtask/test_transform_applicant_type.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_applicant_type.py\nSize: 8.80 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/search/SearchPagination.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchPagination.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nimport tests.src.db.models.factories as f\nfrom src.constants.lookup_constants import ApplicantType\nfrom src.data_migration.transformation.subtask.transform_applicant_type import (\n    TransformApplicantType,\n)\nfrom tests.src.data_migration.transformation.conftest import (\n    BaseTransformTestClass,\n    setup_applicant_type,\n    validate_applicant_type,\n)\n\n\nclass TestTransformApplicantType(BaseTransformTestClass):\n    @pytest.fixture()\n    def transform_applicant_type(self, transform_oracle_data_task):\n        return TransformApplicantType(transform_oracle_data_task)\n\n    def test_process_applicant_types(self, db_session, transform_applicant_type):\n        # Forecast scenarios\n        opportunity_summary_forecast = f.OpportunitySummaryFactory.create(\n            is_forecast=True, revision_number=None, no_link_values=True\n        )\n        forecast_insert1 = setup_applicant_type(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"00\",\n        )\n        forecast_update1 = setup_applicant_type(\n            create_existing=True,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"01\",\n            applicant_type=ApplicantType.COUNTY_GOVERNMENTS,\n        )\n        forecast_update2 = setup_applicant_type(\n            create_existing=True,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"02\",\n            applicant_type=ApplicantType.CITY_OR_TOWNSHIP_GOVERNMENTS,\n        )\n        forecast_delete1 = setup_applicant_type(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"04\",\n            applicant_type=ApplicantType.SPECIAL_DISTRICT_GOVERNMENTS,\n        )\n        forecast_delete2 = setup_applicant_type(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"05\",\n            applicant_type=ApplicantType.INDEPENDENT_SCHOOL_DISTRICTS,\n        )\n        forecast_update_already_processed = setup_applicant_type(\n            create_existing=True,\n            is_already_processed=True,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"06\",\n            applicant_type=ApplicantType.PUBLIC_AND_STATE_INSTITUTIONS_OF_HIGHER_EDUCATION,\n        )\n\n        # Synopsis scenarios\n        opportunity_summary_syn = f.OpportunitySummaryFactory.create(\n            is_forecast=False, revision_number=None, no_link_values=True\n        )\n        syn_insert1 = setup_applicant_type(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"20\",\n        )\n        syn_insert2 = setup_applicant_type(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"21\",\n        )\n        syn_update1 = setup_applicant_type(\n            create_existing=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"22\",\n            applicant_type=ApplicantType.FOR_PROFIT_ORGANIZATIONS_OTHER_THAN_SMALL_BUSINESSES,\n        )\n        syn_update2 = setup_applicant_type(\n            create_existing=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"23\",\n            applicant_type=ApplicantType.SMALL_BUSINESSES,\n        )\n        syn_delete1 = setup_applicant_type(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"25\",\n            applicant_type=ApplicantType.OTHER,\n        )\n        syn_delete2 = setup_applicant_type(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"99\",\n            applicant_type=ApplicantType.UNRESTRICTED,\n        )\n        syn_delete_but_current_missing = setup_applicant_type(\n            create_existing=False,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"07\",\n        )\n        syn_update_already_processed = setup_applicant_type(\n            create_existing=True,\n            is_already_processed=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"08\",\n            applicant_type=ApplicantType.PUBLIC_AND_INDIAN_HOUSING_AUTHORITIES,\n        )\n\n        transform_applicant_type.run_subtask()\n\n        # Validate forecast scenarios\n        validate_applicant_type(\n            db_session, forecast_insert1, expected_applicant_type=ApplicantType.STATE_GOVERNMENTS\n        )\n        validate_applicant_type(\n            db_session, forecast_update1, expected_applicant_type=ApplicantType.COUNTY_GOVERNMENTS\n        )\n        validate_applicant_type(\n            db_session,\n            forecast_update2,\n            expected_applicant_type=ApplicantType.CITY_OR_TOWNSHIP_GOVERNMENTS,\n        )\n        validate_applicant_type(db_session, forecast_delete1, expect_in_db=False)\n        validate_applicant_type(db_session, forecast_delete2, expect_in_db=False)\n        validate_applicant_type(\n            db_session,\n            forecast_update_already_processed,\n            expected_applicant_type=ApplicantType.PUBLIC_AND_STATE_INSTITUTIONS_OF_HIGHER_EDUCATION,\n            expect_values_to_match=False,\n        )\n\n        # Validate synopsis scenarios\n        validate_applicant_type(\n            db_session,\n            syn_insert1,\n            expected_applicant_type=ApplicantType.PRIVATE_INSTITUTIONS_OF_HIGHER_EDUCATION,\n        )\n        validate_applicant_type(\n            db_session, syn_insert2, expected_applicant_type=ApplicantType.INDIVIDUALS\n        )\n        validate_applicant_type(\n            db_session,\n            syn_update1,\n            expected_applicant_type=ApplicantType.FOR_PROFIT_ORGANIZATIONS_OTHER_THAN_SMALL_BUSINESSES,\n        )\n        validate_applicant_type(\n            db_session, syn_update2, expected_applicant_type=ApplicantType.SMALL_BUSINESSES\n        )\n        validate_applicant_type(db_session, syn_delete1, expect_in_db=False)\n        validate_applicant_type(db_session, syn_delete2, expect_in_db=False)\n        validate_applicant_type(\n            db_session,\n            syn_update_already_processed,\n            expected_applicant_type=ApplicantType.PUBLIC_AND_INDIAN_HOUSING_AUTHORITIES,\n            expect_values_to_match=False,\n        )\n        validate_applicant_type(\n            db_session, syn_delete_but_current_missing, expect_in_db=False, was_processed=True\n        )\n\n        # Validate metrics\n        metrics = transform_applicant_type.metrics\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 12\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 3\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 4\n        assert transform_constants.Metrics.TOTAL_ERROR_COUNT not in metrics\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n        # Rerunning will not reprocess anything since there were no errors\n        db_session.commit()  # commit to end any existing transactions as run_subtask starts a new one\n        transform_applicant_type.run_subtask()\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 12\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 3\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 4\n        assert transform_constants.Metrics.TOTAL_ERROR_COUNT not in metrics\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n    @pytest.mark.parametrize(\"is_forecast\", [True, False])\n    def test_process_applicant_type_but_no_opportunity_summary_non_hist(\n        self,\n        db_session,\n        transform_applicant_type,\n        is_forecast,\n    ):\n        opportunity_summary = f.OpportunitySummaryFactory.create(\n            is_forecast=is_forecast, revision_number=None, no_link_values=True\n        )\n\n        source_record = setup_applicant_type(\n            create_existing=False,\n            opportunity_summary=opportunity_summary,\n            legacy_lookup_value=\"00\",\n        )\n\n        with pytest.raises(\n            ValueError,\n            match=\"Applicant type record cannot be processed as the opportunity summary for it does not exist\",\n        ):\n            transform_applicant_type.process_link_applicant_type(source_record, None, None)"}
{"path":"frontend/tests/components/search/SearchResults.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchResults.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/transformation/subtask/test_transform_assistance_listing.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_assistance_listing.py\nSize: 11.22 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/search/SearchResultsHeader.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchResultsHeader.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nimport tests.src.db.models.factories as f\nfrom src.data_migration.transformation.subtask.transform_assistance_listing import (\n    TransformAssistanceListing,\n)\nfrom src.db.models.opportunity_models import Opportunity, OpportunityAssistanceListing\nfrom tests.src.data_migration.transformation.conftest import (\n    BaseTransformTestClass,\n    setup_cfda,\n    validate_assistance_listing,\n)\n\n\nclass TestTransformAssistanceListing(BaseTransformTestClass):\n    @pytest.fixture()\n    def transform_assistance_listing(self, transform_oracle_data_task):\n        return TransformAssistanceListing(transform_oracle_data_task)\n\n    def test_process_opportunity_assistance_listings(\n        self, db_session, transform_assistance_listing\n    ):\n        opportunity1 = f.OpportunityFactory.create(opportunity_assistance_listings=[])\n        cfda_insert1 = setup_cfda(create_existing=False, opportunity=opportunity1)\n        cfda_insert2 = setup_cfda(create_existing=False, opportunity=opportunity1)\n        cfda_update1 = setup_cfda(create_existing=True, opportunity=opportunity1)\n        cfda_delete1 = setup_cfda(create_existing=True, is_delete=True, opportunity=opportunity1)\n        cfda_update_already_processed1 = setup_cfda(\n            create_existing=True, is_already_processed=True, opportunity=opportunity1\n        )\n\n        opportunity2 = f.OpportunityFactory.create(opportunity_assistance_listings=[])\n        cfda_insert3 = setup_cfda(create_existing=False, opportunity=opportunity2)\n        cfda_update_already_processed2 = setup_cfda(\n            create_existing=True, is_already_processed=True, opportunity=opportunity2\n        )\n        cfda_delete_already_processed1 = setup_cfda(\n            create_existing=False,\n            is_already_processed=True,\n            is_delete=True,\n            opportunity=opportunity2,\n        )\n        cfda_delete2 = setup_cfda(create_existing=True, is_delete=True, opportunity=opportunity2)\n\n        opportunity3 = f.OpportunityFactory.create(opportunity_assistance_listings=[])\n        cfda_update2 = setup_cfda(create_existing=True, opportunity=opportunity3)\n        cfda_delete_but_current_missing = setup_cfda(\n            create_existing=False, is_delete=True, opportunity=opportunity3\n        )\n\n        cfda_insert_without_opportunity = setup_cfda(\n            create_existing=False, source_values={\"opportunity_id\": 12345678}, opportunity=None\n        )\n        cfda_delete_without_opportunity = setup_cfda(\n            create_existing=False, source_values={\"opportunity_id\": 34567890}, opportunity=None\n        )\n\n        transform_assistance_listing.run_subtask()\n\n        validate_assistance_listing(db_session, cfda_insert1)\n        validate_assistance_listing(db_session, cfda_insert2)\n        validate_assistance_listing(db_session, cfda_insert3)\n        validate_assistance_listing(db_session, cfda_update1)\n        validate_assistance_listing(db_session, cfda_update2)\n        validate_assistance_listing(db_session, cfda_delete1, expect_in_db=False)\n        validate_assistance_listing(db_session, cfda_delete2, expect_in_db=False)\n\n        # Records that won't have been fetched\n        validate_assistance_listing(\n            db_session,\n            cfda_update_already_processed1,\n            expect_in_db=True,\n            expect_values_to_match=False,\n        )\n        validate_assistance_listing(\n            db_session,\n            cfda_update_already_processed2,\n            expect_in_db=True,\n            expect_values_to_match=False,\n        )\n        validate_assistance_listing(db_session, cfda_delete_already_processed1, expect_in_db=False)\n\n        validate_assistance_listing(db_session, cfda_delete_but_current_missing, expect_in_db=False)\n\n        validate_assistance_listing(db_session, cfda_insert_without_opportunity, expect_in_db=False)\n        validate_assistance_listing(db_session, cfda_delete_without_opportunity, expect_in_db=False)\n\n        metrics = transform_assistance_listing.metrics\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 10\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 3\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_ORPHANED] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n        # Rerunning finds nothing - no metrics update\n        db_session.commit()  # commit to end any existing transactions as run_subtask starts a new one\n        transform_assistance_listing.run_subtask()\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 10\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 3\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_ORPHANED] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n    def test_process_assistance_listing_orphaned_record(\n        self, db_session, transform_assistance_listing\n    ):\n        cfda_insert_without_opportunity = setup_cfda(\n            create_existing=False, source_values={\"opportunity_id\": 987654321}, opportunity=None\n        )\n\n        # Verify it gets marked as transformed\n        assert cfda_insert_without_opportunity.transformed_at is None\n        transform_assistance_listing.process_assistance_listing(\n            cfda_insert_without_opportunity, None, None\n        )\n        assert cfda_insert_without_opportunity.transformed_at is not None\n        assert cfda_insert_without_opportunity.transformation_notes == \"orphaned_cfda\"\n        assert (\n            transform_assistance_listing.metrics[transform_constants.Metrics.TOTAL_RECORDS_ORPHANED]\n            == 1\n        )\n\n        # Verify nothing actually gets created\n        opportunity = (\n            db_session.query(Opportunity)\n            .filter(Opportunity.opportunity_id == cfda_insert_without_opportunity.opportunity_id)\n            .one_or_none()\n        )\n        assert opportunity is None\n        assistance_listing = (\n            db_session.query(OpportunityAssistanceListing)\n            .filter(\n                OpportunityAssistanceListing.opportunity_assistance_listing_id\n                == cfda_insert_without_opportunity.opp_cfda_id\n            )\n            .one_or_none()\n        )\n        assert assistance_listing is None\n\n    def test_process_assistance_listing_delete_but_current_missing(\n        self, db_session, transform_assistance_listing\n    ):\n        opportunity = f.OpportunityFactory.create(opportunity_assistance_listings=[])\n        delete_but_current_missing = setup_cfda(\n            create_existing=False, is_delete=True, opportunity=opportunity\n        )\n\n        transform_assistance_listing.process_assistance_listing(\n            delete_but_current_missing, None, opportunity\n        )\n\n        validate_assistance_listing(db_session, delete_but_current_missing, expect_in_db=False)\n        assert delete_but_current_missing.transformed_at is not None\n        assert delete_but_current_missing.transformation_notes == \"orphaned_delete_record\"\n\n    def test_process_empty_assistance_listings(self, db_session, transform_assistance_listing):\n        \"\"\"Test that assistance listings with empty required fields are skipped\"\"\"\n        # Create opportunities with empty assistance listings\n        opportunity1 = f.OpportunityFactory.create(opportunity_assistance_listings=[])\n\n        # Empty program title\n        empty_program_title = setup_cfda(\n            create_existing=False,\n            opportunity=opportunity1,\n            source_values={\"programtitle\": \"\", \"cfdanumber\": \"12.345\"},\n        )\n\n        # Empty assistance listing number\n        empty_listing_number = setup_cfda(\n            create_existing=False,\n            opportunity=opportunity1,\n            source_values={\"programtitle\": \"Test Program\", \"cfdanumber\": \"\"},\n        )\n\n        # Both empty\n        both_empty = setup_cfda(\n            create_existing=False,\n            opportunity=opportunity1,\n            source_values={\"programtitle\": \"\", \"cfdanumber\": \"\"},\n        )\n\n        # Control - valid record\n        valid_record = setup_cfda(\n            create_existing=False,\n            opportunity=opportunity1,\n            source_values={\"programtitle\": \"Valid Program\", \"cfdanumber\": \"67.890\"},\n        )\n\n        transform_assistance_listing.run_subtask()\n\n        # Verify empty records were skipped and marked appropriately\n        for record in [empty_program_title, empty_listing_number, both_empty]:\n            assert record.transformed_at is not None\n            assert record.transformation_notes == \"empty_assistance_listing\"\n\n            # Verify no record was created in the target table\n            assistance_listing = (\n                db_session.query(OpportunityAssistanceListing)\n                .filter(\n                    OpportunityAssistanceListing.opportunity_assistance_listing_id\n                    == record.opp_cfda_id\n                )\n                .one_or_none()\n            )\n            assert assistance_listing is None\n\n        # Verify valid record was processed\n        validate_assistance_listing(db_session, valid_record)\n\n        # Verify metrics\n        metrics = transform_assistance_listing.metrics\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_SKIPPED] == 3\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 1\n\n    def test_process_empty_assistance_listing_update(\n        self, db_session, transform_assistance_listing\n    ):\n        \"\"\"Test that empty assistance listings are skipped even for updates\"\"\"\n        opportunity = f.OpportunityFactory.create(opportunity_assistance_listings=[])\n\n        # Create a record that exists but will be updated with empty values\n        empty_update = setup_cfda(\n            create_existing=True,\n            opportunity=opportunity,\n            source_values={\"programtitle\": \"\", \"cfdanumber\": \"\"},\n        )\n\n        transform_assistance_listing.run_subtask()\n\n        # Verify record was marked as processed but not updated\n        assert empty_update.transformed_at is not None\n        assert empty_update.transformation_notes == \"empty_assistance_listing\"\n\n        # Verify original record in target table remains unchanged\n        assistance_listing = (\n            db_session.query(OpportunityAssistanceListing)\n            .filter(\n                OpportunityAssistanceListing.opportunity_assistance_listing_id\n                == empty_update.opp_cfda_id\n            )\n            .one()\n        )\n        assert assistance_listing is not None\n        # Verify the values weren't updated to empty\n        assert assistance_listing.program_title != \"\"\n        assert assistance_listing.assistance_listing_number != \"\"\n\n        # Verify metrics\n        metrics = transform_assistance_listing.metrics\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 1\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_SKIPPED] == 1"}
{"path":"frontend/tests/components/search/SearchResultsList.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchResultsList.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/transformation/subtask/test_transform_funding_category.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_funding_category.py\nSize: 15.50 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/components/search/SearchResultsListItem.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchResultsListItem.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nimport tests.src.db.models.factories as f\nfrom src.constants.lookup_constants import FundingCategory\nfrom src.data_migration.transformation.subtask.transform_funding_category import (\n    TransformFundingCategory,\n)\nfrom tests.src.data_migration.transformation.conftest import (\n    BaseTransformTestClass,\n    setup_funding_category,\n    validate_funding_category,\n)\n\n\nclass TestTransformFundingCategory(BaseTransformTestClass):\n    @pytest.fixture()\n    def transform_funding_category(self, transform_oracle_data_task):\n        return TransformFundingCategory(transform_oracle_data_task)\n\n    def test_process_funding_categories(self, db_session, transform_funding_category):\n        opportunity_summary_forecast = f.OpportunitySummaryFactory.create(\n            is_forecast=True, revision_number=None, no_link_values=True\n        )\n        forecast_insert1 = setup_funding_category(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"RA\",\n        )\n        forecast_insert2 = setup_funding_category(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"AG\",\n        )\n        forecast_update1 = setup_funding_category(\n            create_existing=True,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"AR\",\n            funding_category=FundingCategory.ARTS,\n        )\n        forecast_delete1 = setup_funding_category(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"BC\",\n            funding_category=FundingCategory.BUSINESS_AND_COMMERCE,\n        )\n        forecast_delete2 = setup_funding_category(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"CD\",\n            funding_category=FundingCategory.COMMUNITY_DEVELOPMENT,\n        )\n        forecast_update_already_processed = setup_funding_category(\n            create_existing=True,\n            is_already_processed=True,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"CP\",\n            funding_category=FundingCategory.CONSUMER_PROTECTION,\n        )\n\n        opportunity_summary_forecast_hist = f.OpportunitySummaryFactory.create(\n            is_forecast=True, revision_number=3, no_link_values=True\n        )\n        forecast_hist_insert1 = setup_funding_category(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_forecast_hist,\n            legacy_lookup_value=\"DPR\",\n        )\n        forecast_hist_insert2 = setup_funding_category(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_forecast_hist,\n            legacy_lookup_value=\"ED\",\n        )\n        forecast_hist_update1 = setup_funding_category(\n            create_existing=True,\n            opportunity_summary=opportunity_summary_forecast_hist,\n            legacy_lookup_value=\"ELT\",\n            funding_category=FundingCategory.EMPLOYMENT_LABOR_AND_TRAINING,\n        )\n        forecast_hist_delete1 = setup_funding_category(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_forecast_hist,\n            legacy_lookup_value=\"EN\",\n            funding_category=FundingCategory.ENERGY,\n        )\n        forecast_hist_delete_already_processed = setup_funding_category(\n            create_existing=False,\n            is_delete=True,\n            is_already_processed=True,\n            opportunity_summary=opportunity_summary_forecast_hist,\n            legacy_lookup_value=\"ENV\",\n        )\n\n        opportunity_summary_syn = f.OpportunitySummaryFactory.create(\n            is_forecast=False, revision_number=None, no_link_values=True\n        )\n        syn_insert1 = setup_funding_category(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"FN\",\n        )\n        syn_insert2 = setup_funding_category(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"HL\",\n        )\n        syn_update1 = setup_funding_category(\n            create_existing=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"HO\",\n            funding_category=FundingCategory.HOUSING,\n        )\n        syn_delete1 = setup_funding_category(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"HU\",\n            funding_category=FundingCategory.HUMANITIES,\n        )\n        syn_delete2 = setup_funding_category(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"IIJ\",\n            funding_category=FundingCategory.INFRASTRUCTURE_INVESTMENT_AND_JOBS_ACT,\n        )\n        syn_delete_but_current_missing = setup_funding_category(\n            create_existing=False,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"IS\",\n        )\n        syn_update_already_processed = setup_funding_category(\n            create_existing=True,\n            is_already_processed=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"ISS\",\n            funding_category=FundingCategory.INCOME_SECURITY_AND_SOCIAL_SERVICES,\n        )\n\n        opportunity_summary_syn_hist = f.OpportunitySummaryFactory.create(\n            is_forecast=False, revision_number=21, no_link_values=True\n        )\n        syn_hist_insert1 = setup_funding_category(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_syn_hist,\n            legacy_lookup_value=\"LJL\",\n        )\n        syn_hist_insert2 = setup_funding_category(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_syn_hist,\n            legacy_lookup_value=\"NR\",\n        )\n        syn_hist_insert3 = setup_funding_category(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_syn_hist,\n            legacy_lookup_value=\"OZ\",\n        )\n        syn_hist_update1 = setup_funding_category(\n            create_existing=True,\n            opportunity_summary=opportunity_summary_syn_hist,\n            legacy_lookup_value=\"RD\",\n            funding_category=FundingCategory.REGIONAL_DEVELOPMENT,\n        )\n\n        syn_hist_delete1 = setup_funding_category(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_syn_hist,\n            legacy_lookup_value=\"ST\",\n            funding_category=FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT,\n        )\n        syn_hist_delete2 = setup_funding_category(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_syn_hist,\n            legacy_lookup_value=\"T\",\n            funding_category=FundingCategory.TRANSPORTATION,\n        )\n        syn_hist_insert_invalid_type = setup_funding_category(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_syn_hist,\n            legacy_lookup_value=\"XYZ\",\n            funding_category=FundingCategory.HEALTH,\n        )\n\n        transform_funding_category.run_subtask()\n\n        validate_funding_category(\n            db_session, forecast_insert1, expected_funding_category=FundingCategory.RECOVERY_ACT\n        )\n        validate_funding_category(\n            db_session, forecast_insert2, expected_funding_category=FundingCategory.AGRICULTURE\n        )\n        validate_funding_category(\n            db_session,\n            forecast_hist_insert1,\n            expected_funding_category=FundingCategory.DISASTER_PREVENTION_AND_RELIEF,\n        )\n        validate_funding_category(\n            db_session, forecast_hist_insert2, expected_funding_category=FundingCategory.EDUCATION\n        )\n        validate_funding_category(\n            db_session, syn_insert1, expected_funding_category=FundingCategory.FOOD_AND_NUTRITION\n        )\n        validate_funding_category(\n            db_session, syn_insert2, expected_funding_category=FundingCategory.HEALTH\n        )\n        validate_funding_category(\n            db_session,\n            syn_hist_insert1,\n            expected_funding_category=FundingCategory.LAW_JUSTICE_AND_LEGAL_SERVICES,\n        )\n        validate_funding_category(\n            db_session,\n            syn_hist_insert2,\n            expected_funding_category=FundingCategory.NATURAL_RESOURCES,\n        )\n        validate_funding_category(\n            db_session,\n            syn_hist_insert3,\n            expected_funding_category=FundingCategory.OPPORTUNITY_ZONE_BENEFITS,\n        )\n\n        validate_funding_category(\n            db_session, forecast_update1, expected_funding_category=FundingCategory.ARTS\n        )\n        validate_funding_category(\n            db_session,\n            forecast_hist_update1,\n            expected_funding_category=FundingCategory.EMPLOYMENT_LABOR_AND_TRAINING,\n        )\n        validate_funding_category(\n            db_session, syn_update1, expected_funding_category=FundingCategory.HOUSING\n        )\n        validate_funding_category(\n            db_session,\n            syn_hist_update1,\n            expected_funding_category=FundingCategory.REGIONAL_DEVELOPMENT,\n        )\n\n        validate_funding_category(db_session, forecast_delete1, expect_in_db=False)\n        validate_funding_category(db_session, forecast_delete2, expect_in_db=False)\n        validate_funding_category(db_session, forecast_hist_delete1, expect_in_db=False)\n        validate_funding_category(db_session, syn_delete1, expect_in_db=False)\n        validate_funding_category(db_session, syn_delete2, expect_in_db=False)\n        validate_funding_category(db_session, syn_hist_delete1, expect_in_db=False)\n        validate_funding_category(db_session, syn_hist_delete2, expect_in_db=False)\n\n        validate_funding_category(\n            db_session,\n            forecast_update_already_processed,\n            expected_funding_category=FundingCategory.CONSUMER_PROTECTION,\n            expect_values_to_match=False,\n        )\n        validate_funding_category(\n            db_session, forecast_hist_delete_already_processed, expect_in_db=False\n        )\n        validate_funding_category(\n            db_session,\n            syn_update_already_processed,\n            expected_funding_category=FundingCategory.INCOME_SECURITY_AND_SOCIAL_SERVICES,\n            expect_values_to_match=False,\n        )\n\n        validate_funding_category(\n            db_session, syn_delete_but_current_missing, expect_in_db=False, was_processed=True\n        )\n        validate_funding_category(\n            db_session, syn_hist_insert_invalid_type, expect_in_db=False, was_processed=False\n        )\n\n        metrics = transform_funding_category.metrics\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 22\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 7\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 9\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_ERROR_COUNT] == 1\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n        # Rerunning will only attempt to re-process the errors, so total+errors goes up by 1\n        db_session.commit()  # commit to end any existing transactions as run_subtask starts a new one\n        transform_funding_category.run_subtask()\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 23\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 7\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 9\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_ERROR_COUNT] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n    @pytest.mark.parametrize(\n        \"is_forecast,revision_number\", [(True, None), (False, None), (True, 1), (False, 70)]\n    )\n    def test_process_funding_category_but_current_missing(\n        self, db_session, transform_funding_category, is_forecast, revision_number\n    ):\n        opportunity_summary = f.OpportunitySummaryFactory.create(\n            is_forecast=is_forecast, revision_number=revision_number, no_link_values=True\n        )\n        delete_but_current_missing = setup_funding_category(\n            create_existing=False,\n            opportunity_summary=opportunity_summary,\n            legacy_lookup_value=\"00\",\n            is_delete=True,\n        )\n\n        transform_funding_category.process_link_funding_category(\n            delete_but_current_missing, None, opportunity_summary\n        )\n\n        validate_funding_category(db_session, delete_but_current_missing, expect_in_db=False)\n        assert delete_but_current_missing.transformed_at is not None\n        assert delete_but_current_missing.transformation_notes == \"orphaned_delete_record\"\n\n    @pytest.mark.parametrize(\n        \"is_forecast,revision_number,legacy_lookup_value\",\n        [(True, None, \"ab\"), (False, None, \"cd\"), (True, 5, \"ef\"), (False, 10, \"Ag\")],\n    )\n    def test_process_funding_category_but_invalid_lookup_value(\n        self,\n        db_session,\n        transform_funding_category,\n        is_forecast,\n        revision_number,\n        legacy_lookup_value,\n    ):\n        opportunity_summary = f.OpportunitySummaryFactory.create(\n            is_forecast=is_forecast, revision_number=revision_number, no_link_values=True\n        )\n        insert_but_invalid_value = setup_funding_category(\n            create_existing=False,\n            opportunity_summary=opportunity_summary,\n            legacy_lookup_value=legacy_lookup_value,\n        )\n\n        with pytest.raises(ValueError, match=\"Unrecognized funding category\"):\n            transform_funding_category.process_link_funding_category(\n                insert_but_invalid_value, None, opportunity_summary\n            )\n\n    @pytest.mark.parametrize(\n        \"factory_cls\", [f.StagingTfundactcatForecastFactory, f.StagingTfundactcatSynopsisFactory]\n    )\n    def test_process_funding_category_but_no_opportunity_summary_non_hist(\n        self,\n        db_session,\n        transform_funding_category,\n        factory_cls,\n    ):\n        source_record = factory_cls.create(orphaned_record=True)\n\n        with pytest.raises(\n            ValueError,\n            match=\"Funding category record cannot be processed as the opportunity summary for it does not exist\",\n        ):\n            transform_funding_category.process_link_funding_category(source_record, None, None)\n\n    @pytest.mark.parametrize(\n        \"factory_cls\",\n        [f.StagingTfundactcatForecastHistFactory, f.StagingTfundactcatSynopsisHistFactory],\n    )\n    def test_process_funding_category_but_no_opportunity_summary_hist(\n        self,\n        db_session,\n        transform_funding_category,\n        factory_cls,\n    ):\n        source_record = factory_cls.create(orphaned_record=True, revision_number=12)\n        transform_funding_category.process_link_funding_category(source_record, None, None)\n        assert source_record.transformed_at is not None\n        assert source_record.transformation_notes == \"orphaned_historical_record\""}
{"path":"frontend/tests/components/search/SearchSortBy.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/components/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/components/search/SearchSortBy.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.136Z","content":"File: api/tests/src/data_migration/transformation/subtask/test_transform_funding_instrument.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_funding_instrument.py\nSize: 12.52 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/e2e/404.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/404.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nimport tests.src.db.models.factories as f\nfrom src.constants.lookup_constants import FundingInstrument\nfrom src.data_migration.transformation.subtask.transform_funding_instrument import (\n    TransformFundingInstrument,\n)\nfrom tests.src.data_migration.transformation.conftest import (\n    BaseTransformTestClass,\n    setup_funding_instrument,\n    validate_funding_instrument,\n)\n\n\nclass TestTransformFundingInstrument(BaseTransformTestClass):\n    @pytest.fixture()\n    def transform_funding_instrument(self, transform_oracle_data_task):\n        return TransformFundingInstrument(transform_oracle_data_task)\n\n    def test_process_funding_instruments(self, db_session, transform_funding_instrument):\n        opportunity_summary_forecast = f.OpportunitySummaryFactory.create(\n            is_forecast=True, revision_number=None, no_link_values=True\n        )\n        forecast_insert1 = setup_funding_instrument(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"CA\",\n        )\n        forecast_update1 = setup_funding_instrument(\n            create_existing=True,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"G\",\n            funding_instrument=FundingInstrument.GRANT,\n        )\n        forecast_delete1 = setup_funding_instrument(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"PC\",\n            funding_instrument=FundingInstrument.PROCUREMENT_CONTRACT,\n        )\n        forecast_update_already_processed = setup_funding_instrument(\n            create_existing=True,\n            is_already_processed=True,\n            opportunity_summary=opportunity_summary_forecast,\n            legacy_lookup_value=\"O\",\n            funding_instrument=FundingInstrument.OTHER,\n        )\n\n        opportunity_summary_forecast_hist = f.OpportunitySummaryFactory.create(\n            is_forecast=True, revision_number=3, no_link_values=True\n        )\n        forecast_hist_insert1 = setup_funding_instrument(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_forecast_hist,\n            legacy_lookup_value=\"G\",\n        )\n        forecast_hist_delete1 = setup_funding_instrument(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_forecast_hist,\n            legacy_lookup_value=\"CA\",\n            funding_instrument=FundingInstrument.COOPERATIVE_AGREEMENT,\n        )\n        forecast_hist_delete_already_processed = setup_funding_instrument(\n            create_existing=False,\n            is_delete=True,\n            is_already_processed=True,\n            opportunity_summary=opportunity_summary_forecast_hist,\n            legacy_lookup_value=\"O\",\n        )\n        syn_delete_but_current_missing = setup_funding_instrument(\n            create_existing=False,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_forecast_hist,\n            legacy_lookup_value=\"PC\",\n        )\n\n        opportunity_summary_syn = f.OpportunitySummaryFactory.create(\n            is_forecast=False, revision_number=None, no_link_values=True\n        )\n        syn_insert1 = setup_funding_instrument(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"O\",\n        )\n        syn_insert2 = setup_funding_instrument(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"G\",\n        )\n        syn_delete1 = setup_funding_instrument(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"CA\",\n            funding_instrument=FundingInstrument.COOPERATIVE_AGREEMENT,\n        )\n        syn_update_already_processed = setup_funding_instrument(\n            create_existing=True,\n            is_already_processed=True,\n            opportunity_summary=opportunity_summary_syn,\n            legacy_lookup_value=\"PC\",\n            funding_instrument=FundingInstrument.PROCUREMENT_CONTRACT,\n        )\n\n        opportunity_summary_syn_hist = f.OpportunitySummaryFactory.create(\n            is_forecast=False, revision_number=21, no_link_values=True\n        )\n        syn_hist_insert1 = setup_funding_instrument(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_syn_hist,\n            legacy_lookup_value=\"CA\",\n        )\n        syn_hist_update1 = setup_funding_instrument(\n            create_existing=True,\n            opportunity_summary=opportunity_summary_syn_hist,\n            legacy_lookup_value=\"O\",\n            funding_instrument=FundingInstrument.OTHER,\n        )\n        syn_hist_delete1 = setup_funding_instrument(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_syn_hist,\n            legacy_lookup_value=\"PC\",\n            funding_instrument=FundingInstrument.PROCUREMENT_CONTRACT,\n        )\n        syn_hist_delete2 = setup_funding_instrument(\n            create_existing=True,\n            is_delete=True,\n            opportunity_summary=opportunity_summary_syn_hist,\n            legacy_lookup_value=\"G\",\n            funding_instrument=FundingInstrument.GRANT,\n        )\n        syn_hist_insert_invalid_type = setup_funding_instrument(\n            create_existing=False,\n            opportunity_summary=opportunity_summary_syn_hist,\n            legacy_lookup_value=\"X\",\n        )\n\n        transform_funding_instrument.run_subtask()\n\n        validate_funding_instrument(\n            db_session,\n            forecast_insert1,\n            expected_funding_instrument=FundingInstrument.COOPERATIVE_AGREEMENT,\n        )\n        validate_funding_instrument(\n            db_session, forecast_hist_insert1, expected_funding_instrument=FundingInstrument.GRANT\n        )\n        validate_funding_instrument(\n            db_session, syn_insert1, expected_funding_instrument=FundingInstrument.OTHER\n        )\n        validate_funding_instrument(\n            db_session, syn_insert2, expected_funding_instrument=FundingInstrument.GRANT\n        )\n        validate_funding_instrument(\n            db_session,\n            syn_hist_insert1,\n            expected_funding_instrument=FundingInstrument.COOPERATIVE_AGREEMENT,\n        )\n\n        validate_funding_instrument(\n            db_session, forecast_update1, expected_funding_instrument=FundingInstrument.GRANT\n        )\n        validate_funding_instrument(\n            db_session, syn_hist_update1, expected_funding_instrument=FundingInstrument.OTHER\n        )\n\n        validate_funding_instrument(db_session, forecast_delete1, expect_in_db=False)\n        validate_funding_instrument(db_session, forecast_hist_delete1, expect_in_db=False)\n        validate_funding_instrument(db_session, syn_delete1, expect_in_db=False)\n        validate_funding_instrument(db_session, syn_hist_delete1, expect_in_db=False)\n        validate_funding_instrument(db_session, syn_hist_delete2, expect_in_db=False)\n\n        validate_funding_instrument(\n            db_session,\n            forecast_update_already_processed,\n            expected_funding_instrument=FundingInstrument.OTHER,\n            expect_values_to_match=False,\n        )\n        validate_funding_instrument(\n            db_session, forecast_hist_delete_already_processed, expect_in_db=False\n        )\n        validate_funding_instrument(\n            db_session,\n            syn_update_already_processed,\n            expected_funding_instrument=FundingInstrument.PROCUREMENT_CONTRACT,\n            expect_values_to_match=False,\n        )\n\n        validate_funding_instrument(\n            db_session, syn_delete_but_current_missing, expect_in_db=False, was_processed=True\n        )\n        validate_funding_instrument(\n            db_session, syn_hist_insert_invalid_type, expect_in_db=False, was_processed=False\n        )\n\n        metrics = transform_funding_instrument.metrics\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 14\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 5\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 5\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_ERROR_COUNT] == 1\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n        # Rerunning will only attempt to re-process the errors, so total+errors goes up by 2\n        db_session.commit()  # commit to end any existing transactions as run_subtask starts a new one\n        transform_funding_instrument.run_subtask()\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 15\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 5\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 5\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_ERROR_COUNT] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n    @pytest.mark.parametrize(\n        \"is_forecast,revision_number\", [(True, None), (False, None), (True, 1), (False, 4)]\n    )\n    def test_process_funding_instrument_but_current_missing(\n        self, db_session, transform_funding_instrument, is_forecast, revision_number\n    ):\n        opportunity_summary = f.OpportunitySummaryFactory.create(\n            is_forecast=is_forecast, revision_number=revision_number, no_link_values=True\n        )\n        delete_but_current_missing = setup_funding_instrument(\n            create_existing=False,\n            opportunity_summary=opportunity_summary,\n            legacy_lookup_value=\"G\",\n            is_delete=True,\n        )\n\n        transform_funding_instrument.process_link_funding_instrument(\n            delete_but_current_missing, None, opportunity_summary\n        )\n\n        validate_funding_instrument(db_session, delete_but_current_missing, expect_in_db=False)\n        assert delete_but_current_missing.transformed_at is not None\n        assert delete_but_current_missing.transformation_notes == \"orphaned_delete_record\"\n\n    @pytest.mark.parametrize(\n        \"is_forecast,revision_number,legacy_lookup_value\",\n        [(True, None, \"X\"), (False, None, \"4\"), (True, 5, \"Y\"), (False, 10, \"A\")],\n    )\n    def test_process_funding_instrument_but_invalid_lookup_value(\n        self,\n        db_session,\n        transform_funding_instrument,\n        is_forecast,\n        revision_number,\n        legacy_lookup_value,\n    ):\n        opportunity_summary = f.OpportunitySummaryFactory.create(\n            is_forecast=is_forecast, revision_number=revision_number, no_link_values=True\n        )\n        insert_but_invalid_value = setup_funding_instrument(\n            create_existing=False,\n            opportunity_summary=opportunity_summary,\n            legacy_lookup_value=legacy_lookup_value,\n        )\n\n        with pytest.raises(ValueError, match=\"Unrecognized funding instrument\"):\n            transform_funding_instrument.process_link_funding_instrument(\n                insert_but_invalid_value, None, opportunity_summary\n            )\n\n    @pytest.mark.parametrize(\n        \"factory_cls\", [f.StagingTfundinstrForecastFactory, f.StagingTfundinstrSynopsisFactory]\n    )\n    def test_process_funding_instrument_but_no_opportunity_summary_non_hist(\n        self,\n        db_session,\n        transform_funding_instrument,\n        factory_cls,\n    ):\n        source_record = factory_cls.create(orphaned_record=True)\n\n        with pytest.raises(\n            ValueError,\n            match=\"Funding instrument record cannot be processed as the opportunity summary for it does not exist\",\n        ):\n            transform_funding_instrument.process_link_funding_instrument(source_record, None, None)\n\n    @pytest.mark.parametrize(\n        \"factory_cls\",\n        [f.StagingTfundinstrForecastHistFactory, f.StagingTfundinstrSynopsisHistFactory],\n    )\n    def test_process_funding_instrument_but_no_opportunity_summary_hist(\n        self,\n        db_session,\n        transform_funding_instrument,\n        factory_cls,\n    ):\n        source_record = factory_cls.create(orphaned_record=True, revision_number=12)\n        transform_funding_instrument.process_link_funding_instrument(source_record, None, None)\n        assert source_record.transformed_at is not None\n        assert source_record.transformation_notes == \"orphaned_historical_record\""}
{"path":"frontend/tests/e2e/index.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/index.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"File: api/tests/src/data_migration/transformation/subtask/test_transform_opportunity.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_opportunity.py\nSize: 7.63 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/e2e/opportunity.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/opportunity.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nfrom src.data_migration.transformation.subtask.transform_opportunity import TransformOpportunity\nfrom src.services.opportunity_attachments import attachment_util\nfrom src.util import file_util\nfrom tests.src.data_migration.transformation.conftest import (\n    BaseTransformTestClass,\n    setup_opportunity,\n    validate_opportunity,\n)\nfrom tests.src.db.models.factories import OpportunityAttachmentFactory, OpportunityFactory\n\n\nclass TestTransformOpportunity(BaseTransformTestClass):\n    @pytest.fixture()\n    def transform_opportunity(self, transform_oracle_data_task, truncate_staging_tables, s3_config):\n        return TransformOpportunity(transform_oracle_data_task, s3_config)\n\n    def test_process_opportunities(self, db_session, transform_opportunity):\n        ordinary_delete = setup_opportunity(\n            create_existing=True, is_delete=True, all_fields_null=True\n        )\n        ordinary_delete2 = setup_opportunity(\n            create_existing=True, is_delete=True, all_fields_null=False\n        )\n        delete_but_current_missing = setup_opportunity(create_existing=False, is_delete=True)\n\n        basic_insert = setup_opportunity(create_existing=False)\n        basic_insert2 = setup_opportunity(create_existing=False, all_fields_null=True)\n        basic_insert3 = setup_opportunity(create_existing=False)\n\n        basic_update = setup_opportunity(\n            create_existing=True,\n        )\n        basic_update2 = setup_opportunity(create_existing=True, all_fields_null=True)\n        basic_update3 = setup_opportunity(create_existing=True, all_fields_null=True)\n        basic_update4 = setup_opportunity(create_existing=True)\n\n        # Something else deleted it\n        already_processed_insert = setup_opportunity(\n            create_existing=False, is_already_processed=True\n        )\n        already_processed_update = setup_opportunity(\n            create_existing=True, is_already_processed=True\n        )\n\n        insert_that_will_fail = setup_opportunity(\n            create_existing=False, source_values={\"oppcategory\": \"X\"}\n        )\n\n        transform_opportunity.run_subtask()\n\n        validate_opportunity(db_session, ordinary_delete, expect_in_db=False)\n        validate_opportunity(db_session, ordinary_delete2, expect_in_db=False)\n        validate_opportunity(db_session, delete_but_current_missing, expect_in_db=False)\n\n        validate_opportunity(db_session, basic_insert)\n        validate_opportunity(db_session, basic_insert2)\n        validate_opportunity(db_session, basic_insert3)\n\n        validate_opportunity(db_session, basic_update)\n        validate_opportunity(db_session, basic_update2)\n        validate_opportunity(db_session, basic_update3)\n        validate_opportunity(db_session, basic_update4)\n\n        validate_opportunity(db_session, already_processed_insert, expect_in_db=False)\n        validate_opportunity(db_session, already_processed_update, expect_values_to_match=False)\n\n        validate_opportunity(db_session, insert_that_will_fail, expect_in_db=False)\n\n        metrics = transform_opportunity.metrics\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 11\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 2\n        # Note this insert counts the case where the category fails\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 3\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_ERROR_COUNT] == 1\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n        # Rerunning does mostly nothing, it will attempt to re-process the one that errored\n        # but otherwise won't find anything else\n        db_session.commit()  # commit to end any existing transactions as run_subtask starts a new one\n        transform_opportunity.run_subtask()\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 12\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 2\n        # Note this insert counts the case where the category fails\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 3\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_ERROR_COUNT] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n    def test_process_opportunity_invalid_category(self, db_session, transform_opportunity):\n        # This will error in the transform as that isn't a category we have configured\n        insert_that_will_fail = setup_opportunity(\n            create_existing=False, source_values={\"oppcategory\": \"X\"}\n        )\n\n        with pytest.raises(ValueError, match=\"Unrecognized opportunity category\"):\n            transform_opportunity.process_opportunity(insert_that_will_fail, None)\n\n        validate_opportunity(db_session, insert_that_will_fail, expect_in_db=False)\n\n    def test_process_opportunity_delete_with_attachments(\n        self, db_session, transform_opportunity, s3_config\n    ):\n\n        source_opportunity = setup_opportunity(create_existing=False, is_delete=True)\n\n        target_opportunity = OpportunityFactory.create(\n            opportunity_id=source_opportunity.opportunity_id, opportunity_attachments=[]\n        )\n\n        attachments = []\n        for i in range(10):\n            s3_path = attachment_util.get_s3_attachment_path(\n                f\"my_file{i}.txt\", i, target_opportunity, s3_config\n            )\n\n            with file_util.open_stream(s3_path, \"w\") as outfile:\n                outfile.write(f\"This is the {i}th file\")\n\n            attachment = OpportunityAttachmentFactory.create(\n                opportunity=target_opportunity, file_location=s3_path\n            )\n            attachments.append(attachment)\n\n        transform_opportunity.process_opportunity(source_opportunity, target_opportunity)\n\n        validate_opportunity(db_session, source_opportunity, expect_in_db=False)\n\n        # Verify all of the files were deleted\n        for attachment in attachments:\n            assert file_util.file_exists(attachment.file_location) is False\n\n    def test_process_opportunity_update_to_non_draft_with_attachments(\n        self, db_session, transform_opportunity, s3_config\n    ):\n\n        source_opportunity = setup_opportunity(\n            create_existing=False, source_values={\"is_draft\": \"N\"}\n        )\n\n        target_opportunity = OpportunityFactory.create(\n            opportunity_id=source_opportunity.opportunity_id,\n            is_draft=True,\n            opportunity_attachments=[],\n        )\n\n        attachments = []\n        for i in range(10):\n            s3_path = attachment_util.get_s3_attachment_path(\n                f\"my_file{i}.txt\", i, target_opportunity, s3_config\n            )\n            assert s3_path.startswith(s3_config.draft_files_bucket_path) is True\n\n            with file_util.open_stream(s3_path, \"w\") as outfile:\n                outfile.write(f\"This is the {i}th file\")\n\n            attachment = OpportunityAttachmentFactory.create(\n                opportunity=target_opportunity, file_location=s3_path\n            )\n            attachments.append(attachment)\n\n        transform_opportunity.process_opportunity(source_opportunity, target_opportunity)\n\n        validate_opportunity(db_session, source_opportunity)\n\n        # Verify all of the files were moved to the public bucket\n        for attachment in attachments:\n            assert attachment.file_location.startswith(s3_config.public_files_bucket_path) is True\n            assert file_util.file_exists(attachment.file_location) is True"}
{"path":"frontend/tests/e2e/playwrightUtils.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/playwrightUtils.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"File: api/tests/src/data_migration/transformation/subtask/test_transform_opportunity_attachment.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_opportunity_attachment.py\nSize: 10.10 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/e2e/process.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/process.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"import tests.src.db.models.factories as f\nfrom src.data_migration.transformation import transform_constants\nfrom src.data_migration.transformation.subtask.transform_opportunity_attachment import (\n    TransformOpportunityAttachment,\n)\nfrom src.services.opportunity_attachments import attachment_util\nfrom src.util import file_util\nfrom tests.src.data_migration.transformation.conftest import (\n    BaseTransformTestClass,\n    setup_opportunity_attachment,\n    validate_opportunity_attachment,\n)\n\n\nclass TestTransformOpportunitySummary(BaseTransformTestClass):\n\n    @pytest.fixture()\n    def transform_opportunity_attachment(self, transform_oracle_data_task, s3_config):\n        return TransformOpportunityAttachment(transform_oracle_data_task, s3_config)\n\n    def test_transform_opportunity_attachment(\n        self, db_session, transform_opportunity_attachment, s3_config\n    ):\n\n        opportunity1 = f.OpportunityFactory.create(opportunity_attachments=[])\n\n        insert1 = setup_opportunity_attachment(\n            create_existing=False, opportunity=opportunity1, config=s3_config\n        )\n        insert2 = setup_opportunity_attachment(\n            create_existing=False, opportunity=opportunity1, config=s3_config\n        )\n\n        update1 = setup_opportunity_attachment(\n            create_existing=True, opportunity=opportunity1, config=s3_config\n        )\n        update2 = setup_opportunity_attachment(\n            create_existing=True, opportunity=opportunity1, config=s3_config\n        )\n\n        delete1 = setup_opportunity_attachment(\n            create_existing=True,\n            is_delete=True,\n            opportunity=opportunity1,\n            config=s3_config,\n        )\n\n        opportunity2 = f.OpportunityFactory.create(opportunity_attachments=[])\n\n        insert3 = setup_opportunity_attachment(\n            create_existing=False, opportunity=opportunity2, config=s3_config\n        )\n        update3 = setup_opportunity_attachment(\n            create_existing=True, opportunity=opportunity2, config=s3_config\n        )\n        delete2 = setup_opportunity_attachment(\n            create_existing=True,\n            is_delete=True,\n            opportunity=opportunity2,\n            config=s3_config,\n        )\n\n        already_processed_insert = setup_opportunity_attachment(\n            create_existing=False,\n            opportunity=opportunity2,\n            is_already_processed=True,\n            config=s3_config,\n        )\n        already_processed_update = setup_opportunity_attachment(\n            create_existing=True,\n            opportunity=opportunity2,\n            is_already_processed=True,\n            config=s3_config,\n        )\n\n        delete_but_current_missing = setup_opportunity_attachment(\n            create_existing=False,\n            opportunity=opportunity2,\n            is_delete=True,\n            config=s3_config,\n        )\n\n        # Draft opportunity\n        opportunity3 = f.OpportunityFactory.create(is_draft=True, opportunity_attachments=[])\n        insert4 = setup_opportunity_attachment(\n            create_existing=False, opportunity=opportunity3, config=s3_config\n        )\n        update4 = setup_opportunity_attachment(\n            create_existing=True, opportunity=opportunity3, config=s3_config\n        )\n        delete3 = setup_opportunity_attachment(\n            create_existing=True,\n            is_delete=True,\n            opportunity=opportunity3,\n            config=s3_config,\n        )\n\n        transform_opportunity_attachment.run_subtask()\n\n        validate_opportunity_attachment(db_session, insert1)\n        validate_opportunity_attachment(db_session, insert2)\n        validate_opportunity_attachment(db_session, insert3)\n        validate_opportunity_attachment(db_session, insert4)\n\n        validate_opportunity_attachment(db_session, update1)\n        validate_opportunity_attachment(db_session, update2)\n        validate_opportunity_attachment(db_session, update3)\n        validate_opportunity_attachment(db_session, update4)\n\n        validate_opportunity_attachment(db_session, delete1, expect_in_db=False)\n        validate_opportunity_attachment(db_session, delete2, expect_in_db=False)\n        validate_opportunity_attachment(db_session, delete3, expect_in_db=False)\n\n        validate_opportunity_attachment(db_session, already_processed_insert, expect_in_db=False)\n        validate_opportunity_attachment(\n            db_session, already_processed_update, expect_values_to_match=False\n        )\n\n        validate_opportunity_attachment(db_session, delete_but_current_missing, expect_in_db=False)\n\n        metrics = transform_opportunity_attachment.metrics\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 12\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 3\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n        db_session.commit()  # commit to end any existing transactions as run_subtask starts a new one\n        transform_opportunity_attachment.run_subtask()\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 12\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 3\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 4\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n    def test_transform_opportunity_attachment_delete_but_current_missing(\n        self, db_session, transform_opportunity_attachment, s3_config\n    ):\n        opportunity = f.OpportunityFactory.create(opportunity_attachments=[])\n        delete_but_current_missing = setup_opportunity_attachment(\n            create_existing=False,\n            opportunity=opportunity,\n            is_delete=True,\n            config=s3_config,\n        )\n\n        transform_opportunity_attachment.process_opportunity_attachment(\n            delete_but_current_missing, None, opportunity\n        )\n\n        validate_opportunity_attachment(db_session, delete_but_current_missing, expect_in_db=False)\n        assert delete_but_current_missing.transformed_at is not None\n        assert delete_but_current_missing.transformation_notes == \"orphaned_delete_record\"\n\n    def test_transform_opportunity_attachment_no_opportunity(\n        self, db_session, transform_opportunity_attachment, s3_config\n    ):\n        opportunity = f.OpportunityFactory.create(opportunity_attachments=[])\n        insert = setup_opportunity_attachment(\n            create_existing=False, opportunity=opportunity, config=s3_config\n        )\n\n        # Don't pass the opportunity in - as if it wasn't found\n        with pytest.raises(\n            ValueError,\n            match=\"Opportunity attachment cannot be processed as the opportunity for it does not exist\",\n        ):\n            transform_opportunity_attachment.process_opportunity_attachment(insert, None, None)\n\n        assert insert.transformed_at is None\n\n    def test_transform_opportunity_attachment_update_file_renamed(\n        self, db_session, transform_opportunity_attachment, s3_config\n    ):\n        opportunity = f.OpportunityFactory.create(is_draft=False, opportunity_attachments=[])\n        update = setup_opportunity_attachment(\n            # Don't create the existing, we'll do that below\n            create_existing=False,\n            opportunity=opportunity,\n            config=s3_config,\n        )\n\n        old_s3_path = attachment_util.get_s3_attachment_path(\n            \"old_file_name.txt\", update.syn_att_id, opportunity, s3_config\n        )\n\n        with file_util.open_stream(old_s3_path, \"w\") as outfile:\n            outfile.write(f.fake.sentence(25))\n\n        target_attachment = f.OpportunityAttachmentFactory.create(\n            attachment_id=update.syn_att_id,\n            opportunity=opportunity,\n            file_location=old_s3_path,\n            file_name=\"old_file_name.txt\",\n        )\n\n        transform_opportunity_attachment.process_opportunity_attachment(\n            update, target_attachment, opportunity\n        )\n\n        validate_opportunity_attachment(db_session, update)\n\n        # Verify the old file name was deleted\n        assert file_util.file_exists(old_s3_path) is False\n\n    def test_transform_opportunity_attachment_delete_file_missing_on_s3(\n        self, db_session, transform_opportunity_attachment, s3_config\n    ):\n        opportunity = f.OpportunityFactory.create(opportunity_attachments=[])\n\n        synopsis_attachment = f.StagingTsynopsisAttachmentFactory.create(\n            opportunity=None,\n            opportunity_id=opportunity.opportunity_id,\n            is_deleted=True,\n        )\n\n        # Make a realistic path, but don't actually create the file\n        s3_path = attachment_util.get_s3_attachment_path(\n            synopsis_attachment.file_name, synopsis_attachment.syn_att_id, opportunity, s3_config\n        )\n\n        target_attachment = f.OpportunityAttachmentFactory.create(\n            attachment_id=synopsis_attachment.syn_att_id,\n            opportunity=opportunity,\n            file_location=s3_path,\n        )\n\n        # This won't error, s3 delete object doesn't error if the object doesn't exist\n        transform_opportunity_attachment.process_opportunity_attachment(\n            synopsis_attachment, target_attachment, opportunity\n        )\n\n        validate_opportunity_attachment(db_session, synopsis_attachment, expect_in_db=False)\n\n    def test_transform_opportunity_attachment_null_file_lob(\n        self, db_session, transform_opportunity_attachment, s3_config\n    ):\n        opportunity = f.OpportunityFactory.create(opportunity_attachments=[])\n        insert = setup_opportunity_attachment(\n            create_existing=False,\n            opportunity=opportunity,\n            config=s3_config,\n            source_values={\"file_lob\": None},\n        )\n\n        with pytest.raises(ValueError, match=\"Attachment is null, cannot copy\"):\n            transform_opportunity_attachment.process_opportunity_attachment(\n                insert, None, opportunity\n            )\n\n        assert insert.transformed_at is None"}
{"path":"frontend/tests/e2e/research.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/research.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"File: api/tests/src/data_migration/transformation/subtask/test_transform_opportunity_summary.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation/subtask\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/subtask/test_transform_opportunity_summary.py\nSize: 11.90 KB\nLast Modified: 2025-02-14T17:08:26.460Z"}
{"path":"frontend/tests/e2e/saved-grants.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/saved-grants.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"import src.data_migration.transformation.transform_constants as transform_constants\nimport tests.src.db.models.factories as f\nfrom src.data_migration.transformation.subtask.transform_opportunity_summary import (\n    TransformOpportunitySummary,\n)\nfrom tests.src.data_migration.transformation.conftest import (\n    BaseTransformTestClass,\n    setup_synopsis_forecast,\n    validate_opportunity_summary,\n)\n\n\nclass TestTransformOpportunitySummary(BaseTransformTestClass):\n    @pytest.fixture()\n    def transform_opportunity_summary(self, transform_oracle_data_task):\n        return TransformOpportunitySummary(transform_oracle_data_task)\n\n    def test_process_opportunity_summaries(self, db_session, transform_opportunity_summary):\n        # Basic inserts\n        opportunity1 = f.OpportunityFactory.create(\n            no_current_summary=True, opportunity_assistance_listings=[]\n        )\n        forecast_insert1 = setup_synopsis_forecast(\n            is_forecast=True, revision_number=None, create_existing=False, opportunity=opportunity1\n        )\n        synopsis_insert1 = setup_synopsis_forecast(\n            is_forecast=False, revision_number=None, create_existing=False, opportunity=opportunity1\n        )\n        forecast_hist_insert1 = setup_synopsis_forecast(\n            is_forecast=True, revision_number=1, create_existing=False, opportunity=opportunity1\n        )\n        synopsis_hist_insert1 = setup_synopsis_forecast(\n            is_forecast=False, revision_number=1, create_existing=False, opportunity=opportunity1\n        )\n\n        # Mix of updates and inserts, somewhat resembling what happens when summary objects\n        # get moved to the historical table (we'd update the synopsis/forecast records, and create new historical)\n        opportunity2 = f.OpportunityFactory.create(\n            no_current_summary=True, opportunity_assistance_listings=[]\n        )\n        forecast_update1 = setup_synopsis_forecast(\n            is_forecast=True, revision_number=None, create_existing=True, opportunity=opportunity2\n        )\n        synopsis_update1 = setup_synopsis_forecast(\n            is_forecast=False, revision_number=None, create_existing=True, opportunity=opportunity2\n        )\n        forecast_hist_update1 = setup_synopsis_forecast(\n            is_forecast=True, revision_number=1, create_existing=True, opportunity=opportunity2\n        )\n        synopsis_hist_update1 = setup_synopsis_forecast(\n            is_forecast=False, revision_number=1, create_existing=True, opportunity=opportunity2\n        )\n        forecast_hist_insert2 = setup_synopsis_forecast(\n            is_forecast=True, revision_number=2, create_existing=False, opportunity=opportunity2\n        )\n        synopsis_hist_insert2 = setup_synopsis_forecast(\n            is_forecast=False, revision_number=2, create_existing=False, opportunity=opportunity2\n        )\n\n        # Mix of inserts, updates, and deletes\n        opportunity3 = f.OpportunityFactory.create(\n            no_current_summary=True, opportunity_assistance_listings=[]\n        )\n        forecast_delete1 = setup_synopsis_forecast(\n            is_forecast=True,\n            revision_number=None,\n            create_existing=True,\n            is_delete=True,\n            opportunity=opportunity3,\n            is_existing_current_opportunity_summary=True,\n        )\n        synopsis_delete1 = setup_synopsis_forecast(\n            is_forecast=False,\n            revision_number=None,\n            create_existing=True,\n            is_delete=True,\n            opportunity=opportunity3,\n        )\n        forecast_hist_insert3 = setup_synopsis_forecast(\n            is_forecast=True, revision_number=2, create_existing=False, opportunity=opportunity3\n        )\n        synopsis_hist_update2 = setup_synopsis_forecast(\n            is_forecast=False,\n            revision_number=1,\n            create_existing=True,\n            source_values={\"action_type\": \"D\"},\n            opportunity=opportunity3,\n        )\n\n        # A few error scenarios\n        opportunity4 = f.OpportunityFactory.create(\n            no_current_summary=True, opportunity_assistance_listings=[]\n        )\n        forecast_delete_but_current_missing = setup_synopsis_forecast(\n            is_forecast=True,\n            revision_number=None,\n            create_existing=False,\n            is_delete=True,\n            opportunity=opportunity4,\n            is_existing_current_opportunity_summary=True,\n        )\n        synopsis_update_invalid_yn_field = setup_synopsis_forecast(\n            is_forecast=False,\n            revision_number=None,\n            create_existing=True,\n            source_values={\"sendmail\": \"E\"},\n            opportunity=opportunity4,\n        )\n        synopsis_hist_insert_invalid_yn_field = setup_synopsis_forecast(\n            is_forecast=False,\n            revision_number=1,\n            create_existing=False,\n            source_values={\"cost_sharing\": \"1\"},\n            opportunity=opportunity4,\n        )\n        forecast_hist_update_invalid_action_type = setup_synopsis_forecast(\n            is_forecast=True,\n            revision_number=2,\n            create_existing=True,\n            source_values={\"action_type\": \"X\"},\n            opportunity=opportunity4,\n        )\n\n        transform_opportunity_summary.run_subtask()\n\n        validate_opportunity_summary(db_session, forecast_insert1)\n        validate_opportunity_summary(db_session, synopsis_insert1)\n        validate_opportunity_summary(db_session, forecast_hist_insert1)\n        validate_opportunity_summary(db_session, synopsis_hist_insert1)\n        validate_opportunity_summary(db_session, forecast_hist_insert2)\n        validate_opportunity_summary(db_session, synopsis_hist_insert2)\n        validate_opportunity_summary(db_session, forecast_hist_insert3)\n\n        validate_opportunity_summary(db_session, forecast_update1)\n        validate_opportunity_summary(db_session, synopsis_update1)\n        validate_opportunity_summary(db_session, forecast_hist_update1)\n        validate_opportunity_summary(db_session, synopsis_hist_update1)\n        validate_opportunity_summary(db_session, synopsis_hist_update2)\n\n        validate_opportunity_summary(db_session, forecast_delete1, expect_in_db=False)\n        validate_opportunity_summary(db_session, synopsis_delete1, expect_in_db=False)\n\n        validate_opportunity_summary(\n            db_session, forecast_delete_but_current_missing, expect_in_db=False\n        )\n        validate_opportunity_summary(\n            db_session,\n            synopsis_update_invalid_yn_field,\n            expect_in_db=True,\n            expect_values_to_match=False,\n        )\n        validate_opportunity_summary(\n            db_session, synopsis_hist_insert_invalid_yn_field, expect_in_db=False\n        )\n        validate_opportunity_summary(\n            db_session,\n            forecast_hist_update_invalid_action_type,\n            expect_in_db=True,\n            expect_values_to_match=False,\n        )\n\n        metrics = transform_opportunity_summary.metrics\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 18\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 7\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 5\n        assert metrics[transform_constants.Metrics.TOTAL_ERROR_COUNT] == 3\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n        # Rerunning will only attempt to re-process the errors, so total+errors goes up by 3\n        db_session.commit()  # commit to end any existing transactions as run_subtask starts a new one\n        transform_opportunity_summary.run_subtask()\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_PROCESSED] == 21\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_DELETED] == 2\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_INSERTED] == 7\n        assert metrics[transform_constants.Metrics.TOTAL_RECORDS_UPDATED] == 5\n        assert metrics[transform_constants.Metrics.TOTAL_ERROR_COUNT] == 6\n        assert metrics[transform_constants.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED] == 1\n\n    @pytest.mark.parametrize(\n        \"is_forecast,revision_number\", [(True, None), (False, None), (True, 5), (False, 10)]\n    )\n    def test_process_opportunity_summary_delete_but_current_missing(\n        self, db_session, transform_opportunity_summary, is_forecast, revision_number\n    ):\n        opportunity = f.OpportunityFactory.create(\n            no_current_summary=True, opportunity_assistance_listings=[]\n        )\n        delete_but_current_missing = setup_synopsis_forecast(\n            is_forecast=is_forecast,\n            revision_number=revision_number,\n            create_existing=False,\n            is_delete=True,\n            opportunity=opportunity,\n        )\n\n        transform_opportunity_summary.process_opportunity_summary(\n            delete_but_current_missing, None, opportunity\n        )\n\n        validate_opportunity_summary(db_session, delete_but_current_missing, expect_in_db=False)\n        assert delete_but_current_missing.transformed_at is not None\n        assert delete_but_current_missing.transformation_notes == \"orphaned_delete_record\"\n\n    @pytest.mark.parametrize(\n        \"is_forecast,revision_number,source_values,expected_error\",\n        [\n            (True, None, {\"sendmail\": \"z\"}, \"Unexpected Y/N bool value: z\"),\n            (False, None, {\"cost_sharing\": \"v\"}, \"Unexpected Y/N bool value: v\"),\n            (True, 5, {\"action_type\": \"T\"}, \"Unexpected action type value: T\"),\n            (False, 10, {\"action_type\": \"5\"}, \"Unexpected action type value: 5\"),\n        ],\n    )\n    def test_process_opportunity_summary_invalid_value_errors(\n        self,\n        db_session,\n        transform_opportunity_summary,\n        is_forecast,\n        revision_number,\n        source_values,\n        expected_error,\n    ):\n        opportunity = f.OpportunityFactory.create(\n            no_current_summary=True, opportunity_assistance_listings=[]\n        )\n        source_summary = setup_synopsis_forecast(\n            is_forecast=is_forecast,\n            revision_number=revision_number,\n            create_existing=False,\n            opportunity=opportunity,\n            source_values=source_values,\n        )\n\n        with pytest.raises(ValueError, match=expected_error):\n            transform_opportunity_summary.process_opportunity_summary(\n                source_summary, None, opportunity\n            )\n\n    @pytest.mark.parametrize(\"is_forecast\", [True, False])\n    def test_process_opportunity_summary_but_no_opportunity_non_hist(\n        self,\n        db_session,\n        transform_opportunity_summary,\n        is_forecast,\n    ):\n        source_record = setup_synopsis_forecast(\n            is_forecast=is_forecast,\n            revision_number=None,\n            create_existing=False,\n            opportunity=None,\n            source_values={\"opportunity_id\": 12121212},\n        )\n\n        with pytest.raises(\n            ValueError,\n            match=\"Opportunity summary cannot be processed as the opportunity for it does not exist\",\n        ):\n            transform_opportunity_summary.process_opportunity_summary(source_record, None, None)\n\n    @pytest.mark.parametrize(\"is_forecast,revision_number\", [(True, 10), (False, 9)])\n    def test_process_opportunity_summary_but_no_opportunity_hist(\n        self,\n        db_session,\n        transform_opportunity_summary,\n        is_forecast,\n        revision_number,\n    ):\n        source_record = setup_synopsis_forecast(\n            is_forecast=is_forecast,\n            revision_number=revision_number,\n            create_existing=False,\n            opportunity=None,\n            source_values={\"opportunity_id\": 12121212},\n        )\n\n        transform_opportunity_summary.process_opportunity_summary(source_record, None, None)\n\n        validate_opportunity_summary(db_session, source_record, expect_in_db=False)\n        assert source_record.transformed_at is not None\n        assert source_record.transformation_notes == \"orphaned_historical_record\""}
{"path":"frontend/tests/e2e/search/search-download.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/search/search-download.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"File: api/tests/src/data_migration/transformation/test_transform_oracle_data_task.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/test_transform_oracle_data_task.py\nSize: 25.80 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/e2e/search/search-error.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/search/search-error.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"import tests.src.db.models.factories as f\nfrom src.constants.lookup_constants import ApplicantType, FundingCategory, FundingInstrument\nfrom src.data_migration.transformation.transform_oracle_data_task import TransformOracleDataTask\nfrom src.db.models import staging\nfrom src.db.models.opportunity_models import Opportunity\nfrom tests.conftest import BaseTestClass\nfrom tests.src.data_migration.transformation.conftest import (\n    get_summary_from_source,\n    setup_agency,\n    setup_cfda,\n    setup_opportunity,\n    setup_synopsis_forecast,\n    validate_agency,\n    validate_applicant_type,\n    validate_assistance_listing,\n    validate_funding_category,\n    validate_funding_instrument,\n    validate_opportunity,\n    validate_opportunity_summary,\n    validate_summary_and_nested,\n)\n\n\nclass TestTransformFullRunTask(BaseTestClass):\n    # The above tests validated we could run the tests\n\n    @pytest.fixture()\n    def truncate_all_staging_tables(self, db_session):\n        # Iterate over all the staging tables and truncate them to avoid\n        # any collisions with prior test data. There are no foreign keys\n        # between these tables, so the order doesn't matter here.\n        for table in staging.metadata.tables.values():\n            db_session.query(table).delete()\n\n    @pytest.fixture()\n    def transform_oracle_data_task(\n        self,\n        db_session,\n        enable_factory_create,\n        truncate_opportunities,\n        truncate_all_staging_tables,\n        s3_config,\n    ) -> TransformOracleDataTask:\n        return TransformOracleDataTask(db_session)\n\n    def test_all_inserts(self, db_session, transform_oracle_data_task):\n        # Test that we're fully capable of processing inserts across an entire opportunity record\n        parent_agency = setup_agency(\"INSERTAGENCY\", create_existing=False)\n        subagency = setup_agency(\"INSERTAGENCY-ABC\", create_existing=False)\n\n        opportunity = setup_opportunity(\n            create_existing=False, source_values={\"owningagency\": \"INSERTAGENCY-ABC\"}\n        )\n\n        cfda1 = setup_cfda(create_existing=False, opportunity=opportunity)\n        cfda2 = setup_cfda(create_existing=False, opportunity=opportunity)\n\n        ### Forecast\n        forecast = setup_synopsis_forecast(\n            create_existing=False, is_forecast=True, revision_number=None, opportunity=opportunity\n        )\n        f.StagingTapplicanttypesForecastFactory(forecast=forecast, at_id=\"01\")\n        # This is a duplicate record (same at_id, but will have a different at_frcst_id), verifying we handle duplicates\n        f.StagingTapplicanttypesForecastFactory(forecast=forecast, at_id=\"01\")\n        f.StagingTfundactcatForecastFactory(forecast=forecast, fac_id=\"RA\")\n        f.StagingTfundactcatForecastFactory(forecast=forecast, fac_id=\"HO\")\n        # Duplicate here too\n        f.StagingTfundactcatForecastFactory(forecast=forecast, fac_id=\"HO\")\n\n        f.StagingTfundinstrForecastFactory(forecast=forecast, fi_id=\"CA\")\n        f.StagingTfundinstrForecastFactory(forecast=forecast, fi_id=\"G\")\n        # Duplicate here as well\n        f.StagingTfundinstrForecastFactory(forecast=forecast, fi_id=\"G\")\n\n        ### Synopsis (has some invalid values)\n        synopsis = setup_synopsis_forecast(\n            create_existing=False, is_forecast=False, revision_number=None, opportunity=opportunity\n        )\n        f.StagingTapplicanttypesSynopsisFactory(synopsis=synopsis, at_id=\"06\")\n        f.StagingTapplicanttypesSynopsisFactory(synopsis=synopsis, at_id=\"07\")\n        f.StagingTapplicanttypesSynopsisFactory(synopsis=synopsis, at_id=\"11\")\n        # Invalid value\n        f.StagingTapplicanttypesSynopsisFactory(synopsis=synopsis, at_id=\"x\")\n        f.StagingTfundactcatSynopsisFactory(synopsis=synopsis, fac_id=\"ACA\")\n        f.StagingTfundactcatSynopsisFactory(synopsis=synopsis, fac_id=\"O\")\n        # Invalid value\n        f.StagingTfundactcatSynopsisFactory(synopsis=synopsis, fac_id=\"BOB\")\n        f.StagingTfundinstrSynopsisFactory(synopsis=synopsis, fi_id=\"G\")\n        # Invalid value\n        f.StagingTfundinstrSynopsisFactory(synopsis=synopsis, fi_id=\"x\")\n\n        transform_oracle_data_task.run()\n\n        created_opportunity: Opportunity = (\n            db_session.query(Opportunity)\n            .filter(Opportunity.opportunity_id == opportunity.opportunity_id)\n            .one_or_none()\n        )\n\n        # Validate that all of the expected records were created\n        # not worrying about all of the transforms specifically here,\n        # just that everything is in place\n\n        assert created_opportunity is not None\n        validate_opportunity(db_session, opportunity)\n        assert {\n            al.opportunity_assistance_listing_id\n            for al in created_opportunity.opportunity_assistance_listings\n        } == {cfda1.opp_cfda_id, cfda2.opp_cfda_id}\n\n        assert len(created_opportunity.all_opportunity_summaries) == 2\n\n        created_forecast = get_summary_from_source(db_session, forecast)\n        assert created_forecast is not None\n        validate_summary_and_nested(\n            db_session,\n            forecast,\n            [ApplicantType.COUNTY_GOVERNMENTS],\n            [FundingCategory.RECOVERY_ACT, FundingCategory.HOUSING],\n            [FundingInstrument.GRANT, FundingInstrument.COOPERATIVE_AGREEMENT],\n        )\n        validate_summary_and_nested(\n            db_session,\n            synopsis,\n            [\n                ApplicantType.PUBLIC_AND_STATE_INSTITUTIONS_OF_HIGHER_EDUCATION,\n                ApplicantType.FEDERALLY_RECOGNIZED_NATIVE_AMERICAN_TRIBAL_GOVERNMENTS,\n                ApplicantType.OTHER_NATIVE_AMERICAN_TRIBAL_ORGANIZATIONS,\n            ],\n            [FundingCategory.AFFORDABLE_CARE_ACT, FundingCategory.OTHER],\n            [FundingInstrument.GRANT],\n        )\n\n        validate_agency(db_session, parent_agency)\n        validate_agency(db_session, subagency)\n\n        assert {\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_PROCESSED: 24,\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_INSERTED: 18,\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_UPDATED: 0,\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_DELETED: 0,\n            transform_oracle_data_task.Metrics.TOTAL_DUPLICATE_RECORDS_SKIPPED: 3,\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_ORPHANED: 0,\n            transform_oracle_data_task.Metrics.TOTAL_ERROR_COUNT: 3,\n        }.items() <= transform_oracle_data_task.metrics.items()\n\n    def test_mix_of_inserts_updates_deletes(self, db_session, transform_oracle_data_task):\n        parent_agency = setup_agency(\"UPDATEAGENCY\", create_existing=True)\n        subagency = setup_agency(\n            \"UPDATEAGENCY-XYZ\",\n            create_existing=True,\n            deleted_fields={\"AgencyContactEMail2\", \"ldapGp\", \"description\"},\n        )\n\n        existing_opportunity = f.OpportunityFactory(\n            no_current_summary=True, opportunity_assistance_listings=[], agency_code=\"UPDATEAGENCY\"\n        )\n        opportunity = f.StagingTopportunityFactory(\n            opportunity_id=existing_opportunity.opportunity_id, cfdas=[]\n        )\n\n        cfda_insert = setup_cfda(create_existing=False, opportunity=existing_opportunity)\n        cfda_update = setup_cfda(create_existing=True, opportunity=existing_opportunity)\n        setup_cfda(create_existing=True, is_delete=True, opportunity=existing_opportunity)\n\n        ### Forecast (update)\n        summary_forecast = f.OpportunitySummaryFactory(\n            is_forecast=True, opportunity=existing_opportunity, no_link_values=True\n        )\n        forecast_update = f.StagingTforecastFactory(opportunity=opportunity)\n\n        ## Forecast applicant type\n        # insert\n        f.StagingTapplicanttypesForecastFactory(forecast=forecast_update, at_id=\"01\")\n        # update\n        f.StagingTapplicanttypesForecastFactory(\n            forecast=forecast_update, at_id=\"02\", at_frcst_id=1000\n        )\n        f.LinkOpportunitySummaryApplicantTypeFactory(\n            opportunity_summary=summary_forecast,\n            applicant_type=ApplicantType.CITY_OR_TOWNSHIP_GOVERNMENTS,\n            legacy_applicant_type_id=1000,\n        )\n        # delete\n        f.StagingTapplicanttypesForecastFactory(\n            forecast=forecast_update, at_id=\"04\", at_frcst_id=1001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryApplicantTypeFactory(\n            opportunity_summary=summary_forecast,\n            applicant_type=ApplicantType.SPECIAL_DISTRICT_GOVERNMENTS,\n            legacy_applicant_type_id=1001,\n        )\n\n        ## Forecast funding category\n        # insert\n        f.StagingTfundactcatForecastFactory(forecast=forecast_update, fac_id=\"OZ\")\n        # update\n        f.StagingTfundactcatForecastFactory(\n            forecast=forecast_update, fac_id=\"NR\", fac_frcst_id=2000\n        )\n        f.LinkOpportunitySummaryFundingCategoryFactory(\n            opportunity_summary=summary_forecast,\n            funding_category=FundingCategory.NATURAL_RESOURCES,\n            legacy_funding_category_id=2000,\n        )\n        # delete\n        f.StagingTfundactcatForecastFactory(\n            forecast=forecast_update, fac_id=\"ST\", fac_frcst_id=2001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryFundingCategoryFactory(\n            opportunity_summary=summary_forecast,\n            funding_category=FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT,\n            legacy_funding_category_id=2001,\n        )\n\n        ## Forecast funding instrument\n        # insert\n        f.StagingTfundinstrForecastFactory(forecast=forecast_update, fi_id=\"G\")\n        # update\n        f.StagingTfundinstrForecastFactory(forecast=forecast_update, fi_id=\"CA\", fi_frcst_id=3000)\n        f.LinkOpportunitySummaryFundingInstrumentFactory(\n            opportunity_summary=summary_forecast,\n            funding_instrument=FundingInstrument.COOPERATIVE_AGREEMENT,\n            legacy_funding_instrument_id=3000,\n        )\n        # delete\n        f.StagingTfundinstrForecastFactory(\n            forecast=forecast_update, fi_id=\"O\", fi_frcst_id=3001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryFundingInstrumentFactory(\n            opportunity_summary=summary_forecast,\n            funding_instrument=FundingInstrument.OTHER,\n            legacy_funding_instrument_id=3001,\n        )\n\n        ### Synopsis (not modified as the update was already processed)\n        summary_synopsis = f.OpportunitySummaryFactory(\n            is_forecast=False, opportunity=existing_opportunity, no_link_values=True\n        )\n        synopsis_already_processed = f.StagingTsynopsisFactory(\n            opportunity=opportunity, already_transformed=True\n        )\n\n        ## Synopsis applicant type (many duplicates)\n        # insert\n        f.StagingTapplicanttypesSynopsisFactory(synopsis=synopsis_already_processed, at_id=\"99\")\n        f.StagingTapplicanttypesSynopsisFactory(synopsis=synopsis_already_processed, at_id=\"99\")\n        f.StagingTapplicanttypesSynopsisFactory(synopsis=synopsis_already_processed, at_id=\"99\")\n        # update\n        f.StagingTapplicanttypesSynopsisFactory(\n            synopsis=synopsis_already_processed, at_id=\"07\", at_syn_id=1000\n        )\n        f.StagingTapplicanttypesSynopsisFactory(\n            synopsis=synopsis_already_processed, at_id=\"07\", at_syn_id=11000\n        )\n        f.LinkOpportunitySummaryApplicantTypeFactory(\n            opportunity_summary=summary_synopsis,\n            applicant_type=ApplicantType.FEDERALLY_RECOGNIZED_NATIVE_AMERICAN_TRIBAL_GOVERNMENTS,\n            legacy_applicant_type_id=1000,\n        )\n        # delete\n        f.StagingTapplicanttypesSynopsisFactory(\n            synopsis=synopsis_already_processed, at_id=\"21\", at_syn_id=1001, is_deleted=True\n        )\n        # this will actually error because we don't yet handle these dupe deletes\n        f.StagingTapplicanttypesSynopsisFactory(\n            synopsis=synopsis_already_processed, at_id=\"21\", at_syn_id=11001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryApplicantTypeFactory(\n            opportunity_summary=summary_synopsis,\n            applicant_type=ApplicantType.INDIVIDUALS,\n            legacy_applicant_type_id=1001,\n        )\n\n        ## Synopsis funding category\n        # insert\n        f.StagingTfundactcatSynopsisFactory(synopsis=synopsis_already_processed, fac_id=\"IIJ\")\n        f.StagingTfundactcatSynopsisFactory(synopsis=synopsis_already_processed, fac_id=\"IIJ\")\n        f.StagingTfundactcatSynopsisFactory(synopsis=synopsis_already_processed, fac_id=\"IIJ\")\n        f.StagingTfundactcatSynopsisFactory(synopsis=synopsis_already_processed, fac_id=\"IIJ\")\n        # update\n        f.StagingTfundactcatSynopsisFactory(\n            synopsis=synopsis_already_processed, fac_id=\"FN\", fac_syn_id=2000\n        )\n        f.StagingTfundactcatSynopsisFactory(\n            synopsis=synopsis_already_processed, fac_id=\"FN\", fac_syn_id=20000\n        )\n        f.StagingTfundactcatSynopsisFactory(\n            synopsis=synopsis_already_processed, fac_id=\"FN\", fac_syn_id=21000\n        )\n        f.StagingTfundactcatSynopsisFactory(\n            synopsis=synopsis_already_processed, fac_id=\"FN\", fac_syn_id=22000\n        )\n        f.LinkOpportunitySummaryFundingCategoryFactory(\n            opportunity_summary=summary_synopsis,\n            funding_category=FundingCategory.FOOD_AND_NUTRITION,\n            legacy_funding_category_id=2000,\n        )\n        # delete\n        f.StagingTfundactcatSynopsisFactory(\n            synopsis=synopsis_already_processed, fac_id=\"HL\", fac_syn_id=2001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryFundingCategoryFactory(\n            opportunity_summary=summary_synopsis,\n            funding_category=FundingCategory.HEALTH,\n            legacy_funding_category_id=2001,\n        )\n\n        ## Synopsis funding instrument\n        # insert\n        f.StagingTfundinstrSynopsisFactory(synopsis=synopsis_already_processed, fi_id=\"PC\")\n        f.StagingTfundinstrSynopsisFactory(synopsis=synopsis_already_processed, fi_id=\"PC\")\n        f.StagingTfundinstrSynopsisFactory(synopsis=synopsis_already_processed, fi_id=\"PC\")\n        f.StagingTfundinstrSynopsisFactory(synopsis=synopsis_already_processed, fi_id=\"PC\")\n        f.StagingTfundinstrSynopsisFactory(synopsis=synopsis_already_processed, fi_id=\"PC\")\n        # update\n        f.StagingTfundinstrSynopsisFactory(\n            synopsis=synopsis_already_processed, fi_id=\"O\", fi_syn_id=3000\n        )\n        f.StagingTfundinstrSynopsisFactory(\n            synopsis=synopsis_already_processed, fi_id=\"O\", fi_syn_id=30000\n        )\n        f.StagingTfundinstrSynopsisFactory(\n            synopsis=synopsis_already_processed, fi_id=\"O\", fi_syn_id=31000\n        )\n        f.LinkOpportunitySummaryFundingInstrumentFactory(\n            opportunity_summary=summary_synopsis,\n            funding_instrument=FundingInstrument.OTHER,\n            legacy_funding_instrument_id=3000,\n        )\n        # delete\n        f.StagingTfundinstrSynopsisFactory(\n            synopsis=synopsis_already_processed, fi_id=\"G\", fi_syn_id=3001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryFundingInstrumentFactory(\n            opportunity_summary=summary_synopsis,\n            funding_instrument=FundingInstrument.GRANT,\n            legacy_funding_instrument_id=3001,\n        )\n\n        transform_oracle_data_task.run()\n\n        updated_opportunity: Opportunity = (\n            db_session.query(Opportunity)\n            .filter(Opportunity.opportunity_id == opportunity.opportunity_id)\n            .one_or_none()\n        )\n\n        assert updated_opportunity is not None\n        validate_opportunity(db_session, opportunity)\n        assert {\n            al.opportunity_assistance_listing_id\n            for al in updated_opportunity.opportunity_assistance_listings\n        } == {cfda_insert.opp_cfda_id, cfda_update.opp_cfda_id}\n\n        validate_summary_and_nested(\n            db_session,\n            forecast_update,\n            [ApplicantType.COUNTY_GOVERNMENTS, ApplicantType.CITY_OR_TOWNSHIP_GOVERNMENTS],\n            [FundingCategory.OPPORTUNITY_ZONE_BENEFITS, FundingCategory.NATURAL_RESOURCES],\n            [FundingInstrument.GRANT, FundingInstrument.COOPERATIVE_AGREEMENT],\n        )\n        validate_summary_and_nested(\n            db_session,\n            synopsis_already_processed,\n            [\n                ApplicantType.UNRESTRICTED,\n                ApplicantType.FEDERALLY_RECOGNIZED_NATIVE_AMERICAN_TRIBAL_GOVERNMENTS,\n            ],\n            [\n                FundingCategory.INFRASTRUCTURE_INVESTMENT_AND_JOBS_ACT,\n                FundingCategory.FOOD_AND_NUTRITION,\n            ],\n            [FundingInstrument.PROCUREMENT_CONTRACT, FundingInstrument.OTHER],\n            expect_values_to_match=False,\n        )\n        validate_agency(db_session, parent_agency)\n        validate_agency(db_session, subagency, deleted_fields={\"ldapGp\", \"description\"})\n\n        assert {\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_PROCESSED: 41,\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_INSERTED: 7,\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_UPDATED: 11,\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_DELETED: 7,\n            transform_oracle_data_task.Metrics.TOTAL_DUPLICATE_RECORDS_SKIPPED: 15,\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_ORPHANED: 0,\n            transform_oracle_data_task.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED: 1,\n        }.items() <= transform_oracle_data_task.metrics.items()\n\n    def test_delete_opportunity_with_deleted_children(self, db_session, transform_oracle_data_task):\n        agency = setup_agency(\"AGENCYXYZ\", create_existing=True)\n\n        # We create an opportunity with a synopsis/forecast record, and various other child values\n        # We then delete all of them at once. Deleting the opportunity will recursively delete the others\n        # but we'll still have delete events for the others - this verfies how we handle that.\n\n        existing_opportunity = f.OpportunityFactory(\n            no_current_summary=True,\n            opportunity_assistance_listings=[],\n            agency_code=\"AGENCYXYZ\",\n            opportunity_attachments=[],\n        )\n        opportunity = f.StagingTopportunityFactory(\n            opportunity_id=existing_opportunity.opportunity_id, cfdas=[], is_deleted=True\n        )\n\n        cfda = setup_cfda(create_existing=True, is_delete=True, opportunity=existing_opportunity)\n\n        ### Forecast - has several children that will be deleted\n        summary_forecast = f.OpportunitySummaryFactory(\n            is_forecast=True, opportunity=existing_opportunity, no_link_values=True\n        )\n        forecast = f.StagingTforecastFactory(opportunity=opportunity, is_deleted=True)\n        forecast_applicant_type = f.StagingTapplicanttypesForecastFactory(\n            forecast=forecast, at_id=\"04\", at_frcst_id=91001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryApplicantTypeFactory(\n            opportunity_summary=summary_forecast,\n            applicant_type=ApplicantType.SPECIAL_DISTRICT_GOVERNMENTS,\n            legacy_applicant_type_id=91001,\n        )\n        forecast_funding_category = f.StagingTfundactcatForecastFactory(\n            forecast=forecast, fac_id=\"ST\", fac_frcst_id=92001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryFundingCategoryFactory(\n            opportunity_summary=summary_forecast,\n            funding_category=FundingCategory.SCIENCE_TECHNOLOGY_AND_OTHER_RESEARCH_AND_DEVELOPMENT,\n            legacy_funding_category_id=92001,\n        )\n        forecast_funding_instrument = f.StagingTfundinstrForecastFactory(\n            forecast=forecast, fi_id=\"O\", fi_frcst_id=93001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryFundingInstrumentFactory(\n            opportunity_summary=summary_forecast,\n            funding_instrument=FundingInstrument.OTHER,\n            legacy_funding_instrument_id=93001,\n        )\n\n        ### Synopsis\n        summary_synopsis = f.OpportunitySummaryFactory(\n            is_forecast=False, opportunity=existing_opportunity, no_link_values=True\n        )\n        synopsis = f.StagingTsynopsisFactory(opportunity=opportunity, is_deleted=True)\n        synopsis_applicant_type = f.StagingTapplicanttypesSynopsisFactory(\n            synopsis=synopsis, at_id=\"21\", at_syn_id=81001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryApplicantTypeFactory(\n            opportunity_summary=summary_synopsis,\n            applicant_type=ApplicantType.INDIVIDUALS,\n            legacy_applicant_type_id=81001,\n        )\n        synopsis_funding_category = f.StagingTfundactcatSynopsisFactory(\n            synopsis=synopsis, fac_id=\"HL\", fac_syn_id=82001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryFundingCategoryFactory(\n            opportunity_summary=summary_synopsis,\n            funding_category=FundingCategory.HEALTH,\n            legacy_funding_category_id=82001,\n        )\n        synopsis_funding_instrument = f.StagingTfundinstrSynopsisFactory(\n            synopsis=synopsis, fi_id=\"G\", fi_syn_id=83001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryFundingInstrumentFactory(\n            opportunity_summary=summary_synopsis,\n            funding_instrument=FundingInstrument.GRANT,\n            legacy_funding_instrument_id=83001,\n        )\n        # Need to put an expire all so SQLAlchemy doesn't read from its cache\n        # otherwise when it does the recursive deletes, it doesn't see the later-added link table objects\n        db_session.expire_all()\n\n        transform_oracle_data_task.run_task()\n\n        # verify everything is not in the DB\n        validate_opportunity(db_session, opportunity, expect_in_db=False)\n        validate_assistance_listing(db_session, cfda, expect_in_db=False)\n        validate_opportunity_summary(db_session, forecast, expect_in_db=False)\n        validate_opportunity_summary(db_session, synopsis, expect_in_db=False)\n\n        validate_applicant_type(db_session, forecast_applicant_type, expect_in_db=False)\n        validate_applicant_type(db_session, synopsis_applicant_type, expect_in_db=False)\n\n        validate_funding_category(db_session, forecast_funding_category, expect_in_db=False)\n        validate_funding_category(db_session, synopsis_funding_category, expect_in_db=False)\n\n        validate_funding_instrument(db_session, forecast_funding_instrument, expect_in_db=False)\n        validate_funding_instrument(db_session, synopsis_funding_instrument, expect_in_db=False)\n\n        validate_agency(db_session, agency)\n\n        assert {\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_PROCESSED: 11,\n            # Despite processing 11 records, only the opportunity is actually deleted directly\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_DELETED: 1,\n            f\"opportunity.{transform_oracle_data_task.Metrics.TOTAL_RECORDS_DELETED}\": 1,\n            transform_oracle_data_task.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED: 9,\n        }.items() <= transform_oracle_data_task.metrics.items()\n\n    def test_delete_opportunity_summary_with_deleted_children(\n        self, db_session, transform_oracle_data_task\n    ):\n        # Similar to the above test, but we're leaving the opportunity alone and just deleting\n        # an opportunity summary. Should be the same thing, just on a smaller scale.\n        existing_opportunity = f.OpportunityFactory(\n            no_current_summary=True, opportunity_assistance_listings=[]\n        )\n        opportunity = f.StagingTopportunityFactory(\n            opportunity_id=existing_opportunity.opportunity_id, cfdas=[], already_transformed=True\n        )\n\n        summary_synopsis = f.OpportunitySummaryFactory(\n            is_forecast=False, opportunity=existing_opportunity, no_link_values=True\n        )\n        synopsis = f.StagingTsynopsisFactory(opportunity=opportunity, is_deleted=True)\n        synopsis_applicant_type = f.StagingTapplicanttypesSynopsisFactory(\n            synopsis=synopsis, at_id=\"21\", at_syn_id=71001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryApplicantTypeFactory(\n            opportunity_summary=summary_synopsis,\n            applicant_type=ApplicantType.INDIVIDUALS,\n            legacy_applicant_type_id=71001,\n        )\n        synopsis_funding_category = f.StagingTfundactcatSynopsisFactory(\n            synopsis=synopsis, fac_id=\"HL\", fac_syn_id=72001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryFundingCategoryFactory(\n            opportunity_summary=summary_synopsis,\n            funding_category=FundingCategory.HEALTH,\n            legacy_funding_category_id=72001,\n        )\n        synopsis_funding_instrument = f.StagingTfundinstrSynopsisFactory(\n            synopsis=synopsis, fi_id=\"G\", fi_syn_id=73001, is_deleted=True\n        )\n        f.LinkOpportunitySummaryFundingInstrumentFactory(\n            opportunity_summary=summary_synopsis,\n            funding_instrument=FundingInstrument.GRANT,\n            legacy_funding_instrument_id=73001,\n        )\n        # Need to put an expire all so SQLAlchemy doesn't read from its cache\n        # otherwise when it does the recursive deletes, it doesn't see the later-added link table objects\n        db_session.expire_all()\n\n        transform_oracle_data_task.run_task()\n\n        # verify everything is not in the DB\n        validate_opportunity(\n            db_session, opportunity, expect_in_db=True, expect_values_to_match=False\n        )\n        validate_opportunity_summary(db_session, synopsis, expect_in_db=False)\n        validate_applicant_type(db_session, synopsis_applicant_type, expect_in_db=False)\n        validate_funding_category(db_session, synopsis_funding_category, expect_in_db=False)\n        validate_funding_instrument(db_session, synopsis_funding_instrument, expect_in_db=False)\n\n        assert {\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_PROCESSED: 4,\n            # Despite processing 4 records, only the opportunity_summary is actually deleted directly\n            transform_oracle_data_task.Metrics.TOTAL_RECORDS_DELETED: 1,\n            f\"opportunity_summary.{transform_oracle_data_task.Metrics.TOTAL_RECORDS_DELETED}\": 1,\n            transform_oracle_data_task.Metrics.TOTAL_DELETE_ORPHANS_SKIPPED: 3,\n        }.items() <= transform_oracle_data_task.metrics.items()"}
{"path":"frontend/tests/e2e/search/search-loading.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/search/search-loading.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"File: api/tests/src/data_migration/transformation/test_transform_util.py\nLanguage: py\nType: code\nDirectory: api/tests/src/data_migration/transformation\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/data_migration/transformation/test_transform_util.py\nSize: 4.58 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/e2e/search/search-navigate.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/search/search-navigate.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"import pytest\nfrom freezegun import freeze_time\n\nfrom src.constants.lookup_constants import OpportunityCategory\nfrom src.data_migration.transformation import transform_util\nfrom tests.src.db.models.factories import OpportunityFactory, StagingTopportunityFactory\n\n\n@pytest.mark.parametrize(\n    \"value,expected_value\",\n    [\n        # Just check a few\n        (\"D\", OpportunityCategory.DISCRETIONARY),\n        (\"M\", OpportunityCategory.MANDATORY),\n        (\"O\", OpportunityCategory.OTHER),\n        (None, None),\n        (\" \", None),\n        (\"\", None),\n    ],\n)\ndef test_transform_opportunity_category(value, expected_value):\n    assert transform_util.transform_opportunity_category(value) == expected_value\n\n\n@pytest.mark.parametrize(\"value\", [\"A\", \"B\", \"mandatory\", \"other\", \"hello\"])\ndef test_transform_opportunity_category_unexpected_value(value):\n    with pytest.raises(ValueError, match=\"Unrecognized opportunity category\"):\n        transform_util.transform_opportunity_category(value)\n\n\n@pytest.mark.parametrize(\n    \"created_date,last_upd_date,expected_created_at,expected_updated_at\",\n    [\n        ### Using string timestamps rather than defining the dates directly for readability\n        # A few happy scenarios\n        (\n            \"2020-01-01T12:00:00\",\n            \"2020-06-01T12:00:00\",\n            \"2020-01-01T17:00:00+00:00\",\n            \"2020-06-01T16:00:00+00:00\",\n        ),\n        (\n            \"2021-01-31T21:30:15\",\n            \"2021-12-31T23:59:59\",\n            \"2021-02-01T02:30:15+00:00\",\n            \"2022-01-01T04:59:59+00:00\",\n        ),\n        # Leap year handling\n        (\n            \"2024-02-28T23:00:59\",\n            \"2024-02-29T19:10:10\",\n            \"2024-02-29T04:00:59+00:00\",\n            \"2024-03-01T00:10:10+00:00\",\n        ),\n        # last_upd_date is None, created_date is used for both\n        (\"2020-05-31T16:32:08\", None, \"2020-05-31T20:32:08+00:00\", \"2020-05-31T20:32:08+00:00\"),\n        (\"2020-07-15T20:00:00\", None, \"2020-07-16T00:00:00+00:00\", \"2020-07-16T00:00:00+00:00\"),\n        # both input values are None, the current time is used (which we set for the purposes of this test below)\n        (None, None, \"2023-05-10T12:00:00+00:00\", \"2023-05-10T12:00:00+00:00\"),\n    ],\n)\n@freeze_time(\"2023-05-10 12:00:00\", tz_offset=0)\ndef test_transform_update_create_timestamp(\n    created_date, last_upd_date, expected_created_at, expected_updated_at\n):\n    created_datetime = datetime.fromisoformat(created_date) if created_date is not None else None\n    last_upd_datetime = datetime.fromisoformat(last_upd_date) if last_upd_date is not None else None\n\n    source = StagingTopportunityFactory.build(\n        created_date=created_datetime, last_upd_date=last_upd_datetime\n    )\n    destination = OpportunityFactory.build()\n\n    transform_util.transform_update_create_timestamp(source, destination)\n\n    assert destination.created_at == datetime.fromisoformat(expected_created_at)\n    assert destination.updated_at == datetime.fromisoformat(expected_updated_at)\n\n\n@pytest.mark.parametrize(\n    \"value,expected_value\",\n    [\n        (\"Y\", True),\n        (\"N\", False),\n        (\"Yes\", True),\n        (\"No\", False),\n        (\"Y\", True),\n        (\"n\", False),\n        (\"yes\", True),\n        (\"no\", False),\n        (\"\", None),\n        (\" \", None),\n        (None, None),\n    ],\n)\ndef test_convert_yn_boolean(value, expected_value):\n    assert transform_util.convert_yn_bool(value) == expected_value\n\n\n@pytest.mark.parametrize(\"value\", [\"X\", \"Z\", \"1\", \"0\", \"yEs\", \"nO\"])\ndef test_convert_yn_boolean_unexpected_value(value):\n    with pytest.raises(ValueError, match=\"Unexpected Y/N bool value\"):\n        transform_util.convert_yn_bool(value)\n\n\n@pytest.mark.parametrize(\n    \"value,expected_value\", [(\"D\", True), (\"U\", False), (\"\", False), (\" \", False), (None, False)]\n)\ndef test_convert_action_type_to_is_deleted(value, expected_value):\n    assert transform_util.convert_action_type_to_is_deleted(value) == expected_value\n\n\n@pytest.mark.parametrize(\"value\", [\"A\", \"B\", \"d\", \"u\"])\ndef test_convert_action_type_to_is_deleted_unexpected_value(value):\n    with pytest.raises(ValueError, match=\"Unexpected action type value\"):\n        transform_util.convert_action_type_to_is_deleted(value)\n\n\n@pytest.mark.parametrize(\n    \"value,expected_value\",\n    [\n        (\"1\", 1),\n        (\"0\", 0),\n        (\"123123123\", 123123123),\n        (\"-5\", -5),\n        (\"\", None),\n        (\" \", None),\n        (None, None),\n        (\"words\", None),\n        (\"zero\", None),\n        (\"n/a\", None),\n    ],\n)\ndef test_convert_numeric_str_to_int(value, expected_value):\n    assert transform_util.convert_numeric_str_to_int(value) == expected_value"}
{"path":"frontend/tests/e2e/search/search-no-results.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/search/search-no-results.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"File: api/tests/src/db/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/db\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/e2e/search/search.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/search/search.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":""}
{"path":"frontend/tests/e2e/search/searchSpecUtil.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/search/searchSpecUtil.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"File: api/tests/src/db/models/__init__.py\nLanguage: py\nType: model\nDirectory: api/tests/src/db/models\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/e2e/search/searchUtil.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/search/searchUtil.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":""}
{"path":"frontend/tests/e2e/sign-in.spec.ts","language":"typescript","type":"code","directory":"frontend/tests/e2e","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/e2e/sign-in.spec.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"File: api/tests/src/db/models/factories.py\nLanguage: py\nType: model\nDirectory: api/tests/src/db/models\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/factories.py\nSize: 63.03 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/errors.test.ts","language":"typescript","type":"code","directory":"frontend/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/errors.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"These factories are used to generate test data for the tests. They are\nused both for generating in memory objects and for generating objects\nthat are persisted to the database.\n\nThe factories are based on the `factory_boy` library. See\nhttps://factoryboy.readthedocs.io/en/latest/ for more information.\n\"\"\"\n\nimport random\nfrom datetime import datetime\nfrom typing import Optional\n\nimport factory\nimport factory.fuzzy\nimport faker\nfrom faker.providers import BaseProvider\nfrom sqlalchemy import func\nfrom sqlalchemy.orm import scoped_session\n\nimport src.adapters.db as db\nimport src.db.models.extract_models as extract_models\nimport src.db.models.foreign as foreign\nimport src.db.models.opportunity_models as opportunity_models\nimport src.db.models.staging as staging\nimport src.db.models.user_models as user_models\nimport src.util.datetime_util as datetime_util\nfrom src.constants.lookup_constants import (\n    AgencyDownloadFileType,\n    AgencySubmissionNotificationSetting,\n    ApplicantType,\n    ExternalUserType,\n    ExtractType,\n    FundingCategory,\n    FundingInstrument,\n    OpportunityCategory,\n    OpportunityCategoryLegacy,\n    OpportunityStatus,\n)\nfrom src.db.models import agency_models\nfrom src.util import file_util\n\n\ndef sometimes_none(factory_value, none_chance: float = 0.5):\n    return factory.Maybe(\n        decider=factory.LazyAttribute(lambda s: random.random() > none_chance),\n        yes_declaration=factory_value,\n        no_declaration=None,\n    )\n\n\nclass CustomProvider(BaseProvider):\n    \"\"\"\n    This class is a custom faker provider that can be used to generate\n    fake data for our specific scenarios.\n\n    The name of the functions defined in this class is the name of the individual provider.\n    For example, the \"agency_code\" method below can be called by doing either of the following::\n\n        fake.agency_code()\n\n        factory.Faker(\"agency_code\")\n\n    Below we register this provider class with both the faker instance we setup, as well as\n    the underlying one backing the factory's faker instance.\n\n    See: https://faker.readthedocs.io/en/master/#how-to-create-a-provider\n    \"\"\"\n\n    # Ideally use agencies here that actually exist so anyone\n    # using data generated by this for front-end dev actually\n    # has valid values\n    AGENCIES = [\n        \"USAID\",\n        \"USAID-SAF\",\n        \"USAID-ETH\",\n        \"DOC\",\n        \"DOC-EDA\",\n        \"DOC-NIST\",\n        \"DOC-DOCNOAAERA\",\n        \"DOI\",\n        \"DOI-NPS\",\n        \"DOI-FWS\",\n        \"DOI-BLM\",\n        \"DOI-USGS1\",\n        \"DOI-BOR\",\n        \"DOI-BOR-MP\",\n        \"DOI-BIA\",\n        \"DOI-BOR-LC\",\n        \"DOI-BOEM\",\n        \"HHS\",\n        \"HHS-NIH11\",\n        \"HHS-CDC\",\n        \"HHS-HRSA\",\n        \"HHS-ACF\",\n        \"HHS-CDC-CGH\",\n        \"HHS-ACL\",\n        \"HHS-ACF-OHS\",\n        \"HHS-CDC-HHSCDCERA\",\n        \"HHS-FDA\",\n        \"HHS-OPHS\",\n        \"HHS-SAMHS-SAMHSA\",\n        \"HHS-SAMHS\",\n        \"HHS-IHS\",\n        \"HHS-AHRQ\",\n        \"HHS-CDC-NCCDPHP\",\n        \"HHS-AOA\",\n        \"HHS-CMS\",\n        \"HHS-ACF-FYSB\",\n        \"HHS-CDC-NCHHSTP\",\n        \"HHS-ACF-OPRE\",\n        \"EPA\",\n        \"DOD\",\n        \"DOD-AMRAA\",\n        \"DOD-ONR\",\n        \"DOD-AMC\",\n        \"DOD-DARPA-DSO\",\n        \"DOD-AFRL\",\n        \"DOD-COE-FW\",\n        \"DOD-COE\",\n        \"DOD-AFOSR\",\n        \"DOD-ONR-FAC\",\n        \"DOD-DARPA-MTO\",\n        \"DOD-COE-ERDC\",\n        \"USDA\",\n        \"USDA-NIFA\",\n        \"USDA-NRCS\",\n        \"USDA-FNS1\",\n        \"USDA-CSREE\",\n        \"USDA-FS\",\n        \"USDA-RUS\",\n        \"USDA-RBCS\",\n        \"USDA-FAS\",\n        \"USDA-AMS\",\n        \"NSF\",\n        \"DOE\",\n        \"DOE-GFO\",\n        \"DOE-NETL\",\n        \"DOE-ARPAE\",\n        \"USDOJ\",\n        \"USDOJ-OJP-BJA\",\n        \"USDOJ-OJP-OJJDP\",\n        \"USDOJ-OJP-NIJ\",\n        \"USDOJ-BOP-NIC\",\n        \"USDOJ-OJP-OVC\",\n        \"USDOJ-OJP-OVW\",\n        \"USDOJ-OJP-BJS\",\n    ]\n\n    # Various words we can use when building the agency names\n    # Stuff that sounds like it might be an agency, even if its not exactly the name\n    AGENCY_WORDS = [\n        \"Agriculture\",\n        \"Commerce\",\n        \"Defense\",\n        \"Education\",\n        \"Economics\",\n        \"Energy\",\n        \"Health\",\n        \"Housing\",\n        \"Justice\",\n        \"Labor\",\n        \"State\",\n        \"Interior\",\n        \"Transportation\",\n        \"Science\",\n        \"Arts\",\n    ]\n\n    AGENCY_NAME_FORMATS = [\n        \"Department of {{agency_word}}\",\n        \"Department of the {{agency_word}}\",\n        \"Agency for {{agency_word}}\",\n        \"National {{agency_word}} Administration\",\n    ]\n\n    AGENCY_CONTACT_DESC_FORMATS = [\n        \"{{name}}\\n{{job}}\\n555-###-####\\n{{email}}\",\n        \"{{relevant_url}} Contact Center\\nHours of operation are 24 hours a day, 7 days a week.\\n{{email}}\",\n        \"Webmaster\\n{{email}}\",\n    ]\n\n    # Rather than generate any random URL in our data, use those\n    # that are vaguely relevant to avoid linking to anything outside\n    # of the grants ecosystem that could cause confusion in test data\n    # (ie. either a website we work with, or a very generic one)\n    RELEVANT_URLS = [\"google.com\", \"grants.gov\", \"simpler.grants.gov\", \"sam.gov\"]\n\n    ADDITIONAL_INFO_DESC_FORMATS = [\n        \"Full Announcement\",\n        \"Grants.gov\",\n        \"Link to grant on {{relevant_url}}\",\n        \"Program Announcement\",\n        \"Click on the link to see the full announcement.\",\n        \"Division of {{company}}\",\n    ]\n\n    # Opportunity title uses several other existing providers\n    # to generate titles. Anything in {{ }} is calling a provider\n    # with that name.\n    OPPORTUNITY_TITLE_FORMATS = [\n        \"Research into {{job}} industry\",\n        \"Embassy program for {{job}} in {{country}}\",\n        \"{{name}} Foundation Grant for {{bs}}\",\n        \"{{company}} {{year}} award\",\n    ]\n\n    SUMMARY_DESCRIPTION_FORMATS = [\n        \"{{agency_code}} is looking to further investigate this topic. {{paragraph}}\",\n        \"<p>{{paragraph}}</p><p><br></p><p>{{paragraph}}</p>\",\n        \"The purpose of this Notice of Funding Opportunity (NOFO) is to support research into {{job}} and how we might {{catch_phrase}}.\",\n        \"<div>{{paragraph:long}} <a href='{{relevant_url}}'>{{sentence}}</a> {{paragraph:long}}</div> <div>{{paragraph:long}} <a href='{{relevant_url}}'>{{sentence}}</a> {{paragraph:long}}</div>\",\n    ]\n\n    # In the formatting, ? becomes a random letter, # becomes a random digit\n    OPPORTUNITY_NUMBER_FORMATS = [\n        \"???-###-FY{{year}}-###\",\n        \"{{agency_code}}-##-###\",\n        \"???#######\",\n        \"??-##-???-###\",\n        \"{{word}}-###-##\",\n    ]\n\n    YN_BOOLEAN_VALUES = [\"Y\", \"N\"]\n\n    YN_YESNO_BOOLEAN_VALUES = [\"Y\", \"N\", \"Yes\", \"No\"]\n\n    def agency_code(self) -> str:\n        return self.random_element(self.AGENCIES)\n\n    def agency_word(self) -> str:\n        return self.random_element(self.AGENCY_WORDS)\n\n    def agency_name(self) -> str:\n        pattern = self.random_element(self.AGENCY_NAME_FORMATS)\n        return self.generator.parse(pattern)\n\n    def agency_contact_description(self) -> str:\n        # bothify turns any ? into letters, and # into digits\n        pattern = self.bothify(self.random_element(self.AGENCY_CONTACT_DESC_FORMATS))\n        return self.generator.parse(pattern)\n\n    def relevant_url(self):\n        return self.random_element(self.RELEVANT_URLS)\n\n    def additional_info_desc(self):\n        pattern = self.random_element(self.ADDITIONAL_INFO_DESC_FORMATS)\n        return self.generator.parse(pattern)\n\n    def opportunity_number(self) -> str:\n        # bothify turns any ? into letters, and # into digits\n        pattern = self.bothify(self.random_element(self.OPPORTUNITY_NUMBER_FORMATS))\n        return self.generator.parse(pattern).upper()\n\n    def opportunity_title(self) -> str:\n        pattern = self.random_element(self.OPPORTUNITY_TITLE_FORMATS)\n        return self.generator.parse(pattern)\n\n    def summary_description(self) -> str:\n        self.generator.set_arguments(\"long\", {\"nb_sentences\": 25})\n        pattern = self.random_element(self.SUMMARY_DESCRIPTION_FORMATS)\n        return self.generator.parse(pattern)\n\n    def yn_boolean(self) -> str:\n        return self.random_element(self.YN_BOOLEAN_VALUES)\n\n    def yn_yesno_boolean(self) -> str:\n        return self.random_element(self.YN_YESNO_BOOLEAN_VALUES)\n\n\nfake = faker.Faker()\nfake.add_provider(CustomProvider)\nfactory.Faker.add_provider(CustomProvider)\n\n_db_session: Optional[db.Session] = None\n\n\ndef get_db_session() -> db.Session:\n    # _db_session is only set in the pytest fixture `enable_factory_create`\n    # so that tests do not unintentionally write to the database.\n    if _db_session is None:\n        raise Exception(\n            \"\"\"Factory db_session is not initialized.\n\n            If your tests don't need to cover database behavior, consider\n            calling the `build()` method instead of `create()` on the factory to\n            not persist the generated model.\n\n            If running tests that actually need data in the DB, pull in the\n            `enable_factory_create` fixture to initialize the db_session.\n            \"\"\"\n        )\n\n    return _db_session\n\n\n# The scopefunc ensures that the session gets cleaned up after each test\n# it implicitly calls `remove()` on the session.\n# see https://docs.sqlalchemy.org/en/20/orm/contextual.html\nSession = scoped_session(lambda: get_db_session(), scopefunc=lambda: get_db_session())\n\n\nclass Generators:\n    Now = factory.LazyFunction(datetime.now)\n    UtcNow = factory.LazyFunction(datetime_util.utcnow)\n    UuidObj = factory.Faker(\"uuid4\", cast_to=None)\n    PhoneNumber = factory.Sequence(lambda n: f\"123-456-{n:04}\")\n\n\nclass BaseFactory(factory.alchemy.SQLAlchemyModelFactory):\n\n    class Meta:\n        abstract = True\n        sqlalchemy_session = Session\n        sqlalchemy_session_persistence = \"commit\"\n\n\nclass OpportunityFactory(BaseFactory):\n    class Meta:\n        model = opportunity_models.Opportunity\n\n    @classmethod\n    def _setup_next_sequence(cls):\n        if _db_session is not None:\n            value = _db_session.query(\n                func.max(opportunity_models.Opportunity.opportunity_id)\n            ).scalar()\n            if value is not None:\n                return value + 1\n\n        return 1\n\n    opportunity_id = factory.Sequence(lambda n: n)\n\n    opportunity_number = factory.Faker(\"opportunity_number\")\n    opportunity_title = factory.Faker(\"opportunity_title\")\n\n    agency_code = factory.Faker(\"agency_code\")\n\n    category = factory.fuzzy.FuzzyChoice(OpportunityCategory)\n    # only set the category explanation if category is Other\n    category_explanation = factory.Maybe(\n        decider=factory.LazyAttribute(lambda o: o.category == OpportunityCategory.OTHER),\n        yes_declaration=factory.Faker(\"sentence\", nb_words=3),\n        no_declaration=None,\n    )\n\n    is_draft = False  # Because we filter out drafts, just default these to False\n\n    revision_number = 0  # We'll want to consider how we handle this when we add history\n\n    opportunity_assistance_listings = factory.RelatedFactoryList(\n        \"tests.src.db.models.factories.OpportunityAssistanceListingFactory\",\n        factory_related_name=\"opportunity\",\n        size=lambda: random.randint(1, 3),\n    )\n\n    # By default we'll add a current opportunity summary which will be POSTED\n    # if you'd like to easily modify the values, see the possible traits below in the\n    # Params class section\n    current_opportunity_summary = factory.RelatedFactory(\n        \"tests.src.db.models.factories.CurrentOpportunitySummaryFactory\",\n        factory_related_name=\"opportunity\",\n    )\n\n    opportunity_attachments = []  # Use has_attachments=True to add attachments\n\n    class Params:\n        # These are common scenarios we might want for an opportunity.\n        # Simply pass the in `trait_name=True` to the factory when making an object\n        # and all of these will be set for you on the relevant models\n        # See: https://factoryboy.readthedocs.io/en/stable/reference.html#traits\n\n        no_current_summary = factory.Trait(current_opportunity_summary=None)\n\n        # We set a trait for the OpportunitySummaryFactory for each of these as well as set the opportunity status\n        is_posted_summary = factory.Trait(current_opportunity_summary__is_posted_summary=True)\n        is_forecasted_summary = factory.Trait(\n            current_opportunity_summary__is_forecasted_summary=True\n        )\n        is_closed_summary = factory.Trait(current_opportunity_summary__is_closed_summary=True)\n        is_archived_non_forecast_summary = factory.Trait(\n            current_opportunity_summary__is_archived_non_forecast_summary=True\n        )\n        is_archived_forecast_summary = factory.Trait(\n            current_opportunity_summary__is_archived_forecast_summary=True\n        )\n\n        has_long_descriptions = factory.Trait(\n            current_opportunity_summary__has_long_descriptions=True\n        )\n\n        # Set all nullable fields to null\n        all_fields_null = factory.Trait(\n            agency_code=None,\n            category=None,\n            category_explanation=None,\n            current_opportunity_summary=None,\n            opportunity_assistance_listings=None,\n        )\n\n        # Set the timestamps in the past rather than using the default of \"now\"\n        timestamps_in_past = factory.Trait(\n            created_at=factory.Faker(\"date_time_between\", start_date=\"-5y\", end_date=\"-3y\"),\n            updated_at=factory.Faker(\"date_time_between\", start_date=\"-3y\", end_date=\"-1y\"),\n        )\n\n        has_attachments = factory.Trait(\n            opportunity_attachments=factory.RelatedFactoryList(\n                \"tests.src.db.models.factories.OpportunityAttachmentFactory\",\n                factory_related_name=\"opportunity\",\n                size=lambda: random.randint(1, 2),\n            )\n        )\n\n\nclass OpportunitySummaryFactory(BaseFactory):\n    class Meta:\n        model = opportunity_models.OpportunitySummary\n\n    opportunity = factory.SubFactory(OpportunityFactory, current_opportunity_summary=None)\n    opportunity_id = factory.LazyAttribute(lambda s: s.opportunity.opportunity_id)\n\n    summary_description = factory.Faker(\"summary_description\")\n    is_cost_sharing = factory.Faker(\"boolean\")\n\n    # By default generate non-forecasts which affects several fields\n    is_forecast = False\n\n    # Forecasted records don't have a close date\n    close_date = factory.Maybe(\n        decider=factory.LazyAttribute(lambda s: s.is_forecast),\n        # If forecasted, don't set a close date\n        yes_declaration=None,\n        # otherwise a future date\n        no_declaration=factory.Faker(\"date_between\", start_date=\"+2w\", end_date=\"+3w\"),\n    )\n    close_date_description = factory.Maybe(\n        decider=factory.LazyAttribute(lambda s: s.close_date is None),\n        yes_declaration=None,\n        no_declaration=factory.Faker(\"paragraph\", nb_sentences=1),\n    )\n\n    # Just a random recent post date\n    post_date = factory.Faker(\"date_between\", start_date=\"-3w\", end_date=\"-1d\")\n\n    # By default set to a date in the future\n    archive_date = factory.Faker(\"date_between\", start_date=\"+3w\", end_date=\"+4w\")\n\n    unarchive_date = None\n\n    expected_number_of_awards = factory.Faker(\"random_int\", min=1, max=25)\n    estimated_total_program_funding = factory.Faker(\n        \"random_int\", min=10_000, max=10_000_000, step=5_000\n    )\n    award_floor = factory.LazyAttribute(\n        lambda s: s.estimated_total_program_funding // s.expected_number_of_awards\n    )\n    award_ceiling = factory.LazyAttribute(lambda s: s.estimated_total_program_funding)\n\n    additional_info_url = factory.Faker(\"relevant_url\")\n    additional_info_url_description = factory.Faker(\"additional_info_desc\")\n\n    funding_category_description = factory.Maybe(\n        decider=factory.LazyAttribute(lambda s: fake.boolean()),  # random chance to include value\n        yes_declaration=factory.Faker(\"paragraph\", nb_sentences=1),\n        no_declaration=None,\n    )\n    applicant_eligibility_description = factory.Maybe(\n        decider=factory.LazyAttribute(lambda s: fake.boolean()),  # random chance to include value\n        yes_declaration=factory.Faker(\"paragraph\", nb_sentences=5),\n        no_declaration=None,\n    )\n\n    agency_code = factory.LazyAttribute(lambda s: s.opportunity.agency_code)\n    agency_name = factory.Faker(\"agency_name\")\n    agency_phone_number = Generators.PhoneNumber\n    agency_contact_description = factory.Faker(\"agency_contact_description\")\n    agency_email_address = factory.Faker(\"email\")\n    agency_email_address_description = factory.LazyAttribute(\n        lambda s: f\"Contact {s.agency_name} via email\"\n    )\n\n    # Forecasted values are only set if is_forecast=True\n    forecasted_post_date = factory.Maybe(\n        decider=factory.LazyAttribute(lambda s: s.is_forecast),\n        # If forecasted, set it in the future\n        yes_declaration=factory.Faker(\"date_between\", start_date=\"+2w\", end_date=\"+3w\"),\n        # otherwise don't set\n        no_declaration=None,\n    )\n    forecasted_close_date = factory.Maybe(\n        decider=factory.LazyAttribute(lambda s: s.is_forecast),\n        # If forecasted, set it in the future\n        yes_declaration=factory.Faker(\"date_between\", start_date=\"+6w\", end_date=\"+12w\"),\n        # otherwise don't set\n        no_declaration=None,\n    )\n    forecasted_close_date_description = factory.Maybe(\n        decider=factory.LazyAttribute(lambda s: s.forecasted_close_date is None),\n        yes_declaration=None,\n        no_declaration=factory.Faker(\"paragraph\", nb_sentences=1),\n    )\n    forecasted_award_date = factory.Maybe(\n        decider=factory.LazyAttribute(lambda s: s.is_forecast),\n        # If forecasted, set it in the future\n        yes_declaration=factory.Faker(\"date_between\", start_date=\"+26w\", end_date=\"+30w\"),\n        # otherwise don't set\n        no_declaration=None,\n    )\n    forecasted_project_start_date = factory.Maybe(\n        decider=factory.LazyAttribute(lambda s: s.is_forecast),\n        # If forecasted, set it in the future\n        yes_declaration=factory.Faker(\"date_between\", start_date=\"+30w\", end_date=\"+52w\"),\n        # otherwise don't set\n        no_declaration=None,\n    )\n    fiscal_year = factory.LazyAttribute(\n        lambda s: s.forecasted_project_start_date.year if s.forecasted_project_start_date else None\n    )\n\n    is_deleted = False\n\n    # Generally, current summaries won't have the revision number set\n    revision_number = None\n\n    version_number = factory.Faker(\"random_int\", min=8, max=13)\n\n    funding_instruments = factory.Faker(\n        \"random_elements\",\n        length=random.randint(1, 3),\n        elements=[f for f in FundingInstrument],\n        unique=True,\n    )\n    funding_categories = factory.Faker(\n        \"random_elements\",\n        length=random.randint(1, 3),\n        elements=[f for f in FundingCategory],\n        unique=True,\n    )\n    applicant_types = factory.Faker(\n        \"random_elements\",\n        length=random.randint(1, 3),\n        elements=[a for a in ApplicantType],\n        unique=True,\n    )\n\n    created_at = factory.Faker(\"date_time_between\", start_date=\"-5y\", end_date=\"-3y\")\n    updated_at = factory.LazyAttribute(\n        lambda o: fake.date_time_between(start_date=o.created_at, end_date=\"-1y\")\n    )\n\n    class Params:\n        # These are common overrides we might want for an opportunity summary.\n        # Simply pass the in `trait_name=True` to the factory when making an object\n        # and all of these will be set for you, overriding the above defaults\n        # See: https://factoryboy.readthedocs.io/en/stable/reference.html#traits\n\n        # The default state of values above assumes an \"active\" summary that has been\n        # posted, so only need to configure the is_forecast field for these two\n        is_posted_summary = factory.Trait(is_forecast=False)\n        is_forecasted_summary = factory.Trait(is_forecast=True)\n\n        # For these scenarios, adjust a few dates to make more sense\n        is_closed_summary = factory.Trait(\n            is_forecast=False,\n            post_date=factory.Faker(\"date_between\", start_date=\"-6w\", end_date=\"-5w\"),\n            close_date=factory.Faker(\"date_between\", start_date=\"-3w\", end_date=\"-1w\"),\n        )\n\n        is_archived_non_forecast_summary = factory.Trait(\n            is_forecast=False,\n            post_date=factory.Faker(\"date_between\", start_date=\"-6w\", end_date=\"-5w\"),\n            close_date=factory.Faker(\"date_between\", start_date=\"-3w\", end_date=\"-2w\"),\n            archive_date=factory.Faker(\"date_between\", start_date=\"-2w\", end_date=\"-1w\"),\n        )\n        is_archived_forecast_summary = factory.Trait(\n            is_forecast=True,\n            post_date=factory.Faker(\"date_between\", start_date=\"-6w\", end_date=\"-5w\"),\n            archive_date=factory.Faker(\"date_between\", start_date=\"-2w\", end_date=\"-1w\"),\n        )\n\n        is_non_public_non_forecast_summary = factory.Trait(\n            is_forecast=False,\n            post_date=factory.Faker(\"date_between\", start_date=\"+3w\", end_date=\"+4w\"),\n        )\n        is_non_public_forecast_summary = factory.Trait(\n            is_forecast=True,\n            post_date=factory.Faker(\"date_between\", start_date=\"+3w\", end_date=\"+4w\"),\n        )\n\n        # Set all nullable fields to null\n        all_fields_null = factory.Trait(\n            summary_description=None,\n            is_cost_sharing=None,\n            post_date=None,\n            close_date=None,\n            close_date_description=None,\n            archive_date=None,\n            unarchive_date=None,\n            expected_number_of_awards=None,\n            estimated_total_program_funding=None,\n            award_floor=None,\n            award_ceiling=None,\n            additional_info_url=None,\n            additional_info_url_description=None,\n            forecasted_post_date=None,\n            forecasted_close_date=None,\n            forecasted_close_date_description=None,\n            forecasted_award_date=None,\n            forecasted_project_start_date=None,\n            fiscal_year=None,\n            modification_comments=None,\n            funding_category_description=None,\n            applicant_eligibility_description=None,\n            agency_code=None,\n            agency_name=None,\n            agency_phone_number=None,\n            agency_contact_description=None,\n            agency_email_address=None,\n            agency_email_address_description=None,\n            is_deleted=None,\n            funding_instruments=[],\n            funding_categories=[],\n            applicant_types=[],\n        )\n\n        no_link_values = factory.Trait(\n            link_funding_instruments=[],\n            link_funding_categories=[],\n            link_applicant_types=[],\n        )\n\n        has_long_descriptions = factory.Trait(\n            summary_description=factory.Faker(\"paragraph\", nb_sentences=60),\n            close_date_description=factory.Faker(\"paragraph\", nb_sentences=30),\n        )\n\n\nclass CurrentOpportunitySummaryFactory(BaseFactory):\n    class Meta:\n        model = opportunity_models.CurrentOpportunitySummary\n\n    opportunity = factory.SubFactory(OpportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda a: a.opportunity.opportunity_id)\n\n    opportunity_summary = factory.SubFactory(\n        OpportunitySummaryFactory, opportunity=factory.SelfAttribute(\"..opportunity\")\n    )\n    opportunity_summary_id = factory.LazyAttribute(\n        lambda a: a.opportunity_summary.opportunity_summary_id\n    )\n\n    opportunity_status = OpportunityStatus.POSTED\n\n    class Params:\n        is_posted_summary = factory.Trait(\n            opportunity_status=OpportunityStatus.POSTED, opportunity_summary__is_posted_summary=True\n        )\n        is_forecasted_summary = factory.Trait(\n            opportunity_status=OpportunityStatus.FORECASTED,\n            opportunity_summary__is_forecasted_summary=True,\n        )\n        is_closed_summary = factory.Trait(\n            opportunity_status=OpportunityStatus.CLOSED, opportunity_summary__is_closed_summary=True\n        )\n        is_archived_non_forecast_summary = factory.Trait(\n            opportunity_status=OpportunityStatus.ARCHIVED,\n            opportunity_summary__is_archived_non_forecast_summary=True,\n        )\n        is_archived_forecast_summary = factory.Trait(\n            opportunity_status=OpportunityStatus.ARCHIVED,\n            opportunity_summary__is_archived_forecast_summary=True,\n        )\n\n        has_long_descriptions = factory.Trait(\n            opportunity_summary__has_long_descriptions=True,\n        )\n\n\nclass OpportunityAssistanceListingFactory(BaseFactory):\n    class Meta:\n        model = opportunity_models.OpportunityAssistanceListing\n\n    @classmethod\n    def _setup_next_sequence(cls):\n        if _db_session is not None:\n            value = _db_session.query(\n                func.max(\n                    opportunity_models.OpportunityAssistanceListing.opportunity_assistance_listing_id\n                )\n            ).scalar()\n            if value is not None:\n                return value + 1\n\n        return 1\n\n    opportunity_assistance_listing_id = factory.Sequence(lambda n: n)\n\n    opportunity = factory.SubFactory(OpportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda a: a.opportunity.opportunity_id)\n\n    program_title = factory.Faker(\"company\")\n    assistance_listing_number = factory.LazyFunction(\n        lambda: f\"{fake.random_int(min=1, max=99):02}.{fake.random_int(min=1, max=999):03}\"\n    )\n\n    class Params:\n        # Set the timestamps in the past rather than using the default of \"now\"\n        timestamps_in_past = factory.Trait(\n            created_at=factory.Faker(\"date_time_between\", start_date=\"-5y\", end_date=\"-3y\"),\n            updated_at=factory.Faker(\"date_time_between\", start_date=\"-3y\", end_date=\"-1y\"),\n        )\n\n\nclass LinkOpportunitySummaryFundingInstrumentFactory(BaseFactory):\n    class Meta:\n        model = opportunity_models.LinkOpportunitySummaryFundingInstrument\n\n    opportunity_summary = factory.SubFactory(OpportunitySummaryFactory)\n    opportunity_summary_id = factory.LazyAttribute(\n        lambda f: f.opportunity_summary.opportunity_summary_id\n    )\n\n    # We use an iterator here to keep the values unique when generated by the opportunity factory\n    funding_instrument = factory.Iterator(FundingInstrument)\n\n\nclass LinkOpportunitySummaryFundingCategoryFactory(BaseFactory):\n    class Meta:\n        model = opportunity_models.LinkOpportunitySummaryFundingCategory\n\n    opportunity_summary = factory.SubFactory(OpportunitySummaryFactory)\n    opportunity_summary_id = factory.LazyAttribute(\n        lambda f: f.opportunity_summary.opportunity_summary_id\n    )\n\n    # We use an iterator here to keep the values unique when generated by the opportunity factory\n    funding_category = factory.Iterator(FundingCategory)\n\n\nclass LinkOpportunitySummaryApplicantTypeFactory(BaseFactory):\n    class Meta:\n        model = opportunity_models.LinkOpportunitySummaryApplicantType\n\n    opportunity_summary = factory.SubFactory(OpportunitySummaryFactory)\n    opportunity_summary_id = factory.LazyAttribute(\n        lambda f: f.opportunity_summary.opportunity_summary_id\n    )\n\n    # We use an iterator here to keep the values unique when generated by the opportunity factory\n    applicant_type = factory.Iterator(ApplicantType)\n\n\nclass OpportunityAttachmentFactory(BaseFactory):\n    class Meta:\n        model = opportunity_models.OpportunityAttachment\n\n    opportunity = factory.SubFactory(OpportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda a: a.opportunity.opportunity_id)\n\n    # Whatever you pass in for file_contents will end up in the file, but\n    # not included anywhere on the model itself\n    file_contents = factory.Faker(\"sentence\")\n    # NOTE: If you want the file to properly get written to s3 for tests/locally\n    # make sure the bucket actually exists\n    file_location = factory.LazyAttribute(\n        lambda o: f\"s3://local-mock-public-bucket/opportunities/{o.opportunity_id}/attachments/{fake.random_int(min=1, max=100_000_000)}/{o.file_name}\"\n    )\n    mime_type = factory.Faker(\"mime_type\")\n    file_name = factory.Faker(\"file_name\")\n    file_description = factory.Faker(\"sentence\")\n    file_size_bytes = factory.Faker(\"random_int\", min=1000, max=10000000)\n\n    created_at = factory.Faker(\"date_time_between\", start_date=\"-1y\", end_date=\"now\")\n    updated_at = factory.LazyAttribute(\n        lambda o: fake.date_time_between(start_date=o.created_at, end_date=\"now\")\n    )\n\n    @classmethod\n    def _build(cls, model_class, *args, **kwargs):\n        kwargs.pop(\"file_contents\")  # Don't file for build strategy\n        super()._build(model_class, *args, **kwargs)\n\n    @classmethod\n    def _create(cls, model_class, *args, **kwargs):\n        file_contents = kwargs.pop(\"file_contents\")\n        attachment = super()._create(model_class, *args, **kwargs)\n\n        try:\n            with file_util.open_stream(attachment.file_location, \"w\") as my_file:\n                my_file.write(file_contents)\n        except Exception as e:\n            raise Exception(\n                f\"\"\"There was an error writing your attachment to {attachment.file_location}.\n\n                Does this location exist? If you are running in unit tests, make sure\n                `enable_factory_create` is pulled in as a fixture to your test.\n\n                If you are running locally outside of unit tests, make sure that `make init-localstack` has run.\n                \"\"\"\n            ) from e\n\n        return attachment\n\n\nclass AgencyContactInfoFactory(BaseFactory):\n    class Meta:\n        model = agency_models.AgencyContactInfo\n\n    contact_name = factory.Faker(\"name\")\n    address_line_1 = factory.Faker(\"street_address\")\n    address_line_2 = sometimes_none(factory.Sequence(lambda n: f\"Room {n}\"))\n    city = factory.Faker(\"city\")\n    state = factory.Faker(\"state_abbr\")\n    zip_code = factory.Faker(\"street_address\")\n    phone_number = factory.Faker(\"basic_phone_number\")\n    primary_email = factory.Faker(\"email\")\n    secondary_email = sometimes_none(factory.Faker(\"email\"))\n\n\nclass AgencyFactory(BaseFactory):\n    class Meta:\n        model = agency_models.Agency\n\n    agency_name = factory.Faker(\"agency_name\")\n\n    agency_code = factory.Iterator(CustomProvider.AGENCIES)\n\n    sub_agency_code = factory.LazyAttribute(lambda a: a.agency_code.split(\"-\")[0])\n\n    assistance_listing_number = factory.Faker(\"random_int\", min=1, max=999)\n\n    agency_submission_notification_setting = factory.fuzzy.FuzzyChoice(\n        AgencySubmissionNotificationSetting\n    )\n\n    agency_contact_info = factory.SubFactory(AgencyContactInfoFactory)\n    agency_contact_info_id = factory.LazyAttribute(\n        lambda a: a.agency_contact_info.agency_contact_info_id if a.agency_contact_info else None\n    )\n\n    is_test_agency = False\n\n    ldap_group = factory.LazyAttribute(lambda a: a.agency_code)\n    description = factory.LazyAttribute(lambda a: a.agency_name)\n    label = factory.LazyAttribute(lambda a: a.agency_name)\n    is_multilevel_agency = factory.Faker(\"boolean\")\n    is_multiproject = factory.Faker(\"boolean\")\n    has_system_to_system_certificate = factory.Faker(\"boolean\")\n    can_view_packages_in_grace_period = factory.Faker(\"boolean\")\n    is_image_workspace_enabled = factory.Faker(\"boolean\")\n    is_validation_workspace_enabled = factory.Faker(\"boolean\")\n\n    top_level_agency_id = None\n    agency_download_file_types = factory.Faker(\n        \"random_elements\",\n        length=random.randint(1, 2),\n        elements=[a for a in AgencyDownloadFileType],\n        unique=True,\n    )\n\n    # Create the contact info first and use its ID\n    agency_contact_info = factory.SubFactory(AgencyContactInfoFactory)\n    agency_contact_info_id = factory.LazyAttribute(\n        lambda a: a.agency_contact_info.agency_contact_info_id if a.agency_contact_info else None\n    )\n\n\n####################################\n# Staging Table Factories\n####################################\n\n\nLEGACY_APPLICANT_TYPE_IDS = [\n    \"00\",\n    \"01\",\n    \"02\",\n    \"04\",\n    \"05\",\n    \"06\",\n    \"07\",\n    \"08\",\n    \"11\",\n    \"12\",\n    \"13\",\n    \"20\",\n    \"21\",\n    \"22\",\n    \"23\",\n    \"25\",\n    \"99\",\n]\nLEGACY_FUNDING_CATEGORY_IDS = [\n    \"RA\",\n    \"AG\",\n    \"AR\",\n    \"BC\",\n    \"CD\",\n    \"CP\",\n    \"DPR\",\n    \"ED\",\n    \"ELT\",\n    \"EN\",\n    \"ENV\",\n    \"FN\",\n    \"HL\",\n    \"HO\",\n    \"HU\",\n    \"IIJ\",\n    \"IS\",\n    \"ISS\",\n    \"LJL\",\n    \"NR\",\n    \"OZ\",\n    \"RD\",\n    \"ST\",\n    \"T\",\n    \"ACA\",\n    \"O\",\n]\nLEGACY_FUNDING_INSTRUMENT_IDS = [\"CA\", \"G\", \"PC\", \"O\"]\n\n\nclass TopportunityFactory(BaseFactory):\n    class Meta:\n        abstract = True\n\n    opportunity_id = factory.Sequence(lambda n: n)\n\n    oppnumber = factory.Faker(\"opportunity_number\")\n    opptitle = factory.Faker(\"opportunity_title\")\n\n    owningagency = factory.Faker(\"agency_code\")\n\n    oppcategory = factory.fuzzy.FuzzyChoice(OpportunityCategoryLegacy)\n    # only set the category explanation if category is Other\n    category_explanation = factory.Maybe(\n        decider=factory.LazyAttribute(lambda o: o.oppcategory == OpportunityCategoryLegacy.OTHER),\n        yes_declaration=factory.Faker(\"sentence\", nb_words=5),\n        no_declaration=None,\n    )\n\n    is_draft = factory.fuzzy.FuzzyChoice([\"N\", \"S\"])\n\n    revision_number = factory.Faker(\"random_int\", min=1, max=10)\n\n    created_date = factory.Faker(\"date_time_between\", start_date=\"-10y\", end_date=\"-5y\")\n    last_upd_date = sometimes_none(\n        factory.Faker(\"date_time_between\", start_date=\"-5y\", end_date=\"now\")\n    )\n\n\nclass TopportunityCfdaFactory(BaseFactory):\n    class Meta:\n        abstract = True\n\n    opp_cfda_id = factory.Sequence(lambda n: n)\n    opportunity_id = factory.Sequence(lambda n: n)\n\n    programtitle = factory.Faker(\"company\")\n    cfdanumber = factory.LazyFunction(\n        lambda: f\"{fake.random_int(min=1, max=99):02}.{fake.random_int(min=1, max=999):03}\"\n    )\n\n    created_date = factory.Faker(\"date_time_between\", start_date=\"-10y\", end_date=\"-5y\")\n    last_upd_date = sometimes_none(\n        factory.Faker(\"date_time_between\", start_date=\"-5y\", end_date=\"now\")\n    )\n\n\nclass TsynopsisFactory(BaseFactory):\n    class Meta:\n        abstract = True\n\n    opportunity_id = factory.Sequence(lambda n: n)\n\n    posting_date = factory.Faker(\"date_between\", start_date=\"-3w\", end_date=\"now\")\n    response_date = factory.Faker(\"date_between\", start_date=\"+2w\", end_date=\"+3w\")\n    archive_date = factory.Faker(\"date_between\", start_date=\"+3w\", end_date=\"+4w\")\n    unarchive_date = sometimes_none(\n        factory.Faker(\"date_between\", start_date=\"+6w\", end_date=\"+7w\"), none_chance=0.9\n    )\n    syn_desc = factory.Faker(\"summary_description\")\n    oth_cat_fa_desc = sometimes_none(factory.Faker(\"paragraph\", nb_sentences=1))\n\n    cost_sharing = sometimes_none(factory.Faker(\"yn_yesno_boolean\"), none_chance=0.1)\n    # These int values are stored as strings\n    number_of_awards = sometimes_none(\n        factory.LazyFunction(lambda: str(fake.random_int(1, 25))), none_chance=0.1\n    )\n    est_funding = sometimes_none(\n        factory.LazyFunction(lambda: str(fake.random_int(25_000, 25_000_000, step=5_000))),\n        none_chance=0.1,\n    )\n    award_ceiling = sometimes_none(\n        factory.LazyFunction(lambda: str(fake.random_int(10_000, 25_000, step=5_000))),\n        none_chance=0.1,\n    )\n    award_floor = sometimes_none(\n        factory.LazyFunction(lambda: str(fake.random_int(0, 10_000, step=5_000))), none_chance=0.1\n    )\n\n    fd_link_url = factory.Faker(\"relevant_url\")\n    fd_link_desc = factory.Faker(\"additional_info_desc\")\n    agency_contact_desc = factory.Faker(\"agency_contact_description\")\n    ac_email_addr = factory.Faker(\"email\")\n    ac_email_desc = factory.LazyAttribute(lambda s: f\"Contact {s.ac_name} via email\")\n    a_sa_code = factory.Faker(\"agency_code\")\n    ac_phone_number = Generators.PhoneNumber\n    ac_name = factory.Faker(\"agency_name\")\n\n    created_date = factory.Faker(\"date_time_between\", start_date=\"-10y\", end_date=\"-5y\")\n    last_upd_date = sometimes_none(\n        factory.Faker(\"date_time_between\", start_date=\"-5y\", end_date=\"now\")\n    )\n    create_ts = factory.Faker(\"date_time_between\", start_date=\"-10y\", end_date=\"-5y\")\n    sendmail = sometimes_none(factory.Faker(\"yn_yesno_boolean\"))\n    response_date_desc = sometimes_none(factory.Faker(\"paragraph\", nb_sentences=2))\n    applicant_elig_desc = sometimes_none(factory.Faker(\"paragraph\", nb_sentences=5))\n    version_nbr = factory.Faker(\"random_int\", min=0, max=10)\n    modification_comments = sometimes_none(factory.Faker(\"paragraph\", nb_sentences=1))\n    publisheruid = sometimes_none(factory.Faker(\"first_name\"))\n    publisher_profile_id = sometimes_none(factory.Faker(\"random_int\", min=1, max=99_999))\n\n\nclass TsynopsisAttachmentFactory(BaseFactory):\n    class Meta:\n        abstract = True\n\n    syn_att_id: factory.Sequence(lambda n: n)\n    opportunity_id: factory.Sequence(lambda n: n)\n    att_revision_number = factory.Faker(\"random_int\", min=1000, max=10000000)\n    att_type: factory.Faker(\"att_type\")\n    mime_type = factory.Faker(\"mime_type\")\n    link_url = factory.Faker(\"relevant_url\")\n    file_name = factory.Faker(\"file_name\", category=\"text\")\n    file_desc = factory.Faker(\"sentence\")\n    file_lob = factory.LazyFunction(lambda: fake.sentence(25).encode())\n    file_lob_size = factory.LazyAttribute(lambda x: len(x.file_lob) if x.file_lob else 0)\n    create_date = factory.Faker(\"date_time_between\", start_date=\"-1y\", end_date=\"now\")\n    created_date = factory.LazyAttribute(\n        lambda o: fake.date_time_between(start_date=o.create_date, end_date=\"now\")\n    )\n    last_upd_date = factory.LazyAttribute(\n        lambda o: fake.date_time_between(start_date=o.created_date, end_date=\"now\")\n    )\n    creator_id = factory.Faker(\"first_name\")\n    last_upd_id = factory.Faker(\"first_name\")\n    syn_att_folder_id = factory.Faker(\"random_int\", min=1000, max=10000000)\n\n\nclass TforecastFactory(BaseFactory):\n    class Meta:\n        abstract = True\n\n    opportunity_id = factory.Sequence(lambda n: n)\n\n    posting_date = factory.Faker(\"date_between\", start_date=\"-3w\", end_date=\"now\")\n    archive_date = factory.Faker(\"date_between\", start_date=\"+3w\", end_date=\"+4w\")\n    forecast_desc = factory.Faker(\"summary_description\")\n    oth_cat_fa_desc = sometimes_none(factory.Faker(\"paragraph\", nb_sentences=1))\n\n    cost_sharing = sometimes_none(factory.Faker(\"yn_yesno_boolean\"), none_chance=0.1)\n    # These int values are stored as strings\n    number_of_awards = sometimes_none(\n        factory.LazyFunction(lambda: str(fake.random_int(1, 25))), none_chance=0.1\n    )\n    est_funding = sometimes_none(\n        factory.LazyFunction(lambda: str(fake.random_int(25_000, 25_000_000, step=5_000))),\n        none_chance=0.1,\n    )\n    award_ceiling = sometimes_none(\n        factory.LazyFunction(lambda: str(fake.random_int(10_000, 25_000, step=5_000))),\n        none_chance=0.1,\n    )\n    award_floor = sometimes_none(\n        factory.LazyFunction(lambda: str(fake.random_int(0, 10_000, step=5_000))), none_chance=0.1\n    )\n\n    fd_link_url = factory.Faker(\"relevant_url\")\n    fd_link_desc = factory.Faker(\"additional_info_desc\")\n    ac_email_addr = factory.Faker(\"email\")\n    ac_email_desc = factory.LazyAttribute(lambda s: f\"Contact {s.ac_name} via email\")\n    agency_code = factory.Faker(\"agency_code\")\n    ac_phone = Generators.PhoneNumber\n    ac_name = factory.Faker(\"agency_name\")\n\n    created_date = factory.Faker(\"date_time_between\", start_date=\"-10y\", end_date=\"-5y\")\n    last_upd_date = sometimes_none(\n        factory.Faker(\"date_time_between\", start_date=\"-5y\", end_date=\"now\")\n    )\n    create_ts = factory.Faker(\"date_time_between\", start_date=\"-10y\", end_date=\"-5y\")\n    sendmail = sometimes_none(factory.Faker(\"yn_yesno_boolean\"))\n    applicant_elig_desc = sometimes_none(factory.Faker(\"paragraph\", nb_sentences=5))\n    version_nbr = factory.Faker(\"random_int\", min=0, max=10)\n    modification_comments = sometimes_none(factory.Faker(\"paragraph\", nb_sentences=1))\n    publisheruid = sometimes_none(factory.Faker(\"first_name\"))\n    publisher_profile_id = sometimes_none(factory.Faker(\"random_int\", min=1, max=99_999))\n\n    est_synopsis_posting_date = sometimes_none(\n        factory.Faker(\"date_between\", start_date=\"+2w\", end_date=\"+3w\")\n    )\n    est_appl_response_date = sometimes_none(\n        factory.Faker(\"date_between\", start_date=\"+4w\", end_date=\"+6w\")\n    )\n    est_appl_response_date_desc = sometimes_none(factory.Faker(\"paragraph\", nb_sentences=1))\n    est_award_date = sometimes_none(\n        factory.Faker(\"date_between\", start_date=\"+26w\", end_date=\"+30w\")\n    )\n    est_project_start_date = sometimes_none(\n        factory.Faker(\"date_between\", start_date=\"+30w\", end_date=\"+52w\")\n    )\n    fiscal_year = factory.LazyAttribute(\n        lambda f: f.est_project_start_date.year if f.est_project_start_date else None\n    )\n\n\nclass TapplicanttypesFactory(BaseFactory):\n    # Base abstract factory for both TapplicanttypesForecast and TapplicanttypesSynopsis\n    class Meta:\n        abstract = True\n\n    at_id = factory.Iterator(LEGACY_APPLICANT_TYPE_IDS)\n\n    opportunity_id = factory.Sequence(lambda n: n)\n\n    created_date = factory.Faker(\"date_time_between\", start_date=\"-10y\", end_date=\"-5y\")\n    last_upd_date = sometimes_none(\n        factory.Faker(\"date_time_between\", start_date=\"-5y\", end_date=\"now\")\n    )\n\n    last_upd_id = factory.Faker(\"first_name\")\n    creator_id = factory.Faker(\"first_name\")\n\n\nclass TfundactcatFactory(BaseFactory):\n    class Meta:\n        abstract = True\n\n    fac_id = factory.Iterator(LEGACY_FUNDING_CATEGORY_IDS)\n\n    opportunity_id = factory.Sequence(lambda n: n)\n\n    created_date = factory.Faker(\"date_time_between\", start_date=\"-10y\", end_date=\"-5y\")\n    last_upd_date = sometimes_none(\n        factory.Faker(\"date_time_between\", start_date=\"-5y\", end_date=\"now\")\n    )\n\n    creator_id = factory.Faker(\"first_name\")\n    last_upd_id = factory.Faker(\"first_name\")\n\n\nclass TfundinstrFactory(BaseFactory):\n    class Meta:\n        abstract = True\n\n    fi_id = factory.Iterator(LEGACY_FUNDING_INSTRUMENT_IDS)\n\n    opportunity_id = factory.Sequence(lambda n: n)\n\n    created_date = factory.Faker(\"date_time_between\", start_date=\"-10y\", end_date=\"-5y\")\n    last_upd_date = sometimes_none(\n        factory.Faker(\"date_time_between\", start_date=\"-5y\", end_date=\"now\")\n    )\n\n    creator_id = factory.Faker(\"first_name\")\n    last_upd_id = factory.Faker(\"first_name\")\n\n\n####################################\n# Staging Table Factories\n####################################\n\n\nclass AbstractStagingFactory(BaseFactory):\n    class Meta:\n        abstract = True\n\n    # Default to being a new insert/update\n    is_deleted = False\n    transformed_at = None\n\n    class Params:\n        already_transformed = factory.Trait(\n            transformed_at=factory.Faker(\"date_time_between\", start_date=\"-7d\", end_date=\"-1d\")\n        )\n\n\nclass StagingTopportunityFactory(TopportunityFactory, AbstractStagingFactory):\n    class Meta:\n        model = staging.opportunity.Topportunity\n\n    cfdas = factory.RelatedFactoryList(\n        \"tests.src.db.models.factories.StagingTopportunityCfdaFactory\",\n        factory_related_name=\"opportunity\",\n        size=lambda: random.randint(1, 3),\n    )\n\n    class Params:\n        # Trait to set all nullable fields to None\n        all_fields_null = factory.Trait(\n            oppnumber=None,\n            revision_number=None,\n            opptitle=None,\n            owningagency=None,\n            oppcategory=None,\n            category_explanation=None,\n        )\n\n\nclass StagingTopportunityCfdaFactory(TopportunityCfdaFactory, AbstractStagingFactory):\n    class Meta:\n        model = staging.opportunity.TopportunityCfda\n\n    opportunity = factory.SubFactory(StagingTopportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.opportunity.opportunity_id)\n\n    class Params:\n        # Trait to set all nullable fields to None\n        all_fields_null = factory.Trait(\n            programtitle=None,\n            cfdanumber=None,\n        )\n\n\nclass StagingTsynopsisFactory(TsynopsisFactory, AbstractStagingFactory):\n    class Meta:\n        model = staging.synopsis.Tsynopsis\n\n    opportunity = factory.SubFactory(StagingTopportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.opportunity.opportunity_id)\n\n\nclass StagingTsynopsisHistFactory(StagingTsynopsisFactory):\n    class Meta:\n        model = staging.synopsis.TsynopsisHist\n\n    revision_number = factory.Faker(\"random_int\", min=1, max=25)\n    action_type = \"U\"  # Update, put D for deleted\n\n\nclass StagingTforecastFactory(TforecastFactory, AbstractStagingFactory):\n    class Meta:\n        model = staging.forecast.Tforecast\n\n    opportunity = factory.SubFactory(StagingTopportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.opportunity.opportunity_id)\n\n\nclass StagingTforecastHistFactory(StagingTforecastFactory):\n    class Meta:\n        model = staging.forecast.TforecastHist\n\n    revision_number = factory.Faker(\"random_int\", min=1, max=25)\n    action_type = \"U\"  # Update, put D for deleted\n\n\nclass StagingTapplicanttypesForecastFactory(TapplicanttypesFactory, AbstractStagingFactory):\n    class Meta:\n        model = staging.forecast.TapplicanttypesForecast\n\n    at_frcst_id = factory.Sequence(lambda n: n)\n\n    forecast = factory.SubFactory(StagingTforecastFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.forecast.opportunity_id)\n\n    class Params:\n        orphaned_record = factory.Trait(\n            forecast=None, opportunity_id=factory.Faker(\"random_int\", min=10_000, max=50_000)\n        )\n\n\nclass StagingTapplicanttypesForecastHistFactory(StagingTapplicanttypesForecastFactory):\n    class Meta:\n        model = staging.forecast.TapplicanttypesForecastHist\n\n    forecast = factory.SubFactory(StagingTforecastHistFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.forecast.opportunity_id)\n    revision_number = factory.LazyAttribute(lambda s: s.forecast.revision_number)\n\n\nclass StagingTapplicanttypesSynopsisFactory(TapplicanttypesFactory, AbstractStagingFactory):\n    class Meta:\n        model = staging.synopsis.TapplicanttypesSynopsis\n\n    at_syn_id = factory.Sequence(lambda n: n)\n\n    synopsis = factory.SubFactory(StagingTsynopsisFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.synopsis.opportunity_id)\n\n    class Params:\n        orphaned_record = factory.Trait(\n            synopsis=None, opportunity_id=factory.Faker(\"random_int\", min=10_000, max=50_000)\n        )\n\n\nclass StagingTapplicanttypesSynopsisHistFactory(StagingTapplicanttypesSynopsisFactory):\n    class Meta:\n        model = staging.synopsis.TapplicanttypesSynopsisHist\n\n    synopsis = factory.SubFactory(StagingTsynopsisHistFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.synopsis.opportunity_id)\n    revision_number = factory.LazyAttribute(lambda s: s.synopsis.revision_number)\n\n\nclass StagingTfundactcatForecastFactory(TfundactcatFactory, AbstractStagingFactory):\n    class Meta:\n        model = staging.forecast.TfundactcatForecast\n\n    fac_frcst_id = factory.Sequence(lambda n: n)\n\n    forecast = factory.SubFactory(StagingTforecastFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.forecast.opportunity_id)\n\n    class Params:\n        orphaned_record = factory.Trait(\n            forecast=None, opportunity_id=factory.Faker(\"random_int\", min=10_000, max=50_000)\n        )\n\n\nclass StagingTfundactcatForecastHistFactory(StagingTfundactcatForecastFactory):\n    class Meta:\n        model = staging.forecast.TfundactcatForecastHist\n\n    forecast = factory.SubFactory(StagingTforecastHistFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.forecast.opportunity_id)\n    revision_number = factory.LazyAttribute(lambda s: s.forecast.revision_number)\n\n\nclass StagingTfundactcatSynopsisFactory(TfundactcatFactory, AbstractStagingFactory):\n    class Meta:\n        model = staging.synopsis.TfundactcatSynopsis\n\n    fac_syn_id = factory.Sequence(lambda n: n)\n\n    synopsis = factory.SubFactory(StagingTsynopsisFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.synopsis.opportunity_id)\n\n    class Params:\n        orphaned_record = factory.Trait(\n            synopsis=None, opportunity_id=factory.Faker(\"random_int\", min=10_000, max=50_000)\n        )\n\n\nclass StagingTfundactcatSynopsisHistFactory(StagingTfundactcatSynopsisFactory):\n    class Meta:\n        model = staging.synopsis.TfundactcatSynopsisHist\n\n    synopsis = factory.SubFactory(StagingTsynopsisHistFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.synopsis.opportunity_id)\n    revision_number = factory.LazyAttribute(lambda s: s.synopsis.revision_number)\n\n\nclass StagingTfundinstrForecastFactory(TfundinstrFactory, AbstractStagingFactory):\n    class Meta:\n        model = staging.forecast.TfundinstrForecast\n\n    fi_frcst_id = factory.Sequence(lambda n: n)\n\n    forecast = factory.SubFactory(StagingTforecastFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.forecast.opportunity_id)\n\n    class Params:\n        orphaned_record = factory.Trait(\n            forecast=None, opportunity_id=factory.Faker(\"random_int\", min=10_000, max=50_000)\n        )\n\n\nclass StagingTfundinstrForecastHistFactory(StagingTfundinstrForecastFactory):\n    class Meta:\n        model = staging.forecast.TfundinstrForecastHist\n\n    forecast = factory.SubFactory(StagingTforecastHistFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.forecast.opportunity_id)\n    revision_number = factory.LazyAttribute(lambda s: s.forecast.revision_number)\n\n\nclass StagingTfundinstrSynopsisFactory(TfundinstrFactory, AbstractStagingFactory):\n    class Meta:\n        model = staging.synopsis.TfundinstrSynopsis\n\n    fi_syn_id = factory.Sequence(lambda n: n)\n\n    synopsis = factory.SubFactory(StagingTsynopsisFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.synopsis.opportunity_id)\n\n    class Params:\n        orphaned_record = factory.Trait(\n            synopsis=None, opportunity_id=factory.Faker(\"random_int\", min=10_000, max=50_000)\n        )\n\n\nclass StagingTfundinstrSynopsisHistFactory(StagingTfundinstrSynopsisFactory):\n    class Meta:\n        model = staging.synopsis.TfundinstrSynopsisHist\n\n    synopsis = factory.SubFactory(StagingTsynopsisHistFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.synopsis.opportunity_id)\n    revision_number = factory.LazyAttribute(lambda s: s.synopsis.revision_number)\n\n\nclass StagingTgroupsFactory(AbstractStagingFactory):\n    class Meta:\n        model = staging.tgroups.Tgroups\n\n    keyfield = \"\"\n    value = \"\"\n\n    created_date = factory.Faker(\"date_time_between\", start_date=\"-10y\", end_date=\"-5y\")\n    last_upd_date = sometimes_none(\n        factory.Faker(\"date_time_between\", start_date=\"-5y\", end_date=\"now\")\n    )\n\n    last_upd_id = factory.Faker(\"first_name\")\n    creator_id = factory.Faker(\"first_name\")\n\n\nclass StagingTsynopsisAttachmentFactory(TsynopsisAttachmentFactory, AbstractStagingFactory):\n    class Meta:\n        model = staging.attachment.TsynopsisAttachment\n\n    opportunity = factory.SubFactory(StagingTopportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda o: o.opportunity.opportunity_id)\n\n\n####################################\n# Foreign Table Factories\n####################################\n\n\nclass ForeignTopportunityFactory(TopportunityFactory):\n    class Meta:\n        model = foreign.opportunity.Topportunity\n\n    @classmethod\n    def _setup_next_sequence(cls):\n        if _db_session is not None:\n            value = _db_session.query(\n                func.max(foreign.opportunity.Topportunity.opportunity_id)\n            ).scalar()\n            if value is not None:\n                return value + 1\n\n        return 1\n\n    cfdas = factory.RelatedFactoryList(\n        \"tests.src.db.models.factories.ForeignTopportunityCfdaFactory\",\n        factory_related_name=\"opportunity\",\n        size=lambda: random.randint(1, 3),\n    )\n\n\nclass ForeignTopportunityCfdaFactory(TopportunityCfdaFactory):\n    class Meta:\n        model = foreign.opportunity.TopportunityCfda\n\n    @classmethod\n    def _setup_next_sequence(cls):\n        if _db_session is not None:\n            value = _db_session.query(\n                func.max(foreign.opportunity.TopportunityCfda.opp_cfda_id)\n            ).scalar()\n            if value is not None:\n                return value + 1\n\n        return 1\n\n    opportunity = factory.SubFactory(ForeignTopportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.opportunity.opportunity_id)\n\n\nclass ForeignTsynopsisFactory(TsynopsisFactory):\n    class Meta:\n        model = foreign.synopsis.Tsynopsis\n\n    opportunity = factory.SubFactory(ForeignTopportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.opportunity.opportunity_id)\n\n\nclass ForeignTsynopsisHistFactory(ForeignTsynopsisFactory):\n    class Meta:\n        model = foreign.synopsis.TsynopsisHist\n\n    revision_number = factory.Faker(\"random_int\", min=1, max=25)\n    action_type = \"U\"  # Update, put D for deleted\n\n\nclass ForeignTforecastFactory(TforecastFactory):\n    class Meta:\n        model = foreign.forecast.Tforecast\n\n    opportunity = factory.SubFactory(ForeignTopportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.opportunity.opportunity_id)\n\n\nclass ForeignTforecastHistFactory(ForeignTforecastFactory):\n    class Meta:\n        model = foreign.forecast.TforecastHist\n\n    revision_number = factory.Faker(\"random_int\", min=1, max=25)\n    action_type = \"U\"  # Update, put D for deleted\n\n\nclass ForeignTapplicanttypesForecastFactory(TapplicanttypesFactory):\n    class Meta:\n        model = foreign.forecast.TapplicanttypesForecast\n\n    at_frcst_id = factory.Sequence(lambda n: n)\n\n    forecast = factory.SubFactory(ForeignTforecastFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.forecast.opportunity_id)\n\n\nclass ForeignTapplicanttypesForecastHistFactory(ForeignTapplicanttypesForecastFactory):\n    class Meta:\n        model = foreign.forecast.TapplicanttypesForecastHist\n\n    forecast = factory.SubFactory(ForeignTforecastHistFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.forecast.opportunity_id)\n    revision_number = factory.LazyAttribute(lambda s: s.forecast.revision_number)\n\n\nclass ForeignTapplicanttypesSynopsisFactory(TapplicanttypesFactory):\n    class Meta:\n        model = foreign.synopsis.TapplicanttypesSynopsis\n\n    at_syn_id = factory.Sequence(lambda n: n)\n\n    synopsis = factory.SubFactory(ForeignTsynopsisFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.synopsis.opportunity_id)\n\n\nclass ForeignTapplicanttypesSynopsisHistFactory(ForeignTapplicanttypesSynopsisFactory):\n    class Meta:\n        model = foreign.synopsis.TapplicanttypesSynopsisHist\n\n    synopsis = factory.SubFactory(ForeignTsynopsisHistFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.synopsis.opportunity_id)\n    revision_number = factory.LazyAttribute(lambda s: s.synopsis.revision_number)\n\n\nclass ForeignTfundactcatForecastFactory(TfundactcatFactory):\n    class Meta:\n        model = foreign.forecast.TfundactcatForecast\n\n    fac_frcst_id = factory.Sequence(lambda n: n)\n\n    forecast = factory.SubFactory(ForeignTforecastFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.forecast.opportunity_id)\n\n\nclass ForeignTfundactcatForecastHistFactory(ForeignTfundactcatForecastFactory):\n    class Meta:\n        model = foreign.forecast.TfundactcatForecastHist\n\n    forecast = factory.SubFactory(ForeignTforecastHistFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.forecast.opportunity_id)\n    revision_number = factory.LazyAttribute(lambda s: s.forecast.revision_number)\n\n\nclass ForeignTfundactcatSynopsisFactory(TfundactcatFactory):\n    class Meta:\n        model = foreign.synopsis.TfundactcatSynopsis\n\n    fac_syn_id = factory.Sequence(lambda n: n)\n\n    synopsis = factory.SubFactory(ForeignTsynopsisFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.synopsis.opportunity_id)\n\n\nclass ForeignTfundactcatSynopsisHistFactory(ForeignTfundactcatSynopsisFactory):\n    class Meta:\n        model = foreign.synopsis.TfundactcatSynopsisHist\n\n    synopsis = factory.SubFactory(ForeignTsynopsisHistFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.synopsis.opportunity_id)\n    revision_number = factory.LazyAttribute(lambda s: s.synopsis.revision_number)\n\n\nclass ForeignTfundinstrForecastFactory(TfundinstrFactory):\n    class Meta:\n        model = foreign.forecast.TfundinstrForecast\n\n    fi_frcst_id = factory.Sequence(lambda n: n)\n\n    forecast = factory.SubFactory(ForeignTforecastFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.forecast.opportunity_id)\n\n\nclass ForeignTfundinstrForecastHistFactory(ForeignTfundinstrForecastFactory):\n    class Meta:\n        model = foreign.forecast.TfundinstrForecastHist\n\n    forecast = factory.SubFactory(ForeignTforecastHistFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.forecast.opportunity_id)\n    revision_number = factory.LazyAttribute(lambda s: s.forecast.revision_number)\n\n\nclass ForeignTfundinstrSynopsisFactory(TfundinstrFactory):\n    class Meta:\n        model = staging.synopsis.TfundinstrSynopsis\n\n    fi_syn_id = factory.Sequence(lambda n: n)\n\n    synopsis = factory.SubFactory(StagingTsynopsisFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.synopsis.opportunity_id)\n\n\nclass ForeignTfundinstrSynopsisHistFactory(ForeignTfundinstrSynopsisFactory):\n    class Meta:\n        model = foreign.synopsis.TfundinstrSynopsisHist\n\n    synopsis = factory.SubFactory(ForeignTsynopsisHistFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.synopsis.opportunity_id)\n    revision_number = factory.LazyAttribute(lambda s: s.synopsis.revision_number)\n\n\nclass ForeignTsynopsisAttachmentFactory(TsynopsisAttachmentFactory):\n    class Meta:\n        model = foreign.attachment.TsynopsisAttachment\n\n    opportunity = factory.SubFactory(ForeignTopportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda o: o.opportunity.opportunity_id)\n\n\n##\n# Pseudo-factories\n##\n\n\nclass StagingTgroupsAgencyFactory(factory.DictFactory):\n    \"\"\"\n        This does not need to be called directly, and instead you should use\n    create_tgroups_agency (defined below) in order to call this.\n\n    We use this to help organize factories / the ability to override and set\n    values for the tgroups agency data which is spread across many rows.\n\n    Note: Any value that is \"None\" will not be included in the created\n          tgroups records (empty strings, or strings of values like \"null\" will be)\n    \"\"\"\n\n    AgencyName = factory.Faker(\"agency_name\")\n    AgencyCode = \"\"  # see: create_tgroups_agency for how this gets set\n    AgencyCFDA = factory.Faker(\"random_int\", min=1, max=99)\n    AgencyDownload = factory.Faker(\"random_int\", min=1, max=3)\n    AgencyNotify = factory.Faker(\"random_int\", min=1, max=3)\n    AgencyEnroll = \"\"  # see: create_tgroups_agency for how this gets set\n\n    AgencyContactName = factory.Faker(\"name\")\n    AgencyContactAddress1 = factory.Faker(\"street_address\")\n    AgencyContactAddress2 = factory.Maybe(\n        decider=factory.LazyAttribute(lambda s: random.random() > 0.5),\n        yes_declaration=factory.Sequence(lambda n: f\"Room {n}\"),\n        no_declaration=\"NULL\",\n    )\n    AgencyContactCity = factory.Faker(\"city\")\n    AgencyContactState = factory.Faker(\"state_abbr\")\n    AgencyContactZipCode = factory.Faker(\"postcode\")\n    AgencyContactTelephone = Generators.PhoneNumber\n    AgencyContactEMail = factory.Faker(\"email\")\n    AgencyContactEMail2 = sometimes_none(factory.Faker(\"email\"))\n\n    ldapGp = \"\"  # see: create_tgroups_agency for how this gets set\n    description = factory.LazyAttribute(lambda g: g.AgencyName)\n    label = factory.LazyAttribute(lambda g: g.AgencyName)\n    multilevel = sometimes_none(\"TRUE\", none_chance=0.8)\n\n    HasS2SCert = sometimes_none(factory.Faker(\"yn_boolean\"), none_chance=0.8)\n    ViewPkgsInGracePeriod = sometimes_none(factory.Faker(\"yn_boolean\"), none_chance=0.8)\n    multiproject = sometimes_none(factory.Faker(\"yn_boolean\"), none_chance=0.8)\n    ImageWS = sometimes_none(factory.Faker(\"yn_boolean\"), none_chance=0.8)\n    ValidationWS = sometimes_none(factory.Faker(\"yn_boolean\"), none_chance=0.8)\n\n\ndef create_tgroups_agency(\n    agency_code: str,\n    is_deleted: bool = False,\n    is_already_processed: bool = False,\n    deleted_fields: set | None = None,\n    already_processed_fields: set | None = None,\n    **kwargs,\n) -> list[staging.tgroups.Tgroups]:\n    # The agency_code value is actually just the first bit (the top-level agency)\n    kwargs.setdefault(\"AgencyCode\", agency_code.split(\"-\")[-1])\n    kwargs.setdefault(\"AgencyEnroll\", agency_code)\n    kwargs.setdefault(\"ldapGp\", agency_code)\n\n    field_values = StagingTgroupsAgencyFactory.build(**kwargs)\n\n    groups = []\n\n    field_prefix = f\"Agency-{agency_code}-\"\n\n    if already_processed_fields is None:\n        already_processed_fields = set()\n\n    if deleted_fields is None:\n        deleted_fields = set()\n\n    for field_name, value in field_values.items():\n        if value is None:\n            continue\n\n        is_field_already_processed = is_already_processed or field_name in already_processed_fields\n        is_field_deleted = is_deleted or field_name in deleted_fields\n\n        tgroup = StagingTgroupsFactory.create(\n            keyfield=field_prefix + field_name,\n            value=str(value),\n            is_deleted=is_field_deleted,\n            already_transformed=is_field_already_processed,\n        )\n\n        groups.append(tgroup)\n\n    return groups\n\n\nclass OpportunityChangeAuditFactory(BaseFactory):\n    class Meta:\n        model = opportunity_models.OpportunityChangeAudit\n\n    opportunity = factory.SubFactory(OpportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda s: s.opportunity.opportunity_id)\n\n\nclass UserFactory(BaseFactory):\n    class Meta:\n        model = user_models.User\n\n    user_id = Generators.UuidObj\n\n\nclass LinkExternalUserFactory(BaseFactory):\n    class Meta:\n        model = user_models.LinkExternalUser\n\n    link_external_user_id = Generators.UuidObj\n    external_user_id = Generators.UuidObj\n\n    user = factory.SubFactory(UserFactory)\n    user_id = factory.LazyAttribute(lambda s: s.user.user_id)\n\n    external_user_type = factory.fuzzy.FuzzyChoice(ExternalUserType)\n\n    email = factory.Faker(\"email\")\n\n\nclass UserNotificationLogFactory(BaseFactory):\n    class Meta:\n        model = user_models.UserNotificationLog\n\n    user_notification_log_id = Generators.UuidObj\n\n    user = factory.SubFactory(UserFactory)\n    user_id = factory.LazyAttribute(lambda s: s.user.user_id)\n\n    notification_reason = \"test\"\n    notification_sent = True\n\n\nclass LoginGovStateFactory(BaseFactory):\n    class Meta:\n        model = user_models.LoginGovState\n\n    login_gov_state_id = Generators.UuidObj\n    nonce = Generators.UuidObj\n\n\nclass ExtractMetadataFactory(BaseFactory):\n    class Meta:\n        model = extract_models.ExtractMetadata\n\n    extract_type = factory.fuzzy.FuzzyChoice(ExtractType)\n    file_name = factory.Faker(\"file_name\")\n    file_path = \"s3://bucket/key\"\n    file_size_bytes = factory.Faker(\"random_int\", min=1, max=1000000)\n\n\nclass UserTokenSessionFactory(BaseFactory):\n    class Meta:\n        model = user_models.UserTokenSession\n\n    user = factory.SubFactory(UserFactory)\n    user_id = factory.LazyAttribute(lambda s: s.user.user_id)\n\n    token_id = Generators.UuidObj\n\n    expires_at = factory.Faker(\"date_time_between\", start_date=\"+1d\", end_date=\"+10d\")\n\n    is_valid = True\n\n\nclass UserSavedOpportunityFactory(BaseFactory):\n    class Meta:\n        model = user_models.UserSavedOpportunity\n\n    user = factory.SubFactory(UserFactory)\n    user_id = factory.LazyAttribute(lambda o: o.user.user_id)\n\n    opportunity = factory.SubFactory(OpportunityFactory)\n    opportunity_id = factory.LazyAttribute(lambda o: o.opportunity.opportunity_id)\n\n\nclass UserSavedSearchFactory(BaseFactory):\n    class Meta:\n        model = user_models.UserSavedSearch\n\n    user = factory.SubFactory(UserFactory)\n    user_id = factory.LazyAttribute(lambda s: s.user.user_id)\n\n    saved_search_id = Generators.UuidObj\n\n    name = factory.Faker(\"sentence\")\n\n    search_query = factory.LazyAttribute(lambda s: s.search_query)\n\n    last_notified_at = factory.Faker(\"date_time_between\", start_date=\"-5y\", end_date=\"-3y\")\n\n    searched_opportunity_ids = factory.LazyAttribute(lambda _: random.sample(range(1, 1000), 5))"}
{"path":"frontend/tests/hooks/useFeatureFlags.test.ts","language":"typescript","type":"code","directory":"frontend/tests/hooks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/hooks/useFeatureFlags.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"File: api/tests/src/db/models/lookup/__init__.py\nLanguage: py\nType: model\nDirectory: api/tests/src/db/models/lookup\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/lookup/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/hooks/usePrevious.test.ts","language":"typescript","type":"code","directory":"frontend/tests/hooks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/hooks/usePrevious.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":""}
{"path":"frontend/tests/hooks/useSearchParamUpdater.test.ts","language":"typescript","type":"code","directory":"frontend/tests/hooks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/hooks/useSearchParamUpdater.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.137Z","content":"File: api/tests/src/db/models/lookup/test_lookup.py\nLanguage: py\nType: model\nDirectory: api/tests/src/db/models/lookup\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/lookup/test_lookup.py\nSize: 3.37 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/jest.setup.js","language":"javascript","type":"code","directory":"frontend/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/jest.setup.js","size":1644826,"lastModified":"2025-02-14T17:08:31.137Z","content":"import pytest\n\nfrom src.adapters.db.type_decorators.postgres_type_decorators import LookupColumn\nfrom src.db.models import metadata\nfrom src.db.models.lookup import LookupConfig, LookupStr\n\n\nclass EnumX(StrEnum):\n    A = \"A\"\n    B = \"B\"\n\n\nclass EnumY(StrEnum):\n    C = \"C\"\n    D = \"D\"\n\n\nclass EnumZ(StrEnum):\n    # Str values overlap with X and Y\n    B = \"B\"\n    C = \"C\"\n\n\ndef test_lookup_config():\n    config = LookupConfig(\n        [\n            LookupStr(EnumX.A, 1),\n            LookupStr(EnumX.B, 2),\n            LookupStr(EnumY.C, 3),\n            LookupStr(EnumY.D, 4),\n        ]\n    )\n\n    assert config.get_int_for_enum(EnumX.A) == 1\n    assert config.get_int_for_enum(EnumX.B) == 2\n    assert config.get_int_for_enum(EnumY.C) == 3\n    assert config.get_int_for_enum(EnumY.D) == 4\n\n    assert config.get_enum_for_int(1) == EnumX.A\n    assert config.get_enum_for_int(2) == EnumX.B\n    assert config.get_enum_for_int(3) == EnumY.C\n    assert config.get_enum_for_int(4) == EnumY.D\n\n\ndef test_lookup_config_duplicate_enum_str():\n    with pytest.raises(AttributeError, match=\"Duplicate lookup_enum B defined\"):\n        LookupConfig(\n            [\n                LookupStr(EnumX.A, 1),\n                LookupStr(EnumX.B, 2),\n                LookupStr(EnumZ.B, 3),\n            ]\n        )\n\n\ndef test_lookup_config_duplicate_lookup_val():\n    with pytest.raises(AttributeError, match=\"Duplicate lookup_val 1 defined\"):\n        LookupConfig(\n            [\n                LookupStr(EnumX.A, 1),\n                LookupStr(EnumX.B, 1),\n            ]\n        )\n\n\ndef test_lookup_config_missing_mapping():\n    with pytest.raises(\n        AttributeError,\n        match=\"Lookup config must define a mapping for all enum values, the following were missing: {<EnumX.B: 'B'>}\",\n    ):\n        LookupConfig([LookupStr(EnumX.A, 1)])\n\n\ndef test_lookup_config_negative():\n    with pytest.raises(\n        AttributeError,\n        match=\"Only positive lookup_val values are allowed\",\n    ):\n        LookupConfig([LookupStr(EnumX.A, -1)])\n\n\ndef test_lookup_config_zero():\n    with pytest.raises(\n        AttributeError,\n        match=\"Only positive lookup_val values are allowed\",\n    ):\n        LookupConfig([LookupStr(EnumX.A, 0)])\n\n\ndef test_lookup_columns_named_in_db_correctly():\n    \"\"\"\n    If you are seeing this fail, here's an example of a valid one (context below)::\n\n    pay_type: Mapped[PayType] = mapped_column(\n        \"pay_type_id\",  # <<< Make sure to specifically define the column name as the first parameter\n        LookupColumn(LkAdditionalPayType),\n        ForeignKey(LkAdditionalPayType.additional_pay_type_id),\n        nullable=False,\n    )\n\n    We want our lookup columns to always be named `<lookup_type>_id` in the DB\n    so that it's clear they are an ID pointing to another table and not the actual value.\n\n    It's fine if our in-code value doesn't have ID, as we aren't working with an ID,\n    we are instead working with the enum directly.\n\n    \"\"\"\n\n    # This will validate the behavior of all columns in every table derived\n    # from the PostgresBase class which is everything we'll ever generate migrations for\n    for table in metadata.tables.values():\n        for column in table.columns:\n            if isinstance(column.type, LookupColumn):\n                assert column.name.endswith(\n                    \"_id\"\n                ), f\"Lookup column {table.name}.{column.name} must be named with '_id' suffix\""}
{"path":"frontend/tests/pages/dev/feature-flags/page.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/pages/dev/feature-flags","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/pages/dev/feature-flags/page.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.137Z","content":"File: api/tests/src/db/models/lookup/test_lookup_registry.py\nLanguage: py\nType: model\nDirectory: api/tests/src/db/models/lookup\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/lookup/test_lookup_registry.py\nSize: 3.17 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/pages/maintenance/page.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/pages/maintenance","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/pages/maintenance/page.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.137Z","content":"import pytest\nfrom sqlalchemy import Column, Integer\n\nfrom src.db.models.lookup import Lookup, LookupConfig, LookupRegistry, LookupStr, LookupTable\n\n\nclass TmpEnum(StrEnum):\n    A = \"A\"\n    B = \"B\"\n    C = \"C\"\n\n\nclass AnotherEnum(StrEnum):\n    D = \"D\"\n    E = \"E\"\n\n\nTMP_LOOKUP_CONFIG = LookupConfig(\n    [\n        LookupStr(TmpEnum.A, 1),\n        LookupStr(TmpEnum.B, 2),\n        LookupStr(TmpEnum.C, 3),\n    ]\n)\n\n\nclass LkTmp(LookupTable):\n    __abstract__ = True  # Mark it abstract so SQLAlchemy and Alembic ignore it\n    __tablename__ = \"lk_tmp\"\n\n    tmp_id: int = Column(Integer, primary_key=True)\n\n    @classmethod\n    def from_lookup(cls, lookup: Lookup) -> \"LookupTable\":\n        pass\n\n\ndef test_lookup_registry():\n    try:\n        # This is the equivalent of adding @LookupRegistry.register_lookup(TmpLookup)\n        # on top of the LkTmp class, but without defining that at the module level\n        # that way we can reuse those classes.\n        LookupRegistry.register_lookup(TMP_LOOKUP_CONFIG)(LkTmp)\n\n        sync_values = LookupRegistry.get_sync_values()\n        assert LkTmp in sync_values  # Make sure it got added\n        assert sync_values[LkTmp] is TMP_LOOKUP_CONFIG\n\n        assert LookupRegistry.get_enum_for_lookup_int(LkTmp, 1) == TmpEnum.A\n        assert LookupRegistry.get_enum_for_lookup_int(LkTmp, 2) == TmpEnum.B\n        assert LookupRegistry.get_enum_for_lookup_int(LkTmp, 3) == TmpEnum.C\n        assert LookupRegistry.get_enum_for_lookup_int(LkTmp, 0) is None\n        assert LookupRegistry.get_enum_for_lookup_int(LkTmp, 4) is None\n        assert LookupRegistry.get_enum_for_lookup_int(LkTmp, None) is None\n\n        assert LookupRegistry.get_lookup_int_for_enum(LkTmp, TmpEnum.A) == 1\n        assert LookupRegistry.get_lookup_int_for_enum(LkTmp, TmpEnum.B) == 2\n        assert LookupRegistry.get_lookup_int_for_enum(LkTmp, TmpEnum.C) == 3\n        assert LookupRegistry.get_lookup_int_for_enum(LkTmp, AnotherEnum.D) is None\n        assert LookupRegistry.get_lookup_int_for_enum(LkTmp, AnotherEnum.E) is None\n        assert LookupRegistry.get_lookup_int_for_enum(LkTmp, None) is None\n    finally:\n        # Because the registry is global, remove LkTmp\n        # so it doesn't affect other tests that run\n        del LookupRegistry._lookup_registry[LkTmp]\n\n    # Verify we cleaned up properly\n    assert LkTmp not in LookupRegistry.get_sync_values()\n\n\ndef test_lookup_member_not_registered():\n    with pytest.raises(\n        Exception, match=\"Table lk_tmp does not have a registered lookup_config via register_lookup\"\n    ):\n        LookupRegistry.get_enum_for_lookup_int(LkTmp, 1)\n\n\ndef test_lookup_registry_duplicate_table():\n    try:\n        LookupRegistry.register_lookup(TMP_LOOKUP_CONFIG)(LkTmp)\n\n        with pytest.raises(\n            Exception,\n            match=\"Cannot attach lookup mapping to table lk_tmp, table already registered\",\n        ):\n            LookupRegistry.register_lookup(TMP_LOOKUP_CONFIG)(LkTmp)\n\n    finally:\n        # Because the registry is global, remove LkTmp\n        # so it doesn't affect other tests that run\n        del LookupRegistry._lookup_registry[LkTmp]\n\n    # Verify we cleaned up properly\n    assert LkTmp not in LookupRegistry.get_sync_values()"}
{"path":"frontend/tests/pages/not-found.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/pages","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/pages/not-found.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/db/models/lookup/test_sync_lookup_values.py\nLanguage: py\nType: model\nDirectory: api/tests/src/db/models/lookup\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/lookup/test_sync_lookup_values.py\nSize: 4.13 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/pages/page.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/pages","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/pages/page.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":"import pytest\nfrom sqlalchemy import inspect\n\nimport src.adapters.db as db\nimport src.db.models as db_models\nfrom src.db.models.lookup import LookupConfig, LookupRegistry, LookupStr, LookupTable\nfrom src.db.models.lookup.sync_lookup_values import sync_lookup_values\nfrom src.db.models.lookup_models import LkOpportunityCategory\nfrom tests.lib import db_testing\n\n\n@pytest.fixture\ndef schema_no_lookup(monkeypatch) -> db.PostgresDBClient:\n    \"\"\"\n    Create a test schema, if it doesn't already exist, and drop it after the\n    test completes.\n\n    This is similar to what the db_client fixture does but does not create any tables in the\n    schema.\n    \"\"\"\n    with db_testing.create_isolated_db(\n        monkeypatch, f\"test_lookup_{uuid.uuid4().int}_\"\n    ) as db_client:\n        db_models.metadata.create_all(bind=db_client._engine)\n        # Skipping the sync that normally occurs to do in tests below\n        yield db_client\n\n\ndef validate_lookup_synced_to_table(\n    db_session, table: Type[LookupTable], lookup_config: LookupConfig\n):\n    db_lookup_values = db_session.query(table).all()\n\n    assert len(db_lookup_values) == len(lookup_config.get_lookups())\n\n    primary_key = inspect(table).primary_key[0].name\n\n    # Verify the values match by seeing if we can convert the descriptions\n    for db_lookup_value in db_lookup_values:\n        id_value = getattr(db_lookup_value, primary_key)\n        lookup_value = lookup_config.get_lookup_for_int(id_value)\n\n        assert lookup_value is not None\n        assert db_lookup_value.description == lookup_value.get_description()\n\n\ndef test_sync_lookup_for_table_sanity(db_session):\n    # Note that db_session calls in a fixture that\n    # does the syncing, this test is making sure our tests\n    # are working with the correct data.\n    sync_values = LookupRegistry.get_sync_values()\n\n    assert len(sync_values) > 0\n\n    # Verify all of our values are in the DB\n    for table, lookup in sync_values.items():\n        validate_lookup_synced_to_table(db_session, table, lookup)\n\n\nclass NewOpportunityCategory(StrEnum):\n    A = \"A\"\n    B = \"B\"\n    C = \"C\"\n    D = \"D\"\n    E = \"E\"\n    F = \"F\"\n    G = \"G\"\n\n\nNEW_OPPORTUNITY_CATEGORY_CONFIG = LookupConfig(\n    [\n        LookupStr(NewOpportunityCategory.A, 1),\n        LookupStr(NewOpportunityCategory.B, 2),\n        LookupStr(NewOpportunityCategory.C, 3),\n        LookupStr(NewOpportunityCategory.D, 4),\n        LookupStr(NewOpportunityCategory.E, 5),\n        LookupStr(NewOpportunityCategory.F, 6),\n        LookupStr(NewOpportunityCategory.G, 7),\n    ]\n)\n\n\ndef test_sync_lookup_for_table(schema_no_lookup, caplog: pytest.LogCaptureFixture):\n    caplog.set_level(logging.INFO)\n    with schema_no_lookup.get_session() as db_session:\n        sync_values = LookupRegistry.get_sync_values()\n        for table in sync_values.keys():\n            assert db_session.query(table).count() == 0\n\n    # Sync the lookup values to the DB\n    sync_lookup_values(schema_no_lookup)\n\n    with schema_no_lookup.get_session() as db_session:\n        for table, lookup in sync_values.items():\n            validate_lookup_synced_to_table(db_session, table, lookup)\n\n    # Running sync again won't cause any change\n    caplog.clear()\n    sync_lookup_values(schema_no_lookup)\n    assert \"No modified lookup values for table lk_opportunity_category\" in caplog.text\n\n    # Modify the lookup values used for one of the lookups\n    # in order to test that updates work, but reset it afterwards\n    # to avoid breaking other tests.\n    existing_suffix_config = LookupRegistry._lookup_registry[LkOpportunityCategory]\n    try:\n        LookupRegistry._lookup_registry[LkOpportunityCategory] = NEW_OPPORTUNITY_CATEGORY_CONFIG\n        caplog.clear()\n        sync_lookup_values(schema_no_lookup)\n        assert \"Updated lookup value in table lk_opportunity_category to\" in caplog.text\n\n        with schema_no_lookup.get_session() as db_session:\n            validate_lookup_synced_to_table(\n                db_session, LkOpportunityCategory, NEW_OPPORTUNITY_CATEGORY_CONFIG\n            )\n\n    finally:\n        LookupRegistry._lookup_registry[LkOpportunityCategory] = existing_suffix_config"}
{"path":"frontend/tests/pages/process/page.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/pages/process","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/pages/process/page.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/db/models/test_factories.py\nLanguage: py\nType: model\nDirectory: api/tests/src/db/models\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/test_factories.py\nSize: 2.92 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/pages/research/page.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/pages/research","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/pages/research/page.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":"import pytest\n\nimport src.adapters.db as db\nfrom src.constants.lookup_constants import OpportunityCategory\nfrom src.db.models.opportunity_models import Opportunity\nfrom tests.src.db.models.factories import OpportunityFactory\n\nopportunity_params = {\n    \"opportunity_number\": 100123456,\n    \"opportunity_title\": \"Study math\",\n    \"agency_code\": \"ABC-XYZ\",\n    \"category\": OpportunityCategory.CONTINUATION,\n    \"is_draft\": False,\n}\n\n\ndef validate_opportunity_record(opportunity: Opportunity, expected_values=None):\n    if expected_values:\n        assert opportunity.opportunity_id is not None\n        for k, v in expected_values.items():\n            opportunity_v = getattr(opportunity, k)\n            if isinstance(opportunity_v, datetime):\n                opportunity_v = opportunity_v.isoformat()\n\n            assert str(opportunity_v) == str(v)\n\n    else:\n        # Otherwise just validate the values are set\n        assert opportunity.opportunity_id is not None\n        assert opportunity.opportunity_title is not None\n        assert opportunity.opportunity_number is not None\n        assert opportunity.is_draft is not None\n        assert opportunity.category is not None\n\n\ndef test_opportunity_factory_build():\n    # Build doesn't use the DB\n\n    # Build sets the values\n    opportunity = OpportunityFactory.build()\n    validate_opportunity_record(opportunity)\n\n    opportunity = OpportunityFactory.build(**opportunity_params)\n    validate_opportunity_record(opportunity, opportunity_params)\n\n\ndef test_factory_create_uninitialized_db_session():\n    # DB factory access is disabled from tests unless you add the\n    # 'enable_factory_create' fixture.\n    with pytest.raises(Exception, match=\"Factory db_session is not initialized.\"):\n        OpportunityFactory.create()\n\n\ndef test_opportunity_factory_create(enable_factory_create, db_session: db.Session):\n    # Create actually writes a record to the DB when run\n    # so we'll check the DB directly as well.\n    opportunity = OpportunityFactory.create()\n    validate_opportunity_record(opportunity)\n\n    db_record = (\n        db_session.query(Opportunity)\n        .filter(Opportunity.opportunity_id == opportunity.opportunity_id)\n        .one_or_none()\n    )\n    # Make certain the DB record matches the factory one.\n    validate_opportunity_record(db_record, db_record.for_json())\n\n    opportunity = OpportunityFactory.create(**opportunity_params)\n    validate_opportunity_record(opportunity, opportunity_params)\n\n    db_record = (\n        db_session.query(Opportunity)\n        .filter(Opportunity.opportunity_id == opportunity.opportunity_id)\n        .one_or_none()\n    )\n    # Make certain the DB record matches the factory one.\n    validate_opportunity_record(db_record, db_record.for_json())\n\n    # Make certain nullable fields can be overriden\n    null_params = {\"agency_code\": None}\n    opportunity = OpportunityFactory.create(**null_params)\n    validate_opportunity_record(opportunity, null_params)"}
{"path":"frontend/tests/pages/saved-grants/page.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/pages/saved-grants","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/pages/saved-grants/page.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/db/models/test_relationships.py\nLanguage: py\nType: model\nDirectory: api/tests/src/db/models\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/models/test_relationships.py\nSize: 14.47 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/pages/search/page.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/pages/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/pages/search/page.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":"import random\n\nimport pytest\nfrom sqlalchemy import select\n\nfrom src.constants.lookup_constants import ApplicantType, FundingCategory, FundingInstrument\nfrom src.db.models.agency_models import Agency\nfrom src.db.models.lookup_models import LkApplicantType, LkFundingCategory, LkFundingInstrument\nfrom src.db.models.opportunity_models import (\n    CurrentOpportunitySummary,\n    Opportunity,\n    OpportunityAssistanceListing,\n    OpportunityAttachment,\n    OpportunitySummary,\n)\nfrom tests.src.db.models.factories import (\n    AgencyFactory,\n    CurrentOpportunitySummaryFactory,\n    OpportunityAssistanceListingFactory,\n    OpportunityAttachmentFactory,\n    OpportunityFactory,\n    OpportunitySummaryFactory,\n)\n\n\ndef setup_opportunity():\n    # For setting up an opportunity, we always want to create something\n    # with multiple records of each type (where possible) so we can later check if those were deleted\n    agency = AgencyFactory.create(agency_code=f\"XYZ-ABC-{random.randint(1, 100_000_000)}\")\n\n    opportunity = OpportunityFactory.create(\n        opportunity_assistance_listings=[],\n        opportunity_attachments=[],\n        no_current_summary=True,\n        agency_code=agency.agency_code,\n    )\n\n    OpportunityAssistanceListingFactory.create_batch(size=3, opportunity=opportunity)\n\n    OpportunityAttachmentFactory.create_batch(size=3, opportunity=opportunity)\n\n    # Note each of these will in turn have several funding instruments, categories, and applicant types\n    opportunity_summary_posted = OpportunitySummaryFactory.create(\n        opportunity=opportunity, is_posted_summary=True\n    )\n    OpportunitySummaryFactory.create(opportunity=opportunity, is_forecasted_summary=True)\n\n    OpportunitySummaryFactory.create(\n        opportunity=opportunity, is_posted_summary=True, revision_number=1\n    )\n    OpportunitySummaryFactory.create(\n        opportunity=opportunity, is_forecasted_summary=True, revision_number=2\n    )\n\n    CurrentOpportunitySummaryFactory.create(\n        opportunity=opportunity, opportunity_summary=opportunity_summary_posted\n    )\n\n    return opportunity\n\n\ndef validate_db_records(\n    db_session,\n    opportunity: Opportunity,\n    is_opportunity_deleted: bool = False,\n    expected_assistance_listing_ids: list | None = None,\n    expected_attachment_ids: list | None = None,\n    is_current_opportunity_summary_deleted: bool = False,\n    expected_opportunity_summary_ids: list | None = None,\n    is_agency_deleted: bool = False,\n):\n    db_opportunity = (\n        db_session.query(Opportunity)\n        .where(Opportunity.opportunity_id == opportunity.opportunity_id)\n        .one_or_none()\n    )\n    assert (db_opportunity is None) == is_opportunity_deleted\n\n    db_assistance_listing_ids = (\n        db_session.execute(\n            select(OpportunityAssistanceListing.opportunity_assistance_listing_id).where(\n                OpportunityAssistanceListing.opportunity_id == opportunity.opportunity_id\n            )\n        )\n        .scalars()\n        .all()\n    )\n    if expected_assistance_listing_ids is None:\n        expected_assistance_listing_ids = []\n    assert set(db_assistance_listing_ids) == set(expected_assistance_listing_ids)\n\n    db_attachment_ids = (\n        db_session.execute(\n            select(OpportunityAttachment.attachment_id).where(\n                OpportunityAttachment.opportunity_id == opportunity.opportunity_id\n            )\n        )\n        .scalars()\n        .all()\n    )\n    if expected_attachment_ids is None:\n        expected_attachment_ids = []\n    assert set(db_attachment_ids) == set(expected_attachment_ids)\n\n    db_current_opp_summary = (\n        db_session.query(CurrentOpportunitySummary)\n        .where(CurrentOpportunitySummary.opportunity_id == opportunity.opportunity_id)\n        .one_or_none()\n    )\n    assert (db_current_opp_summary is None) == is_current_opportunity_summary_deleted\n\n    db_opportunity_summaries = (\n        db_session.execute(\n            select(OpportunitySummary.opportunity_summary_id).where(\n                OpportunitySummary.opportunity_id == opportunity.opportunity_id\n            )\n        )\n        .scalars()\n        .all()\n    )\n    if expected_opportunity_summary_ids is None:\n        expected_opportunity_summary_ids = []\n    assert set(db_opportunity_summaries) == set(expected_opportunity_summary_ids)\n\n    db_agency = (\n        db_session.query(Agency).where(Agency.agency_code == opportunity.agency).one_or_none()\n    )\n    assert (db_agency is None) == is_agency_deleted\n\n\ndef test_delete_opportunity(db_session, enable_factory_create):\n    opportunity = setup_opportunity()\n\n    db_session.delete(opportunity)\n    db_session.commit()\n    db_session.expunge_all()\n\n    validate_db_records(\n        db_session,\n        opportunity,\n        is_opportunity_deleted=True,\n        expected_assistance_listing_ids=[],\n        expected_attachment_ids=[],\n        is_current_opportunity_summary_deleted=True,\n        expected_opportunity_summary_ids=[],\n        is_agency_deleted=False,\n    )\n\n\ndef test_delete_opportunity_summary_is_current(db_session, enable_factory_create):\n    opportunity = setup_opportunity()\n    summary = opportunity.current_opportunity_summary.opportunity_summary\n    assistance_listings = opportunity.opportunity_assistance_listings\n    opportunity_summaries = opportunity.all_opportunity_summaries\n    attachments = opportunity.opportunity_attachments\n\n    db_session.delete(summary)\n    db_session.commit()\n    db_session.expunge_all()\n\n    validate_db_records(\n        db_session,\n        opportunity,\n        is_opportunity_deleted=False,\n        expected_assistance_listing_ids=[\n            a.opportunity_assistance_listing_id for a in assistance_listings\n        ],\n        expected_attachment_ids=[a.attachment_id for a in attachments],\n        is_current_opportunity_summary_deleted=True,\n        expected_opportunity_summary_ids=[\n            o.opportunity_summary_id\n            for o in opportunity_summaries\n            if o.opportunity_summary_id != summary.opportunity_summary_id\n        ],\n        is_agency_deleted=False,\n    )\n\n\ndef test_delete_opportunity_summary_is_not_current(db_session, enable_factory_create):\n    opportunity = setup_opportunity()\n    assistance_listings = opportunity.opportunity_assistance_listings\n    opportunity_summaries = opportunity.all_opportunity_summaries\n    attachments = opportunity.opportunity_attachments\n\n    # Make the summary we delete be the non-current one\n    summary = None\n    for opportunity_summary in opportunity_summaries:\n        if (\n            opportunity_summary.opportunity_summary_id\n            != opportunity.current_opportunity_summary.opportunity_summary_id\n        ):\n            summary = opportunity_summary\n            break\n\n    db_session.delete(summary)\n    db_session.commit()\n    db_session.expunge_all()\n\n    validate_db_records(\n        db_session,\n        opportunity,\n        is_opportunity_deleted=False,\n        expected_assistance_listing_ids=[\n            a.opportunity_assistance_listing_id for a in assistance_listings\n        ],\n        expected_attachment_ids=[a.attachment_id for a in attachments],\n        is_current_opportunity_summary_deleted=False,\n        expected_opportunity_summary_ids=[\n            o.opportunity_summary_id\n            for o in opportunity_summaries\n            if o.opportunity_summary_id != summary.opportunity_summary_id\n        ],\n        is_agency_deleted=False,\n    )\n\n\ndef test_delete_current_opportunity_summary(db_session, enable_factory_create):\n    opportunity = setup_opportunity()\n    assistance_listings = opportunity.opportunity_assistance_listings\n    opportunity_summaries = opportunity.all_opportunity_summaries\n    attachments = opportunity.opportunity_attachments\n\n    db_session.delete(opportunity.current_opportunity_summary)\n    db_session.commit()\n    db_session.expunge_all()\n\n    # Deleting the current_opportunity_summary record should not delete\n    # anything else as it's just a useful linkage between opportunity and opportunity_summary\n    validate_db_records(\n        db_session,\n        opportunity,\n        is_opportunity_deleted=False,\n        expected_assistance_listing_ids=[\n            a.opportunity_assistance_listing_id for a in assistance_listings\n        ],\n        expected_attachment_ids=[a.attachment_id for a in attachments],\n        is_current_opportunity_summary_deleted=True,\n        expected_opportunity_summary_ids=[o.opportunity_summary_id for o in opportunity_summaries],\n        is_agency_deleted=False,\n    )\n\n\ndef test_delete_opportunity_assistance_listing(db_session, enable_factory_create):\n    opportunity = setup_opportunity()\n    assistance_listings = opportunity.opportunity_assistance_listings\n    assistance_listing_to_delete = assistance_listings[-1]\n    opportunity_summaries = opportunity.all_opportunity_summaries\n    attachments = opportunity.opportunity_attachments\n\n    db_session.delete(assistance_listing_to_delete)\n    db_session.commit()\n    db_session.expunge_all()\n\n    validate_db_records(\n        db_session,\n        opportunity,\n        is_opportunity_deleted=False,\n        expected_assistance_listing_ids=[\n            a.opportunity_assistance_listing_id\n            for a in assistance_listings\n            if a.opportunity_assistance_listing_id\n            != assistance_listing_to_delete.opportunity_assistance_listing_id\n        ],\n        expected_attachment_ids=[a.attachment_id for a in attachments],\n        is_current_opportunity_summary_deleted=False,\n        expected_opportunity_summary_ids=[o.opportunity_summary_id for o in opportunity_summaries],\n        is_agency_deleted=False,\n    )\n\n\ndef test_delete_attachments(db_session, enable_factory_create):\n    opportunity = setup_opportunity()\n    assistance_listings = opportunity.opportunity_assistance_listings\n    opportunity_summaries = opportunity.all_opportunity_summaries\n    attachments = opportunity.opportunity_attachments\n\n    attachment_to_delete = attachments[-1]\n\n    db_session.delete(attachment_to_delete)\n    db_session.commit()\n    db_session.expunge_all()\n\n    validate_db_records(\n        db_session,\n        opportunity,\n        is_opportunity_deleted=False,\n        expected_assistance_listing_ids=[\n            a.opportunity_assistance_listing_id for a in assistance_listings\n        ],\n        expected_attachment_ids=[\n            a.attachment_id\n            for a in attachments\n            if a.attachment_id != attachment_to_delete.attachment_id\n        ],\n        is_current_opportunity_summary_deleted=False,\n        expected_opportunity_summary_ids=[o.opportunity_summary_id for o in opportunity_summaries],\n        is_agency_deleted=False,\n    )\n\n\ndef test_delete_agency(db_session, enable_factory_create):\n    opportunity = setup_opportunity()\n    agency = opportunity.agency_record\n    assistance_listings = opportunity.opportunity_assistance_listings\n    opportunity_summaries = opportunity.all_opportunity_summaries\n    attachments = opportunity.opportunity_attachments\n\n    db_session.delete(agency)\n    db_session.commit()\n    db_session.expunge_all()\n\n    # Deleting the agency should not affect the opportunity\n    validate_db_records(\n        db_session,\n        opportunity,\n        is_opportunity_deleted=False,\n        expected_assistance_listing_ids=[\n            a.opportunity_assistance_listing_id for a in assistance_listings\n        ],\n        expected_attachment_ids=[a.attachment_id for a in attachments],\n        is_current_opportunity_summary_deleted=False,\n        expected_opportunity_summary_ids=[o.opportunity_summary_id for o in opportunity_summaries],\n        is_agency_deleted=True,\n    )\n\n\ndef test_delete_link_values(db_session, enable_factory_create):\n    opportunity = setup_opportunity()\n    assistance_listings = opportunity.opportunity_assistance_listings\n    opportunity_summaries = opportunity.all_opportunity_summaries\n    attachments = opportunity.opportunity_attachments\n\n    summary = opportunity.current_opportunity_summary.opportunity_summary\n    funding_category_to_delete = summary.link_funding_categories[-1]\n    funding_instrument_to_delete = summary.link_funding_instruments[0]\n    applicant_type_to_delete = summary.link_applicant_types[-1]\n\n    db_session.delete(funding_category_to_delete)\n    db_session.delete(funding_instrument_to_delete)\n    db_session.delete(applicant_type_to_delete)\n    db_session.commit()\n    db_session.expunge_all()\n\n    validate_db_records(\n        db_session,\n        opportunity,\n        is_opportunity_deleted=False,\n        expected_assistance_listing_ids=[\n            a.opportunity_assistance_listing_id for a in assistance_listings\n        ],\n        expected_attachment_ids=[a.attachment_id for a in attachments],\n        is_current_opportunity_summary_deleted=False,\n        expected_opportunity_summary_ids=[o.opportunity_summary_id for o in opportunity_summaries],\n        is_agency_deleted=False,\n    )\n\n    # Additional sanity test that the relationship to the lookup tables didn't cause any sort of deletes\n    funding_categories = db_session.query(LkFundingCategory).all()\n    assert len(funding_categories) == len([f for f in FundingCategory])\n\n    funding_instruments = db_session.query(LkFundingInstrument).all()\n    assert len(funding_instruments) == len([f for f in FundingInstrument])\n\n    applicant_types = db_session.query(LkApplicantType).all()\n    assert len(applicant_types) == len([a for a in ApplicantType])\n\n\ndef test_delete_child_agency(db_session, enable_factory_create):\n    parent_agency = AgencyFactory.create(agency_code=f\"TOP{random.randint(1, 100_000_000)}\")\n    child_agency = AgencyFactory.create(\n        agency_code=parent_agency.agency_code + \"-xyz\", top_level_agency=parent_agency\n    )\n\n    db_session.delete(child_agency)\n    db_session.commit()\n    db_session.expunge_all()\n\n    # Deleting the child should not affect the parent\n    db_agency = (\n        db_session.query(Agency).filter(Agency.agency_id == parent_agency.agency_id).one_or_none()\n    )\n    assert db_agency is not None\n\n\ndef test_delete_parent_agency(db_session, enable_factory_create):\n    parent_agency = AgencyFactory.create(agency_code=f\"TOP{random.randint(1, 100_000_000)}\")\n    AgencyFactory.create(\n        agency_code=parent_agency.agency_code + \"-xyz\", top_level_agency=parent_agency\n    )\n\n    # Trying to delete the parent will give a foreign key constraint error, we don't\n    # have the relationships setup in a way that would support this right now\n    with pytest.raises(Exception, match=\"violates foreign key constraint\"):\n        db_session.delete(parent_agency)\n        db_session.commit()\n        db_session.expunge_all()"}
{"path":"frontend/tests/pages/subscribe/SubscriptionForm.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/pages/subscribe","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/pages/subscribe/SubscriptionForm.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/db/test_migrations.py\nLanguage: py\nType: code\nDirectory: api/tests/src/db\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/db/test_migrations.py\nSize: 2.69 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/pages/subscribe/SubscriptionSubmitButton.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/pages/subscribe","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/pages/subscribe/SubscriptionSubmitButton.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":"import alembic.command as command\nimport pytest\nfrom alembic.script import ScriptDirectory\nfrom alembic.script.revision import MultipleHeads\nfrom alembic.util.exc import CommandError\n\nimport src.adapters.db as db\nfrom src.db.migrations.run import alembic_cfg\nfrom tests.lib import db_testing\n\n\n@pytest.fixture\ndef empty_schema(monkeypatch) -> db.DBClient:\n    \"\"\"\n    Create a test schema, if it doesn't already exist, and drop it after the\n    test completes.\n\n    This is similar to what the db_client fixture does but does not create any tables in the\n    schema.\n    \"\"\"\n    with db_testing.create_isolated_db(\n        monkeypatch, f\"test_migrations_{uuid.uuid4().int}_\"\n    ) as db_client:\n        yield db_client\n\n\ndef test_only_single_head_revision_in_migrations():\n    script = ScriptDirectory.from_config(alembic_cfg)\n\n    try:\n        # This will raise if there are multiple heads\n        script.get_current_head()\n        multihead_situation = False\n    except CommandError as e:\n        # re-raise anything not expected\n        if not isinstance(e.__cause__, MultipleHeads):\n            raise\n\n        multihead_situation = True\n\n    # raising assertion error here instead of in `except` block to avoid pytest\n    # printing the huge stacktrace of the multi-head exception, which in this\n    # case we don't really care about the details, just using it as a flag\n    if multihead_situation:\n        raise AssertionError(\n            \"Multi-head migration issue: run `make db-migrate-merge-heads` to resolve\"\n        )\n\n\ndef test_db_setup_via_alembic_migration(\n    empty_schema, caplog: pytest.LogCaptureFixture, capsys: pytest.CaptureFixture\n):\n    \"\"\"\n    All of our tests run using temporary DB schemas. However the alembic\n    migrations are generated with the schema hardcoded (eg. \"api\") and trying to make alembic\n    work in a test requires intercepting those function calls to swap in our\n    test schema. While this is doable, we'd need to do it for more than a dozen\n    functions with varying signatures, which feels too brittle and complex\n    to be a valuable test\n    \"\"\"\n\n    caplog.set_level(logging.INFO)\n    # Tell Alembic to run all migrations, generating SQL commands for each\n    command.upgrade(alembic_cfg, \"base:head\", sql=True)\n\n    # Verify that the upgrades ran and that at least one specific query is present\n    # Alembic just writes to stdout, so capsys captures that.\n    assert \"Running upgrade\" in caplog.text\n    assert \"CREATE TABLE api.opportunity\" in capsys.readouterr().out\n\n\ndef test_db_init_with_migrations(empty_schema):\n    # Verify the DB session works after initializing the migrations\n    db_session = empty_schema.get_session()\n    db_session.close()"}
{"path":"frontend/tests/pages/subscribe/page.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/pages/subscribe","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/pages/subscribe/page.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/logging/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/logging/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/pages/subscribe/subscribeEmailAction.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/pages/subscribe","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/pages/subscribe/subscribeEmailAction.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":""}
{"path":"frontend/tests/playwright.config.ts","language":"typescript","type":"code","directory":"frontend/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/playwright.config.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/logging/test_audit.py\nLanguage: py\nType: code\nDirectory: api/tests/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/logging/test_audit.py\nSize: 7.96 KB\nLast Modified: 2025-02-14T17:08:26.461Z"}
{"path":"frontend/tests/react-utils.tsx","language":"typescript","type":"code","directory":"frontend/tests","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/react-utils.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":"import logging\nimport os\nimport pathlib\nimport signal\nimport socket\nimport subprocess\nimport sys\nimport urllib.error\nimport urllib.parse\nimport urllib.request\nfrom typing import Any, Callable\n\nimport pytest\n\nimport src.logging.audit as audit\n\n# Do not run these tests alongside the rest of the test suite since\n# this tests adds an audit hook that interfere with other tests,\n# and at the time of writing there isn't a known way to remove\n# audit hooks.\npytestmark = pytest.mark.audit\n\n\n@pytest.fixture(scope=\"session\")\ndef init_audit_hook():\n    audit.init()\n\n\ntest_audit_hook_data = [\n    pytest.param(eval, (\"1+1\", None, None), [{\"msg\": \"exec\"}], id=\"eval\"),\n    pytest.param(exec, (\"1+1\", None, None), [{\"msg\": \"exec\"}], id=\"exec\"),\n    pytest.param(\n        open,\n        (\"/dev/null\", \"w\"),\n        [\n            {\n                \"msg\": \"open\",\n                \"audit.args.path\": \"/dev/null\",\n                \"audit.args.mode\": \"w\",\n            }\n        ],\n        id=\"open\",\n    ),\n    pytest.param(\n        os.rename,\n        (\"/tmp/oldname\", \"/tmp/newname\"),\n        [\n            {\n                \"msg\": \"os.rename\",\n                \"audit.args.src\": \"/tmp/oldname\",\n                \"audit.args.dst\": \"/tmp/newname\",\n            }\n        ],\n        id=\"os.rename\",\n    ),\n    pytest.param(\n        subprocess.Popen,\n        ([\"/usr/bin/git\", \"log\", \"HEAD~1..HEAD\"],),\n        [\n            {\n                \"msg\": \"subprocess.Popen\",\n                \"audit.args.executable\": \"/usr/bin/git\",\n                \"audit.args.args\": [\"/usr/bin/git\", \"log\", \"HEAD~1..HEAD\"],\n            }\n        ],\n        id=\"subprocess.Popen\",\n    ),\n    pytest.param(\n        os.open,\n        (\"/dev/null\", os.O_RDWR | os.O_CREAT, 0o777),\n        [\n            {\n                \"msg\": \"open\",\n                \"audit.args.path\": \"/dev/null\",\n                \"audit.args.mode\": None,\n            }\n        ],\n        id=\"os.open\",\n    ),\n    pytest.param(\n        sys.addaudithook,\n        (lambda *args: None,),\n        [{\"msg\": \"sys.addaudithook\"}],\n        id=\"sys.addaudithook\",\n    ),\n    pytest.param(\n        socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect,\n        ((\"www.python.org\", 80),),\n        [{\"msg\": \"socket.connect\", \"audit.args.address\": (\"www.python.org\", 80)}],\n        id=\"socket.connect\",\n    ),\n    pytest.param(\n        socket.getaddrinfo,\n        (\"www.python.org\", 80),\n        [{\"msg\": \"socket.getaddrinfo\", \"audit.args.host\": \"www.python.org\", \"audit.args.port\": 80}],\n        id=\"socket.getaddrinfo\",\n    ),\n    pytest.param(\n        urllib.request.urlopen,\n        (\"https://www.python.org\",),\n        # urllib.request.urlopen calls socket.getaddrinfo and socket.connect under the hood,\n        # both of which trigger audit log entries\n        [\n            {\n                \"msg\": \"urllib.Request\",\n                \"audit.args.url\": \"https://www.python.org\",\n                \"audit.args.method\": \"GET\",\n            },\n            {\n                \"msg\": \"socket.getaddrinfo\",\n                \"audit.args.host\": \"www.python.org\",\n                \"audit.args.port\": 443,\n            },\n            {\n                \"msg\": \"socket.connect\",\n            },\n        ],\n        id=\"urllib.request.urlopen\",\n    ),\n]\n\n\n@pytest.mark.parametrize(\"func,args,expected_records\", test_audit_hook_data)\ndef test_audit_hook(\n    init_audit_hook,\n    caplog: pytest.LogCaptureFixture,\n    func: Callable,\n    args: tuple[Any],\n    expected_records: list[dict[str, Any]],\n):\n    caplog.set_level(logging.INFO)\n    caplog.clear()\n\n    try:\n        func(*args)\n    except Exception:\n        pass\n\n    assert len(caplog.records) == len(expected_records)\n    for record, expected_record in zip(caplog.records, expected_records, strict=True):\n        assert record.levelname == \"AUDIT\"\n        assert_record_match(record, expected_record)\n\n\ndef test_os_kill(init_audit_hook, caplog: pytest.LogCaptureFixture):\n    # Start a process to kill\n    process = subprocess.Popen(\"cat\")\n    os.kill(process.pid, signal.SIGTERM)\n\n    expected_records = [\n        {\"msg\": \"subprocess.Popen\"},\n        {\n            \"msg\": \"os.kill\",\n            \"audit.args.pid\": process.pid,\n            \"audit.args.sig\": signal.SIGTERM,\n        },\n    ]\n\n    assert len(caplog.records) == len(expected_records)\n    for record, expected_record in zip(caplog.records, expected_records, strict=True):\n        assert record.levelname == \"AUDIT\"\n        assert_record_match(record, expected_record)\n\n\ndef test_do_not_log_popen_env(\n    init_audit_hook, caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch\n):\n    monkeypatch.setenv(\"FOO\", \"SENSITIVE-DATA\")\n    subprocess.Popen([\"ls\"], env=os.environ)\n    for record in caplog.records:\n        assert \"SENSITIVE-DATA\" not in str(record.__dict__)\n\n\ndef test_do_not_log_request_data(\n    init_audit_hook,\n    caplog: pytest.LogCaptureFixture,\n):\n    data = urllib.parse.urlencode({\"foo\": \"SENSITIVE-DATA\"}).encode()\n    req = urllib.request.Request(\"https://www.python.org\", data=data)\n    req.add_header(\"X-Bar\", \"SENSITIVE-DATA\")\n    try:\n        urllib.request.urlopen(req)\n    except urllib.error.HTTPError:\n        pass\n\n    for record in caplog.records:\n        assert \"SENSITIVE-DATA\" not in str(record.__dict__)\n\n\ndef test_repeated_audit_logs(\n    init_audit_hook, caplog: pytest.LogCaptureFixture, tmp_path: pathlib.Path\n):\n    caplog.set_level(logging.INFO)\n    caplog.clear()\n\n    for _ in range(1000):\n        open(tmp_path / \"repeated-audit-logs\", \"w\")\n\n    expected_records = [\n        {\"msg\": \"open\", \"count\": 1},\n        {\"msg\": \"open\", \"count\": 2},\n        {\"msg\": \"open\", \"count\": 3},\n        {\"msg\": \"open\", \"count\": 4},\n        {\"msg\": \"open\", \"count\": 5},\n        {\"msg\": \"open\", \"count\": 6},\n        {\"msg\": \"open\", \"count\": 7},\n        {\"msg\": \"open\", \"count\": 8},\n        {\"msg\": \"open\", \"count\": 9},\n        {\"msg\": \"open\", \"count\": 10},\n        {\"msg\": \"open\", \"count\": 20},\n        {\"msg\": \"open\", \"count\": 30},\n        {\"msg\": \"open\", \"count\": 40},\n        {\"msg\": \"open\", \"count\": 50},\n        {\"msg\": \"open\", \"count\": 60},\n        {\"msg\": \"open\", \"count\": 70},\n        {\"msg\": \"open\", \"count\": 80},\n        {\"msg\": \"open\", \"count\": 90},\n        {\"msg\": \"open\", \"count\": 100},\n        {\"msg\": \"open\", \"count\": 200},\n        {\"msg\": \"open\", \"count\": 300},\n        {\"msg\": \"open\", \"count\": 400},\n        {\"msg\": \"open\", \"count\": 500},\n        {\"msg\": \"open\", \"count\": 600},\n        {\"msg\": \"open\", \"count\": 700},\n        {\"msg\": \"open\", \"count\": 800},\n        {\"msg\": \"open\", \"count\": 900},\n        {\"msg\": \"open\", \"count\": 1000},\n    ]\n\n    assert len(caplog.records) == len(expected_records)\n    for record, expected_record in zip(caplog.records, expected_records, strict=True):\n        assert record.levelname == \"AUDIT\"\n        assert_record_match(record, expected_record)\n\n\n# Test utility data structure used by audit module\ndef test_least_recently_used_dict():\n    lru_dict = audit.LeastRecentlyUsedDict(maxsize=4)\n\n    assert lru_dict[\"a\"] == 0\n    assert len(lru_dict) == 0\n\n    lru_dict[\"a\"] = 10\n    lru_dict[\"b\"] = 20\n    lru_dict[\"c\"] = 30\n    lru_dict[\"d\"] = 40\n\n    assert len(lru_dict) == 4\n    assert tuple(lru_dict.items()) == ((\"a\", 10), (\"b\", 20), (\"c\", 30), (\"d\", 40))\n    assert lru_dict[\"a\"] == 10\n    assert lru_dict[\"b\"] == 20\n    assert lru_dict[\"c\"] == 30\n    assert lru_dict[\"d\"] == 40\n    assert lru_dict[\"e\"] == 0\n    assert len(lru_dict) == 4\n\n    lru_dict[\"a\"] += 1  # Write existing a, move to end\n    assert len(lru_dict) == 4\n    assert tuple(lru_dict.items()) == ((\"b\", 20), (\"c\", 30), (\"d\", 40), (\"a\", 11))\n\n    lru_dict[\"f\"] = 50  # Write new key f, and evict oldest b\n    lru_dict[\"c\"] += 1  # Write existing c, move to end, and evict oldest d\n    lru_dict[\"g\"] = 60  # Write new key g, and evict oldest d\n    assert len(lru_dict) == 4\n    assert tuple(lru_dict.items()) == ((\"a\", 11), (\"f\", 50), (\"c\", 31), (\"g\", 60))\n\n\ndef assert_record_match(record: logging.LogRecord, expected_record: dict[str, Any]):\n    for key, value in expected_record.items():\n        assert record.__dict__[key] == value"}
{"path":"frontend/tests/services/auth/session.test.ts","language":"typescript","type":"code","directory":"frontend/tests/services/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/services/auth/session.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/logging/test_flask_logger.py\nLanguage: py\nType: code\nDirectory: api/tests/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/logging/test_flask_logger.py\nSize: 4.27 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"frontend/tests/services/auth/sessionUtils.test.ts","language":"typescript","type":"code","directory":"frontend/tests/services/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/services/auth/sessionUtils.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":"import pytest\nfrom flask import Flask\n\nimport src.logging.flask_logger as flask_logger\nfrom tests.lib.assertions import assert_dict_contains\n\n\n@pytest.fixture\ndef logger():\n    logger = logging.getLogger(\"src\")\n    before_level = logger.level\n\n    logger.setLevel(logging.DEBUG)\n    handler = logging.StreamHandler(sys.stdout)\n    logger.addHandler(handler)\n    yield logger\n    logger.setLevel(before_level)\n    logger.removeHandler(handler)\n\n\n@pytest.fixture\ndef app(logger):\n    app = Flask(\"test_app_name\")\n\n    @app.get(\"/hello/<name>\")\n    def hello(name):\n        logging.getLogger(\"src.hello\").info(f\"hello, {name}!\")\n        return \"ok\"\n\n    flask_logger.init_app(logger, app)\n    return app\n\n\ntest_request_lifecycle_logs_data = [\n    pytest.param(\n        \"/hello/jane\",\n        [\n            {\"msg\": \"start request\"},\n            {\"msg\": \"hello, jane!\"},\n            {\n                \"msg\": \"end request\",\n                \"response.status_code\": 200,\n                \"response.content_length\": 2,\n                \"response.content_type\": \"text/html; charset=utf-8\",\n                \"response.mimetype\": \"text/html\",\n            },\n        ],\n        id=\"200\",\n    ),\n    pytest.param(\n        \"/notfound\",\n        [\n            {\"msg\": \"start request\"},\n            {\n                \"msg\": \"end request\",\n                \"response.status_code\": 404,\n                \"response.content_length\": 207,\n                \"response.content_type\": \"text/html; charset=utf-8\",\n                \"response.mimetype\": \"text/html\",\n            },\n        ],\n        id=\"404\",\n    ),\n]\n\n\n@pytest.mark.parametrize(\n    \"route,expected_extras\",\n    test_request_lifecycle_logs_data,\n)\ndef test_request_lifecycle_logs(\n    app: Flask, caplog: pytest.LogCaptureFixture, route, expected_extras\n):\n    app.test_client().get(route)\n\n    # Assert that the log messages are present\n    # There should be the route log message that is logged in the before_request handler\n    # as part of every request, followed by the log message in the route handler itself.\n    # then the log message in the after_request handler.\n\n    assert len(caplog.records) == len(expected_extras)\n    for record, expected_extra in zip(caplog.records, expected_extras, strict=True):\n        assert_dict_contains(record.__dict__, expected_extra)\n\n\ndef test_app_context_extra_attributes(app: Flask, caplog: pytest.LogCaptureFixture):\n    # Assert that extra attributes related to the app context are present in all log records\n    expected_extra = {\"app.name\": \"test_app_name\"}\n\n    app.test_client().get(\"/hello/jane\")\n\n    assert len(caplog.records) > 0\n    for record in caplog.records:\n        assert_dict_contains(record.__dict__, expected_extra)\n\n\ndef test_request_context_extra_attributes(app: Flask, caplog: pytest.LogCaptureFixture):\n    # Assert that the extra attributes related to the request context are present in all log records\n    expected_extra = {\n        \"request.id\": \"\",\n        \"request.method\": \"GET\",\n        \"request.path\": \"/hello/jane\",\n        \"request.url_rule\": \"/hello/<name>\",\n        \"request.query.up\": \"high\",\n        \"request.query.down\": \"low\",\n    }\n\n    app.test_client().get(\"/hello/jane?up=high&down=low\")\n\n    assert len(caplog.records) > 0\n    for record in caplog.records:\n        assert_dict_contains(record.__dict__, expected_extra)\n\n\ndef test_add_extra_log_data_for_current_request(app: Flask, caplog: pytest.LogCaptureFixture):\n    @app.get(\"/pet/<name>\")\n    def pet(name):\n        flask_logger.add_extra_data_to_current_request_logs({\"pet.name\": name})\n        logging.getLogger(\"test.pet\").info(f\"petting {name}\")\n        return \"ok\"\n\n    app.test_client().get(\"/pet/kitty\")\n\n    last_record = caplog.records[-1]\n    assert_dict_contains(last_record.__dict__, {\"pet.name\": \"kitty\"})\n\n\ndef test_log_response_time(app: Flask, caplog: pytest.LogCaptureFixture):\n    @app.get(\"/sleep\")\n    def sleep():\n        time.sleep(0.1)  # 0.1 s = 100 ms\n        return \"ok\"\n\n    app.test_client().get(\"/sleep\")\n\n    last_record = caplog.records[-1]\n    assert \"response.time_ms\" in last_record.__dict__\n    response_time_ms = last_record.__dict__[\"response.time_ms\"]\n    expected_response_time_ms = 100  # ms\n    allowed_error = 25  # ms\n\n    assert response_time_ms == pytest.approx(expected_response_time_ms, abs=allowed_error)"}
{"path":"frontend/tests/services/auth/useUser.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/services/auth","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/services/auth/useUser.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/logging/test_formatters.py\nLanguage: py\nType: code\nDirectory: api/tests/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/logging/test_formatters.py\nSize: 2.85 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"frontend/tests/services/featureFlags/FeatureFlagManager.test.ts","language":"typescript","type":"code","directory":"frontend/tests/services/featureFlags","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/services/featureFlags/FeatureFlagManager.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":"import pytest\n\nimport src.logging.formatters as formatters\nfrom tests.lib.assertions import assert_dict_contains\n\n\ndef test_json_formatter(capsys: pytest.CaptureFixture):\n    logger = logging.getLogger(\"test_json_formatter\")\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(formatters.JsonFormatter())\n    logger.addHandler(console_handler)\n\n    datetime_now = datetime.now()\n    date_now = datetime_now.date()\n    decimal_field = Decimal(\"12.34567\")\n    uuid_field = uuid4()\n    set_field = {uuid4(), uuid4()}\n    list_field = [1, 2, 3, 4]\n    exception_field = ValueError(\"my exception message\")\n    logger.warning(\n        \"hello %s\",\n        \"interpolated_string\",\n        extra={\n            \"foo\": \"bar\",\n            \"int_field\": 5,\n            \"bool_field\": True,\n            \"none_field\": None,\n            \"datetime_field\": datetime_now,\n            \"date_field\": date_now,\n            \"decimal_field\": decimal_field,\n            \"uuid_field\": uuid_field,\n            \"set_field\": set_field,\n            \"list_field\": list_field,\n            \"exception_field\": exception_field,\n        },\n    )\n\n    json_record = json.loads(capsys.readouterr().err)\n\n    expected = {\n        \"name\": \"test_json_formatter\",\n        \"message\": \"hello interpolated_string\",\n        \"msg\": \"hello %s\",\n        \"levelname\": \"WARNING\",\n        \"levelno\": 30,\n        \"filename\": \"test_formatters.py\",\n        \"module\": \"test_formatters\",\n        \"funcName\": \"test_json_formatter\",\n        \"foo\": \"bar\",\n        \"int_field\": 5,\n        \"bool_field\": True,\n        \"none_field\": None,\n        \"datetime_field\": datetime_now.isoformat(),\n        \"date_field\": date_now.isoformat(),\n        \"decimal_field\": str(decimal_field),\n        \"uuid_field\": str(uuid_field),\n        \"set_field\": [str(u) for u in set_field],\n        \"list_field\": list_field,\n        \"exception_field\": str(exception_field),\n    }\n    assert_dict_contains(json_record, expected)\n    logger.removeHandler(console_handler)\n\n\ndef test_human_readable_formatter(capsys: pytest.CaptureFixture):\n    logger = logging.getLogger(\"test_human_readable_formatter\")\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(formatters.HumanReadableFormatter())\n    logger.addHandler(console_handler)\n\n    logger.warning(\"hello %s\", \"interpolated_string\", extra={\"foo\": \"bar\"})\n\n    text = capsys.readouterr().err\n    created_time = text[:12]\n    rest = text[12:]\n    assert re.match(r\"^\\d{2}:\\d{2}:\\d{2}\\.\\d{3}\", created_time)\n    assert (\n        rest\n        == \"  test_human_readable_formatter       \\x1b[0m test_human_readable_formatter \\x1b[31mWARNING \\x1b[0m \\x1b[31mhello interpolated_string                         \\x1b[0m \\x1b[34mfoo=bar\\x1b[0m\\n\"\n    )\n    logger.removeHandler(console_handler)"}
{"path":"frontend/tests/services/featureFlags/featureFlagHelpers.test.ts","language":"typescript","type":"code","directory":"frontend/tests/services/featureFlags","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/services/featureFlags/featureFlagHelpers.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/logging/test_logging.py\nLanguage: py\nType: code\nDirectory: api/tests/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/logging/test_logging.py\nSize: 2.98 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"frontend/tests/services/fetch/FetcherHelpers.test.ts","language":"typescript","type":"code","directory":"frontend/tests/services/fetch","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/services/fetch/FetcherHelpers.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":"import pytest\n\nimport src.logging\nimport src.logging.formatters as formatters\nfrom tests.lib.assertions import assert_dict_contains\n\n\n@pytest.fixture\ndef init_test_logger(caplog: pytest.LogCaptureFixture, monkeypatch: pytest.MonkeyPatch):\n    caplog.set_level(logging.DEBUG)\n    monkeypatch.setenv(\"LOG_FORMAT\", \"human-readable\")\n    with src.logging.init(\"test_logging\"):\n        yield\n\n\n@pytest.mark.parametrize(\n    \"log_format,expected_formatter\",\n    [\n        (\"human-readable\", formatters.HumanReadableFormatter),\n        (\"json\", formatters.JsonFormatter),\n    ],\n)\ndef test_init(caplog: pytest.LogCaptureFixture, monkeypatch, log_format, expected_formatter):\n    caplog.set_level(logging.DEBUG)\n    monkeypatch.setenv(\"LOG_FORMAT\", log_format)\n\n    with src.logging.init(\"test_logging\"):\n        records = caplog.records\n        assert len(records) == 2\n        assert re.match(\n            r\"^start test_logging: \\w+ [0-9.]+ \\w+, hostname \\S+, pid \\d+, user \\d+\\(\\w+\\)$\",\n            records[0].message,\n        )\n        assert re.match(r\"^invoked as:\", records[1].message)\n\n        formatter_types = [type(handler.formatter) for handler in logging.root.handlers]\n        assert expected_formatter in formatter_types\n\n\ndef test_log_exception(init_test_logger, caplog):\n    logger = logging.getLogger(__name__)\n\n    try:\n        raise Exception(\"example exception\")\n    except Exception:\n        logger.exception(\n            \"test log message %s\", \"example_arg\", extra={\"key1\": \"value1\", \"key2\": \"value2\"}\n        )\n\n    last_record: logging.LogRecord = caplog.records[-1]\n\n    assert last_record.message == \"test log message example_arg\"\n    assert last_record.funcName == \"test_log_exception\"\n    assert last_record.threadName == \"MainThread\"\n    assert last_record.exc_text.startswith(\"Traceback (most recent call last)\")\n    assert last_record.exc_text.endswith(\"Exception: example exception\")\n    assert last_record.__dict__[\"key1\"] == \"value1\"\n    assert last_record.__dict__[\"key2\"] == \"value2\"\n\n\n@pytest.mark.parametrize(\n    \"args,extra,expected\",\n    [\n        pytest.param(\n            (\"ssn: 123456789\",),\n            None,\n            {\"message\": \"ssn: *********\"},\n            id=\"pii in msg\",\n        ),\n        pytest.param(\n            (\"pii\",),\n            {\"foo\": \"bar\", \"tin\": \"123456789\", \"dashed-ssn\": \"123-45-6789\"},\n            {\n                \"message\": \"pii\",\n                \"foo\": \"bar\",\n                \"tin\": \"*********\",\n                \"dashed-ssn\": \"*********\",\n            },\n            id=\"pii in extra\",\n        ),\n        pytest.param(\n            (\"%s %s\", \"text\", \"123456789\"),\n            None,\n            {\"message\": \"text *********\"},\n            id=\"pii in interpolation args\",\n        ),\n    ],\n)\ndef test_mask_pii(init_test_logger, caplog: pytest.LogCaptureFixture, args, extra, expected):\n    logger = logging.getLogger(__name__)\n\n    logger.info(*args, extra=extra)\n\n    assert len(caplog.records) == 1\n    assert_dict_contains(caplog.records[0].__dict__, expected)"}
{"path":"frontend/tests/services/fetch/Fetchers.test.ts","language":"typescript","type":"code","directory":"frontend/tests/services/fetch","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/services/fetch/Fetchers.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/logging/test_pii.py\nLanguage: py\nType: code\nDirectory: api/tests/src/logging\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/logging/test_pii.py\nSize: 1.98 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"frontend/tests/services/fetch/fetchers/SearchFetcher.test.ts","language":"typescript","type":"code","directory":"frontend/tests/services/fetch/fetchers","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/services/fetch/fetchers/SearchFetcher.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":"import pytest\n\nimport src.logging.pii as pii\n\n\n@pytest.mark.parametrize(\n    \"input,expected\",\n    [\n        (\"\", \"\"),\n        (\"1234\", \"1234\"),\n        (1234, 1234),\n        (None, None),\n        (\"hostname ip-10-11-12-134.ec2.internal\", \"hostname ip-10-11-12-134.ec2.internal\"),\n        ({}, {}),\n        (\"123456789\", \"*********\"),\n        (123456789, \"*********\"),\n        (\"123-45-6789\", \"*********\"),\n        (\"123456789 test\", \"********* test\"),\n        (\"test 123456789\", \"test *********\"),\n        (\"test 123456789 test\", \"test ********* test\"),\n        (\"test=999000000.\", \"test=*********.\"),\n        (\"test=999000000,\", \"test=*********,\"),\n        (999000000.5, 999000000.5),\n        ({\"a\": \"x\", \"b\": \"999000000\"}, \"{'a': 'x', 'b': '*********'}\"),\n    ],\n)\ndef test_mask_pii(input, expected):\n    assert pii._mask_pii(input) == expected\n\n\n@pytest.mark.parametrize(\n    \"input_value,expected_output\",\n    [\n        # Basic SSN patterns that should be masked\n        (\"123456789\", \"*********\"),\n        (\"123-45-6789\", \"*********\"),\n        # IP addresses that should not be masked\n        (\"ip-10-11-12-134\", \"ip-10-11-12-134\"),\n        # Floating point numbers that should not be masked\n        (\"5.999000000\", \"5.999000000\"),\n        (\"999000000.5\", \"999000000.5\"),\n        (\"0.999000000\", \"0.999000000\"),\n        (\"999.000000\", \"999.000000\"),\n        # Mixed content\n        (\"SSN: 123456789 Amount: 999000000.5\", \"SSN: ********* Amount: 999000000.5\"),\n        (\"IP: ip-10-11-12-134 SSN: 123-45-6789\", \"IP: ip-10-11-12-134 SSN: *********\"),\n    ],\n)\ndef test_mask_pii_logging_floats(input_value, expected_output):\n    # Create a LogRecord with the test value\n    record = logging.LogRecord(\n        name=\"test\",\n        level=logging.INFO,\n        pathname=\"test.py\",\n        lineno=1,\n        msg=input_value,\n        args=(),\n        exc_info=None,\n    )\n\n    # Apply PII masking\n    pii.mask_pii(record)\n\n    # Check that the message was properly masked\n    assert record.msg == expected_output"}
{"path":"frontend/tests/services/fetch/fetchers/clientSearchResultsDownloadFetcher.test.ts","language":"typescript","type":"code","directory":"frontend/tests/services/fetch/fetchers","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/services/fetch/fetchers/clientSearchResultsDownloadFetcher.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/pagination/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/pagination\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/pagination/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"frontend/tests/services/fetch/fetchers/opportunityFetcher.test.ts","language":"typescript","type":"code","directory":"frontend/tests/services/fetch/fetchers","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/services/fetch/fetchers/opportunityFetcher.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":""}
{"path":"frontend/tests/services/withFeatureFlag.test.tsx","language":"typescript","type":"code","directory":"frontend/tests/services","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/services/withFeatureFlag.test.tsx","size":318844,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/pagination/test_paginator.py\nLanguage: py\nType: code\nDirectory: api/tests/src/pagination\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/pagination/test_paginator.py\nSize: 5.62 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"frontend/tests/utils/dateUtil.test.ts","language":"typescript","type":"code","directory":"frontend/tests/utils","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/utils/dateUtil.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":"from src.db.models.opportunity_models import (\n    CurrentOpportunitySummary,\n    LinkOpportunitySummaryApplicantType,\n    LinkOpportunitySummaryFundingCategory,\n    LinkOpportunitySummaryFundingInstrument,\n    Opportunity,\n    OpportunitySummary,\n)\nfrom src.pagination.paginator import Paginator\nfrom tests.src.db.models.factories import OpportunityFactory\n\nDEFAULT_OPPORTUNITY_PARAMS = {\n    \"opportunity_title\": \"opportunity of a lifetime\",\n    \"opportunity_number\": \"XYZ-111\",\n    \"is_draft\": True,\n}\n\n\n@pytest.fixture\ndef create_opportunities(db_session, enable_factory_create):\n    # Clear any prior opportunities from other tests so we're only fetching\n    # records we created here.\n    # Note that we can't just do db_session.query(Opportunity).delete() as the cascade deletes won't work automatically:\n    # https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-queryguide-update-delete-caveats\n    # but if we do it individually they will\n    opportunities = db_session.query(Opportunity).all()\n    for opp in opportunities:\n        db_session.delete(opp)\n\n    # 5 with the default params\n    OpportunityFactory.create_batch(5, **DEFAULT_OPPORTUNITY_PARAMS)\n\n    # 4 with a different last name\n    params = DEFAULT_OPPORTUNITY_PARAMS | {\"opportunity_title\": \"something else\"}\n    OpportunityFactory.create_batch(4, **params)\n\n    # 3 with a different opportunity number\n    params = DEFAULT_OPPORTUNITY_PARAMS | {\"opportunity_number\": \"XYZ-222\"}\n    OpportunityFactory.create_batch(3, **params)\n\n    # 2 that aren't drafts\n    params = DEFAULT_OPPORTUNITY_PARAMS | {\"is_draft\": False}\n    OpportunityFactory.create_batch(2, **params)\n\n    # 1 that is different in all ways\n    params = DEFAULT_OPPORTUNITY_PARAMS | {\n        \"opportunity_title\": \"something else\",\n        \"opportunity_number\": \"XYZ-222\",\n        \"is_draft\": False,\n    }\n    OpportunityFactory.create_batch(1, **params)\n\n\ndef test_paginator(db_session, create_opportunities):\n    # A base \"select * from opportunity\" query\n    base_stmt = select(Opportunity)\n\n    # Verify that with no additional filters, we get everything\n    paginator = Paginator(Opportunity, base_stmt, db_session, page_size=6)\n    assert paginator.page_size == 6\n    assert paginator.total_pages == 3\n    assert paginator.total_records == 15\n\n    # The pages are generated at the expected length\n    assert len(paginator.page_at(1)) == 6\n    assert len(paginator.page_at(2)) == 6\n    assert len(paginator.page_at(3)) == 3\n    assert len(paginator.page_at(4)) == 0\n\n    # Verify when filtering by last name\n    stmt = base_stmt.filter(Opportunity.opportunity_title == \"something else\")\n    paginator = Paginator(Opportunity, stmt, db_session, page_size=10)\n    assert paginator.page_size == 10\n    assert paginator.total_pages == 1\n    assert paginator.total_records == 5\n\n    assert len(paginator.page_at(1)) == 5\n    assert len(paginator.page_at(2)) == 0\n\n    # Verify when filtering by opportunity number\n    stmt = base_stmt.filter(Opportunity.opportunity_number == \"XYZ-222\")\n    paginator = Paginator(Opportunity, stmt, db_session, page_size=1)\n    assert paginator.page_size == 1\n    assert paginator.total_pages == 4\n    assert paginator.total_records == 4\n\n    assert len(paginator.page_at(1)) == 1\n    assert len(paginator.page_at(2)) == 1\n    assert len(paginator.page_at(3)) == 1\n    assert len(paginator.page_at(4)) == 1\n    assert len(paginator.page_at(5)) == 0\n\n    # Verify when filtering by is_draft\n    stmt = base_stmt.filter(Opportunity.is_draft.is_(False))\n    paginator = Paginator(Opportunity, stmt, db_session, page_size=100)\n    assert paginator.page_size == 100\n    assert paginator.total_pages == 1\n    assert paginator.total_records == 3\n\n    assert len(paginator.page_at(1)) == 3\n    assert len(paginator.page_at(2)) == 0\n\n    # Verify when filtering by all fields\n    stmt = base_stmt.filter(\n        Opportunity.opportunity_title == \"something else\",\n        Opportunity.opportunity_number == \"XYZ-222\",\n        Opportunity.is_draft.is_(False),\n    )\n    paginator = Paginator(Opportunity, stmt, db_session)\n    assert paginator.page_size == 25\n    assert paginator.total_pages == 1\n    assert paginator.total_records == 1\n\n    assert len(paginator.page_at(1)) == 1\n    assert len(paginator.page_at(2)) == 0\n\n    # Verify when filtering to zero results\n    stmt = base_stmt.filter(Opportunity.opportunity_title == \"something that won't be found\")\n    paginator = Paginator(Opportunity, stmt, db_session)\n    assert paginator.page_size == 25\n    assert paginator.total_pages == 0\n    assert paginator.total_records == 0\n\n    assert len(paginator.page_at(1)) == 0\n\n    # Verify when adding joins, the counts continue to be correct\n    # If we didn't have distinct in the count function, we'd end up with\n    # every opportunity being counted extra for each Link table value\n    stmt = (\n        base_stmt.join(CurrentOpportunitySummary)\n        .join(\n            OpportunitySummary,\n            CurrentOpportunitySummary.opportunity_summary_id\n            == OpportunitySummary.opportunity_summary_id,\n        )\n        .join(LinkOpportunitySummaryFundingInstrument)\n        .join(LinkOpportunitySummaryFundingCategory)\n        .join(LinkOpportunitySummaryApplicantType)\n    )\n    paginator = Paginator(Opportunity, stmt, db_session, page_size=6)\n    assert paginator.page_size == 6\n    assert paginator.total_pages == 3\n    assert paginator.total_records == 15\n\n\n@pytest.mark.parametrize(\"page_size\", [0, -1, -2])\ndef test_page_size_zero_or_negative(db_session, page_size):\n    with pytest.raises(ValueError, match=\"Page size must be at least 1\"):\n        Paginator(Opportunity, select(Opportunity), db_session, page_size)"}
{"path":"frontend/tests/utils/generalUtils.test.ts","language":"typescript","type":"code","directory":"frontend/tests/utils","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/utils/generalUtils.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/schemas/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/schemas\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/schemas/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"frontend/tests/utils/getRoutes.test.ts","language":"typescript","type":"code","directory":"frontend/tests/utils","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/utils/getRoutes.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":""}
{"path":"frontend/tests/utils/isSummary.test.ts","language":"typescript","type":"code","directory":"frontend/tests/utils","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tests/utils/isSummary.test.ts","size":324715,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/schemas/extension/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/schemas/extension\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/schemas/extension/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"frontend/tsconfig.json","language":"json","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tsconfig.json","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":""}
{"path":"frontend/tsconfig.ts-jest.json","language":"json","type":"code","directory":"frontend","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/frontend/tsconfig.ts-jest.json","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/schemas/extension/test_schema_fields.py\nLanguage: py\nType: code\nDirectory: api/tests/src/schemas/extension\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/schemas/extension/test_schema_fields.py\nSize: 2.98 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/README.md","language":"markdown","type":"code","directory":"infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.138Z","content":"import pytest\nfrom marshmallow import ValidationError\n\nfrom src.api.schemas.extension import fields\nfrom tests.src.schemas.schema_validation_utils import (\n    DummySchema,\n    EnumA,\n    EnumB,\n    FieldTestSchema,\n    get_expected_validation_errors,\n    get_invalid_field_test_schema_req,\n    get_valid_field_test_schema_req,\n    validate_errors,\n)\n\n\ndef test_enum_field():\n    schema = DummySchema()\n\n    both_ab_field = schema.declared_fields[\"both_ab\"]\n\n    # Make sure the multi enum can deserialize to both enums and reserialize to a string\n    for e in EnumA:\n        deserialized_value = both_ab_field._deserialize(str(e), None, None)\n        assert deserialized_value == e\n        assert isinstance(deserialized_value, EnumA)\n\n        serialized_value = both_ab_field._serialize(e, None, None)\n        assert isinstance(serialized_value, str)\n    for e in EnumB:\n        deserialized_value = both_ab_field._deserialize(str(e), None, None)\n        assert deserialized_value == e\n        assert isinstance(deserialized_value, EnumB)\n\n        serialized_value = both_ab_field._serialize(e, None, None)\n        assert isinstance(serialized_value, str)\n\n    with pytest.raises(\n        ValidationError, match=\"Must be one of: value1, value2, value3, value4, value5, value6.\"\n    ):\n        both_ab_field._deserialize(\"not_a_value\", None, None)\n\n\n@pytest.mark.parametrize(\n    \"payload,expected_errors\",\n    [(get_invalid_field_test_schema_req(), get_expected_validation_errors())],\n)\ndef test_fields(payload, expected_errors):\n    errors = FieldTestSchema().validate(payload)\n    validate_errors(errors, expected_errors)\n\n\ndef test_fields_ignore_unknowns():\n    unknown_key = \"UNKNOWN\"\n    payload = {**get_valid_field_test_schema_req(), unknown_key: \"EXCLUDED\"}\n    result = FieldTestSchema().load(payload)\n    assert unknown_key not in result\n\n\ndef test_fields_configured_properly():\n    \"\"\"\n    This is a sanity-test to verify we have properly\n    overriden and defined all the default error codes\n    that Marshmallow uses.\n\n    If you see this test failing after updating our\n    dependency on Marshmallow, likely just need to add\n    a configuration to the relevant class' \"error_mapping\" object\n    \"\"\"\n    relevant_classes = []\n    for _, obj in inspect.getmembers(fields):\n        if inspect.isclass(obj) and issubclass(obj, fields.MixinField):\n            relevant_classes.append(obj)\n\n    for relevant_class in relevant_classes:\n        if relevant_class == fields.Enum:\n            # We don't derive from the original and made a custom enum field\n            # so the default error messages aren't relevant\n            assert relevant_class.error_mapping.keys() == {\"unknown\"}\n            continue\n\n        # We want to make sure all keys are configured, but we also may have more\n        required_error_message_keys = relevant_class.default_error_messages.keys()\n        configured_error_message_keys = relevant_class.error_mapping.keys()\n        assert configured_error_message_keys >= required_error_message_keys"}
{"path":"infra/accounts/.terraform.lock.hcl","language":"unknown","type":"code","directory":"infra/accounts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/accounts/.terraform.lock.hcl","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/schemas/schema_validation_utils.py\nLanguage: py\nType: code\nDirectory: api/tests/src/schemas\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/schemas/schema_validation_utils.py\nSize: 12.63 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/accounts/alarms.tf","language":"unknown","type":"code","directory":"infra/accounts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/accounts/alarms.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"from src.api.schemas.extension import MarshmallowErrorContainer, Schema, fields, validators\nfrom src.validation.validation_constants import ValidationErrorType\n\n#############################\n# Validation Error Messages\n#############################\nMISSING_DATA = MarshmallowErrorContainer(\n    ValidationErrorType.REQUIRED, \"Missing data for required field.\"\n)\nINVALID_INTEGER = MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid integer.\")\nINVALID_INTEGER_32BIT = MarshmallowErrorContainer(\n    ValidationErrorType.MIN_OR_MAX_VALUE,\n    \"Must be greater than or equal to -2147483648 and less than or equal to 2147483647.\",\n)\nINVALID_STRING = MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid string.\")\nINVALID_STRING_PATTERN = MarshmallowErrorContainer(\n    ValidationErrorType.FORMAT, \"String does not match expected pattern.\"\n)\nINVALID_DATE = MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid date.\")\nINVALID_DATETIME = MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid datetime.\")\nINVALID_BOOLEAN = MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid boolean.\")\nINVALID_SCHEMA_MSG = MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Invalid input type.\")\nINVALID_SCHEMA = {\"_schema\": [INVALID_SCHEMA_MSG]}\nINVALID_LIST = MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid list.\")\nINVALID_UUID = MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid UUID.\")\nINVALID_DECIMAL = MarshmallowErrorContainer(ValidationErrorType.INVALID, \"Not a valid decimal.\")\nINVALID_SPECIAL_DECIMAL = MarshmallowErrorContainer(\n    ValidationErrorType.SPECIAL_NUMERIC,\n    \"Special numeric values (nan or infinity) are not permitted.\",\n)\nINVALID_EMAIL = MarshmallowErrorContainer(ValidationErrorType.FORMAT, \"Not a valid email address.\")\nUNKNOWN_FIELD = MarshmallowErrorContainer(ValidationErrorType.UNKNOWN, \"Unknown field.\")\n\n\n########################\n# Validation Utilities\n########################\ndef get_random_string(length: int):\n    return \"\".join(choice(ascii_uppercase) for i in range(length))\n\n\ndef get_enum_error_msg(*enums: Type[Enum]):\n    possible_values = []\n    for enum in enums:\n        possible_values.extend([e.value for e in enum])\n\n    return MarshmallowErrorContainer(\n        ValidationErrorType.INVALID_CHOICE, f\"Must be one of: {', '.join(possible_values)}.\"\n    )\n\n\ndef get_one_of_error_msg(choices: list[str]):\n    choices_text = \", \".join([c for c in choices])\n\n    return MarshmallowErrorContainer(\n        ValidationErrorType.INVALID_CHOICE, f\"Value must be one of: {choices_text}\"\n    )\n\n\ndef get_min_length_error_msg(length: int):\n    return MarshmallowErrorContainer(\n        ValidationErrorType.MIN_LENGTH, f\"Shorter than minimum length {length}.\"\n    )\n\n\ndef get_max_length_error_msg(length: int):\n    return MarshmallowErrorContainer(\n        ValidationErrorType.MAX_LENGTH, f\"Longer than maximum length {length}.\"\n    )\n\n\ndef get_length_range_error_msg(min: int, max: int):\n    return MarshmallowErrorContainer(\n        ValidationErrorType.MIN_OR_MAX_LENGTH, f\"Length must be between {min} and {max}.\"\n    )\n\n\ndef get_length_equal_error_msg(equal: int):\n    return MarshmallowErrorContainer(ValidationErrorType.EQUALS, f\"Length must be {equal}.\")\n\n\ndef get_max_or_min_value_error_msg(min: int = -2147483648, max: int = 2147483647):\n    # defaults are the 32-bit integer min/max\n    return MarshmallowErrorContainer(\n        ValidationErrorType.MIN_OR_MAX_VALUE,\n        f\"Must be greater than or equal to {min} and less than or equal to {max}.\",\n    )\n\n\ndef validate_errors(actual_errors, expected_errors):\n    assert len(actual_errors) == len(\n        expected_errors\n    ), f\"Expected {len(expected_errors)}, but had {len(actual_errors)} errors\"\n    for field_name in actual_errors:\n        assert field_name in expected_errors, f\"{field_name} in errors but not expected\"\n        assert (\n            expected_errors[field_name] == actual_errors[field_name]\n        ), f\"Actual error for {field_name}: {str(actual_errors[field_name])} but received {str(expected_errors[field_name])}\"\n\n\n########################\n# Schemas for testing\n########################\n\n\nclass EnumA(StrEnum):\n    VALUE1 = \"value1\"\n    VALUE2 = \"value2\"\n    VALUE3 = \"value3\"\n\n\nclass EnumB(StrEnum):\n    VALUE4 = \"value4\"\n    VALUE5 = \"value5\"\n    VALUE6 = \"value6\"\n\n\nclass DummySchema(Schema):\n    both_ab = fields.Enum(EnumA, EnumB)\n\n\nclass InnerTestSchema(Schema):\n    inner_str = fields.String()\n    inner_required_str = fields.String(required=True)\n\n\nclass FieldTestSchema(Schema):\n    field_str = fields.String()\n    field_str_required = fields.String(required=True)\n    field_str_min = fields.String(validate=[validators.Length(min=2)])\n    field_str_max = fields.String(validate=[validators.Length(max=3)])\n    field_str_min_and_max = fields.String(validate=[validators.Length(min=2, max=3)])\n    field_str_equal = fields.String(validate=[validators.Length(equal=3)])\n    field_str_regex = fields.String(validate=[validators.Regexp(\"^\\\\d{3}$\")])\n    field_str_email = fields.String(validate=[validators.Email()])\n\n    field_int = fields.Integer()\n    field_int_required = fields.Integer(required=True)\n    field_int_strict = fields.Integer(strict=True)\n    field_int_32bit = fields.Integer(restrict_to_32bit_int=True)\n\n    field_bool = fields.Boolean()\n    field_bool_required = fields.Boolean(required=True)\n\n    field_decimal = fields.Decimal()\n    field_decimal_required = fields.Decimal(required=True)\n    field_decimal_special = fields.Decimal(allow_nan=False)\n\n    field_uuid = fields.UUID()\n    field_uuid_required = fields.UUID(required=True)\n\n    field_date = fields.Date()\n    field_date_required = fields.Date(required=True)\n    field_date_format = fields.Date(format=\"iso8601\")\n\n    field_datetime = fields.DateTime()\n    field_datetime_required = fields.DateTime(required=True)\n    field_datetime_format = fields.DateTime(format=\"iso8601\")\n\n    field_list = fields.List(fields.Boolean())\n    field_list_required = fields.List(fields.Integer(), required=True)\n    field_list_indexed = fields.List(fields.Integer())\n\n    field_nested = fields.Nested(InnerTestSchema())\n    field_nested_invalid = fields.Nested(InnerTestSchema())\n    field_nested_required = fields.Nested(InnerTestSchema(), required=True)\n\n    field_list_nested = fields.List(fields.Nested(InnerTestSchema()))\n    field_list_nested_invalid = fields.List(fields.Nested(InnerTestSchema()))\n    field_list_nested_required = fields.List(fields.Nested(InnerTestSchema()), required=True)\n\n    # There's no \"invalid\" raw field it doesn't serialize/deserialize\n    field_raw_required = fields.Raw(required=True)\n\n    field_enum = fields.Enum(EnumA)\n    field_enum_invalid_choice = fields.Enum(EnumA)\n    field_enum_required = fields.Enum(EnumB, required=True)\n\n\n########################\n# Requests for the above schema\n########################\n\n\ndef get_valid_field_test_schema_req():\n    return {\n        \"field_str\": \"text\",\n        \"field_str_required\": \"text\",\n        \"field_str_min\": \"abcd\",\n        \"field_str_max\": \"a\",\n        \"field_str_min_and_max\": \"ab\",\n        \"field_str_equal\": \"abc\",\n        \"field_str_regex\": \"123\",\n        \"field_str_email\": \"person@example.com\",\n        \"field_int\": 1,\n        \"field_int_required\": 2,\n        \"field_int_strict\": 3,\n        \"field_int_32bit\": 4,\n        \"field_bool\": True,\n        \"field_bool_required\": False,\n        \"field_decimal\": \"2.5\",\n        \"field_decimal_required\": \"555\",\n        \"field_decimal_special\": \"4\",\n        \"field_uuid\": \"1234a5b6-7c8d-90ef-1ab2-c3d45678e9f0\",\n        \"field_uuid_required\": \"1234a5b6-7c8d-90ef-1ab2-c3d45678e9f0\",\n        \"field_date\": \"2000-01-01\",\n        \"field_date_required\": \"2010-02-02\",\n        \"field_date_format\": \"2020-03-03\",\n        \"field_datetime\": \"2000-01-01T00:01:01Z\",\n        \"field_datetime_required\": \"2010-02-02T00:02:02Z\",\n        \"field_datetime_format\": \"2020-03-03T00:03:03Z\",\n        \"field_list\": [True],\n        \"field_list_required\": [],\n        \"field_list_indexed\": [1, 2, 3],\n        \"field_nested\": {\n            \"inner_str\": \"text\",\n            \"inner_required_str\": \"text\",\n        },\n        \"field_nested_invalid\": {\n            \"inner_str\": \"text\",\n            \"inner_required_str\": \"text\",\n        },\n        \"field_nested_required\": {\"inner_str\": \"text\", \"inner_required_str\": \"present\"},\n        \"field_list_nested\": [\n            {\"inner_str\": \"text\", \"inner_required_str\": \"present\"},\n            {\"inner_str\": \"text\", \"inner_required_str\": \"present\"},\n        ],\n        \"field_list_nested_invalid\": [],\n        \"field_list_nested_required\": [],\n        \"field_raw_required\": {},\n        \"field_enum\": EnumA.VALUE1,\n        \"field_enum_invalid_choice\": EnumA.VALUE2,\n        \"field_enum_required\": EnumB.VALUE4,\n    }\n\n\ndef get_invalid_field_test_schema_req():\n    return {\n        \"field_str\": 1234,\n        # field_str_required not present\n        \"field_str_min\": \"a\",\n        \"field_str_max\": \"abcdef\",\n        \"field_str_min_and_max\": \"a\",\n        \"field_str_equal\": \"a\",\n        \"field_str_regex\": \"abc\",\n        \"field_str_email\": \"not an email\",\n        \"field_int\": {},\n        # field_int_required not present\n        \"field_int_strict\": \"123\",\n        \"field_int_32bit\": 1_000_000_000_000_000,\n        \"field_bool\": 1234,\n        # field_bool_required not present\n        \"field_decimal\": \"hello\",\n        # field_decimal_required not present\n        \"field_decimal_special\": \"NaN\",\n        \"field_uuid\": \"hello\",\n        # field_uuid_required not present\n        \"field_date\": 1234,\n        # field_date_required not present\n        \"field_date_format\": \"20220202020202\",\n        \"field_datetime\": 1234,\n        # field_datetime_required not present\n        \"field_datetime_format\": \"02022020 7-20PM PDT\",\n        \"field_list\": \"not_a_list\",\n        # field_list_required not present\n        \"field_list_indexed\": [\"text\", 1, \"text\"],\n        \"field_nested\": {\n            \"inner_str\": 1234,\n            # inner_required_str not present\n        },\n        \"field_nested_invalid\": 5678,\n        # field_nested_required not present\n        \"field_list_nested\": [\n            {\"inner_str\": 5678, \"inner_required_str\": \"present\"},\n            {\"inner_str\": \"valid\"},  # inner_required_str not present\n            54321,\n        ],\n        \"field_list_nested_invalid\": 54321,\n        # field_list_nested_required not present\n        # field_raw_required not present\n        \"field_enum\": 12345,\n        \"field_enum_invalid_choice\": \"notvalid\",\n    }\n\n\ndef get_expected_validation_errors():\n    # This is the expected output of the above\n    # get_invalid_field_test_schema_req function\n    return {\n        \"field_str\": [INVALID_STRING],\n        \"field_str_required\": [MISSING_DATA],\n        \"field_str_min\": [get_min_length_error_msg(2)],\n        \"field_str_max\": [get_max_length_error_msg(3)],\n        \"field_str_min_and_max\": [get_length_range_error_msg(2, 3)],\n        \"field_str_equal\": [get_length_equal_error_msg(3)],\n        \"field_str_regex\": [INVALID_STRING_PATTERN],\n        \"field_str_email\": [INVALID_EMAIL],\n        \"field_int\": [INVALID_INTEGER],\n        \"field_int_required\": [MISSING_DATA],\n        \"field_int_strict\": [INVALID_INTEGER],\n        \"field_int_32bit\": [INVALID_INTEGER_32BIT],\n        \"field_bool\": [INVALID_BOOLEAN],\n        \"field_bool_required\": [MISSING_DATA],\n        \"field_decimal\": [INVALID_DECIMAL],\n        \"field_decimal_required\": [MISSING_DATA],\n        \"field_decimal_special\": [INVALID_SPECIAL_DECIMAL],\n        \"field_uuid\": [INVALID_UUID],\n        \"field_uuid_required\": [MISSING_DATA],\n        \"field_date\": [INVALID_DATE],\n        \"field_date_required\": [MISSING_DATA],\n        \"field_date_format\": [INVALID_DATE],\n        \"field_datetime\": [INVALID_DATETIME],\n        \"field_datetime_required\": [MISSING_DATA],\n        \"field_datetime_format\": [INVALID_DATETIME],\n        \"field_list\": [INVALID_LIST],\n        \"field_list_required\": [MISSING_DATA],\n        \"field_list_indexed\": {0: [INVALID_INTEGER], 2: [INVALID_INTEGER]},\n        \"field_nested\": {\"inner_str\": [INVALID_STRING], \"inner_required_str\": [MISSING_DATA]},\n        \"field_nested_invalid\": INVALID_SCHEMA,\n        \"field_nested_required\": [MISSING_DATA],\n        \"field_list_nested\": {\n            0: {\"inner_str\": [INVALID_STRING]},\n            1: {\"inner_required_str\": [MISSING_DATA]},\n            2: INVALID_SCHEMA,\n        },\n        \"field_list_nested_invalid\": [INVALID_LIST],\n        \"field_list_nested_required\": [MISSING_DATA],\n        \"field_raw_required\": [MISSING_DATA],\n        \"field_enum\": [get_enum_error_msg(EnumA)],\n        \"field_enum_invalid_choice\": [get_enum_error_msg(EnumA)],\n        \"field_enum_required\": [MISSING_DATA],\n    }"}
{"path":"infra/accounts/logs.tf","language":"unknown","type":"code","directory":"infra/accounts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/accounts/logs.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/search/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/search\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/search/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/accounts/main.tf","language":"unknown","type":"code","directory":"infra/accounts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/accounts/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":""}
{"path":"infra/accounts/outputs.tf","language":"unknown","type":"code","directory":"infra/accounts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/accounts/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/search/backend/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/search/backend\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/search/backend/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/accounts/simpler-grants-gov.315341936575.s3.tfbackend","language":"unknown","type":"code","directory":"infra/accounts","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/accounts/simpler-grants-gov.315341936575.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":""}
{"path":"infra/analytics/app-config/build_repository.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/build_repository.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/search/backend/test_load_opportunities_to_index.py\nLanguage: py\nType: code\nDirectory: api/tests/src/search/backend\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/search/backend/test_load_opportunities_to_index.py\nSize: 13.07 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/analytics/app-config/dev.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/dev.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"import pytest\nfrom sqlalchemy import select\n\nfrom src.db.models.opportunity_models import OpportunityChangeAudit\nfrom src.search.backend.load_opportunities_to_index import (\n    LoadOpportunitiesToIndex,\n    LoadOpportunitiesToIndexConfig,\n)\nfrom src.util import file_util\nfrom src.util.datetime_util import get_now_us_eastern_datetime\nfrom tests.conftest import BaseTestClass\nfrom tests.src.db.models.factories import (\n    AgencyFactory,\n    OpportunityAttachmentFactory,\n    OpportunityChangeAuditFactory,\n    OpportunityFactory,\n)\n\n\nclass TestLoadOpportunitiesToIndexFullRefresh(BaseTestClass):\n    @pytest.fixture(scope=\"class\")\n    def load_opportunities_to_index(self, db_session, search_client, opportunity_index_alias):\n        config = LoadOpportunitiesToIndexConfig(\n            alias_name=opportunity_index_alias, index_prefix=\"test-load-opps\"\n        )\n        return LoadOpportunitiesToIndex(db_session, search_client, True, config)\n\n    def test_load_opportunities_to_index(\n        self,\n        truncate_opportunities,\n        enable_factory_create,\n        search_client,\n        opportunity_index_alias,\n        load_opportunities_to_index,\n    ):\n        # Create an agency that some records will be connected to\n        agency = AgencyFactory.create(agency_code=\"FUN-AGENCY\", is_test_agency=False)\n\n        # Create 25 opportunities we will load into the search index\n        opportunities = []\n        opportunities.extend(\n            OpportunityFactory.create_batch(\n                size=6,\n                is_posted_summary=True,\n                agency_code=agency.agency_code,\n                opportunity_attachments=[],\n            )\n        )\n        opportunities.extend(\n            OpportunityFactory.create_batch(\n                size=3, is_forecasted_summary=True, opportunity_attachments=[]\n            )\n        )\n        opportunities.extend(\n            OpportunityFactory.create_batch(\n                size=2, is_closed_summary=True, opportunity_attachments=[]\n            )\n        )\n        opportunities.extend(\n            OpportunityFactory.create_batch(\n                size=8, is_archived_non_forecast_summary=True, opportunity_attachments=[]\n            )\n        )\n        opportunities.extend(\n            OpportunityFactory.create_batch(\n                size=6,\n                is_archived_forecast_summary=True,\n                agency_code=agency.agency_code,\n                opportunity_attachments=[],\n            )\n        )\n\n        # Create some opportunities that won't get fetched / loaded into search\n        OpportunityFactory.create_batch(size=3, is_draft=True, opportunity_attachments=[])\n        OpportunityFactory.create_batch(size=4, no_current_summary=True, opportunity_attachments=[])\n\n        AgencyFactory.create(agency_code=\"MY-TEST-AGENCY\", is_test_agency=True)\n        OpportunityFactory.create_batch(\n            size=3, agency_code=\"MY-TEST-AGENCY\", opportunity_attachments=[]\n        )\n\n        for opportunity in opportunities:\n            OpportunityChangeAuditFactory.create(\n                opportunity=opportunity,\n            )\n\n        load_opportunities_to_index.run()\n        # Verify some metrics first\n        assert (\n            len(opportunities)\n            == load_opportunities_to_index.metrics[\n                load_opportunities_to_index.Metrics.RECORDS_LOADED\n            ]\n        )\n\n        # Just do some rough validation that the data is present\n        resp = search_client.search(opportunity_index_alias, {\"size\": 100})\n\n        assert resp.total_records == len(opportunities)\n\n        assert set([opp.opportunity_id for opp in opportunities]) == set(\n            [record[\"opportunity_id\"] for record in resp.records]\n        )\n\n        # Rerunning without changing anything about the data in the DB doesn't meaningfully change anything\n        load_opportunities_to_index.index_name = load_opportunities_to_index.index_name + \"-another\"\n        load_opportunities_to_index.run()\n        resp = search_client.search(opportunity_index_alias, {\"size\": 100})\n\n        assert resp.total_records == len(opportunities)\n\n        assert set([opp.opportunity_id for opp in opportunities]) == set(\n            [record[\"opportunity_id\"] for record in resp.records]\n        )\n\n        assert load_opportunities_to_index.metrics[\n            load_opportunities_to_index.Metrics.RECORDS_LOADED\n        ] == len(opportunities)\n        assert (\n            load_opportunities_to_index.metrics[\n                load_opportunities_to_index.Metrics.TEST_RECORDS_SKIPPED\n            ]\n            == 3\n        )\n\n        # Rerunning but first add a few more opportunities to show up\n        opportunities.extend(OpportunityFactory.create_batch(size=3, opportunity_attachments=[]))\n        load_opportunities_to_index.index_name = (\n            load_opportunities_to_index.index_name + \"-new-data\"\n        )\n        load_opportunities_to_index.run()\n\n        resp = search_client.search(opportunity_index_alias, {\"size\": 100})\n\n        assert resp.total_records == len(opportunities)\n\n        assert set([opp.opportunity_id for opp in opportunities]) == set(\n            [record[\"opportunity_id\"] for record in resp.records]\n        )\n\n    def test_opportunity_attachment_pipeline(\n        self,\n        mock_s3_bucket,\n        db_session,\n        enable_factory_create,\n        load_opportunities_to_index,\n        monkeypatch: pytest.MonkeyPatch,\n        opportunity_index_alias,\n        search_client,\n    ):\n        filename_1 = \"test_file_1.txt\"\n        file_path_1 = f\"s3://{mock_s3_bucket}/{filename_1}\"\n        content = \"I am a file\"\n\n        with file_util.open_stream(file_path_1, \"w\") as outfile:\n            outfile.write(content)\n\n        filename_2 = \"test_file_2.css\"\n        file_path_2 = f\"s3://{mock_s3_bucket}/{filename_2}\"\n\n        opportunity = OpportunityFactory.create(opportunity_attachments=[])\n        OpportunityAttachmentFactory.create(\n            opportunity=opportunity,\n            file_contents=content,\n            file_location=file_path_1,\n            file_name=filename_1,\n        )\n\n        OpportunityAttachmentFactory.create(\n            opportunity=opportunity,\n            file_contents=content,\n            file_location=file_path_2,\n            file_name=filename_2,\n        )\n\n        load_opportunities_to_index.index_name = (\n            load_opportunities_to_index.index_name + \"-pipeline\"\n        )\n\n        load_opportunities_to_index.run()\n\n        resp = search_client.search(opportunity_index_alias, {\"size\": 100})\n\n        record = [d for d in resp.records if d.get(\"opportunity_id\") == opportunity.opportunity_id]\n        attachments = record[0][\"attachments\"]\n\n        # assert only one (allowed) opportunity attachment was uploaded\n        assert len(attachments) == 1\n        # assert correct attachment was uploaded\n        assert attachments[0][\"filename\"] == filename_1\n        # assert data was b64encoded\n        assert attachments[0][\"attachment\"][\"content\"] == content  # decoded b64encoded attachment\n\n\nclass TestLoadOpportunitiesToIndexPartialRefresh(BaseTestClass):\n    @pytest.fixture(scope=\"class\")\n    def load_opportunities_to_index(self, db_session, search_client, opportunity_index_alias):\n        config = LoadOpportunitiesToIndexConfig(\n            alias_name=opportunity_index_alias, index_prefix=\"test-load-opps\"\n        )\n        return LoadOpportunitiesToIndex(db_session, search_client, False, config)\n\n    def test_load_opportunities_to_index(\n        self,\n        truncate_opportunities,\n        enable_factory_create,\n        db_session,\n        search_client,\n        opportunity_index_alias,\n        load_opportunities_to_index,\n    ):\n        index_name = \"partial-refresh-index-\" + get_now_us_eastern_datetime().strftime(\n            \"%Y-%m-%d_%H-%M-%S\"\n        )\n        search_client.create_index(index_name)\n        search_client.swap_alias_index(\n            index_name,\n            load_opportunities_to_index.config.alias_name,\n        )\n\n        # Load a bunch of records into the DB\n        opportunities = []\n        opportunities.extend(\n            OpportunityFactory.create_batch(\n                size=6, is_posted_summary=True, opportunity_attachments=[]\n            )\n        )\n        opportunities.extend(\n            OpportunityFactory.create_batch(\n                size=3, is_forecasted_summary=True, opportunity_attachments=[]\n            )\n        )\n        opportunities.extend(\n            OpportunityFactory.create_batch(\n                size=2, is_closed_summary=True, opportunity_attachments=[]\n            )\n        )\n        opportunities.extend(\n            OpportunityFactory.create_batch(\n                size=8, is_archived_non_forecast_summary=True, opportunity_attachments=[]\n            )\n        )\n        opportunities.extend(\n            OpportunityFactory.create_batch(\n                size=6, is_archived_forecast_summary=True, opportunity_attachments=[]\n            )\n        )\n\n        AgencyFactory.create(agency_code=\"MY-TEST-AGENCY-123\", is_test_agency=True)\n        test_opps = OpportunityFactory.create_batch(\n            size=2, agency_code=\"MY-TEST-AGENCY-123\", opportunity_attachments=[]\n        )\n\n        for opportunity in itertools.chain(opportunities, test_opps):\n            OpportunityChangeAuditFactory.create(opportunity=opportunity, updated_at=None)\n\n        load_opportunities_to_index.run()\n\n        resp = search_client.search(opportunity_index_alias, {\"size\": 100})\n        assert resp.total_records == len(opportunities)\n\n        assert load_opportunities_to_index.metrics[\n            load_opportunities_to_index.Metrics.RECORDS_LOADED\n        ] == len(opportunities)\n        assert load_opportunities_to_index.metrics[\n            load_opportunities_to_index.Metrics.TEST_RECORDS_SKIPPED\n        ] == len(test_opps)\n\n        # Add a few more opportunities that will be created\n        opportunities.extend(\n            OpportunityFactory.create_batch(\n                size=3, is_posted_summary=True, opportunity_attachments=[]\n            )\n        )\n\n        # Delete some opportunities\n        opportunities_to_delete = [opportunities.pop(), opportunities.pop(), opportunities.pop()]\n        for opportunity in opportunities_to_delete:\n            db_session.delete(opportunity)\n\n        # Change the agency on a few to a test agency to delete them\n        opportunities_now_with_test_agency = opportunities[0:3]\n        for opportunity in opportunities_now_with_test_agency:\n            opportunity.agency_code = \"MY-TEST-AGENCY-123\"\n\n        db_session.commit()\n        db_session.expunge_all()\n        load_opportunities_to_index.run()\n\n        resp = search_client.search(opportunity_index_alias, {\"size\": 100})\n        assert resp.total_records == len(opportunities) - 3  # test agency opportunities excluded\n\n        # Running one last time without any changes should be fine as well\n        load_opportunities_to_index.run()\n        resp = search_client.search(opportunity_index_alias, {\"size\": 100})\n        assert resp.total_records == len(opportunities) - 3\n\n    def test_load_opportunities_to_index_index_does_not_exist(self, db_session, search_client):\n        config = LoadOpportunitiesToIndexConfig(\n            alias_name=\"fake-index-that-will-not-exist\", index_prefix=\"test-load-opps\"\n        )\n        load_opportunities_to_index = LoadOpportunitiesToIndex(\n            db_session, search_client, False, config\n        )\n\n        with pytest.raises(RuntimeError, match=\"please run the full refresh job\"):\n            load_opportunities_to_index.run()\n\n    def test_new_opportunity_gets_indexed(\n        self,\n        db_session,\n        load_opportunities_to_index,\n    ):\n        \"\"\"Test that a new opportunity in the queue gets indexed\"\"\"\n        test_opportunity = OpportunityFactory.create(\n            opportunity_attachments=[],\n            is_draft=False,\n        )\n\n        # Add to queue\n        OpportunityChangeAuditFactory.create(opportunity=test_opportunity, updated_at=None)\n\n        load_opportunities_to_index.run()\n\n        # Verify queue was cleared\n        remaining_queue = (\n            db_session.execute(\n                select(OpportunityChangeAudit).where(\n                    OpportunityChangeAudit.opportunity_id == test_opportunity.opportunity_id,\n                    OpportunityChangeAudit.updated_at.is_(None),\n                )\n            )\n            .scalars()\n            .all()\n        )\n        assert len(remaining_queue) == 0\n\n    def test_draft_opportunity_not_indexed(self, db_session, load_opportunities_to_index):\n        \"\"\"Test that draft opportunities are not indexed\"\"\"\n        test_opportunity = OpportunityFactory.create(is_draft=True, opportunity_attachments=[])\n\n        # Add to queue\n        OpportunityChangeAuditFactory.create(opportunity=test_opportunity, updated_at=None)\n        now = get_now_us_eastern_datetime()\n\n        # Verify queue was not cleared\n        remaining_queue = (\n            db_session.execute(\n                select(OpportunityChangeAudit).where(\n                    OpportunityChangeAudit.opportunity_id == test_opportunity.opportunity_id,\n                    OpportunityChangeAudit.updated_at <= now,\n                )\n            )\n            .scalars()\n            .all()\n        )\n        assert len(remaining_queue) == 1"}
{"path":"infra/analytics/app-config/env-config/database.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/env-config/database.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/services/extracts_v1/test_get_extracts.py\nLanguage: py\nType: service\nDirectory: api/tests/src/services/extracts_v1\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/services/extracts_v1/test_get_extracts.py\nSize: 7.62 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/analytics/app-config/env-config/environment-variables.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/env-config/environment-variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"import pytest\n\nfrom src.constants.lookup_constants import ExtractType\nfrom src.db.models.extract_models import ExtractMetadata\nfrom src.pagination.pagination_models import PaginationParams, SortDirection, SortOrderParams\nfrom src.search.search_models import DateSearchFilter\nfrom src.services.extracts_v1.get_extracts import ExtractFilters, ExtractListParams, get_extracts\nfrom tests.src.db.models.factories import ExtractMetadataFactory\n\n\n@pytest.fixture(autouse=True)\ndef clear_extracts(db_session):\n    db_session.query(ExtractMetadata).delete()\n    db_session.commit()\n    yield\n\n\ndef test_get_extracts_no_filters(\n    enable_factory_create,\n    db_session,\n):\n    ExtractMetadataFactory.create_batch(3)\n\n    params = ExtractListParams(\n        pagination=PaginationParams(\n            page_size=10,\n            page_offset=1,\n            order_by=\"created_at\",\n            sort_direction=SortDirection.ASCENDING,\n        )\n    )\n\n    extracts, pagination_info = get_extracts(db_session, params)\n    assert len(extracts) == 3\n    assert pagination_info.total_records == 3\n\n\ndef test_get_extracts_with_type_filter(\n    enable_factory_create,\n    db_session,\n):\n    ExtractMetadataFactory.create_batch(3, extract_type=ExtractType.OPPORTUNITIES_CSV)\n    ExtractMetadataFactory.create_batch(3, extract_type=ExtractType.OPPORTUNITIES_JSON)\n\n    params = ExtractListParams(\n        pagination=PaginationParams(\n            page_size=10,\n            page_offset=1,\n            order_by=\"created_at\",\n            sort_direction=SortDirection.ASCENDING,\n        ),\n        filters=ExtractFilters(extract_type=\"opportunities_json\"),\n    )\n\n    extracts, _ = get_extracts(db_session, params)\n    assert len(extracts) == 3\n    assert all(r.extract_type == \"opportunities_json\" for r in extracts)\n\n\ndef test_get_extracts_with_date_filter(enable_factory_create, db_session):\n    ExtractMetadataFactory.create_batch(3, created_at=datetime(2024, 1, 15))\n    ExtractMetadataFactory.create_batch(3, created_at=datetime(2024, 1, 25))\n\n    params = ExtractListParams(\n        pagination=PaginationParams(\n            page_size=10,\n            page_offset=1,\n            order_by=\"created_at\",\n            sort_direction=SortDirection.ASCENDING,\n        ),\n        filters=ExtractFilters(\n            created_at=DateSearchFilter(\n                start_date=date(2024, 1, 10),\n                end_date=date(2024, 1, 20),\n                sort_direction=SortDirection.ASCENDING,\n            )\n        ),\n    )\n\n    extracts, _ = get_extracts(db_session, params)\n    assert len(extracts) == 3\n    assert extracts[0].created_at == datetime(2024, 1, 15, tzinfo=timezone.utc)\n\n\ndef test_get_extracts_pagination(enable_factory_create, db_session):\n    ExtractMetadataFactory.create_batch(3)\n\n    params = ExtractListParams(\n        pagination=PaginationParams(\n            page_size=2,\n            page_offset=1,\n            order_by=\"created_at\",\n            sort_direction=SortDirection.ASCENDING,\n        )\n    )\n\n    extracts, pagination_info = get_extracts(db_session, params)\n    assert len(extracts) == 2\n    assert pagination_info.total_records == 3\n    assert pagination_info.total_pages == 2\n\n    # Test second page\n    params.pagination.page_offset = 2\n    extracts, pagination_info = get_extracts(db_session, params)\n    assert len(extracts) == 1\n\n\ndef test_get_extracts_ordering(db_session, enable_factory_create):\n    # Create test data with different values for ordering\n    extract1 = ExtractMetadataFactory.create(\n        extract_type=ExtractType.OPPORTUNITIES_JSON,\n        file_name=\"b_file.json\",\n        created_at=datetime(2025, 1, 1, tzinfo=timezone.utc),\n    )\n    extract2 = ExtractMetadataFactory.create(\n        extract_type=ExtractType.OPPORTUNITIES_CSV,\n        file_name=\"a_file.csv\",\n        created_at=datetime(2025, 1, 2, tzinfo=timezone.utc),\n    )\n    extract3 = ExtractMetadataFactory.create(\n        extract_type=ExtractType.OPPORTUNITIES_CSV,\n        file_name=\"c_file.csv\",\n        created_at=datetime(2025, 1, 3, tzinfo=timezone.utc),\n    )\n\n    extracts = [extract1, extract2, extract3]\n\n    # Test ordering by created_at\n    params_created_asc = ExtractListParams(\n        pagination=PaginationParams(\n            sort_order=[\n                SortOrderParams(order_by=\"created_at\", sort_direction=SortDirection.ASCENDING)\n            ],\n            page_size=10,\n            page_offset=1,\n        ),\n        filters=ExtractFilters(created_at=DateSearchFilter(start_date=date(2025, 1, 1))),\n    )\n    results_created_asc, _ = get_extracts(db_session, params_created_asc)\n    assert [x.extract_metadata_id for x in results_created_asc] == [\n        x.extract_metadata_id for x in sorted(extracts, key=lambda x: x.created_at)\n    ]\n\n    params_created_desc = ExtractListParams(\n        pagination=PaginationParams(\n            sort_order=[\n                SortOrderParams(order_by=\"created_at\", sort_direction=SortDirection.DESCENDING)\n            ],\n            page_size=10,\n            page_offset=1,\n        ),\n        filters=ExtractFilters(created_at=DateSearchFilter(start_date=date(2025, 1, 1))),\n    )\n    results_created_desc, _ = get_extracts(db_session, params_created_desc)\n    assert [x.extract_metadata_id for x in results_created_desc] == [\n        x.extract_metadata_id for x in sorted(extracts, key=lambda x: x.created_at, reverse=True)\n    ]\n\n    # Test ordering by extract_type\n    params_type_asc = ExtractListParams(\n        pagination=PaginationParams(\n            sort_order=[\n                SortOrderParams(order_by=\"extract_type\", sort_direction=SortDirection.ASCENDING)\n            ],\n            page_size=10,\n            page_offset=1,\n        ),\n        filters=ExtractFilters(created_at=DateSearchFilter(start_date=date(2025, 1, 1))),\n    )\n    results_type_asc, _ = get_extracts(db_session, params_type_asc)\n    assert results_type_asc[0].extract_type == ExtractType.OPPORTUNITIES_CSV\n    assert results_type_asc[-1].extract_type == ExtractType.OPPORTUNITIES_JSON\n\n    params_type_desc = ExtractListParams(\n        pagination=PaginationParams(\n            sort_order=[\n                SortOrderParams(order_by=\"extract_type\", sort_direction=SortDirection.DESCENDING)\n            ],\n            page_size=10,\n            page_offset=1,\n        ),\n        filters=ExtractFilters(created_at=DateSearchFilter(start_date=date(2025, 1, 1))),\n    )\n    results_type_desc, _ = get_extracts(db_session, params_type_desc)\n    assert results_type_desc[-1].extract_type == ExtractType.OPPORTUNITIES_CSV\n    assert results_type_desc[0].extract_type == ExtractType.OPPORTUNITIES_JSON\n\n    # Test ordering by file_name\n    params_name_asc = ExtractListParams(\n        pagination=PaginationParams(\n            sort_order=[\n                SortOrderParams(order_by=\"file_name\", sort_direction=SortDirection.ASCENDING)\n            ],\n            page_size=10,\n            page_offset=1,\n        ),\n        filters=ExtractFilters(created_at=DateSearchFilter(start_date=date(2025, 1, 1))),\n    )\n    results_name_asc, _ = get_extracts(db_session, params_name_asc)\n    assert [x.file_name for x in results_name_asc] == sorted([x.file_name for x in extracts])\n\n    params_name_desc = ExtractListParams(\n        pagination=PaginationParams(\n            sort_order=[\n                SortOrderParams(order_by=\"file_name\", sort_direction=SortDirection.DESCENDING)\n            ],\n            page_size=10,\n            page_offset=1,\n        ),\n        filters=ExtractFilters(created_at=DateSearchFilter(start_date=date(2025, 1, 1))),\n    )\n    results_name_desc, _ = get_extracts(db_session, params_name_desc)\n    assert [x.file_name for x in results_name_desc] == sorted(\n        [x.file_name for x in extracts], reverse=True\n    )"}
{"path":"infra/analytics/app-config/env-config/file_upload_jobs.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/env-config/file_upload_jobs.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/services/opportunity_attachments/__init__.py\nLanguage: py\nType: service\nDirectory: api/tests/src/services/opportunity_attachments\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/services/opportunity_attachments/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/analytics/app-config/env-config/main.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/env-config/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":""}
{"path":"infra/analytics/app-config/env-config/outputs.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/env-config/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/services/opportunity_attachments/test_attachment_util.py\nLanguage: py\nType: service\nDirectory: api/tests/src/services/opportunity_attachments\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/services/opportunity_attachments/test_attachment_util.py\nSize: 1.91 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/analytics/app-config/env-config/scheduled_jobs.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/env-config/scheduled_jobs.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"from src.adapters.aws import S3Config\nfrom src.services.opportunity_attachments import attachment_util\nfrom tests.src.db.models.factories import OpportunityFactory\n\n\n@pytest.mark.parametrize(\n    \"is_draft,opportunity_id,attachment_id,file_name,expected_path\",\n    [\n        (\n            False,\n            123,\n            456,\n            \"my_file.txt\",\n            \"s3://test-public-bucket/opportunities/123/attachments/456/my_file.txt\",\n        ),\n        (\n            True,\n            12345,\n            45678,\n            \"example.pdf\",\n            \"s3://test-draft-bucket/opportunities/12345/attachments/45678/example.pdf\",\n        ),\n        (\n            False,\n            1,\n            1,\n            \"example.docx\",\n            \"s3://test-public-bucket/opportunities/1/attachments/1/example.docx\",\n        ),\n    ],\n)\ndef test_get_s3_attachment_path(is_draft, opportunity_id, attachment_id, file_name, expected_path):\n    config = S3Config(\n        PUBLIC_FILES_BUCKET=\"s3://test-public-bucket\", DRAFT_FILES_BUCKET=\"s3://test-draft-bucket\"\n    )\n\n    opp = OpportunityFactory.build(opportunity_id=opportunity_id, is_draft=is_draft)\n\n    assert (\n        attachment_util.get_s3_attachment_path(file_name, attachment_id, opp, config)\n        == expected_path\n    )\n\n\n@pytest.mark.parametrize(\n    \"existing_file_name,expected_file_name\",\n    [\n        (\"abc.txt\", \"abc.txt\"),\n        (\"my file.pdf\", \"my_file.pdf\"),\n        (\"a.b.c.wav\", \"a.b.c.wav\"),\n        (\"my-valid~file_is.good.txt\", \"my-valid~file_is.good.txt\"),\n        (\"!@#$%^&*()'\\\",/;'myfile.txt\", \"myfile.txt\"),\n        (\"0123456789 |[]\", \"0123456789_\"),\n        (\"many       spaces.txt\", \"many_spaces.txt\"),\n        (\"other\\t\\twhitespace\\n\\nremoved.txt\", \"other_whitespace_removed.txt\"),\n    ],\n)\ndef test_adjust_legacy_file_name(existing_file_name, expected_file_name):\n    assert attachment_util.adjust_legacy_file_name(existing_file_name) == expected_file_name"}
{"path":"infra/analytics/app-config/env-config/variables.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/env-config/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/task/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/task\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/analytics/app-config/feature-flags.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/feature-flags.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":""}
{"path":"infra/analytics/app-config/main.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/task/analytics/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/task/analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/analytics/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/analytics/app-config/outputs.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":""}
{"path":"infra/analytics/app-config/prod.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/prod.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/task/analytics/test_create_analytics_db_csvs.py\nLanguage: py\nType: code\nDirectory: api/tests/src/task/analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/analytics/test_create_analytics_db_csvs.py\nSize: 3.56 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/analytics/app-config/staging.tf","language":"unknown","type":"code","directory":"infra/analytics/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/app-config/staging.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"import pytest\n\nimport src.util.file_util as file_util\nfrom src.task.analytics.create_analytics_db_csvs import (\n    CreateAnalyticsDbCsvsConfig,\n    CreateAnalyticsDbCsvsTask,\n)\nfrom tests.conftest import BaseTestClass\nfrom tests.src.db.models.factories import OpportunityFactory\n\n\ndef validate_file(file_path: str, expected_record_count: int) -> dict:\n    with file_util.open_stream(file_path) as csvfile:\n        records = [record for record in csv.DictReader(csvfile)]\n\n        assert len(records) == expected_record_count\n\n    return records\n\n\nclass TestCreateAnalyticsDbCsvsTask(BaseTestClass):\n\n    @pytest.fixture(scope=\"class\")\n    def opportunities(self, truncate_opportunities, enable_factory_create):\n        # Create a variety of opportunities\n        opps = []\n        opps.extend(OpportunityFactory.create_batch(size=5, is_posted_summary=True))\n        opps.extend(OpportunityFactory.create_batch(size=3, is_forecasted_summary=True))\n        opps.extend(OpportunityFactory.create_batch(size=4, is_closed_summary=True))\n        opps.extend(OpportunityFactory.create_batch(size=2, is_archived_non_forecast_summary=True))\n        opps.extend(OpportunityFactory.create_batch(size=4, is_archived_forecast_summary=True))\n        opps.extend(OpportunityFactory.create_batch(size=2, is_draft=True))\n        opps.extend(OpportunityFactory.create_batch(size=1, no_current_summary=True))\n        return opps\n\n    @pytest.fixture()\n    def task(self, db_session, mock_s3_bucket, test_api_schema):\n        config = CreateAnalyticsDbCsvsConfig(\n            API_ANALYTICS_DB_EXTRACTS_PATH=f\"s3://{mock_s3_bucket}/table-extracts\",\n            API_ANALYTICS_DB_SCHEMA=test_api_schema,\n        )\n        return CreateAnalyticsDbCsvsTask(db_session, config=config)\n\n    def test_create_analytics_db_csvs(self, db_session, task, opportunities):\n        task.run()\n\n        # Validate the opportunity file\n        csv_opps = validate_file(task.config.file_path + \"/opportunity.csv\", len(opportunities))\n        opportunity_ids = set([o.opportunity_id for o in opportunities])\n        csv_opportunity_ids = set([int(record[\"opportunity_id\"]) for record in csv_opps])\n        assert opportunity_ids == csv_opportunity_ids\n\n        # Validate the current opportunity file\n        current_opportunity_summaries = [\n            o.current_opportunity_summary\n            for o in opportunities\n            if o.current_opportunity_summary is not None\n        ]\n        csv_current_summaries = validate_file(\n            task.config.file_path + \"/current_opportunity_summary.csv\",\n            len(current_opportunity_summaries),\n        )\n        current_summary_ids = set(\n            [(o.opportunity_id, o.opportunity_summary_id) for o in current_opportunity_summaries]\n        )\n        csv_current_summary_ids = set(\n            [\n                (int(record[\"opportunity_id\"]), int(record[\"opportunity_summary_id\"]))\n                for record in csv_current_summaries\n            ]\n        )\n        assert current_summary_ids == csv_current_summary_ids\n\n        # Validate the opportunity summary file\n        opportunity_summaries = [o.opportunity_summary for o in current_opportunity_summaries]\n        csv_summaries = validate_file(\n            task.config.file_path + \"/opportunity_summary.csv\", len(opportunity_summaries)\n        )\n        opportunity_summary_ids = set([o.opportunity_summary_id for o in opportunity_summaries])\n        csv_opportunity_summary_ids = set(\n            [int(record[\"opportunity_summary_id\"]) for record in csv_summaries]\n        )\n        assert opportunity_summary_ids == csv_opportunity_summary_ids"}
{"path":"infra/analytics/build-repository/main.tf","language":"unknown","type":"code","directory":"infra/analytics/build-repository","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/build-repository/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"File: api/tests/src/task/notifications/test_generate_notifications.py\nLanguage: py\nType: code\nDirectory: api/tests/src/task/notifications\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/notifications/test_generate_notifications.py\nSize: 16.26 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/analytics/build-repository/shared.s3.tfbackend","language":"unknown","type":"code","directory":"infra/analytics/build-repository","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/build-repository/shared.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.138Z","content":"import pytest\nfrom sqlalchemy import select\n\nimport tests.src.db.models.factories as factories\nfrom src.adapters.aws.pinpoint_adapter import _clear_mock_responses, _get_mock_responses\nfrom src.api.opportunities_v1.opportunity_schemas import OpportunityV1Schema\nfrom src.db.models.user_models import UserNotificationLog, UserSavedOpportunity, UserSavedSearch\nfrom src.task.notifications.generate_notifications import (\n    NotificationConstants,\n    NotificationTask,\n    _strip_pagination_params,\n)\nfrom src.util import datetime_util\nfrom tests.src.api.opportunities_v1.test_opportunity_route_search import OPPORTUNITIES\n\n\n@pytest.fixture\ndef user_with_email(db_session, user, monkeypatch):\n    monkeypatch.setenv(\"PINPOINT_APP_ID\", \"test-app-id\")\n    factories.LinkExternalUserFactory.create(user=user, email=\"test@example.com\")\n    return user\n\n\n@pytest.fixture\ndef setup_search_data(opportunity_index, opportunity_index_alias, search_client):\n    # Load into the search index\n    schema = OpportunityV1Schema()\n    json_records = [schema.dump(opportunity) for opportunity in OPPORTUNITIES]\n    search_client.bulk_upsert(opportunity_index, json_records, \"opportunity_id\")\n\n    # Swap the search index alias\n    search_client.swap_alias_index(opportunity_index, opportunity_index_alias)\n\n\n@pytest.fixture\ndef clear_notification_logs(db_session):\n    \"\"\"Clear all notification logs\"\"\"\n    db_session.query(UserNotificationLog).delete()\n    db_session.query(UserSavedOpportunity).delete()\n    db_session.query(UserSavedSearch).delete()\n\n\ndef test_via_cli(cli_runner, db_session, enable_factory_create, user, user_with_email):\n    \"\"\"Simple test that verifies we can invoke the notification task via CLI\"\"\"\n    result = cli_runner.invoke(args=[\"task\", \"generate-notifications\"])\n\n    assert result.exit_code == 0\n\n\ndef test_search_notifications_cli(\n    cli_runner,\n    db_session,\n    enable_factory_create,\n    user,\n    user_with_email,\n    caplog,\n    clear_notification_logs,\n    setup_search_data,\n):\n    \"\"\"Test that verifies we can collect and send search notifications via CLI\"\"\"\n\n    # Create a saved search that needs notification\n    saved_search = factories.UserSavedSearchFactory.create(\n        user=user,\n        search_query={\"keywords\": \"test\"},\n        name=\"Test Search\",\n        last_notified_at=datetime_util.utcnow() - timedelta(days=1),\n        searched_opportunity_ids=[1, 2, 3],\n    )\n\n    notification_logs_count = (\n        db_session.query(UserNotificationLog)\n        .filter(UserNotificationLog.notification_reason == NotificationConstants.SEARCH_UPDATES)\n        .count()\n    )\n\n    _clear_mock_responses()\n\n    result = cli_runner.invoke(args=[\"task\", \"generate-notifications\"])\n\n    assert result.exit_code == 0\n\n    # Verify expected log messages\n    assert \"Collected search notifications\" in caplog.text\n    assert \"Sending notification to user\" in caplog.text\n\n    # Verify the log contains the correct metrics\n    log_records = [r for r in caplog.records if \"Sending notification to user\" in r.message]\n    assert len(log_records) == 1\n    extra = log_records[0].__dict__\n    assert extra[\"user_id\"] == user.user_id\n    assert extra[\"opportunity_count\"] == 0\n    assert extra[\"search_count\"] == 1\n\n    # Verify notification log was created\n    notification_logs = (\n        db_session.query(UserNotificationLog)\n        .filter(UserNotificationLog.notification_reason == NotificationConstants.SEARCH_UPDATES)\n        .all()\n    )\n    assert len(notification_logs) == notification_logs_count + 1\n\n    # Verify last_notified_at was updated\n    db_session.refresh(saved_search)\n    assert saved_search.last_notified_at > datetime_util.utcnow() - timedelta(minutes=1)\n\n    # Verify email was sent via Pinpoint\n    mock_responses = _get_mock_responses()\n    assert len(mock_responses) == 1\n\n    request = mock_responses[0][0]\n    assert request[\"MessageRequest\"][\"Addresses\"] == {\"test@example.com\": {\"ChannelType\": \"EMAIL\"}}\n\n    # Verify notification log was created\n    notification_logs = (\n        db_session.execute(\n            select(UserNotificationLog).where(UserNotificationLog.user_id == user.user_id)\n        )\n        .scalars()\n        .all()\n    )\n\n    assert len(notification_logs) == 2\n    assert notification_logs[0].notification_sent is True\n\n\ndef test_collect_notifications_cli(\n    cli_runner, db_session, enable_factory_create, user, user_with_email, caplog\n):\n    \"\"\"Simple test that verifies we can invoke the notification task via CLI\"\"\"\n    # Create a saved opportunity that needs notification\n    opportunity = factories.OpportunityFactory.create()\n    saved_opportunity = factories.UserSavedOpportunityFactory.create(\n        user=user,\n        opportunity=opportunity,\n        last_notified_at=opportunity.updated_at - timedelta(days=1),\n    )\n    factories.OpportunityChangeAuditFactory.create(\n        opportunity=opportunity,\n        updated_at=saved_opportunity.last_notified_at + timedelta(minutes=1),\n    )\n\n    result = cli_runner.invoke(args=[\"task\", \"generate-notifications\"])\n\n    assert result.exit_code == 0\n\n    # Verify expected log messages\n    assert \"Collected opportunity notifications\" in caplog.text\n    assert \"Sending notification to user\" in caplog.text\n\n    # Verify the log contains the correct metrics\n    log_records = [r for r in caplog.records if \"Sending notification to user\" in r.message]\n    assert len(log_records) == 1\n    extra = log_records[0].__dict__\n    assert extra[\"user_id\"] == user.user_id\n    assert extra[\"opportunity_count\"] == 1\n    assert extra[\"search_count\"] == 0\n\n\ndef test_last_notified_at_updates(\n    cli_runner, db_session, enable_factory_create, user, user_with_email\n):\n    \"\"\"Test that last_notified_at gets updated after sending notifications\"\"\"\n    # Create an opportunity that was updated after the last notification\n    opportunity = factories.OpportunityFactory.create()\n    saved_opp = factories.UserSavedOpportunityFactory.create(\n        user=user,\n        opportunity=opportunity,\n        last_notified_at=opportunity.updated_at - timedelta(days=1),\n    )\n    factories.OpportunityChangeAuditFactory.create(\n        opportunity=opportunity,\n        updated_at=saved_opp.last_notified_at + timedelta(minutes=1),\n    )\n    # Store the original notification time\n    original_notification_time = saved_opp.last_notified_at\n\n    # Run the notification task\n    result = cli_runner.invoke(args=[\"task\", \"generate-notifications\"])\n    assert result.exit_code == 0\n\n    # Refresh the saved opportunity from the database\n    db_session.refresh(saved_opp)\n\n    # Verify last_notified_at was updated\n    assert saved_opp.last_notified_at > original_notification_time\n    # Verify last_notified_at is now after the opportunity's updated_at\n    assert saved_opp.last_notified_at > opportunity.updated_at\n\n\ndef test_notification_log_creation(\n    cli_runner, db_session, enable_factory_create, clear_notification_logs, user, user_with_email\n):\n    \"\"\"Test that notification logs are created when notifications are sent\"\"\"\n    # Create a saved opportunity that needs notification\n    opportunity = factories.OpportunityFactory.create()\n    saved_opportunity = factories.UserSavedOpportunityFactory.create(\n        user=user,\n        opportunity=opportunity,\n        last_notified_at=opportunity.updated_at - timedelta(days=1),\n    )\n\n    factories.OpportunityChangeAuditFactory.create(\n        opportunity=opportunity,\n        updated_at=saved_opportunity.last_notified_at + timedelta(minutes=1),\n    )\n\n    # Run the notification task\n    result = cli_runner.invoke(args=[\"task\", \"generate-notifications\"])\n    assert result.exit_code == 0\n\n    # Verify notification log was created\n    notification_logs = db_session.query(UserNotificationLog).all()\n    assert len(notification_logs) == 1\n\n    log = notification_logs[0]\n    assert log.user_id == user.user_id\n    assert log.notification_reason == NotificationConstants.OPPORTUNITY_UPDATES\n    assert log.notification_sent is True\n\n\ndef test_no_notification_log_when_no_updates(\n    cli_runner, db_session, enable_factory_create, clear_notification_logs, user, user_with_email\n):\n    \"\"\"Test that no notification log is created when there are no updates\"\"\"\n    # Create a saved opportunity that doesn't need notification\n    opportunity = factories.OpportunityFactory.create()\n    factories.UserSavedOpportunityFactory.create(\n        user=user,\n        opportunity=opportunity,\n        last_notified_at=opportunity.updated_at + timedelta(minutes=1),  # After the update\n    )\n\n    # Run the notification task\n    result = cli_runner.invoke(args=[\"task\", \"generate-notifications\"])\n    assert result.exit_code == 0\n\n    # Verify no notification log was created\n    notification_logs = db_session.query(UserNotificationLog).all()\n    assert len(notification_logs) == 0\n\n\ndef test_combined_notifications_cli(\n    cli_runner,\n    db_session,\n    enable_factory_create,\n    user,\n    user_with_email,\n    caplog,\n    clear_notification_logs,\n):\n    \"\"\"Test that verifies we can handle both opportunity and search notifications together\"\"\"\n    # Create a saved opportunity that needs notification\n    opportunity = factories.OpportunityFactory.create()\n    saved_opportunity = factories.UserSavedOpportunityFactory.create(\n        user=user,\n        opportunity=opportunity,\n        last_notified_at=opportunity.updated_at - timedelta(days=1),\n    )\n    factories.OpportunityChangeAuditFactory.create(\n        opportunity=opportunity,\n        updated_at=saved_opportunity.last_notified_at + timedelta(minutes=1),\n    )\n\n    # Create a saved search that needs notification\n    saved_search = factories.UserSavedSearchFactory.create(\n        user=user,\n        search_query={\"keywords\": \"test\"},\n        name=\"Test Search\",\n        last_notified_at=datetime_util.utcnow() - timedelta(days=1),\n        searched_opportunity_ids=[1, 2, 3],\n    )\n\n    result = cli_runner.invoke(args=[\"task\", \"generate-notifications\"])\n\n    assert result.exit_code == 0\n\n    # Verify expected log messages\n    assert \"Collected opportunity notifications\" in caplog.text\n    assert \"Collected search notifications\" in caplog.text\n    assert \"Sending notification to user\" in caplog.text\n\n    # Verify the log contains the correct metrics\n    log_records = [r for r in caplog.records if \"Sending notification to user\" in r.message]\n    assert len(log_records) == 1\n    extra = log_records[0].__dict__\n    assert extra[\"user_id\"] == user.user_id\n    assert extra[\"opportunity_count\"] == 1\n    assert extra[\"search_count\"] == 1\n\n    # Verify notification logs were created for both types\n    notification_logs = db_session.query(UserNotificationLog).all()\n    assert len(notification_logs) == 2\n\n    notification_reasons = {log.notification_reason for log in notification_logs}\n    assert notification_reasons == {\n        NotificationConstants.OPPORTUNITY_UPDATES,\n        NotificationConstants.SEARCH_UPDATES,\n    }\n\n    # Verify last_notified_at was updated for both\n    db_session.refresh(saved_opportunity)\n    db_session.refresh(saved_search)\n    assert saved_opportunity.last_notified_at > datetime_util.utcnow() - timedelta(minutes=1)\n    assert saved_search.last_notified_at > datetime_util.utcnow() - timedelta(minutes=1)\n\n\ndef test_grouped_search_queries_cli(\n    cli_runner,\n    db_session,\n    enable_factory_create,\n    clear_notification_logs,\n    user,\n    user_with_email,\n):\n    \"\"\"Test that verifies we properly handle multiple users with the same search query\"\"\"\n    # Create two users with the same search query\n    user1 = factories.UserFactory.create()\n    user2 = factories.UserFactory.create()\n    factories.LinkExternalUserFactory.create(user=user1, email=\"user1@example.com\")\n    factories.LinkExternalUserFactory.create(user=user2, email=\"user2@example.com\")\n\n    same_search_query = {\"keywords\": \"shared search\"}\n\n    # Create saved searches with the same query but different results\n    saved_search1 = factories.UserSavedSearchFactory.create(\n        user=user1,\n        search_query=same_search_query,\n        name=\"User 1 Search\",\n        last_notified_at=datetime_util.utcnow() - timedelta(days=1),\n        searched_opportunity_ids=[1, 2, 3],\n    )\n\n    saved_search2 = factories.UserSavedSearchFactory.create(\n        user=user2,\n        search_query=same_search_query,\n        name=\"User 2 Search\",\n        last_notified_at=datetime_util.utcnow() - timedelta(days=1),\n        searched_opportunity_ids=[4, 5, 6],\n    )\n\n    result = cli_runner.invoke(args=[\"task\", \"generate-notifications\"])\n\n    assert result.exit_code == 0\n\n    # Verify notification logs were created for both users\n    notification_logs = (\n        db_session.query(UserNotificationLog)\n        .filter(UserNotificationLog.notification_reason == NotificationConstants.SEARCH_UPDATES)\n        .all()\n    )\n    assert len(notification_logs) == 2\n\n    # Verify each user got their own notification\n    user_ids = {log.user_id for log in notification_logs}\n    assert user_ids == {user1.user_id, user2.user_id}\n\n    # Verify both searches were updated with the same new results\n    db_session.refresh(saved_search1)\n    db_session.refresh(saved_search2)\n\n    assert saved_search1.searched_opportunity_ids == saved_search2.searched_opportunity_ids\n    assert saved_search1.last_notified_at > datetime_util.utcnow() - timedelta(minutes=1)\n    assert saved_search2.last_notified_at > datetime_util.utcnow() - timedelta(minutes=1)\n\n\ndef test_search_notifications_on_index_change(\n    cli_runner,\n    db_session,\n    enable_factory_create,\n    user,\n    user_with_email,\n    opportunity_index,\n    search_client,\n    clear_notification_logs,\n):\n    \"\"\"Test that verifies notifications are generated when search results change due to index updates\"\"\"\n    # Create a saved search with initial results\n    saved_search = factories.UserSavedSearchFactory.create(\n        user=user,\n        search_query={\"keywords\": \"test\"},\n        name=\"Test Search\",\n        last_notified_at=datetime_util.utcnow() - timedelta(days=1),\n        searched_opportunity_ids=[1, 2],  # Initial results\n    )\n\n    # Update the search index with new data that will change the results\n    schema = OpportunityV1Schema()\n    new_opportunity = factories.OpportunityFactory.create(\n        opportunity_id=999,\n        opportunity_title=\"New Test Opportunity\",\n    )\n    factories.OpportunitySummaryFactory.build(\n        opportunity=new_opportunity,\n        summary_description=\"This should appear in test search results\",\n    )\n    json_record = schema.dump(new_opportunity)\n    search_client.bulk_upsert(opportunity_index, [json_record], \"opportunity_id\")\n\n    # Run the notification task\n    task = NotificationTask(db_session, search_client)\n    task.run()\n\n    # Verify notification log was created due to changed results\n    notification_logs = (\n        db_session.query(UserNotificationLog)\n        .filter(\n            UserNotificationLog.user_id == user.user_id,\n            UserNotificationLog.notification_reason == NotificationConstants.SEARCH_UPDATES,\n        )\n        .all()\n    )\n    assert len(notification_logs) == 1\n\n    # Verify the saved search was updated with new results\n    db_session.refresh(saved_search)\n    assert 999 in saved_search.searched_opportunity_ids  # New opportunity should be in results\n    assert saved_search.last_notified_at > datetime_util.utcnow() - timedelta(minutes=1)\n\n    # Run the task again - should not generate new notifications since results haven't changed\n    task_rerun = NotificationTask(db_session, search_client)\n    task_rerun.run()\n\n    notification_logs = (\n        db_session.query(UserNotificationLog)\n        .filter(\n            UserNotificationLog.user_id == user.user_id,\n            UserNotificationLog.notification_reason == NotificationConstants.SEARCH_UPDATES,\n        )\n        .all()\n    )\n    assert len(notification_logs) == 1  # Should still only be one notification\n\n\ndef test_pagination_params_are_stripped_from_search_query(\n    cli_runner, db_session, enable_factory_create, user, clear_notification_logs\n):\n    \"\"\"Test that pagination parameters are stripped from search queries\"\"\"\n    saved_search = factories.UserSavedSearchFactory.create(\n        user=user,\n        search_query={\n            \"query\": \"test\",\n            \"pagination\": {\"page\": 1, \"per_page\": 10},\n        },\n        name=\"Test Search\",\n        last_notified_at=datetime_util.utcnow() - timedelta(days=1),\n        searched_opportunity_ids=[1, 2],\n    )\n\n    params = _strip_pagination_params(saved_search.search_query)\n    assert params.keys() == {\"query\"}"}
{"path":"infra/analytics/database/.terraform.lock.hcl","language":"unknown","type":"code","directory":"infra/analytics/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/database/.terraform.lock.hcl","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"File: api/tests/src/task/opportunities/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/task/opportunities\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/opportunities/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.462Z"}
{"path":"infra/analytics/database/dev.s3.tfbackend","language":"unknown","type":"code","directory":"infra/analytics/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/database/dev.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":""}
{"path":"infra/analytics/database/main.tf","language":"unknown","type":"code","directory":"infra/analytics/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/database/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"File: api/tests/src/task/opportunities/test_export_opportunity_data_task.py\nLanguage: py\nType: code\nDirectory: api/tests/src/task/opportunities\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/opportunities/test_export_opportunity_data_task.py\nSize: 4.14 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/analytics/database/outputs.tf","language":"unknown","type":"code","directory":"infra/analytics/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/database/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"import pytest\n\nimport src.util.file_util as file_util\nfrom src.api.opportunities_v1.opportunity_schemas import OpportunityV1Schema\nfrom src.constants.lookup_constants import ExtractType\nfrom src.db.models.extract_models import ExtractMetadata\nfrom src.task.opportunities.export_opportunity_data_task import (\n    ExportOpportunityDataConfig,\n    ExportOpportunityDataTask,\n)\nfrom tests.conftest import BaseTestClass\nfrom tests.src.db.models.factories import OpportunityFactory\n\n\nclass TestExportOpportunityDataTask(BaseTestClass):\n    @pytest.fixture\n    def export_opportunity_data_task(self, db_session, mock_s3_bucket):\n        config = ExportOpportunityDataConfig(\n            PUBLIC_FILES_OPPORTUNITY_DATA_EXTRACTS_PATH=f\"s3://{mock_s3_bucket}/\"\n        )\n        return ExportOpportunityDataTask(db_session, config)\n\n    def test_export_opportunity_data_task(\n        self,\n        db_session,\n        truncate_opportunities,\n        enable_factory_create,\n        export_opportunity_data_task,\n    ):\n        db_session.query(ExtractMetadata).delete()\n        db_session.commit()\n\n        # Create 25 opportunities we will load\n        opportunities = []\n        opportunities.extend(OpportunityFactory.create_batch(size=6, is_posted_summary=True))\n        opportunities.extend(OpportunityFactory.create_batch(size=3, is_forecasted_summary=True))\n        opportunities.extend(OpportunityFactory.create_batch(size=2, is_closed_summary=True))\n        opportunities.extend(\n            OpportunityFactory.create_batch(size=8, is_archived_non_forecast_summary=True)\n        )\n        opportunities.extend(\n            OpportunityFactory.create_batch(size=6, is_archived_forecast_summary=True)\n        )\n\n        # Create some opportunities that won't get fetched / exported\n        OpportunityFactory.create_batch(size=3, is_draft=True)\n        OpportunityFactory.create_batch(size=4, no_current_summary=True)\n\n        export_opportunity_data_task.run()\n\n        # Verify some metrics first\n        # Make sure the opportunities we have created matches the number\n        # That get exported\n        assert (\n            len(opportunities)\n            == export_opportunity_data_task.metrics[\n                export_opportunity_data_task.Metrics.RECORDS_EXPORTED\n            ]\n        )\n\n        expected_opportunity_ids = set([opp.opportunity_id for opp in opportunities])\n        # Verify csv file contents\n        with file_util.open_stream(export_opportunity_data_task.csv_file, \"r\") as infile:\n            reader = csv.DictReader(infile)\n            assert expected_opportunity_ids == set(\n                [int(record[\"opportunity_id\"]) for record in reader]\n            )\n\n        # Verify JSON file contents\n        with file_util.open_stream(export_opportunity_data_task.json_file, \"r\") as infile:\n            # Parse JSON File\n            json_opportunities = json.load(infile)\n\n            assert expected_opportunity_ids == set(\n                [int(record[\"opportunity_id\"]) for record in json_opportunities[\"opportunities\"]]\n            )\n\n            schema = OpportunityV1Schema(many=True)\n\n            errors = schema.validate(json_opportunities[\"opportunities\"])\n            assert len(errors) == 0\n\n        # Verify ExtractMetadata entries were created\n        metadata_entries = db_session.query(ExtractMetadata).all()\n        assert len(metadata_entries) == 2\n\n        # Verify JSON metadata\n        json_metadata = next(\n            m for m in metadata_entries if m.extract_type == ExtractType.OPPORTUNITIES_JSON\n        )\n        assert json_metadata.file_name.endswith(\".json\")\n        assert json_metadata.file_name.startswith(\"opportunity_data-\")\n        assert json_metadata.file_path == export_opportunity_data_task.json_file\n        assert json_metadata.file_size_bytes > 0\n\n        # Verify CSV metadata\n        csv_metadata = next(\n            m for m in metadata_entries if m.extract_type == ExtractType.OPPORTUNITIES_CSV\n        )\n        assert csv_metadata.file_name.endswith(\".csv\")\n        assert csv_metadata.file_name.startswith(\"opportunity_data-\")\n        assert csv_metadata.file_path == export_opportunity_data_task.csv_file\n        assert csv_metadata.file_size_bytes > 0"}
{"path":"infra/analytics/database/prod.s3.tfbackend","language":"unknown","type":"code","directory":"infra/analytics/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/database/prod.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"File: api/tests/src/task/opportunities/test_set_current_opportunities_task.py\nLanguage: py\nType: code\nDirectory: api/tests/src/task/opportunities\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/opportunities/test_set_current_opportunities_task.py\nSize: 25.90 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/analytics/database/staging.s3.tfbackend","language":"unknown","type":"code","directory":"infra/analytics/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/database/staging.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"import pytest\n\nfrom src.constants.lookup_constants import OpportunityStatus\nfrom src.db.models.opportunity_models import Opportunity, OpportunitySummary\nfrom src.task.opportunities.set_current_opportunities_task import SetCurrentOpportunitiesTask\nfrom src.util.datetime_util import get_now_us_eastern_date\nfrom tests.conftest import BaseTestClass\nfrom tests.src.db.models.factories import (\n    CurrentOpportunitySummaryFactory,\n    OpportunityFactory,\n    OpportunitySummaryFactory,\n)\n\n# All tests will use this date as the current date\nCURRENT_DATE = date(2024, 3, 25)\n\n# To avoid the need to define dates constantly below, create a few static dates here we can reuse\n# that are more readable than many different dates\nLAST_YEAR = date(2023, 3, 25)\nLAST_MONTH = date(2024, 2, 25)\nYESTERDAY = date(2024, 3, 24)\nTOMORROW = date(2024, 3, 26)\nNEXT_MONTH = date(2024, 4, 25)\nNEXT_YEAR = date(2025, 4, 25)\n\n\n######################################################\n# Date sets for forecast and non-forecast summaries\n######################################################\n@dataclass\nclass SummaryInfo:\n    is_forecast: bool = False\n    post_date: date | None = None\n    close_date: date | None = None\n    archive_date: date | None = None\n\n\n### Non-forecast\n# No post date\nNON_FORECAST_NONE_POST_DATE_1 = SummaryInfo(False, None, NEXT_MONTH, NEXT_YEAR)\nNON_FORECAST_NONE_POST_DATE_2 = SummaryInfo(False, None, YESTERDAY, YESTERDAY)\nNON_FORECAST_NONE_POST_DATE_3 = SummaryInfo(False, None, None, None)\n# before post date\nNON_FORECAST_BEFORE_POST_DATE_1 = SummaryInfo(False, TOMORROW, NEXT_MONTH, NEXT_YEAR)\nNON_FORECAST_BEFORE_POST_DATE_2 = SummaryInfo(False, TOMORROW, None, NEXT_YEAR)\nNON_FORECAST_BEFORE_POST_DATE_3 = SummaryInfo(False, TOMORROW, NEXT_MONTH, None)\nNON_FORECAST_BEFORE_POST_DATE_4 = SummaryInfo(False, TOMORROW, None, None)\n# on post date, before close date\nNON_FORECAST_ON_POST_DATE_1 = SummaryInfo(False, CURRENT_DATE, NEXT_MONTH, NEXT_YEAR)\n# after post date, before close date\nNON_FORECAST_AFTER_POST_DATE_1 = SummaryInfo(False, YESTERDAY, NEXT_MONTH, NEXT_YEAR)\nNON_FORECAST_AFTER_POST_DATE_2 = SummaryInfo(False, YESTERDAY, NEXT_MONTH, None)\n# after post date, on close date\nNON_FORECAST_ON_CLOSE_DATE_1 = SummaryInfo(False, YESTERDAY, CURRENT_DATE, NEXT_YEAR)\nNON_FORECAST_ON_CLOSE_DATE_2 = SummaryInfo(False, YESTERDAY, CURRENT_DATE, None)\n# after close date, before archive date\nNON_FORECAST_AFTER_CLOSE_DATE_1 = SummaryInfo(False, LAST_MONTH, YESTERDAY, NEXT_MONTH)\nNON_FORECAST_AFTER_CLOSE_DATE_2 = SummaryInfo(False, YESTERDAY, YESTERDAY, NEXT_MONTH)\nNON_FORECAST_AFTER_CLOSE_DATE_3 = SummaryInfo(False, LAST_YEAR, LAST_MONTH, TOMORROW)\nNON_FORECAST_AFTER_CLOSE_DATE_4 = SummaryInfo(False, LAST_YEAR, LAST_YEAR, None)\n# after close date, on archive date\nNON_FORECAST_ON_ARCHIVE_DATE_1 = SummaryInfo(False, LAST_YEAR, LAST_MONTH, CURRENT_DATE)\nNON_FORECAST_ON_ARCHIVE_DATE_2 = SummaryInfo(False, LAST_YEAR, LAST_YEAR, CURRENT_DATE)\n# after archive date\nNON_FORECAST_AFTER_ARCHIVE_DATE_1 = SummaryInfo(False, LAST_YEAR, LAST_YEAR, YESTERDAY)\nNON_FORECAST_AFTER_ARCHIVE_DATE_2 = SummaryInfo(False, LAST_YEAR, None, LAST_MONTH)\n\n### Forecast scenarios (note these won't ever have a close date)\n# Null post date\nFORECAST_NONE_POST_DATE_1 = SummaryInfo(True, None, None, YESTERDAY)\nFORECAST_NONE_POST_DATE_2 = SummaryInfo(True, None, None, None)\n# before post date\nFORECAST_BEFORE_POST_DATE_1 = SummaryInfo(True, TOMORROW, None, NEXT_MONTH)\nFORECAST_BEFORE_POST_DATE_2 = SummaryInfo(True, NEXT_MONTH, None, None)\n# on post date, before archive date\nFORECAST_ON_POST_DATE_1 = SummaryInfo(True, CURRENT_DATE, None, NEXT_MONTH)\nFORECAST_ON_POST_DATE_2 = SummaryInfo(True, CURRENT_DATE, None, None)\nFORECAST_ON_POST_DATE_3 = SummaryInfo(True, CURRENT_DATE, None, TOMORROW)\n# after post date, before archive date\nFORECAST_AFTER_POST_DATE_1 = SummaryInfo(True, LAST_MONTH, None, NEXT_MONTH)\nFORECAST_AFTER_POST_DATE_2 = SummaryInfo(True, LAST_YEAR, None, TOMORROW)\nFORECAST_AFTER_POST_DATE_3 = SummaryInfo(True, LAST_MONTH, None, None)\n# after post date, on archive date\nFORECAST_ON_ARCHIVE_DATE_1 = SummaryInfo(True, LAST_MONTH, None, CURRENT_DATE)\nFORECAST_ON_ARCHIVE_DATE_2 = SummaryInfo(True, LAST_YEAR, None, CURRENT_DATE)\n# after archive date\nFORECAST_AFTER_ARCHIVE_DATE_1 = SummaryInfo(True, LAST_MONTH, None, YESTERDAY)\nFORECAST_AFTER_ARCHIVE_DATE_2 = SummaryInfo(True, LAST_YEAR, None, LAST_MONTH)\n\n\nclass OpportunityContainer:\n    def __init__(self, is_draft: bool = False) -> None:\n        self.opportunity = OpportunityFactory.create(no_current_summary=True, is_draft=is_draft)\n        self.expected_current_summary: OpportunitySummary | None = None\n\n    def with_summary(\n        self,\n        post_date: date | None = None,\n        close_date: date | None = None,\n        archive_date: date | None = None,\n        is_forecast: bool = False,\n        revision_number: int | None = None,\n        is_deleted: bool = False,\n        is_expected_current: bool = False,\n        is_already_current: bool = False,\n    ):\n        opportunity_summary = OpportunitySummaryFactory.create(\n            opportunity=self.opportunity,\n            post_date=post_date,\n            close_date=close_date,\n            archive_date=archive_date,\n            is_forecast=is_forecast,\n            revision_number=revision_number,\n            is_deleted=is_deleted,\n        )\n\n        if is_expected_current:\n            self.expected_current_summary = opportunity_summary\n\n        if is_already_current:\n            CurrentOpportunitySummaryFactory.create(\n                opportunity=self.opportunity, opportunity_summary=opportunity_summary\n            )\n\n        return self\n\n\ndef validate_current_opportunity(\n    db_session, container: OpportunityContainer, expected_status: OpportunityStatus | None\n):\n    # Force all opportunity changes to be flushed to the DB and removed from any session cache\n    db_session.commit()\n    db_session.expunge_all()\n\n    opportunity = (\n        db_session.query(Opportunity)\n        .where(Opportunity.opportunity_id == container.opportunity.opportunity_id)\n        .one_or_none()\n    )\n    current_opportunity_summary = opportunity.current_opportunity_summary\n\n    is_current_none = current_opportunity_summary is None\n    is_none_expected = container.expected_current_summary is None\n\n    assert (\n        is_current_none == is_none_expected\n    ), f\"Expected current opportunity summary to be {container.expected_current_summary} but found {current_opportunity_summary}\"\n\n    if current_opportunity_summary is not None:\n        assert expected_status == current_opportunity_summary.opportunity_status\n        assert (\n            current_opportunity_summary.opportunity_summary_id\n            == container.expected_current_summary.opportunity_summary_id\n        )\n\n\n# These params are used by several tests below and represent\n# scenarios with a single summary. Params are in order:\n#   summary_info, expected_opportunity_status\nSINGLE_SUMMARY_PARAMS = [\n    ### Non-forecast scenarios\n    # Null post date\n    (NON_FORECAST_NONE_POST_DATE_1, None),\n    (NON_FORECAST_NONE_POST_DATE_2, None),\n    (NON_FORECAST_NONE_POST_DATE_3, None),\n    # before post date\n    (NON_FORECAST_BEFORE_POST_DATE_1, None),\n    (NON_FORECAST_BEFORE_POST_DATE_2, None),\n    (NON_FORECAST_BEFORE_POST_DATE_3, None),\n    (NON_FORECAST_BEFORE_POST_DATE_4, None),\n    # on post date, before close date\n    (NON_FORECAST_ON_POST_DATE_1, OpportunityStatus.POSTED),\n    # after post date, before close date\n    (NON_FORECAST_AFTER_POST_DATE_1, OpportunityStatus.POSTED),\n    (NON_FORECAST_AFTER_POST_DATE_2, OpportunityStatus.POSTED),\n    # after post date, on close date\n    (NON_FORECAST_ON_CLOSE_DATE_1, OpportunityStatus.POSTED),\n    (NON_FORECAST_ON_CLOSE_DATE_2, OpportunityStatus.POSTED),\n    # after close date, before archive date\n    (NON_FORECAST_AFTER_CLOSE_DATE_1, OpportunityStatus.CLOSED),\n    (NON_FORECAST_AFTER_CLOSE_DATE_2, OpportunityStatus.CLOSED),\n    (NON_FORECAST_AFTER_CLOSE_DATE_3, OpportunityStatus.CLOSED),\n    (NON_FORECAST_AFTER_CLOSE_DATE_4, OpportunityStatus.CLOSED),\n    # after close date, on archive date\n    (NON_FORECAST_ON_ARCHIVE_DATE_1, OpportunityStatus.CLOSED),\n    (NON_FORECAST_ON_ARCHIVE_DATE_2, OpportunityStatus.CLOSED),\n    # after archive date\n    (NON_FORECAST_AFTER_ARCHIVE_DATE_1, OpportunityStatus.ARCHIVED),\n    (NON_FORECAST_AFTER_ARCHIVE_DATE_2, OpportunityStatus.ARCHIVED),\n    ### Forecast scenarios (note these won't ever have a close date)\n    # Null post date\n    (FORECAST_NONE_POST_DATE_1, None),\n    (FORECAST_NONE_POST_DATE_2, None),\n    # before post date\n    (FORECAST_BEFORE_POST_DATE_1, None),\n    (FORECAST_BEFORE_POST_DATE_2, None),\n    # on post date, before archive date\n    (FORECAST_ON_POST_DATE_1, OpportunityStatus.FORECASTED),\n    (FORECAST_ON_POST_DATE_2, OpportunityStatus.FORECASTED),\n    (FORECAST_ON_POST_DATE_3, OpportunityStatus.FORECASTED),\n    # after post date, before archive date\n    (FORECAST_AFTER_POST_DATE_1, OpportunityStatus.FORECASTED),\n    (FORECAST_AFTER_POST_DATE_2, OpportunityStatus.FORECASTED),\n    (FORECAST_AFTER_POST_DATE_3, OpportunityStatus.FORECASTED),\n    # after post date, on archive date\n    (FORECAST_ON_ARCHIVE_DATE_1, OpportunityStatus.FORECASTED),\n    (FORECAST_ON_ARCHIVE_DATE_2, OpportunityStatus.FORECASTED),\n    # after archive date\n    (FORECAST_AFTER_ARCHIVE_DATE_1, OpportunityStatus.ARCHIVED),\n    (FORECAST_AFTER_ARCHIVE_DATE_2, OpportunityStatus.ARCHIVED),\n]\n\n\nclass TestProcessOpportunity(BaseTestClass):\n    @pytest.fixture(scope=\"class\", autouse=True)\n    def shared_setup(self, truncate_opportunities, enable_factory_create):\n        # Autouse fixture that exists just to call the above two fixtures so we don't\n        # need to include it on every test described below.\n\n        # Note that the truncate only occurs once before the tests run, not between each run\n        pass\n\n    @pytest.fixture\n    def set_current_opportunities_task(self, db_session):\n        return SetCurrentOpportunitiesTask(db_session, CURRENT_DATE)\n\n    def test_process_opportunity_no_summaries(self, set_current_opportunities_task, db_session):\n        container = OpportunityContainer()\n\n        set_current_opportunities_task._process_opportunity(container.opportunity)\n        validate_current_opportunity(db_session, container, None)\n\n    @pytest.mark.parametrize(\n        \"summary_info,expected_opportunity_status\",\n        SINGLE_SUMMARY_PARAMS,\n    )\n    def test_process_opportunity_is_draft(\n        self, set_current_opportunities_task, db_session, summary_info, expected_opportunity_status\n    ):\n        # Regardless of the state of the opportunity summary, if the opportunity\n        # is a draft, it will not get a status\n        container = OpportunityContainer(is_draft=True).with_summary(\n            is_forecast=summary_info.is_forecast,\n            post_date=summary_info.post_date,\n            close_date=summary_info.close_date,\n            archive_date=summary_info.archive_date,\n            is_expected_current=False,\n        )\n\n        set_current_opportunities_task._process_opportunity(container.opportunity)\n        validate_current_opportunity(db_session, container, None)\n\n    @pytest.mark.parametrize(\n        \"summary_info,expected_opportunity_status\",\n        SINGLE_SUMMARY_PARAMS,\n    )\n    def test_single_summary_scenario(\n        self,\n        set_current_opportunities_task,\n        db_session,\n        summary_info,\n        expected_opportunity_status,\n    ):\n        container = (\n            OpportunityContainer()\n            .with_summary(\n                is_forecast=summary_info.is_forecast,\n                post_date=summary_info.post_date,\n                close_date=summary_info.close_date,\n                archive_date=summary_info.archive_date,\n                is_expected_current=True if expected_opportunity_status is not None else False,\n            )\n            .with_summary(\n                # this summary won't ever be chosen as it has a revision number set\n                revision_number=0,\n                is_forecast=summary_info.is_forecast,\n                post_date=YESTERDAY,\n                archive_date=YESTERDAY,\n                is_already_current=True,\n            )\n        )\n\n        set_current_opportunities_task._process_opportunity(container.opportunity)\n        validate_current_opportunity(db_session, container, expected_opportunity_status)\n\n    @pytest.mark.parametrize(\n        \"summary_info,expected_opportunity_status\",\n        SINGLE_SUMMARY_PARAMS,\n    )\n    def test_two_summary_scenarios_one_deleted(\n        self, set_current_opportunities_task, db_session, summary_info, expected_opportunity_status\n    ):\n        # This is identical to the test_single_summary_scenario test above\n        # but we always add a summary of the opposite is_forecasted value\n        # with identical date values, however it is always marked as deleted and won't be used\n        container = (\n            OpportunityContainer()\n            .with_summary(\n                is_forecast=summary_info.is_forecast,\n                post_date=summary_info.post_date,\n                close_date=summary_info.close_date,\n                archive_date=summary_info.archive_date,\n                is_expected_current=True if expected_opportunity_status is not None else False,\n            )\n            .with_summary(\n                is_forecast=not summary_info.is_forecast,\n                post_date=summary_info.post_date,\n                # technically forecasts won't have a close date, but it won't\n                # get to that check in logic anyways, so doesn't matter here\n                close_date=summary_info.close_date,\n                archive_date=summary_info.archive_date,\n                is_deleted=True,\n            )\n        )\n\n        set_current_opportunities_task._process_opportunity(container.opportunity)\n        validate_current_opportunity(db_session, container, expected_opportunity_status)\n\n    @pytest.mark.parametrize(\n        \"expected_summary_info,other_summary_info,expected_opportunity_status\",\n        [\n            ### Each of these scenarios includes one non-forecast, and one forecast summary\n            ### As long as the non-forecast can be used (eg. after post date), it will always\n            ### be chosen over the forecast.\n            # Both null post dates\n            (NON_FORECAST_NONE_POST_DATE_1, FORECAST_NONE_POST_DATE_1, None),\n            (NON_FORECAST_NONE_POST_DATE_3, FORECAST_NONE_POST_DATE_2, None),\n            # Both before post date\n            (NON_FORECAST_BEFORE_POST_DATE_1, FORECAST_BEFORE_POST_DATE_1, None),\n            (NON_FORECAST_BEFORE_POST_DATE_4, FORECAST_BEFORE_POST_DATE_2, None),\n            (NON_FORECAST_BEFORE_POST_DATE_2, FORECAST_BEFORE_POST_DATE_1, None),\n            # Forecast on/after post date, non-forecast before post date\n            (\n                FORECAST_ON_POST_DATE_1,\n                NON_FORECAST_BEFORE_POST_DATE_1,\n                OpportunityStatus.FORECASTED,\n            ),\n            (\n                FORECAST_AFTER_POST_DATE_1,\n                NON_FORECAST_BEFORE_POST_DATE_2,\n                OpportunityStatus.FORECASTED,\n            ),\n            (\n                FORECAST_ON_ARCHIVE_DATE_1,\n                NON_FORECAST_BEFORE_POST_DATE_3,\n                OpportunityStatus.FORECASTED,\n            ),\n            (\n                FORECAST_ON_ARCHIVE_DATE_2,\n                NON_FORECAST_BEFORE_POST_DATE_4,\n                OpportunityStatus.FORECASTED,\n            ),\n            # Forecast after archive date, non-forecast before post date\n            (\n                FORECAST_AFTER_ARCHIVE_DATE_1,\n                NON_FORECAST_BEFORE_POST_DATE_3,\n                OpportunityStatus.ARCHIVED,\n            ),\n            (\n                FORECAST_AFTER_ARCHIVE_DATE_2,\n                NON_FORECAST_BEFORE_POST_DATE_2,\n                OpportunityStatus.ARCHIVED,\n            ),\n            # Forecast any date, non-forecast before post date\n            (NON_FORECAST_ON_POST_DATE_1, FORECAST_AFTER_POST_DATE_3, OpportunityStatus.POSTED),\n            (NON_FORECAST_AFTER_POST_DATE_1, FORECAST_ON_POST_DATE_2, OpportunityStatus.POSTED),\n            (NON_FORECAST_AFTER_POST_DATE_2, FORECAST_ON_ARCHIVE_DATE_1, OpportunityStatus.POSTED),\n            (NON_FORECAST_ON_CLOSE_DATE_1, FORECAST_AFTER_POST_DATE_2, OpportunityStatus.POSTED),\n            (NON_FORECAST_ON_CLOSE_DATE_2, FORECAST_ON_POST_DATE_3, OpportunityStatus.POSTED),\n            (\n                NON_FORECAST_AFTER_POST_DATE_2,\n                FORECAST_AFTER_ARCHIVE_DATE_1,\n                OpportunityStatus.POSTED,\n            ),\n            (NON_FORECAST_ON_CLOSE_DATE_2, FORECAST_AFTER_ARCHIVE_DATE_2, OpportunityStatus.POSTED),\n            # Forecast any date, non-forecast after close date\n            (NON_FORECAST_AFTER_CLOSE_DATE_1, FORECAST_ON_POST_DATE_1, OpportunityStatus.CLOSED),\n            (NON_FORECAST_AFTER_CLOSE_DATE_1, FORECAST_ON_ARCHIVE_DATE_1, OpportunityStatus.CLOSED),\n            (\n                NON_FORECAST_AFTER_CLOSE_DATE_3,\n                FORECAST_AFTER_ARCHIVE_DATE_2,\n                OpportunityStatus.CLOSED,\n            ),\n            (NON_FORECAST_AFTER_CLOSE_DATE_4, FORECAST_AFTER_POST_DATE_3, OpportunityStatus.CLOSED),\n            # Forecast any date, non-forecast after archive date\n            (\n                NON_FORECAST_AFTER_ARCHIVE_DATE_1,\n                FORECAST_AFTER_POST_DATE_2,\n                OpportunityStatus.ARCHIVED,\n            ),\n            (\n                NON_FORECAST_AFTER_ARCHIVE_DATE_2,\n                FORECAST_ON_POST_DATE_3,\n                OpportunityStatus.ARCHIVED,\n            ),\n            (\n                NON_FORECAST_AFTER_ARCHIVE_DATE_1,\n                FORECAST_ON_ARCHIVE_DATE_1,\n                OpportunityStatus.ARCHIVED,\n            ),\n            (\n                NON_FORECAST_AFTER_ARCHIVE_DATE_2,\n                FORECAST_AFTER_ARCHIVE_DATE_1,\n                OpportunityStatus.ARCHIVED,\n            ),\n        ],\n    )\n    def test_two_scenarios_one_forecast_one_non(\n        self,\n        set_current_opportunities_task,\n        db_session,\n        expected_summary_info,\n        other_summary_info,\n        expected_opportunity_status,\n    ):\n        # This tests various scenarios where an opportunity has a forecast and non-forecasted\n        # summary at various different dates.\n        container = (\n            OpportunityContainer()\n            .with_summary(\n                is_forecast=expected_summary_info.is_forecast,\n                post_date=expected_summary_info.post_date,\n                close_date=expected_summary_info.close_date,\n                archive_date=expected_summary_info.archive_date,\n                is_expected_current=True if expected_opportunity_status is not None else False,\n            )\n            .with_summary(\n                is_forecast=other_summary_info.is_forecast,\n                post_date=other_summary_info.post_date,\n                close_date=other_summary_info.close_date,\n                archive_date=other_summary_info.archive_date,\n                revision_number=5,\n            )\n            .with_summary(\n                # Add another record of the same type as the expected, but an older revision\n                # so it won't ever be picked\n                revision_number=1,\n                is_forecast=expected_summary_info.is_forecast,\n                # but the fields within in always would lead to it being marked archived\n                post_date=LAST_YEAR,\n                close_date=LAST_MONTH,\n                archive_date=YESTERDAY,\n            )\n            .with_summary(\n                # Also add a record of the same type as the one we don't plan to pick\n                # that would always be posted/forecasted if it were the most recent\n                revision_number=2,\n                post_date=YESTERDAY,\n                archive_date=NEXT_YEAR,\n            )\n        )\n\n        set_current_opportunities_task._process_opportunity(container.opportunity)\n        validate_current_opportunity(db_session, container, expected_opportunity_status)\n\n\nclass TestSetCurrentOpportunitiesTaskRun(BaseTestClass):\n    @pytest.fixture(scope=\"class\", autouse=True)\n    def shared_setup(self, truncate_opportunities, enable_factory_create):\n        # Autouse fixture that exists just to call the above two fixtures so we don't\n        # need to include it on every test described below.\n\n        # Note that the truncate only occurs once before the tests run, not between each run\n        pass\n\n    @pytest.fixture\n    def set_current_opportunities_task(self, db_session):\n        return SetCurrentOpportunitiesTask(db_session, CURRENT_DATE)\n\n    def test_run(self, db_session, set_current_opportunities_task):\n        # Most of what we wanted to test was done in above tests\n        # this just aims to test a few things on the class itself\n\n        ### Setup a few scenarios\n        # Basic posted scenario that needs to be added\n        container1 = OpportunityContainer().with_summary(\n            is_forecast=NON_FORECAST_AFTER_POST_DATE_2.is_forecast,\n            post_date=NON_FORECAST_AFTER_POST_DATE_2.post_date,\n            close_date=NON_FORECAST_AFTER_POST_DATE_2.close_date,\n            archive_date=NON_FORECAST_AFTER_POST_DATE_2.archive_date,\n            is_expected_current=True,\n        )\n\n        # Basic scenario where the existing summary doesn't need to be changed\n        # but the opportunity status does (factory defaults to posted)\n        container2 = OpportunityContainer().with_summary(\n            is_forecast=FORECAST_AFTER_ARCHIVE_DATE_1.is_forecast,\n            post_date=FORECAST_AFTER_ARCHIVE_DATE_1.post_date,\n            close_date=FORECAST_AFTER_ARCHIVE_DATE_1.close_date,\n            archive_date=FORECAST_AFTER_ARCHIVE_DATE_1.archive_date,\n            is_expected_current=True,\n            is_already_current=True,\n        )\n\n        # a scenario where it has no summaries\n        container3 = OpportunityContainer()\n\n        # A scenario where the existing current summary should be removed entirely\n        # because it is deleted\n        container4 = OpportunityContainer().with_summary(\n            is_forecast=FORECAST_AFTER_POST_DATE_2.is_forecast,\n            post_date=FORECAST_AFTER_POST_DATE_2.post_date,\n            close_date=FORECAST_AFTER_POST_DATE_2.close_date,\n            archive_date=FORECAST_AFTER_POST_DATE_2.archive_date,\n            is_already_current=True,\n            is_deleted=True,\n        )\n\n        # A scenario where the existing current summary should be switched to the other one\n        container5 = (\n            OpportunityContainer()\n            .with_summary(\n                is_forecast=FORECAST_AFTER_ARCHIVE_DATE_1.is_forecast,\n                post_date=FORECAST_AFTER_ARCHIVE_DATE_1.post_date,\n                close_date=FORECAST_AFTER_ARCHIVE_DATE_1.close_date,\n                archive_date=FORECAST_AFTER_ARCHIVE_DATE_1.archive_date,\n                is_already_current=True,\n            )\n            .with_summary(\n                is_forecast=NON_FORECAST_ON_POST_DATE_1.is_forecast,\n                post_date=NON_FORECAST_ON_POST_DATE_1.post_date,\n                close_date=NON_FORECAST_ON_POST_DATE_1.close_date,\n                archive_date=NON_FORECAST_ON_POST_DATE_1.archive_date,\n                is_expected_current=True,\n            )\n        )\n\n        # A scenario where the opportunity summary is valid, but the opportunity is a draft\n        container6 = OpportunityContainer(is_draft=True).with_summary(\n            is_forecast=NON_FORECAST_AFTER_POST_DATE_2.is_forecast,\n            post_date=NON_FORECAST_AFTER_POST_DATE_2.post_date,\n            close_date=NON_FORECAST_AFTER_POST_DATE_2.close_date,\n            archive_date=NON_FORECAST_AFTER_POST_DATE_2.archive_date,\n            is_expected_current=False,\n        )\n\n        set_current_opportunities_task.run()\n\n        validate_current_opportunity(db_session, container1, OpportunityStatus.POSTED)\n        validate_current_opportunity(db_session, container2, OpportunityStatus.ARCHIVED)\n        validate_current_opportunity(db_session, container3, None)\n        validate_current_opportunity(db_session, container4, None)\n        validate_current_opportunity(db_session, container5, OpportunityStatus.POSTED)\n        validate_current_opportunity(db_session, container6, None)\n\n        # Check a few basic metrics that should be set\n        metrics = set_current_opportunities_task.metrics\n\n        assert metrics[set_current_opportunities_task.Metrics.OPPORTUNITY_COUNT] == 6\n        assert metrics[set_current_opportunities_task.Metrics.UNMODIFIED_OPPORTUNITY_COUNT] == 2\n        assert metrics[set_current_opportunities_task.Metrics.MODIFIED_OPPORTUNITY_COUNT] == 4\n\n\ndef test_via_cli(cli_runner, db_session, enable_factory_create):\n    # Simple test that just verifies that we can invoke the script via the CLI\n    # note that the script will always use todays date as the current date, so we\n    # need to generate the scenario from that instead\n\n    today = get_now_us_eastern_date()\n\n    # A basic posted scenario\n    container1 = OpportunityContainer().with_summary(\n        is_forecast=False,\n        post_date=today - timedelta(days=10),\n        close_date=today + timedelta(days=30),\n        archive_date=today + timedelta(days=60),\n        is_expected_current=True,\n    )\n\n    # a basic forecasted scenario with several past revisions\n    container2 = (\n        OpportunityContainer()\n        .with_summary(\n            is_forecast=True,\n            post_date=today - timedelta(days=5),\n            archive_date=today + timedelta(days=60),\n            is_already_current=True,\n            revision_number=2,\n        )\n        .with_summary(\n            is_forecast=True,\n            post_date=today - timedelta(days=5),\n            archive_date=today + timedelta(days=60),\n            revision_number=1,\n        )\n        .with_summary(\n            is_forecast=True,\n            post_date=today - timedelta(days=5),\n            archive_date=today + timedelta(days=120),\n            is_expected_current=True,\n        )\n    )\n\n    cli_runner.invoke(args=[\"task\", \"set-current-opportunities\"])\n\n    validate_current_opportunity(db_session, container1, OpportunityStatus.POSTED)\n    validate_current_opportunity(db_session, container2, OpportunityStatus.FORECASTED)"}
{"path":"infra/analytics/database/variables.tf","language":"unknown","type":"code","directory":"infra/analytics/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/database/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"File: api/tests/src/task/test_ecs_background_task.py\nLanguage: py\nType: code\nDirectory: api/tests/src/task\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/test_ecs_background_task.py\nSize: 2.61 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/analytics/metabase/.terraform.lock.hcl","language":"unknown","type":"code","directory":"infra/analytics/metabase","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/metabase/.terraform.lock.hcl","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"import pytest\n\nfrom src.logging.flask_logger import add_extra_data_to_global_logs\nfrom src.task.ecs_background_task import ecs_background_task\n\n\ndef test_ecs_background_task(app, caplog, monkeypatch_session):\n    monkeypatch_session.setenv(\n        \"LOG_LEVEL_OVERRIDES\",\n        \"newrelic.core.agent=ERROR,newrelic.core.agent_protocol=ERROR,src.adapters.newrelic=ERROR\",\n    )\n\n    # We pull in the app so its initialized\n    # Global logging params like the task name are stored on the app\n    caplog.set_level(logging.INFO)\n\n    @ecs_background_task(task_name=\"my_test_task_name\")\n    def my_test_func(param1, param2):\n        # Add a brief sleep so that we can test the duration logic\n        time.sleep(0.2)  # 0.2s\n        add_extra_data_to_global_logs({\"example_param\": 12345})\n\n        return param1 + param2\n\n    # Verify the function works uneventfully\n    assert my_test_func(1, 2) == 3\n\n    # Filter out newrelic-related logs\n    relevant_records = [\n        record for record in caplog.records if \"newrelic\" not in record.name.lower()\n    ]\n    for record in relevant_records:\n        extra = record.__dict__\n        assert extra[\"task_name\"] == \"my_test_task_name\"\n\n    last_record = relevant_records[-1].__dict__\n    # Make sure the ECS task duration was tracked\n    allowed_error = 0.1\n    assert last_record[\"ecs_task_duration_sec\"] == pytest.approx(0.2, abs=allowed_error)\n    # Make sure the extra we added was put in this automatically\n    assert last_record[\"example_param\"] == 12345\n    assert last_record[\"message\"] == \"Completed ECS task my_test_task_name\"\n\n\ndef test_ecs_background_task_when_erroring(app, caplog, monkeypatch_session):\n    monkeypatch_session.setenv(\n        \"LOG_LEVEL_OVERRIDES\",\n        \"newrelic.core.agent=ERROR,newrelic.core.agent_protocol=ERROR,src.adapters.newrelic=ERROR\",\n    )\n\n    caplog.set_level(logging.INFO)\n\n    @ecs_background_task(task_name=\"my_error_test_task_name\")\n    def my_test_error_func():\n        add_extra_data_to_global_logs({\"another_param\": \"hello\"})\n\n        raise ValueError(\"I am an error\")\n\n    with pytest.raises(ValueError, match=\"I am an error\"):\n        my_test_error_func()\n\n    # Filter out newrelic-related logs\n    relevant_records = [\n        record for record in caplog.records if \"newrelic\" not in record.name.lower()\n    ]\n    for record in relevant_records:\n        extra = record.__dict__\n        assert extra[\"task_name\"] == \"my_error_test_task_name\"\n\n    last_record = relevant_records[-1].__dict__\n\n    assert last_record[\"another_param\"] == \"hello\"\n    assert last_record[\"levelname\"] == \"ERROR\"\n    assert last_record[\"message\"] == \"ECS task failed\""}
{"path":"infra/analytics/metabase/dev.s3.tfbackend","language":"unknown","type":"code","directory":"infra/analytics/metabase","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/metabase/dev.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"File: api/tests/src/task/test_task.py\nLanguage: py\nType: code\nDirectory: api/tests/src/task\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/task/test_task.py\nSize: 3.89 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/analytics/metabase/image_tag.tf","language":"unknown","type":"code","directory":"infra/analytics/metabase","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/metabase/image_tag.tf","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"from src.db.models.extract_models import ExtractMetadata\nfrom src.db.models.task_models import JobLog, JobStatus\nfrom src.task.task import Task\nfrom tests.src.db.models.factories import ExtractMetadataFactory, ExtractType\n\n\nclass DuplicateKeyTask(Task):\n    \"\"\"Test implementation that triggers a duplicate key error\"\"\"\n\n    def run_task(self) -> None:\n        # Create first record\n        extract1 = ExtractMetadataFactory.create(\n            file_path=\"s3://test-bucket/file1.csv\", extract_type=ExtractType.OPPORTUNITIES_CSV\n        )\n\n        # Attempt to create second record with same primary key\n        extract2 = ExtractMetadataFactory.build(\n            file_path=\"s3://test-bucket/file2.csv\", extract_type=ExtractType.OPPORTUNITIES_CSV\n        )\n        extract2.extract_metadata_id = extract1.extract_metadata_id  # Force duplicate PK\n        self.db_session.add(extract2)\n        self.db_session.flush()  # This will trigger the duplicate key error\n\n\nclass SimpleTask(Task):\n    \"\"\"Test implementation of Task\"\"\"\n\n    def run_task(self) -> None:\n        pass\n\n\nclass FailingTask(Task):\n    \"\"\"Test implementation that fails during run_task\"\"\"\n\n    def run_task(self) -> None:\n        raise ValueError(\"Task failed\")\n\n\nclass DBFailingTask(Task):\n    \"\"\"Test implementation that fails during DB operation\"\"\"\n\n    def run_task(self) -> None:\n        # Simulate DB operation failing\n        raise InvalidRequestError(\"DB Error\", None, None)\n\n\ndef test_task_handles_general_error(db_session):\n    \"\"\"Test that task properly handles non-DB errors and rolls back session\"\"\"\n    task = FailingTask(db_session)\n\n    with pytest.raises(ValueError):\n        task.run()\n\n    # Verify job was created and updated to failed status\n    assert task.job is not None\n    assert task.job.job_status == JobStatus.FAILED\n\n    # Verify session is still usable\n    db_session.begin()  # Start a new transaction\n    assert db_session.is_active  # Session should be active with new transaction\n\n\ndef test_task_handles_db_error(db_session):\n    \"\"\"Test that task properly handles DB errors\"\"\"\n    task = DBFailingTask(db_session)\n\n    with pytest.raises(InvalidRequestError):\n        task.run()\n\n    # Verify session was rolled back and is usable\n    db_session.begin()  # Start a new transaction\n    assert db_session.is_active  # Session should be active with new transaction\n\n\ndef test_successful_task_completion(db_session):\n    \"\"\"Test that task completes successfully and updates job status\"\"\"\n    task = SimpleTask(db_session)\n    task.run()\n\n    assert task.job is not None\n    assert task.job.job_status == JobStatus.COMPLETED\n    assert \"task_duration_sec\" in task.metrics\n\n    # Verify session is still usable by starting a new transaction\n    db_session.begin()  # Start a new transaction\n    assert db_session.is_active  # Session should be active with new transaction\n\n\ndef test_task_handles_duplicate_key_error(db_session, enable_factory_create):\n    \"\"\"Test that task properly handles SQLAlchemy errors e.g. integrity errors\"\"\"\n    # Clear any existing ExtractMetadata records\n    db_session.query(ExtractMetadata).delete()\n    db_session.commit()\n\n    task = DuplicateKeyTask(db_session)\n\n    with pytest.raises(IntegrityError):\n        task.run()\n\n    # Verify job was created and updated to failed status\n    assert task.job is not None\n    assert task.job.job_status == JobStatus.FAILED\n\n    # Verify session was rolled back and is usable\n    db_session.begin()  # Start a new transaction\n    assert db_session.is_active  # Session should be active with new transaction\n\n    # Verify only one record exists\n    count = db_session.query(ExtractMetadata).count()\n    assert count == 1\n\n    # Verify the job status is persisted in the database\n    db_job = db_session.query(JobLog).filter_by(job_id=task.job.job_id).first()\n    assert db_job is not None\n    assert db_job.job_status == JobStatus.FAILED"}
{"path":"infra/analytics/metabase/main.tf","language":"unknown","type":"code","directory":"infra/analytics/metabase","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/metabase/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"File: api/tests/src/util/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/analytics/metabase/prod.s3.tfbackend","language":"unknown","type":"code","directory":"infra/analytics/metabase","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/metabase/prod.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":""}
{"path":"infra/analytics/metabase/secrets.tf","language":"unknown","type":"code","directory":"infra/analytics/metabase","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/metabase/secrets.tf","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"File: api/tests/src/util/parametrize_utils.py\nLanguage: py\nType: code\nDirectory: api/tests/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/parametrize_utils.py\nSize: 0.65 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/analytics/metabase/staging.s3.tfbackend","language":"unknown","type":"code","directory":"infra/analytics/metabase","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/metabase/staging.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"from itertools import chain, combinations\n\n\ndef powerset(iterable):\n    \"\"\"\n    Return the power set of an iterable, where each subset is a list.\n    The power set is the set of subsets of a set, including the empty\n    set and the set itself.\n\n    An example use case is if there are a set of roles that a user can have,\n    and you want to parametrize a test with all possible combinations of roles.\n\n    Example:\n    powerset([1,2,3]) --> [] [1] [2] [3] [1,2] [1,3] [2,3] [1,2,3]\n    \"\"\"\n    s = list(iterable)\n    return map(list, chain.from_iterable(combinations(s, r) for r in range(len(s) + 1)))"}
{"path":"infra/analytics/metabase/variables.tf","language":"unknown","type":"code","directory":"infra/analytics/metabase","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/metabase/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"File: api/tests/src/util/test_datetime_util.py\nLanguage: py\nType: code\nDirectory: api/tests/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/test_datetime_util.py\nSize: 2.00 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/analytics/service/.terraform.lock.hcl","language":"unknown","type":"code","directory":"infra/analytics/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/service/.terraform.lock.hcl","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"import pytest\nimport pytz\n\nfrom src.util.datetime_util import adjust_timezone\n\n\n@pytest.mark.parametrize(\n    \"timezone_name, expected_output\",\n    [\n        (\"UTC\", \"2022-01-01T12:00:00+00:00\"),\n        (\"US/Eastern\", \"2022-01-01T07:00:00-05:00\"),\n        (\"US/Central\", \"2022-01-01T06:00:00-06:00\"),\n        (\"US/Mountain\", \"2022-01-01T05:00:00-07:00\"),\n        (\"US/Pacific\", \"2022-01-01T04:00:00-08:00\"),\n        (\"Asia/Tokyo\", \"2022-01-01T21:00:00+09:00\"),\n    ],\n)\ndef test_adjust_timezone_from_utc(timezone_name, expected_output):\n    # Jan 1st 2022 at 12:00pm is the input\n    input_datetime = datetime(2022, 1, 1, 12, 0, 0, tzinfo=timezone.utc)\n\n    # Note that we use the isoformat for validation as a timezone shifted\n    # timezone matches the unshifted one (eg. 12pm UTC and 7am Eastern match)\n    # so comparing the timezone objects themselves is more complicated than\n    # looking at their string representation.\n\n    # Passing in UTC doesn't change the time\n    assert adjust_timezone(input_datetime, timezone_name).isoformat() == expected_output\n\n\n@pytest.mark.parametrize(\n    \"timezone_name, expected_output\",\n    [\n        (\"UTC\", \"2022-06-01T05:00:00+00:00\"),\n        (\"US/Eastern\", \"2022-06-01T01:00:00-04:00\"),\n        (\"US/Central\", \"2022-06-01T00:00:00-05:00\"),\n        (\"US/Mountain\", \"2022-05-31T23:00:00-06:00\"),\n        (\"US/Pacific\", \"2022-05-31T22:00:00-07:00\"),\n        (\"Asia/Tokyo\", \"2022-06-01T14:00:00+09:00\"),\n    ],\n)\ndef test_adjust_timezone_from_non_utc(timezone_name, expected_output):\n    # June 1st 2022 at 01:00am in the Eastern timezone is the input\n    input_datetime = pytz.timezone(\"America/New_York\").localize(datetime(2022, 6, 1, 1, 0, 0))\n\n    # Note that because daylights savings has switched from the above\n    # test, the differences in the timezones is offset by an hour\n    # in a few places that don't observe DST (the US timezones are all 1 hour closer to UTC)\n\n    assert adjust_timezone(input_datetime, timezone_name).isoformat() == expected_output"}
{"path":"infra/analytics/service/api_analytics_bucket.tf","language":"unknown","type":"code","directory":"infra/analytics/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/service/api_analytics_bucket.tf","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"File: api/tests/src/util/test_deploy_metadata.py\nLanguage: py\nType: code\nDirectory: api/tests/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/test_deploy_metadata.py\nSize: 1.90 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/analytics/service/database.tf","language":"unknown","type":"code","directory":"infra/analytics/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/service/database.tf","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"from src.util.deploy_metadata import DeployMetadataConfig\n\n\ndef test_deploy_metadata_with_release_ref(monkeypatch):\n    ref = \"2024.11.27-1\"\n    sha = \"44b954e85c4ca7e3714f9f988a919fae40ec3c98\"\n\n    monkeypatch.setenv(\"DEPLOY_GITHUB_REF\", ref)\n    monkeypatch.setenv(\"DEPLOY_GITHUB_SHA\", sha)\n    monkeypatch.setenv(\"DEPLOY_TIMESTAMP\", \"2024-12-02T21:25:18Z\")\n\n    config = DeployMetadataConfig()\n\n    # Verify the calculated values are there\n    assert config.release_notes == f\"https://github.com/HHS/simpler-grants-gov/releases/tag/{ref}\"\n    assert config.deploy_commit == f\"https://github.com/HHS/simpler-grants-gov/commit/{sha}\"\n    assert config.deploy_datetime_est.isoformat() == \"2024-12-02T16:25:18-05:00\"\n\n\ndef test_deploy_metadata_with_non_release_ref(monkeypatch):\n    sha = \"44b954e85c4ca7e3714f9f988a919fae40ec3c98\"\n\n    monkeypatch.setenv(\"DEPLOY_GITHUB_REF\", \"main\")\n    monkeypatch.setenv(\"DEPLOY_GITHUB_SHA\", sha)\n    monkeypatch.setenv(\"DEPLOY_TIMESTAMP\", \"2024-06-01T03:13:11Z\")\n\n    config = DeployMetadataConfig()\n\n    # Verify the calculated values are there\n    assert config.release_notes == \"https://github.com/HHS/simpler-grants-gov/releases\"\n    assert config.deploy_commit == f\"https://github.com/HHS/simpler-grants-gov/commit/{sha}\"\n    assert config.deploy_datetime_est.isoformat() == \"2024-05-31T23:13:11-04:00\"\n\n\n@freeze_time(\"2024-11-14 12:00:00\", tz_offset=0)\ndef test_deploy_metadata_all_none(monkeypatch):\n    monkeypatch.delenv(\"DEPLOY_GITHUB_REF\")\n    monkeypatch.delenv(\"DEPLOY_GITHUB_SHA\")\n    monkeypatch.delenv(\"DEPLOY_TIMESTAMP\")\n\n    config = DeployMetadataConfig()\n\n    # Verify the calculated values are there\n    assert config.release_notes == \"https://github.com/HHS/simpler-grants-gov/releases\"\n    assert config.deploy_commit == \"https://github.com/HHS/simpler-grants-gov\"\n    assert config.deploy_datetime_est.isoformat() == \"2024-11-14T07:00:00-05:00\""}
{"path":"infra/analytics/service/dev.s3.tfbackend","language":"unknown","type":"code","directory":"infra/analytics/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/service/dev.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"File: api/tests/src/util/test_dict_util.py\nLanguage: py\nType: code\nDirectory: api/tests/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/test_dict_util.py\nSize: 1.58 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/analytics/service/image_tag.tf","language":"unknown","type":"code","directory":"infra/analytics/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/service/image_tag.tf","size":0,"lastModified":"2025-02-14T17:08:31.140Z","content":"from src.util.dict_util import flatten_dict\n\n\n@pytest.mark.parametrize(\n    \"data,expected_output\",\n    [\n        # Scenario 1 - routine case\n        (\n            {\"a\": {\"b\": {\"c\": \"value_c\", \"f\": 5}, \"d\": \"value_d\"}, \"e\": \"value_e\"},\n            {\"a.b.c\": \"value_c\", \"a.b.f\": 5, \"a.d\": \"value_d\", \"e\": \"value_e\"},\n        ),\n        # Scenario 2 - empty\n        ({}, {}),\n        # Scenario 3 - no nesting\n        (\n            {\n                \"a\": \"1\",\n                \"b\": 2,\n                \"c\": True,\n            },\n            {\n                \"a\": \"1\",\n                \"b\": 2,\n                \"c\": True,\n            },\n        ),\n        # Scenario 4 - very nested\n        (\n            {\n                \"a\": {\n                    \"b\": {\n                        \"c\": {\n                            \"d\": {\n                                \"e\": {\n                                    \"f\": {\"g\": {\"h1\": \"h1_value\", \"h2\": [\"h2_value1\", \"h2_value2\"]}}\n                                }\n                            }\n                        }\n                    }\n                }\n            },\n            {\"a.b.c.d.e.f.g.h1\": \"h1_value\", \"a.b.c.d.e.f.g.h2\": [\"h2_value1\", \"h2_value2\"]},\n        ),\n        # Scenario 5 - dictionaries inside non-dictionaries aren't flattened\n        ({\"a\": {\"b\": [{\"list_dict_a\": \"a\"}]}}, {\"a.b\": [{\"list_dict_a\": \"a\"}]}),\n        # Scenario 6 - integer keys should be allowed too\n        ({\"a\": {0: {\"b\": \"b_value\"}, 1: \"c\"}}, {\"a.0.b\": \"b_value\", \"a.1\": \"c\"}),\n    ],\n)\ndef test_flatten_dict(data, expected_output):\n    assert flatten_dict(data) == expected_output"}
{"path":"infra/analytics/service/main.tf","language":"unknown","type":"code","directory":"infra/analytics/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/service/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: api/tests/src/util/test_file_util.py\nLanguage: py\nType: code\nDirectory: api/tests/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/test_file_util.py\nSize: 7.74 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/analytics/service/outputs.tf","language":"unknown","type":"code","directory":"infra/analytics/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/service/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"import boto3\nimport pytest\nfrom smart_open import open as smart_open\n\nimport src.util.file_util as file_util\nimport tests.src.db.models.factories as f\n\n\ndef create_file(root_path, file_path):\n    full_path = os.path.join(root_path, file_path)\n\n    if not file_util.is_s3_path(str(full_path)):\n        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n\n    with smart_open(full_path, mode=\"w\") as outfile:\n        outfile.write(\"hello\")\n\n    return full_path\n\n\n@pytest.mark.parametrize(\n    \"path,is_s3\",\n    [\n        (\"s3://bucket/folder/test.txt\", True),\n        (\"./relative/folder/test.txt\", False),\n        (\"http://example.com/test.txt\", False),\n    ],\n)\ndef test_is_s3_path(path, is_s3):\n    assert file_util.is_s3_path(path) is is_s3\n\n\n@pytest.mark.parametrize(\n    \"path,bucket,prefix\",\n    [\n        (\"s3://my_bucket/my_key\", \"my_bucket\", \"my_key\"),\n        (\"s3://my_bucket/path/to/directory/\", \"my_bucket\", \"path/to/directory/\"),\n        (\"s3://my_bucket/path/to/file.txt\", \"my_bucket\", \"path/to/file.txt\"),\n    ],\n)\ndef test_split_s3_url(path, bucket, prefix):\n    assert file_util.split_s3_url(path) == (bucket, prefix)\n\n\n@pytest.mark.parametrize(\n    \"path,bucket\",\n    [\n        (\"s3://bucket/folder/test.txt\", \"bucket\"),\n        (\"s3://bucket_x/folder\", \"bucket_x\"),\n        (\"s3://bucket-y/folder/\", \"bucket-y\"),\n        (\"s3://bucketz\", \"bucketz\"),\n    ],\n)\ndef test_get_s3_bucket(path, bucket):\n    assert file_util.get_s3_bucket(path) == bucket\n\n\n@pytest.mark.parametrize(\n    \"path,file_key\",\n    [\n        (\"s3://bucket/folder/test.txt\", \"folder/test.txt\"),\n        (\"s3://bucket_x/file.csv\", \"file.csv\"),\n        (\"s3://bucket-y/folder/path/to/abc.zip\", \"folder/path/to/abc.zip\"),\n        (\"./folder/path\", \"./folder/path\"),\n        (\"sftp://folder/filename\", \"filename\"),\n    ],\n)\ndef test_get_s3_file_key(path, file_key):\n    assert file_util.get_s3_file_key(path) == file_key\n\n\n@pytest.mark.parametrize(\n    \"path,file_name\",\n    [\n        (\"s3://bucket/folder/test.txt\", \"test.txt\"),\n        (\"s3://bucket_x/file.csv\", \"file.csv\"),\n        (\"s3://bucket-y/folder/path/to/abc.zip\", \"abc.zip\"),\n        (\"./folder/path\", \"path\"),\n        (\"sftp://filename\", \"filename\"),\n    ],\n)\ndef test_get_s3_file_name(path, file_name):\n    assert file_util.get_file_name(path) == file_name\n\n\ndef test_get_file_length_bytes(tmp_path):\n    test_content = \"Hello, World!\"\n    test_file = tmp_path / \"test.txt\"\n    test_file.write_text(test_content)\n\n    size = file_util.get_file_length_bytes(str(test_file))\n\n    # Verify size matches content length\n    assert size == len(test_content)\n\n\ndef test_get_file_length_bytes_s3_with_content(mock_s3_bucket):\n    \"\"\"Test getting file size from S3 with actual content\"\"\"\n    # Create test content\n    test_content = b\"Test content!\"\n    test_file_path = f\"s3://{mock_s3_bucket}/test/file.txt\"\n\n    # Upload test content to mock S3\n    s3_client = boto3.client(\"s3\")\n    s3_client.put_object(Bucket=mock_s3_bucket, Key=\"test/file.txt\", Body=test_content)\n\n    # Get file size using our utility\n    size = file_util.get_file_length_bytes(test_file_path)\n\n    # Verify size matches content length\n    assert size == len(test_content)\n\n\ndef test_file_exists_local_filesystem(tmp_path):\n    file_path1 = tmp_path / \"test.txt\"\n    file_path2 = tmp_path / \"test2.txt\"\n    file_path3 = tmp_path / \"test3.txt\"\n\n    with file_util.open_stream(file_path1, \"w\") as outfile:\n        outfile.write(\"hello\")\n    with file_util.open_stream(file_path2, \"w\") as outfile:\n        outfile.write(\"hello\")\n    with file_util.open_stream(file_path3, \"w\") as outfile:\n        outfile.write(\"hello\")\n\n    assert file_util.file_exists(file_path1) is True\n    assert file_util.file_exists(file_path2) is True\n    assert file_util.file_exists(file_path3) is True\n    assert file_util.file_exists(tmp_path / \"test4.txt\") is False\n    assert file_util.file_exists(tmp_path / \"test5.txt\") is False\n\n\ndef test_file_exists_s3(mock_s3_bucket):\n    file_path1 = f\"s3://{mock_s3_bucket}/test.txt\"\n    file_path2 = f\"s3://{mock_s3_bucket}/test2.txt\"\n    file_path3 = f\"s3://{mock_s3_bucket}/test3.txt\"\n\n    with file_util.open_stream(file_path1, \"w\") as outfile:\n        outfile.write(\"hello\")\n    with file_util.open_stream(file_path2, \"w\") as outfile:\n        outfile.write(\"hello\")\n    with file_util.open_stream(file_path3, \"w\") as outfile:\n        outfile.write(\"hello\")\n\n    assert file_util.file_exists(file_path1) is True\n    assert file_util.file_exists(file_path2) is True\n    assert file_util.file_exists(file_path3) is True\n    assert file_util.file_exists(f\"s3://{mock_s3_bucket}/test4.txt\") is False\n    assert file_util.file_exists(f\"s3://{mock_s3_bucket}/test5.txt\") is False\n\n\ndef test_copy_file_s3(mock_s3_bucket, other_mock_s3_bucket):\n    file_path = f\"s3://{mock_s3_bucket}/my_file.txt\"\n\n    with file_util.open_stream(file_path, \"w\") as outfile:\n        outfile.write(f.fake.sentence(25))\n\n    other_file_path = f\"s3://{other_mock_s3_bucket}/my_new_file.txt\"\n    file_util.copy_file(file_path, other_file_path)\n\n    assert file_util.file_exists(file_path) is True\n    assert file_util.file_exists(other_file_path) is True\n\n    assert file_util.read_file(file_path) == file_util.read_file(other_file_path)\n\n\ndef test_copy_file_local_disk(tmp_path):\n    file_path = tmp_path / \"my_file.txt\"\n\n    with file_util.open_stream(file_path, \"w\") as outfile:\n        outfile.write(f.fake.sentence(25))\n\n    other_file_path = tmp_path / \"my_file2.txt\"\n    file_util.copy_file(file_path, other_file_path)\n\n    assert file_util.file_exists(file_path) is True\n    assert file_util.file_exists(other_file_path) is True\n\n    assert file_util.read_file(file_path) == file_util.read_file(other_file_path)\n\n\ndef test_move_file_s3(mock_s3_bucket, other_mock_s3_bucket):\n    file_path = f\"s3://{mock_s3_bucket}/my_file_to_copy.txt\"\n\n    contents = f.fake.sentence(25)\n    with file_util.open_stream(file_path, \"w\") as outfile:\n        outfile.write(contents)\n\n    other_file_path = f\"s3://{other_mock_s3_bucket}/my_destination_file.txt\"\n    file_util.move_file(file_path, other_file_path)\n\n    assert file_util.file_exists(file_path) is False\n    assert file_util.file_exists(other_file_path) is True\n\n    assert file_util.read_file(other_file_path) == contents\n\n\ndef test_move_file_local_disk(tmp_path):\n    file_path = tmp_path / \"my_file_to_move.txt\"\n\n    contents = f.fake.sentence(25)\n    with file_util.open_stream(file_path, \"w\") as outfile:\n        outfile.write(contents)\n\n    other_file_path = tmp_path / \"my_moved_file.txt\"\n    file_util.move_file(file_path, other_file_path)\n\n    assert file_util.file_exists(file_path) is False\n    assert file_util.file_exists(other_file_path) is True\n\n    assert file_util.read_file(other_file_path) == contents\n\n\n@pytest.mark.parametrize(\n    \"s3_path,cdn_url,expected\",\n    [\n        (\n            \"s3://local-mock-public-bucket/path/to/file.pdf\",\n            \"https://cdn.example.com\",\n            \"https://cdn.example.com/path/to/file.pdf\",\n        ),\n        (\n            \"s3://local-mock-public-bucket/opportunities/9/attachments/79853231/manager.webm\",\n            \"https://cdn.example.com\",\n            \"https://cdn.example.com/opportunities/9/attachments/79853231/manager.webm\",\n        ),\n        # Test with subdirectory in CDN URL\n        (\n            \"s3://local-mock-public-bucket/file.txt\",\n            \"https://cdn.example.com/assets\",\n            \"https://cdn.example.com/assets/file.txt\",\n        ),\n    ],\n)\ndef test_convert_s3_to_cdn_url(s3_path, cdn_url, expected, s3_config):\n    assert file_util.convert_public_s3_to_cdn_url(s3_path, cdn_url, s3_config) == expected\n\n\ndef test_convert_s3_to_cdn_url_invalid_path(s3_config):\n    with pytest.raises(ValueError, match=\"Expected s3:// path\"):\n        file_util.convert_public_s3_to_cdn_url(\n            \"http://not-s3/file.txt\", \"cdn.example.com\", s3_config\n        )"}
{"path":"infra/analytics/service/prod.s3.tfbackend","language":"unknown","type":"code","directory":"infra/analytics/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/service/prod.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: api/tests/src/util/test_string_utils.py\nLanguage: py\nType: code\nDirectory: api/tests/src/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/src/util/test_string_utils.py\nSize: 0.76 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/analytics/service/secrets.tf","language":"unknown","type":"code","directory":"infra/analytics/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/service/secrets.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"from src.util.string_utils import is_valid_uuid, join_list\n\n\ndef test_join_list():\n    assert join_list(None) == \"\"\n    assert join_list(None, \",\") == \"\"\n    assert join_list(None, \"|\") == \"\"\n    assert join_list([]) == \"\"\n    assert join_list([], \",\") == \"\"\n    assert join_list([], \"|\") == \"\"\n\n    assert join_list([\"a\", \"b\", \"c\"]) == \"a\\nb\\nc\"\n    assert join_list([\"a\", \"b\", \"c\"], \",\") == \"a,b,c\"\n    assert join_list([\"a\", \"b\", \"c\"], \"|\") == \"a|b|c\"\n\n\n@pytest.mark.parametrize(\n    \"value,is_valid\",\n    [\n        (\"20f5484b-88ae-49b0-8af0-3a389b4917dd\", True),\n        (\"abc123\", False),\n        (\"1234\", False),\n        (\"xyz\", False),\n        (\"abc123\", False),\n    ],\n)\ndef test_is_valid_uuid(value, is_valid):\n    assert is_valid_uuid(value) is is_valid"}
{"path":"infra/analytics/service/staging.s3.tfbackend","language":"unknown","type":"code","directory":"infra/analytics/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/service/staging.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: api/tests/util/__init__.py\nLanguage: py\nType: code\nDirectory: api/tests/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/util/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/analytics/service/variables.tf","language":"unknown","type":"code","directory":"infra/analytics/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/analytics/service/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":""}
{"path":"infra/api/app-config/build_repository.tf","language":"unknown","type":"code","directory":"infra/api/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/build_repository.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: api/tests/util/convert_oracle_csvs_to_postgres.py\nLanguage: py\nType: code\nDirectory: api/tests/util\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tests/util/convert_oracle_csvs_to_postgres.py\nSize: 17.80 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/api/app-config/dev.tf","language":"unknown","type":"code","directory":"infra/api/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/dev.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"import csv\nimport logging\n\nimport click\n\nimport src.logging\n\nlogger = logging.getLogger(__name__)\n\n\nEXPECTED_SUFFIX = \"_DATA_TABLE.csv\"\n\nsummary_id = 0\n\n\ndef convert_legacy_category(value: str | None) -> int | None:\n    if value is None or value == \"\":\n        return None\n\n    match value:\n        case \"D\":\n            return 1\n        case \"M\":\n            return 2\n        case \"C\":\n            return 3\n        case \"E\":\n            return 4\n        case \"O\":\n            return 5\n\n    raise Exception(\"Unrecognized category %s\" % value)\n\n\ndef convert_legacy_funding_instrument_type(value: str | None) -> int | None:\n    if value is None or value == \"\":\n        return None\n\n    match value:\n        case \"CA\":  # cooperative_agreement\n            return 1\n        case \"G\":  # grant\n            return 2\n        case \"PC\":  # procurement_contract\n            return 3\n        case \"O\":  # other\n            return 4\n\n    raise Exception(\"Unrecognized funding instrument %s\" % value)\n\n\ndef convert_legacy_funding_category_type(value: str | None) -> int | None:\n    if value is None or value == \"\":\n        return None\n\n    match value:\n        case \"RA\":  # recovery_act\n            return 1\n        case \"AG\":  # agriculture\n            return 2\n        case \"AR\":  # arts\n            return 3\n        case \"BC\":  # business_and_commerce\n            return 4\n        case \"CD\":  # community_development\n            return 5\n        case \"CP\":  # consumer_protection\n            return 6\n        case \"DPR\":  # disaster_prevention_and_relief\n            return 7\n        case \"ED\":  # education\n            return 8\n        case \"ELT\":  # employment_labor_and_training\n            return 9\n        case \"EN\":  # energy\n            return 10\n        case \"ENV\":  # environment\n            return 11\n        case \"FN\":  # food_and_nutrition\n            return 12\n        case \"HL\":  # health\n            return 13\n        case \"HO\":  # housing\n            return 14\n        case \"HU\":  # humanities\n            return 15\n        case \"IIJ\":  # infrastructure_investment_and_jobs_act\n            return 16\n        case \"IS\":  # information_and_statistics\n            return 17\n        case \"ISS\":  # income_security_and_social_services\n            return 18\n        case \"LJL\":  # law_justice_and_legal_services\n            return 19\n        case \"NR\":  # natural_resources\n            return 20\n        case \"OZ\":  # opportunity_zone_benefits\n            return 21\n        case \"RD\":  # regional_development\n            return 22\n        case \"ST\":  # science_technology_and_other_research_and_development\n            return 23\n        case \"T\":  # transportation\n            return 24\n        case \"ACA\":  # affordable_care_act\n            return 25\n        case \"O\":  # other\n            return 26\n\n    raise Exception(\"Unrecognized funding category %s\" % value)\n\n\ndef convert_legacy_applicant_type(value: str | None) -> int | None:\n    if value is None or value == \"\":\n        return None\n\n    match value:\n        case \"00\":  # state_governments\n            return 1\n        case \"01\":  # county_governments\n            return 2\n        case \"02\":  # city_or_township_governments\n            return 3\n        case \"04\":  # special_district_governments\n            return 4\n        case \"05\":  # independent_school_districts\n            return 5\n        case \"06\":  # public_and_state_institutions_of_higher_education\n            return 6\n        case \"07\":  # federally_recognized_native_american_tribal_governments\n            return 8\n        case \"08\":  # public_and_indian_housing_authorities\n            return 10\n        case \"11\":  # other_native_american_tribal_organizations\n            return 9\n        case \"12\":  # nonprofits_non_higher_education_with_501c3\n            return 11\n        case \"13\":  # nonprofits_non_higher_education_without_501c3\n            return 12\n        case \"20\":  # private_institutions_of_higher_education\n            return 7\n        case \"21\":  # individuals\n            return 13\n        case \"22\":  # for_profit_organizations_other_than_small_businesses\n            return 14\n        case \"23\":  # small_businesses\n            return 15\n        case \"25\":  # other\n            return 16\n        case \"99\":  # unrestricted\n            return 17\n\n    raise Exception(\"Unrecognized applicant type: %s\" % value)\n\n\ndef convert_numeric(value: str | None) -> int | None:\n    if value is None or value == \"\":\n        return None\n\n    if value.isnumeric():\n        return int(value)\n\n    # Anything else is just \"none\" or \"not available\" which we'll treat as null\n    return None\n\n\ndef convert_yn_bool(value: str | None) -> bool | None:\n    if value is None or value == \"\":\n        return None\n\n    if value == \"Y\":\n        return True\n\n    if value == \"N\":\n        return False\n\n    raise Exception(\"Unexpected Y/N bool value: %s\" % value)\n\n\ndef get_csv_records(directory: str, table_name: str) -> list[dict[str, str]]:\n    records = []\n    with open(f\"{directory}/{table_name}{EXPECTED_SUFFIX}\") as infile:\n        logger.info(\"Processing %s\", infile.name)\n        reader = csv.DictReader(infile)\n\n        for record in reader:\n            records.append(record)\n\n    return records\n\n\ndef write_csv(directory: str, table_name: str, records: list[dict[str, str]]) -> None:\n    with open(f\"{directory}/{table_name}.csv\", \"w\") as outfile:\n        logger.info(\"Writing %s\", outfile.name)\n        writer = csv.DictWriter(outfile, fieldnames=records[0].keys(), quoting=csv.QUOTE_ALL)\n        writer.writeheader()\n        writer.writerows(records)\n\n\ndef transform_opportunity(record: dict[str, str]) -> dict[str, str]:\n    return {\n        \"opportunity_id\": record[\"OPPORTUNITY_ID\"],\n        \"opportunity_number\": record[\"OPPNUMBER\"],\n        \"opportunity_title\": record[\"OPPTITLE\"],\n        \"agency\": record[\"OWNINGAGENCY\"],\n        \"opportunity_category_id\": convert_legacy_category(record[\"OPPCATEGORY\"]),\n        \"category_explanation\": record[\"CATEGORY_EXPLANATION\"],\n        \"is_draft\": record[\"IS_DRAFT\"] != \"N\",\n        \"revision_number\": record[\"REVISION_NUMBER\"],\n        \"modified_comments\": record[\"MODIFIED_COMMENTS\"],\n        \"publisher_user_id\": record[\"PUBLISHERUID\"],\n        \"publisher_profile_id\": record[\"PUBLISHER_PROFILE_ID\"],\n    }\n\n\ndef transform_forecast(record: dict[str, str]) -> dict[str, str]:\n    global summary_id\n    summary_id += 1\n\n    return {\n        \"opportunity_summary_id\": summary_id,\n        \"opportunity_id\": record.get(\"OPPORTUNITY_ID\"),\n        \"summary_description\": record.get(\"FORECAST_DESC\"),\n        \"is_cost_sharing\": convert_yn_bool(record.get(\"COST_SHARING\")),\n        \"is_forecast\": True,\n        \"post_date\": record.get(\"POSTING_DATE\"),\n        \"close_date\": None,\n        \"close_date_description\": None,\n        \"archive_date\": record.get(\"ARCHIVE_DATE\"),\n        \"unarchive_date\": record.get(\"UNARCHIVE_DATE\"),\n        \"expected_number_of_awards\": convert_numeric(record.get(\"NUMBER_OF_AWARDS\")),\n        \"estimated_total_program_funding\": convert_numeric(record.get(\"EST_FUNDING\")),\n        \"award_floor\": convert_numeric(record.get(\"AWARD_FLOOR\")),\n        \"award_ceiling\": convert_numeric(record.get(\"AWARD_CEILING\")),\n        \"additional_info_url\": record.get(\"FD_LINK_URL\"),\n        \"additional_info_url_description\": record.get(\"FD_LINK_DESC\"),\n        \"forecasted_post_date\": record.get(\"EST_SYNOPSIS_POSTING_DATE\"),\n        \"forecasted_close_date\": record.get(\"EST_APPL_RESPONSE_DATE\"),\n        \"forecasted_close_date_description\": record.get(\"EST_APPL_RESPONSE_DATE_DESC\"),\n        \"forecasted_award_date\": record.get(\"EST_AWARD_DATE\"),\n        \"forecasted_project_start_date\": record.get(\"EST_PROJECT_START_DATE\"),\n        \"fiscal_year\": record.get(\"FISCAL_YEAR\"),\n        \"revision_number\": None,\n        \"modification_comments\": record.get(\"MODIFICATION_COMMENTS\"),\n        \"funding_category_description\": record.get(\"OTH_CAT_FA_DESC\"),\n        \"agency_code\": record.get(\"AGENCY_CODE\"),\n        \"agency_name\": record.get(\"AC_NAME\"),\n        \"agency_phone_number\": record.get(\"AC_PHONE_NUMBER\"),\n        \"agency_contact_description\": record.get(\"AGENCY_CONTACT_DESC\"),\n        \"agency_email_address\": record.get(\"AC_EMAIL_ADDR\"),\n        \"agency_email_address_description\": record.get(\"AC_EMAIL_DESC\"),\n        \"is_deleted\": False,\n        \"can_send_mail\": convert_yn_bool(record.get(\"SENDMAIL\")),\n        \"publisher_profile_id\": record.get(\"PUBLISHER_PROFILE_ID\"),\n        \"publisher_user_id\": record.get(\"PUBLISHERUID\"),\n        \"updated_by\": record.get(\"LAST_UPD_ID\"),\n        \"created_by\": record.get(\"CREATOR_ID\"),\n    }\n\n\ndef transform_synopsis(record: dict[str, str]) -> dict[str, str]:\n    global summary_id\n    summary_id += 1\n\n    return {\n        \"opportunity_summary_id\": summary_id,\n        \"opportunity_id\": record.get(\"OPPORTUNITY_ID\"),\n        \"summary_description\": record.get(\"SYN_DESC\"),\n        \"is_cost_sharing\": convert_yn_bool(record.get(\"COST_SHARING\")),\n        \"is_forecast\": False,\n        \"post_date\": record.get(\"POSTING_DATE\"),\n        \"close_date\": record.get(\"RESPONSE_DATE\"),\n        \"close_date_description\": record.get(\"RESPONSE_DATE_DESC\"),\n        \"archive_date\": record.get(\"ARCHIVE_DATE\"),\n        \"unarchive_date\": record.get(\"UNARCHIVE_DATE\"),\n        \"expected_number_of_awards\": convert_numeric(record.get(\"NUMBER_OF_AWARDS\")),\n        \"estimated_total_program_funding\": convert_numeric(record.get(\"EST_FUNDING\")),\n        \"award_floor\": convert_numeric(record.get(\"AWARD_FLOOR\")),\n        \"award_ceiling\": convert_numeric(record.get(\"AWARD_CEILING\")),\n        \"additional_info_url\": record.get(\"FD_LINK_URL\"),\n        \"additional_info_url_description\": record.get(\"FD_LINK_DESC\"),\n        \"forecasted_post_date\": None,\n        \"forecasted_close_date\": None,\n        \"forecasted_close_date_description\": None,\n        \"forecasted_award_date\": None,\n        \"forecasted_project_start_date\": None,\n        \"fiscal_year\": None,\n        \"revision_number\": None,\n        \"modification_comments\": record.get(\"MODIFICATION_COMMENTS\"),\n        \"funding_category_description\": record.get(\"OTH_CAT_FA_DESC\"),\n        \"agency_code\": record.get(\"A_SA_CODE\"),\n        \"agency_name\": record.get(\"AC_NAME\"),\n        \"agency_phone_number\": record.get(\"AC_PHONE_NUMBER\"),\n        \"agency_contact_description\": record.get(\"AGENCY_CONTACT_DESC\"),\n        \"agency_email_address\": record.get(\"AC_EMAIL_ADDR\"),\n        \"agency_email_address_description\": record.get(\"AC_EMAIL_DESC\"),\n        \"is_deleted\": False,\n        \"can_send_mail\": convert_yn_bool(record.get(\"SENDMAIL\")),\n        \"publisher_profile_id\": record.get(\"PUBLISHER_PROFILE_ID\"),\n        \"publisher_user_id\": record.get(\"PUBLISHERUID\"),\n        \"updated_by\": record.get(\"LAST_UPD_ID\"),\n        \"created_by\": record.get(\"CREATOR_ID\"),\n    }\n\n\ndef transform_cfda(record: dict[str, str]) -> dict[str, str]:\n    return {\n        \"opportunity_assistance_listing_id\": record.get(\"OPP_CFDA_ID\"),\n        \"opportunity_id\": record.get(\"OPPORTUNITY_ID\"),\n        \"assistance_listing_number\": record.get(\"CFDANUMBER\"),\n        \"program_title\": record.get(\"PROGRAMTITLE\"),\n        \"updated_by\": record.get(\"LAST_UPD_ID\"),\n        \"created_by\": record.get(\"CREATOR_ID\"),\n    }\n\n\ndef transform_applicant_types(\n    record: dict[str, str], id_map: dict[str, str], is_forecast: bool\n) -> dict[str, str]:\n    opportunity_id = record.get(\"OPPORTUNITY_ID\")\n    opportunity_summary_id = id_map.get(opportunity_id)\n\n    # this is the primary key ID of the legacy record\n    legacy_id = record.get(\"AT_FRCST_ID\") if is_forecast else record.get(\"AT_SYN_ID\")\n\n    return {\n        \"opportunity_summary_id\": opportunity_summary_id,\n        \"applicant_type_id\": convert_legacy_applicant_type(record.get(\"AT_ID\")),\n        \"legacy_applicant_type_id\": legacy_id,\n        \"updated_by\": record.get(\"LAST_UPD_ID\"),\n        \"created_by\": record.get(\"CREATOR_ID\"),\n    }\n\n\ndef transform_funding_category(\n    record: dict[str, str], id_map: dict[str, str], is_forecast: bool\n) -> dict[str, str]:\n    opportunity_id = record.get(\"OPPORTUNITY_ID\")\n    opportunity_summary_id = id_map.get(opportunity_id)\n\n    # this is the primary key ID of the legacy record\n    legacy_id = record.get(\"FAC_FRCST_ID\") if is_forecast else record.get(\"FAC_SYN_ID\")\n\n    return {\n        \"opportunity_summary_id\": opportunity_summary_id,\n        \"funding_category_id\": convert_legacy_funding_category_type(record.get(\"FAC_ID\")),\n        \"legacy_funding_category_id\": legacy_id,\n        \"updated_by\": record.get(\"LAST_UPD_ID\"),\n        \"created_by\": record.get(\"CREATOR_ID\"),\n    }\n\n\ndef transform_funding_instrument(\n    record: dict[str, str], id_map: dict[str, str], is_forecast: bool\n) -> dict[str, str]:\n    opportunity_id = record.get(\"OPPORTUNITY_ID\")\n    opportunity_summary_id = id_map.get(opportunity_id)\n\n    # this is the primary key ID of the legacy record\n    legacy_id = record.get(\"FI_FRCST_ID\") if is_forecast else record.get(\"FI_SYN_ID\")\n\n    return {\n        \"opportunity_summary_id\": opportunity_summary_id,\n        \"funding_instrument_id\": convert_legacy_funding_instrument_type(record.get(\"FI_ID\")),\n        \"legacy_funding_instrument_id\": legacy_id,\n        \"updated_by\": record.get(\"LAST_UPD_ID\"),\n        \"created_by\": record.get(\"CREATOR_ID\"),\n    }\n\n\ndef process(directory: str) -> None:\n    raw_opportunity_records = get_csv_records(directory, \"TOPPORTUNITY\")\n    opportunity_records = [transform_opportunity(record) for record in raw_opportunity_records]\n\n    opportunity_ids = {record.get(\"opportunity_id\") for record in opportunity_records}\n\n    write_csv(directory, \"opportunity\", opportunity_records)\n\n    raw_forecast_records = get_csv_records(directory, \"TFORECAST\")\n    forecast_records = [transform_forecast(record) for record in raw_forecast_records]\n    raw_synopsis_records = get_csv_records(directory, \"TSYNOPSIS\")\n    synopsis_records = [transform_synopsis(record) for record in raw_synopsis_records]\n    write_csv(directory, \"opportunity_summary\", forecast_records + synopsis_records)\n\n    raw_cfda_records = get_csv_records(directory, \"TOPPORTUNITY_CFDA\")\n    cfda_records = []\n    for record in raw_cfda_records:\n        if record.get(\"OPPORTUNITY_ID\") in opportunity_ids:\n            cfda_records.append(transform_cfda(record))\n    write_csv(directory, \"opportunity_assistance_listing\", cfda_records)\n\n    # before we can process the link lookup tables, we need the new IDs\n    # of the opportunity summary records that we created above.\n    forecast_opportunity_id_to_summary_id_map = {}\n    for record in forecast_records:\n        forecast_opportunity_id_to_summary_id_map[record[\"opportunity_id\"]] = record[\n            \"opportunity_summary_id\"\n        ]\n\n    synopsis_opportunity_id_to_summary_id_map = {}\n    for record in synopsis_records:\n        synopsis_opportunity_id_to_summary_id_map[record[\"opportunity_id\"]] = record[\n            \"opportunity_summary_id\"\n        ]\n\n    # link_opportunity_summary_applicant_type\n    raw_forecast_applicant_types = get_csv_records(directory, \"TAPPLICANTTYPES_FORECAST\")\n    forecast_applicant_types = [\n        transform_applicant_types(record, forecast_opportunity_id_to_summary_id_map, True)\n        for record in raw_forecast_applicant_types\n    ]\n    raw_synopsis_applicant_types = get_csv_records(directory, \"TAPPLICANTTYPES_SYNOPSIS\")\n    synopsis_applicant_types = [\n        transform_applicant_types(record, synopsis_opportunity_id_to_summary_id_map, False)\n        for record in raw_synopsis_applicant_types\n    ]\n    write_csv(\n        directory,\n        \"link_opportunity_summary_applicant_type\",\n        forecast_applicant_types + synopsis_applicant_types,\n    )\n\n    # link_opportunity_summary_funding_instrument\n    raw_forecast_funding_instruments = get_csv_records(directory, \"TFUNDINSTR_FORECAST\")\n    forecast_funding_instruments = [\n        transform_funding_instrument(record, forecast_opportunity_id_to_summary_id_map, True)\n        for record in raw_forecast_funding_instruments\n    ]\n    raw_synopsis_funding_instruments = get_csv_records(directory, \"TFUNDINSTR_SYNOPSIS\")\n    synopsis_funding_instruments = [\n        transform_funding_instrument(record, synopsis_opportunity_id_to_summary_id_map, False)\n        for record in raw_synopsis_funding_instruments\n    ]\n    write_csv(\n        directory,\n        \"link_opportunity_summary_funding_instrument\",\n        forecast_funding_instruments + synopsis_funding_instruments,\n    )\n\n    # link_opportunity_summary_funding_category\n    raw_forecast_funding_categories = get_csv_records(directory, \"TFUNDACTCAT_FORECAST\")\n    forecast_funding_categories = [\n        transform_funding_category(record, forecast_opportunity_id_to_summary_id_map, True)\n        for record in raw_forecast_funding_categories\n    ]\n    raw_synopsis_funding_categories = get_csv_records(directory, \"TFUNDACTCAT_SYNOPSIS\")\n    synopsis_funding_categories = [\n        transform_funding_category(record, synopsis_opportunity_id_to_summary_id_map, False)\n        for record in raw_synopsis_funding_categories\n    ]\n    write_csv(\n        directory,\n        \"link_opportunity_summary_funding_category\",\n        forecast_funding_categories + synopsis_funding_categories,\n    )\n\n\n@click.command()\n@click.option(\"--directory\", required=True)\ndef convert_oracle_csvs_to_postgres(directory: str) -> None:\n    with src.logging.init(\"convert_oracle_csvs_to_postgres\"):\n        logger.info(\"Starting script\")\n\n        process(directory)\n\n        logger.info(\"Done\")\n\n\nif __name__ == \"__main__\":\n    convert_oracle_csvs_to_postgres()"}
{"path":"infra/api/app-config/env-config/database.tf","language":"unknown","type":"code","directory":"infra/api/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/env-config/database.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: api/tool/__init__.py\nLanguage: py\nType: code\nDirectory: api/tool\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tool/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/api/app-config/env-config/environment-variables.tf","language":"unknown","type":"code","directory":"infra/api/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/env-config/environment-variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":""}
{"path":"infra/api/app-config/env-config/file_upload_jobs.tf","language":"unknown","type":"code","directory":"infra/api/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/env-config/file_upload_jobs.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: api/tool/console/__init__.py\nLanguage: py\nType: code\nDirectory: api/tool/console\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tool/console/__init__.py\nSize: 0.00 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/api/app-config/env-config/main.tf","language":"unknown","type":"code","directory":"infra/api/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/env-config/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":""}
{"path":"infra/api/app-config/env-config/outputs.tf","language":"unknown","type":"code","directory":"infra/api/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/env-config/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: api/tool/console/interactive.py\nLanguage: py\nType: code\nDirectory: api/tool/console\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/api/tool/console/interactive.py\nSize: 3.68 KB\nLast Modified: 2025-02-14T17:08:26.463Z"}
{"path":"infra/api/app-config/env-config/s3_buckets.tf","language":"unknown","type":"code","directory":"infra/api/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/env-config/s3_buckets.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"from types import ModuleType\n\nimport rich\nimport rich.panel\nimport rich.pretty\n\nimport src.adapters.db as db\nimport src.db\nimport src.db.models\nimport src.logging\nimport src.util\nimport tests.src.db.models.factories\nfrom src.adapters.db.clients.postgres_client import PostgresDBClient\nfrom src.adapters.db.clients.postgres_config import get_db_config\n\nINTRO = \"\"\"\nSimpler Grants Gov Python console\n\nUseful things:\n\nâ”‚ dbs = database session {db_session}\nâ”‚ f = DB factories module\nâ”‚ u = utilities module\nâ”‚ r = function to reload REPL\n\nExamples:\n\nâ”‚ dbs.query(Opportunity).first()\n| f.OpportunityFactory.create()\n\nTips:\n\nâ˜… Tab-completion is available\nâ˜… History is available (use â†‘â†“ keys)\nâ˜… Use Ctrl+D to exit\n\"\"\"\n\n\ndef interactive_console() -> dict:\n    \"\"\"Set up variables and print a introduction message for the interactive console.\"\"\"\n\n    db_session = connect_to_database()\n\n    print(INTRO.format(**locals()))\n\n    variables = dict()\n    variables.update(vars(src.db.models.opportunity_models))\n    variables.update(vars(src.db.models.lookup_models))\n\n    # This goes after the imports of entire modules, so the console reserved\n    # names (db, fineos, etc) take precedence. This might break some modules\n    # that expect something different under those names.\n    variables.update(locals())\n\n    # DB\n    variables[\"db_session\"] = db_session\n    variables[\"dbs\"] = db_session\n\n    # DB Factories\n    factories_module = tests.src.db.models.factories\n    if isinstance(db_session, db.Session):\n        factories_module._db_session = db_session\n    variables[\"f\"] = tests.src.db.models.factories\n\n    # Easy access to utilities\n    variables[\"u\"] = src.util\n    variables[\"util\"] = src.util\n\n    # Easy reloading of modules imported in REPL, for retrying something after a\n    # code change without dropping out of REPL\n    variables[\"r\"] = reload_repl\n    variables[\"reload\"] = reload_repl\n    variables[\"reload_module\"] = reload_module\n\n    return variables\n\n\ndef connect_to_database() -> db.Session | Exception:\n    db_config = get_db_config()\n\n    # errors sometimes dump sensitive info\n    # (since we're doing locally, we don't need to hide)\n    db_config.hide_sql_parameter_logs = False\n    db_session: db.Session | Exception\n    try:\n        db_session = PostgresDBClient(db_config).get_session()\n    except Exception as err:\n        db_session = err\n\n    return db_session\n\n\ndef reload_repl() -> None:\n    import importlib\n    from sys import modules\n\n    for module in set(modules.values()):\n        # only reload our code\n        if \"<module 'src.\" not in str(module):\n            continue\n\n        # individual database model modules can be particular in how they are\n        # loaded, so don't automatically reload them\n        if \"<module 'src.db.models.\" in str(module):\n            continue\n\n        # reloading the logging initialization and stuff can cause some issues,\n        # avoid it all for now\n        if \"<module 'src.util.logging\" in str(module):\n            continue\n\n        try:\n            importlib.reload(module)\n        except Exception:\n            # there are some problems that are swept under the rug here\n            pass\n\n\ndef reload_module(m: ModuleType) -> None:\n    import importlib\n\n    importlib.reload(m)\n\n\nif __name__ == \"__main__\":\n    with src.logging.init(__package__):\n        interactive_variables = interactive_console()\n        globals().update(interactive_variables)\n        rich.pretty.install(indent_guides=True, max_length=20, max_string=400)"}
{"path":"infra/api/app-config/env-config/scheduled_jobs.tf","language":"unknown","type":"code","directory":"infra/api/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/env-config/scheduled_jobs.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: docker-compose.yml\nLanguage: yml\nType: code\nDirectory: root\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/docker-compose.yml\nSize: 0.15 KB\nLast Modified: 2025-02-14T17:08:26.464Z"}
{"path":"infra/api/app-config/env-config/variables.tf","language":"unknown","type":"code","directory":"infra/api/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/env-config/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"# services:\n  # Define other services or override configurations"}
{"path":"infra/api/app-config/feature-flags.tf","language":"unknown","type":"code","directory":"infra/api/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/feature-flags.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/README.md\nLanguage: md\nType: code\nDirectory: documentation\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/README.md\nSize: 1.12 KB\nLast Modified: 2025-02-14T17:08:26.464Z"}
{"path":"infra/api/app-config/main.tf","language":"unknown","type":"code","directory":"infra/api/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"This folder contains documentation for the Simpler Grants Project.\n\n## Goals\n\nOur vision is for the following to become true:\n\n**_We want Grants.gov to be an extremely simple, accessible, and easy-to-use tool for posting, finding, sharing, and applying for federal financial assistance. Our mission is to increase access to grants and improve the grants experience for everyone._**.\n\nSee [goals.md](./goals.md) for more information about the vision and goals for the project.\n\n## Deliverables\n\nThe [deliverables](./deliverables) directory contains information about our deliverables for the modernization project.\n\n[The product roadmap](https://github.com/orgs/HHS/projects/12) contains brief descriptions of upcoming deliverables. The [individual deliverables](./deliverables/individual_deliverables) directory contains detailed descriptions of the project deliverables.\n\n## Decisions\n\nImportant technical and programmatic decisions are contained in the [decisions](./decisions/) directory, captured using the [architectural decision record format](https://adr.github.io) and written in markdown."}
{"path":"infra/api/app-config/outputs.tf","language":"unknown","type":"code","directory":"infra/api/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/analytics/README.md\nLanguage: md\nType: code\nDirectory: documentation/analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/README.md\nSize: 0.18 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/api/app-config/prod.tf","language":"unknown","type":"code","directory":"infra/api/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/prod.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"Documentation for the operational analytics package can be found in this directory and the [Analytics README.md](../../analytics/README.md)."}
{"path":"infra/api/app-config/staging.tf","language":"unknown","type":"code","directory":"infra/api/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/app-config/staging.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/analytics/development.md\nLanguage: md\nType: code\nDirectory: documentation/analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/development.md\nSize: 5.33 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/api/build-repository/main.tf","language":"unknown","type":"code","directory":"infra/api/build-repository","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/build-repository/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"> [!NOTE]\n> All of the steps on this page should be run from the root of the [`analytics/`](../../analytics/) directory\n\n## Install Prerequisites \n\n## Development Environment: Docker vs. Native\n\nThis package runs in Docker by default, but can also be configured to run natively without Docker. Choose the option that's best for you, and then install the prerequisites for that option:\n\n- [Run with Docker](#run-with-docker)\n- [Run Natively](#run-natively)\n\n#### Run with Docker \n\n**Prerequisites**\n\n- **Docker** [Installation options](https://docs.docker.com/desktop/setup/install/mac-install/) \n- **docker-compose** [Installation options](https://formulae.brew.sh/formula/docker-compose)\n\n#### Run Natively\n\n**Prerequisites**\n\n- **Python version 3.12:** [pyenv](https://github.com/pyenv/pyenv#installation) is one popular option for installing Python, or [asdf](https://asdf-vm.com/)\n- **Poetry:** [Install poetry with the official installer](https://python-poetry.org/docs/#installing-with-the-official-installer) or alternatively use [pipx to install](https://python-poetry.org/docs/#installing-with-pipx)\n- **GitHub CLI:** [Install the GitHub CLI](https://github.com/cli/cli#installation)\n- **Postgres:** [Installation options for macOS](https://www.postgresql.org/download/macosx/)\n- **Psycopg:** [Installation options](https://www.psycopg.org/psycopg3/docs/basic/install.html)\n\n### Install the Package\n\n**Steps**\n\n1. Install all prerequisites\n2. Set up the project: `make install` -- This will install the required packages and prompt you to authenticate with GitHub\n3. Acquire a GitHub Token using one of the methods below\n  - Via AWS (Project Team)\n    - Retrieve GH_TOKEN from [AWS](https://us-east-1.console.aws.amazon.com/systems-manager/parameters/%252Fanalytics%252Fgithub-token/description?region=us-east-1&tab=Table#list_parameter_filters=Name:Contains:analytics%2Fgithub-token)\n  - Create your own in GitHub (Open Source)\n    - Go to https://github.com/settings/tokens\n    - Generate a new token (classic)\n    - Give it the following scopes:\n      - repo\n      - read:org\n      - admin:public_key\n      - project\n4. Add `GH_TOKEN=...` to your environment variables, e.g. in .zshrc or .bashrc\n5. If running natively, add PY_RUN_APPROACH=local to your environment variables\n6. Edit `local.env` and set the value of DB_HOST accordingly\n7. Run `make test-audit` to confirm the application is running correctly\n\n\n## Invoke Commands on the Service\n\n### Using `make` \n\nSeveral `make` commands are defined in the project [`Makefile`](../../analytics/Makefile). Commands can be invoked from the command line, as in the following examples:\n\n- `make install` - Checks that prereqs are installed, installs new dependencies, and prompts for GitHub authentication\n- `make unit-test` - Runs the unit tests and opens a coverage report in a web browser\n- `make e2e-test` - Runs integration and end-to-end tests and opens a coverage report in a web browser\n- `make lint` - Runs [linting and formatting checks](formatting-and-linting.md)\n\n### Using the CLI \n\nThe package includes a CLI that can be used to discover the available commands. To run the CLI, type `poetry run analytics --help` at the command line, and the CLI should respond with a list of available commands.\n\n![Screenshot of passing the --help flag to CLI entry point](../../analytics/static/screenshot-cli-help.png)\n\n## Example Development Tasks\n\n### How To Access Dockerized Postgres DB from MacOS Terminal\n\n1. Start the database container: `sudo docker-compose up -d`\n2. Ensure container is running: `docker-compose ls`\n3. Get your IP address, which will be used in next step: `ifconfig -u | grep 'inet ' | grep -v 127.0.0.1 | cut -d\\  -f2 | head -1` (this will display a value similar to `10.0.1.101`)\n4. Launch the terminal-based front-end to Postgres: `psql -h 10.0.1.101 -p 5432 -U app -W app` (use IP address from previous step for the value of `-h` arg)\n5. Type a PostgresSQL command, e.g.: `\\dir`.\n\n### How To Add New Dataset\n\n1. Create a new python file in `src/analytics/datasets/`\n2. In that file, create a new class that inherits from the `BaseDataset`\n3. Store the names of key columns as either class or instance attributes\n4. If you need to combine multiple source files (or other datasets) to produce this dataset, consider creating a class method that can be used to instantiate this dataset from those sources\n5. Create **at least** one unit test for each method that is implemented with the new class\n\n### How To Add New CLI Entrypoint\n\n1. Add a new function to [`cli.py`](../../analytics/src/analytics/cli.py)\n2. Wrap this function with a [sub-command `typer` decorator](https://typer.tiangolo.com/tutorial/subcommands/single-file/) \n3. If the function accepts parameters, [annotate those parameters](https://typer.tiangolo.com/tutorial/options/name/)\n4. Add *at least* one unit test for the CLI entrypoint, optionally mocking potential side effects of calling the entrypoint\n\n### How to Extend Analytics DB Schema\n\n1. Add a new migration file to [`integrations/etldb/migrations/versions/`](../../analytics/src/analytics/integrations/etldb/migrations/versions) and prefix file name with the next iteration number (ex: `0007_`)\n2. Add valid Postgres SQL to the new integration file\n3. Run the migration command: `make db-migrate` \n\n### How To Run Linters\n\n```bash\nmake lint\n```\n\n### How To Run Unit Tests\n\n```bash\nmake unit-test\n```"}
{"path":"infra/api/build-repository/shared.s3.tfbackend","language":"unknown","type":"code","directory":"infra/api/build-repository","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/build-repository/shared.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/analytics/formatting-and-linting.md\nLanguage: md\nType: code\nDirectory: documentation/analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/formatting-and-linting.md\nSize: 2.50 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/api/database/.terraform.lock.hcl","language":"unknown","type":"code","directory":"infra/api/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/database/.terraform.lock.hcl","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"## Running the checks\n\nRun `make lint` from the root of the `analytics/` package to run all of the linters and formatters.\n\nThis command is also run as part of the [GitHub action](../../.github/workflows/ci-analytics.yml) that is triggered each time changes are pushed to a pull request that modifies the analytics package.\n\n## Linting\n\n### Ruff\n\n[ruff](https://flake8.pycqa.org/en/latest/) is used to enforce a set of best practices for our Python code. Configuration options can be found in [pyproject.toml - tool.ruff](../../analytics/pyproject.toml).\n\n> [!NOTE]\n> We currently enforce [all of ruff's rules](https://docs.astral.sh/ruff/rules/) with just a few exceptions. If we find that the current selection of rules is too cumbersome to adhere to, we can [change which rules are enforced](https://docs.astral.sh/ruff/linter/#rule-selection).\n\n### Pylint\n\n[pylint](https://pylint.readthedocs.io/en/latest/) is used to enforce rules that [ruff doesn't currently support](https://docs.astral.sh/ruff/faq/#how-does-ruffs-linter-compare-to-pylint). Configuration options can be found in [pyproject.toml - tool.pylint](../../analytics/pyproject.toml).\n\n> [!NOTE]\n> Currently we run both ruff and pylint for greater code quality coverage, however, ruff is [implementing more of pylint's rule set](https://github.com/astral-sh/ruff/issues/970). Because ruff is so much faster than pylint, once we consider the overlap between ruff and pylint's rule sets sufficient, we can drop pylint from our list of linters.\n\n### Mypy\n\n[mypy](https://mypy.readthedocs.io/en/stable/) is used to validate and enforce typechecking in python. Configuration options can be found in [pyproject.toml - tool.mypy](../../analytics/pyproject.toml). Type annotations are also enforced with ruff's replacement for the [flake8-annotations rule set](https://docs.astral.sh/ruff/rules/#flake8-annotations-ann).\n\n## Formatting\n\n### Black\n\n[black](https://black.readthedocs.io/en/stable/) is used to format our Python code. Configuration options can be found in [pyproject.toml - tool.black](../../analytics/pyproject.toml). If developers are using VSCode, the `analytics` package also contains a [settings configuration file](../../analytics/.vscode/settings.json) that automatically run black when a file is saved.\n\n### Sorting imports\n\nRather than installing [isort](https://pycqa.github.io/isort/) separately to sort imports, the `analytics` package uses [ruff's isort rule set](https://docs.astral.sh/ruff/rules/#isort-i) to automatically fix the import order."}
{"path":"infra/api/database/dev.s3.tfbackend","language":"unknown","type":"code","directory":"infra/api/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/database/dev.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/analytics/metrics/README.md\nLanguage: md\nType: code\nDirectory: documentation/analytics/metrics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/metrics/README.md\nSize: 0.22 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/api/database/main.tf","language":"unknown","type":"code","directory":"infra/api/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/database/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"This section of the documentation contains descriptions of the metrics we calculate as part of our analytics pipeline.\n\n## List of metrics\n\n1. [Burndown](burndown.md)\n2. [Percent complete](percent-complete.md)"}
{"path":"infra/api/database/outputs.tf","language":"unknown","type":"code","directory":"infra/api/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/database/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/analytics/metrics/burndown.md\nLanguage: md\nType: code\nDirectory: documentation/analytics/metrics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/metrics/burndown.md\nSize: 6.27 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/api/database/prod.s3.tfbackend","language":"unknown","type":"code","directory":"infra/api/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/database/prod.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"Burndown shows the total number of open issues or points per day over a given date range. Burndown charts are often used to answer the following questions:\n\n- *Have we planned work correctly?* For example, if a burndown chart shows several issues or points open by the end of a sprint, it could signal the need to re-examine how we're planning our sprint capacity.\n- *Have we broken work into manageable chunks?* For example, if we see a long plateau where the number of open points remain steady, then suddenly drop, it could be a signal that the amount of work scoped into one ticket is too large.\n- *Are we encountering blockers?* Alternatively, a long plateau could also be a signal of external factors that are blocking forward progress.\n\n> [!NOTE]\n> While burndown can provide helpful signals in the cases above, these signals tell only part of the story. Burndown charts should prompt questions, not represent definitive conclusions.\n\n## Methodology\n\n### Summary\n\nIn order to calculate burndown there are a few key steps:\n\n1. Isolate the set of issues assigned to the sprint (or deliverable) for which we're calculating burndown.\n2. Get the number of issues (or points) opened and closed per day over the date range we care about.\n3. Subtract the number closed from the number opened to get the delta, i.e. the total number of issues/points opened or closed each day.\n4. Cumulatively sum the deltas over the date range to find the running total of open issues (or points) per day.\n\n> [!NOTE]\n> If an issue is unpointed, it *does* count toward burndown by issues, but *does not* count toward burndown by points.\n\n### Step-by-step example: Sprint burndown\n\n<details>\n<summary>Sample input</summary>\n\n| sprint   | issue_title | story_points | opened_date | closed_date | sprint_start | sprint_end |\n| -------- | ----------- | ------------ | ----------- | ----------- | ------------ | ---------- |\n| Sprint 1 | Issue 1     | 2            | 2023-10-30  | 2023-11-02  | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 2     | 1            | 2023-11-01  | None        | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 3     | None         | 2023-11-01  | 2023-11-04  | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 4     | 3            | 2023-11-01  | 2023-11-05  | 2023-11-01   | 2023-11-05 |\n| Sprint 2 | Issue 5     | 3            | 2023-11-02  | 2023-11-07  | 2023-11-06   | 2023-11-10 |\n| Sprint 1 | Issue 6     | 2            | 2023-11-02  | 2023-11-04  | 2023-11-01   | 2023-11-05 |\n| Sprint 2 | Issue 7     | 1            | 2023-11-03  | None        | 2023-11-06   | 2023-11-10 |\n\n</details>\n\n<details>\n<summary>Step 1: Isolate sprint records</summary>\n\n| sprint   | issue_title | story_points | opened_date | closed_date | sprint_start | sprint_end |\n| -------- | ----------- | ------------ | ----------- | ----------- | ------------ | ---------- |\n| Sprint 1 | Issue 1     | 2            | 2023-10-30  | 2023-11-02  | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 2     | 1            | 2023-11-01  | None        | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 3     | None         | 2023-11-01  | 2023-11-04  | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 4     | 3            | 2023-11-01  | 2023-11-05  | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 6     | 2            | 2023-11-02  | 2023-11-04  | 2023-11-01   | 2023-11-05 |\n\n</details>\n\n<details>\n<summary>Step 2: Create date range for burndown</summary>\n\nCreate a date range that runs from the earliest date a ticket was opened to the latest date a ticket was closed *or* to the end of the sprint, whichever is later.\n\n| date       |\n| ---------- |\n| 2023-10-30 |\n| 2023-10-31 |\n| 2023-11-01 |\n| 2023-11-02 |\n| 2023-11-03 |\n| 2023-11-04 |\n| 2023-11-05 |\n\n</details>\n\n<details>\n<summary>Step 3: Group issues/points by date opened</summary>\n\nBy points:\n\n| date       | opened |\n| ---------- | ------ |\n| 2023-10-30 | 2      |\n| 2023-11-01 | 4      |\n| 2023-11-02 | 2      |\n\nBy issues:\n\n| date       | opened |\n| ---------- | ------ |\n| 2023-10-30 | 1      |\n| 2023-11-01 | 3      |\n| 2023-11-02 | 1      |\n\n</details>\n\n<details>\n<summary>Step 4: Group issues/points by date closed</summary>\n\nBy points:\n\n| date       | closed |\n| ---------- | ------ |\n| 2023-11-02 | 2      |\n| 2023-11-04 | 2      |\n| 2023-11-05 | 3      |\n\nBy issues:\n\n| date       | closed |\n| ---------- | ------ |\n| 2023-11-02 | 1      |\n| 2023-11-04 | 2      |\n| 2023-11-05 | 1      |\n\n</details>\n\n<details>\n<summary>Step 5: Join on date and calculate the delta</summary>\n\nBy points:\n\n| date       | opened | closed | delta |\n| ---------- | ------ | ------ | ----- |\n| 2023-10-30 | 2      | 0      | 2     |\n| 2023-10-31 | 0      | 0      | 0     |\n| 2023-11-01 | 4      | 0      | 4     |\n| 2023-11-02 | 2      | 2      | 0     |\n| 2023-11-03 | 0      | 0      | 0     |\n| 2023-11-04 | 0      | 2      | -2    |\n| 2023-11-05 | 0      | 3      | -3    |\n\nBy issues:\n\n| date       | opened | closed | delta |\n| ---------- | ------ | ------ | ----- |\n| 2023-10-30 | 1      | 0      | 1     |\n| 2023-10-31 | 0      | 0      | 0     |\n| 2023-11-01 | 3      | 0      | 3     |\n| 2023-11-02 | 1      | 1      | 0     |\n| 2023-11-03 | 0      | 0      | 0     |\n| 2023-11-04 | 0      | 2      | -2    |\n| 2023-11-05 | 0      | 1      | -1    |\n\n</details>\n\n<details>\n<summary>Step 6: Calculate the running total by cumulatively summing the deltas</summary>\n\nBy points:\n\n| date       | opened | closed | delta | total_open |\n| ---------- | ------ | ------ | ----- | ---------- |\n| 2023-10-30 | 2      | 0      | 2     | 2          |\n| 2023-10-31 | 0      | 0      | 0     | 2          |\n| 2023-11-01 | 4      | 0      | 4     | 6          |\n| 2023-11-02 | 2      | 2      | 0     | 6          |\n| 2023-11-03 | 0      | 0      | 0     | 6          |\n| 2023-11-04 | 0      | 2      | -2    | 4          |\n| 2023-11-05 | 0      | 3      | -3    | 1          |\n\nBy issues:\n\n| date       | opened | closed | delta | total_open |\n| ---------- | ------ | ------ | ----- | ---------- |\n| 2023-10-30 | 1      | 0      | 1     | 1          |\n| 2023-10-31 | 0      | 0      | 0     | 1          |\n| 2023-11-01 | 3      | 0      | 3     | 4          |\n| 2023-11-02 | 1      | 1      | 0     | 4          |\n| 2023-11-03 | 0      | 0      | 0     | 4          |\n| 2023-11-04 | 0      | 2      | -2    | 2          |\n| 2023-11-05 | 0      | 1      | -1    | 1          |\n\n</details>"}
{"path":"infra/api/database/search.tf","language":"unknown","type":"code","directory":"infra/api/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/database/search.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/analytics/metrics/burnup.md\nLanguage: md\nType: code\nDirectory: documentation/analytics/metrics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/metrics/burnup.md\nSize: 5.42 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/api/database/staging.s3.tfbackend","language":"unknown","type":"code","directory":"infra/api/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/database/staging.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"Burnup shows the total number of open issues or points per day over a given date range. burnup charts are often used to answer the following questions:\n\n- *Have we planned work correctly?* For example, if a burnup chart shows several issues or points open by the end of a sprint, it could signal the need to re-examine how we're planning our sprint capacity.\n- *Have we broken work into manageable chunks?* For example, if we see a long plateau where the number of closed issues remain steady, then suddenly increases, it could be a signal that the amount of work scoped into one ticket is too large.\n- *Are we encountering blockers?* Alternatively, a long plateau could also be a signal of external factors that are blocking forward progress.\n\n> [!NOTE]\n> While burnup can provide helpful signals in the cases above, these signals tell only part of the story. burnup charts should prompt questions, not represent definitive conclusions.\n\n## Methodology\n\n### Summary\n\nIn order to calculate burnup there are a few key steps:\n\n1. Isolate the set of issues assigned to the sprint (or deliverable) for which we're calculating burnup.\n2. Get the number of issues (or points) opened and closed per day over the date range we care about.\n3. Get the total number of open issues (or points) and the cumulative sum of issues closed per day over the date range we care about. \n\n> [!NOTE]\n> If an issue is unpointed, it *does* count toward burnup by issues, but *does not* count toward burnup by points.\n\n### Step-by-step example: Sprint burnup\n\n<details>\n<summary>Sample input</summary>\n\n| sprint   | issue_title | story_points | opened_date | closed_date | sprint_start | sprint_end |\n| -------- | ----------- | ------------ | ----------- | ----------- | ------------ | ---------- |\n| Sprint 1 | Issue 1     | 2            | 2023-10-30  | 2023-11-02  | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 2     | 1            | 2023-11-01  | None        | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 3     | None         | 2023-11-01  | 2023-11-04  | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 4     | 3            | 2023-11-01  | 2023-11-05  | 2023-11-01   | 2023-11-05 |\n| Sprint 2 | Issue 5     | 3            | 2023-11-02  | 2023-11-07  | 2023-11-06   | 2023-11-10 |\n| Sprint 1 | Issue 6     | 2            | 2023-11-02  | 2023-11-04  | 2023-11-01   | 2023-11-05 |\n| Sprint 2 | Issue 7     | 1            | 2023-11-03  | None        | 2023-11-06   | 2023-11-10 |\n\n</details>\n\n<details>\n<summary>Step 1: Isolate sprint records</summary>\n\n| sprint   | issue_title | story_points | opened_date | closed_date | sprint_start | sprint_end |\n| -------- | ----------- | ------------ | ----------- | ----------- | ------------ | ---------- |\n| Sprint 1 | Issue 1     | 2            | 2023-10-30  | 2023-11-02  | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 2     | 1            | 2023-11-01  | None        | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 3     | None         | 2023-11-01  | 2023-11-04  | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 4     | 3            | 2023-11-01  | 2023-11-05  | 2023-11-01   | 2023-11-05 |\n| Sprint 1 | Issue 6     | 2            | 2023-11-02  | 2023-11-04  | 2023-11-01   | 2023-11-05 |\n\n</details>\n\n<details>\n<summary>Step 2: Create date range for burnup</summary>\n\nCreate a date range that runs from the earliest date a ticket was opened to the latest date a ticket was closed *or* to the end of the sprint, whichever is later.\n\n| date       |\n| ---------- |\n| 2023-10-30 |\n| 2023-10-31 |\n| 2023-11-01 |\n| 2023-11-02 |\n| 2023-11-03 |\n| 2023-11-04 |\n| 2023-11-05 |\n\n</details>\n\n<details>\n<summary>Step 3: Group issues/points by date opened</summary>\n\nBy points:\n\n| date       | opened |\n| ---------- | ------ |\n| 2023-10-30 | 2      |\n| 2023-11-01 | 4      |\n| 2023-11-02 | 2      |\n\nBy issues:\n\n| date       | opened |\n| ---------- | ------ |\n| 2023-10-30 | 1      |\n| 2023-11-01 | 3      |\n| 2023-11-02 | 1      |\n\n</details>\n\n<details>\n<summary>Step 4: Group issues/points by date closed</summary>\n\nBy points:\n\n| date       | closed |\n| ---------- | ------ |\n| 2023-11-02 | 2      |\n| 2023-11-04 | 2      |\n| 2023-11-05 | 3      |\n\nBy issues:\n\n| date       | closed |\n| ---------- | ------ |\n| 2023-11-02 | 1      |\n| 2023-11-04 | 2      |\n| 2023-11-05 | 1      |\n\n</details>\n\n<details>\n<summary>Step 5: Join on date and calculate the total sums of open and cumulative sum of closed issues</summary>\n\nBy points:\n\n| date       | opened | closed | total_opened|total_closed|\n| ---------- | ------ | ------ | ----------- |------------|\n| 2023-10-30 | 2      | 0      | 2           | 0          |\n| 2023-10-31 | 0      | 0      | 2           | 0          |\n| 2023-11-01 | 4      | 0      | 6           | 0          |\n| 2023-11-02 | 2      | 2      | 8           | 2          |\n| 2023-11-03 | 0      | 0      | 8           | 2          |\n| 2023-11-04 | 0      | 2      | 8           | 4          |\n| 2023-11-05 | 0      | 3      | 8           | 7          |\n\nBy issues:\n\n| date       | opened | closed | total_open |total_closed|\n| ---------- | ------ | ------ | ---------- |------------|\n| 2023-10-30 | 1      | 0      | 1          | 0          |\n| 2023-10-31 | 0      | 0      | 1          | 0          |\n| 2023-11-01 | 3      | 0      | 4          | 0          |\n| 2023-11-02 | 1      | 1      | 4          | 1          |\n| 2023-11-03 | 0      | 0      | 4          | 1          |\n| 2023-11-04 | 0      | 2      | 4          | 3          |\n| 2023-11-05 | 0      | 1      | 4          | 4          |"}
{"path":"infra/api/database/variables.tf","language":"unknown","type":"code","directory":"infra/api/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/database/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/analytics/metrics/percent-complete.md\nLanguage: md\nType: code\nDirectory: documentation/analytics/metrics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/metrics/percent-complete.md\nSize: 3.61 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/api/service/.terraform.lock.hcl","language":"unknown","type":"code","directory":"infra/api/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/service/.terraform.lock.hcl","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"Percent complete shows the percentage of all issues or points assigned to a given deliverable that have been completed. Percent complete charts are often used to answer the following questions:\n\n- *How much work does this deliverable entail?* Part of the reason we display both the percentage and the total number of open/closed tickets is that some deliverables are larger in scope than others. This metric helps us compare both scope of work and progress per deliverable.\n- *How close are we to completing this deliverable?* Often, senior leadership wants to understand our progress toward key deliverables within the project.\n- *Will we be able to hit our target date?* For example, if we are only a few days or weeks out from our target deliverable but are only at 80% completion, it may be a signal that we should cut scope or revise our target.\n\n## Methodology\n\n### Summary\n\nDivide the number of closed issues (or points) by the total number of issues (or points) per deliverable.\n\n> [!NOTE]\n> If an issue is unpointed, it *does* count toward percent complete by issues, but *does not* count toward percent complete by points.\n\n### Step-by-step example: Percent complete by deliverable\n\n<details>\n<summary>Sample input</summary>\n\n| deliverable | issue_title | status | story_points |\n| ----------- | ----------- | ------ | ------------ |\n| Site launch | Issue 1     | open   | 1            |\n| Site launch | Issue 2     | closed | 3            |\n| Site launch | Issue 3     | closed | 2            |\n| Site launch | Issue 4     | open   | 2            |\n| API launch  | Issue 5     | closed | 2            |\n| API launch  | Issue 6     | closed | 1            |\n| API launch  | Issue 7     | open   | 2            |\n| API launch  | Issue 8     | closed | 1            |\n\n</details>\n\n<details>\n<summary>Step 1: Count the total number of issues/points deliverable</summary>\n\nBy points:\n\n| deliverable | total |\n| ----------- | ----- |\n| Site launch | 8     |\n| API launch  | 6     |\n\nBy issues:\n\n| deliverable | total |\n| ----------- | ----- |\n| Site launch | 4     |\n| API launch  | 4     |\n\n</details>\n\n<details>\n<summary>Step 2: Count the number of *closed* issues/points deliverable</summary>\n\nBy points:\n\n| deliverable | closed |\n| ----------- | ------ |\n| Site launch | 5      |\n| API launch  | 4      |\n\nBy issues:\n\n| deliverable | closed |\n| ----------- | ------ |\n| Site launch | 2      |\n| API launch  | 3      |\n\n</details>\n\n<details>\n<summary>Step 3: Join on deliverable and calculate issues/points open</summary>\n\nBy points:\n\n| deliverable | total | closed | open |\n| ----------- | ----- | ------ | ---- |\n| Site launch | 8     | 5      | 3    |\n| API launch  | 6     | 4      | 2    |\n\nBy issues:\n\n| deliverable | total | closed | open |\n| ----------- | ----- | ------ | ---- |\n| Site launch | 4     | 2      | 2    |\n| API launch  | 4     | 3      | 1    |\n\n</details>\n\n<details>\n<summary>Step 3: Calculate percent complete</summary>\n\n> **Note:** While we leave the full decimal in the results dataframe, when we visualize the results, we round to the nearest percentage point and display as a percentage rather than as a decimal.\n\nBy points:\n\n| deliverable | total | closed | open | percent_complete |\n| ----------- | ----- | ------ | ---- | ---------------- |\n| Site launch | 8     | 5      | 3    | 0.625            |\n| API launch  | 6     | 4      | 2    | 0.6666666667     |\n\nBy issues:\n\n| deliverable | total | closed | open | percent_complete |\n| ----------- | ----- | ------ | ---- | ---------------- |\n| Site launch | 4     | 2      | 2    | 0.5              |\n| API launch  | 4     | 3      | 1    | 0.75             |\n\n</details>"}
{"path":"infra/api/service/database.tf","language":"unknown","type":"code","directory":"infra/api/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/service/database.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/analytics/technical-overview.md\nLanguage: md\nType: code\nDirectory: documentation/analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/technical-overview.md\nSize: 1.88 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/api/service/dev.s3.tfbackend","language":"unknown","type":"code","directory":"infra/api/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/service/dev.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"## Key technologies\n\nThe analytics package is written in Python. It marshalls data using pandas, exposes a command line interface using typer, and connects to Postgres via SQLAlchemy:\n\n- [pandas][pandas-docs] ([source code][pandas-source]) - Used to manipulate and analyze data\n- [typer][typer-docs] ([source code][typer-source]) - Used to build the CLI for the analytics package\n- [sqlalchemy][sqlalchemy-docs] ([source code][sqlalchemy-source]) - Used to connect python code to Postgres database\n- [dynaconf][dynaconf-docs] ([source code][dynaconf-source]) - Used to manage configuration variables and secrets\n\n## Integrations\n\nIn order to obtain input data for our pipeline, the analytics package is integrated with GitHub. Integrations of third-party tools, such as GitHub, are encapsulated and maintained as individual modules in [analytics/integrations](../../analytics/src/analytics/integrations/).\n\n### GitHub\n\nWe use the GitHub GraphQL API to export data from GitHub. \n\n- [GitHub integration](../../analytics/src/analytics/integrations/github/)\n\n## Orchestration\n\nIn current practice, the service is triggered daily via an AWS Step Function orchestrated with Terraform. The service may also be triggered on-demand in the AWS console (access privs required). \n\n- [Terraform config file](../infra/analytics/app-config/env-config/scheduled_jobs.tf)\n- [AWS Step Function console](https://us-east-1.console.aws.amazon.com/states/home?region=us-east-1#/statemachines)\n\n<!-- Key technologies -->\n[sqlalchemy-docs]: https://www.sqlalchemy.org\n[sqlalchemy-source]: https://github.com/sqlalchemy/sqlalchemy\n[pandas-docs]: https://pandas.pydata.org/docs/index.html\n[pandas-source]: https://github.com/pandas-dev/pandas\n[typer-docs]: https://typer.tiangolo.com/\n[typer-source]: https://github.com/tiangolo/typer\n[dynaconf-docs]: https://www.dynaconf.com/\n[dynaconf-source]: https://github.com/dynaconf/dynaconf"}
{"path":"infra/api/service/image_tag.tf","language":"unknown","type":"code","directory":"infra/api/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/service/image_tag.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/analytics/testing.md\nLanguage: md\nType: code\nDirectory: documentation/analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/testing.md\nSize: 7.98 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/api/service/main.tf","language":"unknown","type":"code","directory":"infra/api/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/service/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"## Running tests\n\nAll tests are automatically run as part of the [`ci-analytics.yml` GitHub Action](../../.github/workflows/ci-analytics.yml) that is triggered each time changes are pushed to a pull request which modifies the `analytics` package.\n\nThese CI checks calls `make test-audit` which runs the unit tests and validates that test coverage is above 80% for the `analytics` package. During development, you can also use the following commands to run all or part of the test suite.\n\n### Running all tests with coverage\n\nRun `make unit-test` from the root of the `analytics/` sub-directory to run all unit tests and print out a test coverage report.\n\n### Running specific set of tests\n\nTo have greater control over which tests are run, you'll need to use `poetry run pytest` to access the pytest command line. The following commands show you how to invoke progressively smaller subsets of tests:\n\n- `poetry run pytest` Runs all tests automatically collected in [test discovery](https://docs.pytest.org/en/stable/goodpractices.html#conventions-for-python-test-discovery)\n- `poetry run pytest tests/datasets/` Runs all collected tests in the\n  `analytics/tests/datasets/` sub-directory\n- `poetry run pytest tests/dataset/test_base.py` Runs all collected the tests in\n  the `analytics/tests/datasets/test_base.py` file\n- `poetry run pytest tests/dataset/test_sprint_board.py::TestSprintBoard` Runs\n  all tests in the `TestSprintBoard` class in `test_base.py`\n\nVisit the pytest docs for additional information about [testing invocation and usage](https://docs.pytest.org/en/6.2.x/usage.html).\n\n### Using pytest flags\n\nEach of the test execution commands above can be modified or extended with a series of flags. Pytest documentation has [the full list of flags available](https://docs.pytest.org/en/stable/reference/reference.html#command-line-flags), but some commonly used flags include:\n\n- `-x` Stands for \"exit first\" and exits test execution as soon as a test fails\n- `--lf` Stands for \"last fail\" and runs only the tests that failed during the most recent test execution\n- `-v` Stands for \"verbose\" and prints out the name of each test being executed and whether it passed or failed\n- `-vv` Stands for \"very verbose\" and prints out detailed diffs between actual and expected outputs for failed tests\n\n## Writing tests\n\n[pytest](https://docs.pytest.org) is our test runner, which is simple but powerful. If you are new to pytest, reading up on how [fixtures work](https://docs.pytest.org/en/latest/explanation/fixtures.html) in particular might be helpful as it's one area that is a bit different than is common with other runners (and languages).\n\n### Naming\n\npytest automatically discovers tests by [following a number of conventions](https://docs.pytest.org/en/stable/goodpractices.html#conventions-for-python-test-discovery)\n(what it calls \"collection\").\n\nFor this project specifically:\n\n- All tests live under `analytics/tests/`\n- Under `tests/`, the organization mirrors the source code structure\n  - The tests for [`analytics/src/analytics/datasets/`](../../analytics/src/analytics/datasets/)\n    are found at [`analytics/tests/datasets/`](../../analytics/tests/datasets/)\n  - The tests for [`analytics/src/analytics/metrics/`](../../analytics/src/analytics/metrics/)\n    are found at [`analytics/tests/metrics/`](../../analytics/tests/metrics/)\n- Integration tests have their own dedicated `tests/integrations/` testing sub-directory\n- Create `__init__.py` files for each directory. This helps [avoid name conflicts\n  when pytest is resolving tests](https://docs.pytest.org/en/stable/goodpractices.html#tests-outside-application-code).\n- Test files should begin with the `test_` prefix, followed by the module the tests\n  cover, for example, a file `foo.py` will have tests in a file `test_foo.py`.\n- Test cases should begin with the `test_` prefix, followed by the function it's\n  testing and some description of what about the function it is testing.\n  - In `tests/datasets/test_base/test_base.py`, the `test_to_and_from_csv`\n    function is a test (because it begins with `test_`).\n  - Tests can be grouped in classes starting with `Test`, methods that start with\n    `test_` will be picked up as tests, for example `TestFeature::test_scenario`.\n\nThere are occasions where tests may not line up exactly with a single source file, function, or otherwise may need to deviate from this exact structure, but this is the setup in general.\n\n> [!TIP]\n> Try to use test names that describe the expected behavior or the conditions you are testing. For example, `test_return_none_if_no_matching_sprint_exists()` is preferable to `test_get_sprint_name()`.\n>\n> If you can't easily describe the conditions or behavior you're testing, you may need to break your function up into multiple smaller tests. One common pattern is to create a test class (e.g. `TestGetSprintNameFromDate`) that represents the method or function you're testing and then create a test method for each test scenario (e.g. `test_return_previous_sprint_if_date_is_start_of_next_sprint()`)\n\n### conftest files\n\n`conftest.py` files are automatically loaded by pytest, making their contents available to tests without needing to be imported. They are an easy place to put shared test fixtures as well as define other pytest configuration (define hooks, load plugins, define new/override assert behavior, etc.).\n\nThey should never be imported directly.\n\nThe main `tests/conftest.py` holds widely useful fixtures included for all tests. Scoped `conftest.py` files can be created that apply only to the tests below them in the directory hierarchy, for example, the `tests/datasets/conftest.py` file would only be loaded for tests under `tests/datasets/`.\n\n[More info about conftest files](https://docs.pytest.org/en/latest/how-to/fixtures.html?highlight=conftest#scope-sharing-fixtures-across-classes-modules-packages-or-session)\n\n### Testing helpers\n\nIf there is useful functionality that needs to be shared between tests, but is only applicable to testing and is not a fixture, create modules under `tests/helpers/`.\n\nThey can be imported into tests from the path `tests.helpers`, for example, `from tests.helpers.foo import helper_func`.\n\n### Using factories\n\nOne of the more common use cases for helpers is generating factory functions or classes for test data that you want to use in your tests. Currently, the `analytics` package has a series of factory functions in `tests/helpers/factory.py`.\n\nThese factory functions (e.g. `json_issue_row()`) allow developers to generate rows of test data that are customized for the current test, while only declaring the test-specific fields:\n\n```python\nfrom tests.helpers.factor import (\n    DAY_0,\n    DAY_3,\n    sprint_row,\n)\n\ndef test_return_name_if_matching_sprint_exists():\n    \"\"\"Test that correct sprint is returned if date exists in a sprint.\"\"\"\n    # setup - create sample dataset\n    board_data = [\n        sprint_row(issue=1, sprint=1, sprint_start=DAY_0, sprint_length=3),\n        sprint_row(issue=2, sprint=1, sprint_start=DAY_0, sprint_length=3),\n        sprint_row(issue=3, sprint=2, sprint_start=DAY_3, sprint_length=3),\n    ]\n    # rest of the test below\n    ...\n```\n\n### Debugging tests\n\nAs the test suite begins to grow, running the full suite can take an increasingly long time to complete, which makes quickly iterating and debugging individual tests harder to do. In order to speed up iteration, a common workflow to follow when developing or testing a new feature includes:\n\n1. Create a new test (or modify an existing test) add `assert 0` at the bottom of the test to force it to fail.\n2. Run the test suite once with the optional `-x` flag to exit test execution when the new (or modified) test fails.\n3. Iterate on the code and quickly test changes by using the `--lf` flag to rerun just the last failed tests.\n4. Once the test checks for the behavior we expect *and* it runs successfully until `assert 0`, we know that we've implemented the new feature correctly.\n5. Remove `assert 0` from the test then re-run the full test suite (e.g. `poetry run pytest`) to ensure that all tests pass and there are no regressions."}
{"path":"infra/api/service/outputs.tf","language":"unknown","type":"code","directory":"infra/api/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/service/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/analytics/usage.md\nLanguage: md\nType: code\nDirectory: documentation/analytics\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/analytics/usage.md\nSize: 2.36 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/api/service/prod.s3.tfbackend","language":"unknown","type":"code","directory":"infra/api/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/service/prod.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"## Purpose\n\nThis package encapsulates a data-pipeline service and a CLI that can be used to discover and execute data-pipeline commands. This usage guide describes overall usage instructions and some common use cases. \n\n## Pre-Requisites \n\nThis guide assumes the analytics package is installed in the reader's local development environment. For more information about how to install and run the analytics package, visit the [Getting Started Guide for Developers](development.md).\n\n## Data Pipeline Commands\n\n### Using `make`\n\nCurrently available data-pipeline commands are defined in the project [`Makefile`](../../analytics/Makefile). Commands can be invoked from the command line, as in the following examples:\n\n- `make install` - Verify that dependencies are installed \n- `make db-migrate` - Initialize or update analytics database schema \n- `make gh-data-export` - Export issue and sprint data from GitHub to a local flatfile \n- `make gh-transform-and-load` - Transform GitHub project data and load it into the analytics database\n\n### Using the CLI\n\nThe CLI can be used to discover and interactively execute data-pipeline commands.  To run the CLI, type the following into the command line to view a list of available commands:\n\n```bash\npoetry run analytics --help \n```\n\n![Screenshot of passing the --help flag to CLI entry point](../../analytics/static/screenshot-cli-help.png)\n\nDiscover the arguments required for a particular command by appending the `--help` flag to that command:\n\n```bash\npoetry run analytics export gh_issue_data --help\n```\n\n![Screenshot of passing the --help flag to a specific command](../../analytics/static/screenshot-command-help.png)\n\n## Common Use Cases\n\n### Export GitHub Data\n\nExport SGG project data (sprints, issues, epics, deliverables, etc.) from GitHub for local analysis:\n\n```bash\nmake gh-data-export\n```\n\nThe output from this command can be found in the `analytics/data/` directory.\n\n### Transform Github Data and Load into Analytics DB\n\nAfter exporting SGG project data from GitHub, use `make` to transform and load the exported data into the local analytics database instance:\n```bash\nmake gh-transform-and-load\n```\n\nThe same can be achieved with the more verbose and more flexible `poetry` command:\n```bash\npoetry run analytics etl transform_and_load --issue-file ./data/test-etl-01.json --effective-date 2024-10-28 \n```"}
{"path":"infra/api/service/search.tf","language":"unknown","type":"code","directory":"infra/api/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/service/search.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/api/README.md\nLanguage: md\nType: code\nDirectory: documentation/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/README.md\nSize: 0.13 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/api/service/secrets.tf","language":"unknown","type":"code","directory":"infra/api/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/service/secrets.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"Documentation for the Flask-based API can be found in this directory and the [API README.md](../../api/README.md)."}
{"path":"infra/api/service/staging.s3.tfbackend","language":"unknown","type":"code","directory":"infra/api/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/service/staging.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"File: documentation/api/api-details.md\nLanguage: md\nType: code\nDirectory: documentation/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/api-details.md\nSize: 6.44 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/api/service/variables.tf","language":"unknown","type":"code","directory":"infra/api/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/api/service/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.141Z","content":"See [Technical Overview](./technical-overview.md) for details on the technologies used.\n\nEach endpoint is defined as a function using an APIFlask blueprint where we define the schema of the\nrequest and response objects, attach authentication, and otherwise configure the endpoint based on\nseveral decorators attached to the endpoint function.\n\nAn OpenAPI file is generated from this which can be found at [openapi.generated.yml](../../api/openapi.generated.yml).\n\n# Routes\nRoutes and Marshmallow schema are defined in the [api/src/api](../../api/src/api) folder.\n\nA route should only contain what is necessary to configure the inputs/outputs, and setup the DB session.\nA route should generally:\n* Define any request/response schemas\n* Define any additional schemas (ie. headers or path params)\n* Define the authentication for the route\n* Define any DB sessions that the route will require\n* Call the handler\n* Return the response\n\n## Defining a schema\n\nWe define schemas using [Marshmallow](https://marshmallow.readthedocs.io/en/stable/), but use our own derived version\nof the schemas, fields, and validators to be able to produce errors in the format we want. See [error-handling.md](./error-handling.md) for more details.\n\nThese schemas will be used to generate  the OpenAPI schema, setup validations that will run, and make it possible to have a Swagger endpoint available for use\n\nFor example, if we wanted to define an endpoint with a request like:\n```json\n{\n  \"name\": {\n      \"first_name\": \"Bob\",\n      \"last_name\": \"Smith\",\n      \"suffix\": \"SR\"\n  },\n  \"birth_date\": \"1990-01-01\"\n}\n```\nWe would define the Marshmallow schema in-python like so:\n```py\nfrom enum import StrEnum\nfrom src.api.schemas.extension import Schema, fields, validators\nfrom src.api.schemas.response_schema import AbstractResponseSchema\n\nclass Suffix(StrEnum):\n    SENIOR = \"SR\"\n    JUNIOR = \"JR\"\n\nclass NameSchema(Schema):\n    first_name = fields.String(\n        metadata={\"description\": \"First name\", \"example\": \"Jane\"},\n        validate=[validators.Length(max=28)],\n        required=True,\n    )\n    last_name = fields.String(\n        metadata={\"description\": \"Last name\", \"example\": \"Doe\"},\n        validate=[validators.Length(max=28)],\n        required=True,\n    )\n    suffix = fields.Enum(\n        Suffix, metadata={\"description\": \"Suffix\"}\n    )\n\nclass ExampleSchema(Schema):\n    name = fields.Nested(NameSchema())\n    birth_date = fields.Date(metadata={\"description\": \"Their birth date\"})\n\nclass ExampleResponseSchema(AbstractResponseSchema):\n    # Note that AbstractResponseSchema defines a message and status_code field as well\n    data = fields.Nested(ExampleSchema())\n```\n\nAnything specified in the metadata field is passed to the OpenAPI file that backs the swagger endpoint. The values\nthat can be passed through are defined in the [APIFlask docs](https://apiflask.com/openapi/#response-and-request-schema)\nbut it's recommended you try to populate the following:\n- description\n- example - if you want a specific default in the swagger docs\n- type - only necessary if the default can't be determined by APIFlask\n\nYou can specify validators that will be run when the request is being serialized by APIFlask\n\nDefining a response works the exact same way however field validation does not occur on response, only formatting.\nTo keep our response schema following a consistent pattern, we have a few base schema classes like [AbstractResponseSchema](../../api/src/api/schemas/response_schema.py)\nthat you can derive from for shared values like the message.\n\n### Schema tips\n\n`required=True` does not mean a field isn't nullable, it means the field must be set to something.\nIf you have a field named `my_field` that is required then \n- `{\"my_field\": null}` is perfectly valid\n- `{}` would fail validation\n\nIf the value we need to have in the request, and what we want to call it in-code are different, you can specify a\n`data_key` for the request value and Marshmallow will map it for you.\n\nWhen providing an example for a value, consider what is a reasonable test default. Ideally it should pass any validations\nand be representative of real data. For example, we often specify zip codes as several components use those zip codes.\n\nNested fields:\n- If you define a field as `my_field = fields.Nested(AnotherSchema())`, don't provide a metadata as in some cases it seems to cause an issue with openapi generation\n\n## Defining an endpoint\n\nTo define an endpoint, you need to attach a route definition to a function.\n\nFor example, if we wanted to use the above schema and create a `POST /example-route/<uuid:example_id>` we would\ndefine it like so:\n\n\n```py\nfrom src.auth.api_key_auth import api_key_auth\nfrom apiflask import APIBlueprint\nimport src.api.response as response\n\nexample_blueprint = APIBlueprint(\"Example\", __name__, tag=\"Example\")\n\n@example_blueprint.post(\"/example-route/<string:example_id>\")\n@example_blueprint.input(ExampleSchema, arg_name=\"body\") # As defined above, arg_name is used to map to the field in the function below\n@example_blueprint.output(ExampleResponseSchema)\n@example_blueprint.auth_required(api_key_auth) # The HTTPTokenAuth object that has a registered authentication function\ndef post_example_route(example_id: str, body: dict) -> response.ApiResponse:\n    # Implement API logic\n\n    return response.ApiResponse(\"Example success message\", data={\"example_id\": \"abcd1234\"})\n```\n\nThe API itself is defined first, including any route parameters. We then specify the schema that will define\nthe input and output of the request. Finally we define any authentication + specifically specify the auth used\nso that OpenAPI will give you an authentication pop-up to fill out for specific endpoints.\n\nWhen you define a blueprint like this, it needs to be registered with the APIFlask app, which we do in [app.py](../../api/src/app.py)\nby calling `app.register_blueprints(example_blueprint)`.\n\n# Swagger\n\nThe Swagger UI  can be reached locally at [http://localhost:8080/docs](http://localhost:8080/docs) when running the API.\n![Swagger UI](./images/swagger-ui.png)\n\nThe actual openapi spec generated that backs this swagger UI can be seen at [http://localhost:8080/openapi.json](http://localhost:8080/openapi.json)\n\nEach of the endpoints you've described in API blueprints will appear here, organized based on their defined tags. For any endpoints with authentication added, you can add your authentication information by selecting `Authorize` in the top right.\n![Swagger Auth](./images/swagger-auth.png)\n\nAll model schemas defined can be found at the bottom of the UI."}
{"path":"infra/example.s3.tfbackend","language":"unknown","type":"code","directory":"infra","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/example.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/api-versioning.md\nLanguage: md\nType: code\nDirectory: documentation/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/api-versioning.md\nSize: 4.14 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/frontend/app-config/build_repository.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/build_repository.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"This document will detail how we handle versioning our API right now, as well a long-term idea that we will want to circle back to.\n\n## Why versioning?\n\nWhile many changes we plan to make to our API should be backwards-compatible, there will be times when we have to make\nchanges that would break our clients from calling our API (eg. making a field required that was previously optional).\n\nBy having versioned endpoints, we can give our clients time to update their calls until the old version of the endpoint can be fully deprecated.\n\n# Current Implementation\n\nEndpoints are versioned in their routes. For example, if our `GET` opportunity endpoint is `GET /v1/opportunities/:opportunityId` and\nwe made a change that was not backwards compatible (let's say we change the authentication approach), then we'd make an entirely\nnew endpoint called `GET /v2/opportunities/:opportunityId`. The `v1` endpoint would still exist, and most of the logic would likely be shared\nbetween the implementations, but they would be two entirely separate endpoints as far as API routing is concerned.\n\nWhile we are in the early phases of developing the API, prior to having any significant users of the API, we won't worry about\nmaintaining the backwards compatability. We won't start adding new versions of the API until we would begin impacting production\nsystems of our users.\n\n# Long-term idea using headers\n\nAn alternative approach we could take in the future is to instead put the versioning in a header parameter (like the `content-type` field) similar to [Stripe](https://stripe.com/blog/api-versioning).\n\nHowever, that approach has a few complexities due to the libraries we use for defining schemas and endpoints:\n\n* [webargs](https://webargs.readthedocs.io/en/latest/index.html) - a generic framework for parsing HTTP requests, and can serialize/deserialize using Marshmallow schemas.\n* [apispec](https://apispec.readthedocs.io/en/latest/) - a tool that can generate OpenAPI specifications from Marshmallow schemas.\n* [APIFlask](https://apiflask.com/) - the framework we use that connects Flask, webargs, and apispec together for us.\n\nWhen we construct a route, we specify the input & output Marshmallow schemas, which APIFlask passes to webargs & apispec. The OpenAPI specification is generated entirely at app start-up, and\nis a static file (we also generate the [file statically](https://github.com/HHS/simpler-grants-gov/blob/main/api/openapi.generated.yml) in our repo using the same underlying code).\nFor webargs, it registers the schemas with the routes, and when the internal routing of a request is going on, it will use the registered schema to do validation.\n\nWebargs does seem to have support for choosing the schema for an endpoint during request handling, as the \"schema\" can be either a schema or a `callable` object that\ntakes in the request and returns a `Schema` object. That request object contains the header, so a callable could be used to choose the right schema for a users given request.\n\nHowever, the `input` and `output` methods for APIFlask do not take in multiple Schema or a callable, and would need to be rewritten.\n\n## Rough Ideas\nModify the [input](https://apiflask.com/api/blueprint/#apiflask.scaffold.APIScaffold.input) method in APIFlask to be capable of taking in a callable. We could\neither override the method, or look into making the change in APIFlask itself which is open source.\n\nWhen this method calls webargs' [use_args](https://webargs.readthedocs.io/en/latest/api.html#webargs.core.Parser.use_args) method, instead pass in a callable\nthat returns a `Schema` based on the header.\n\nWe also need to further investigate the apispec setup to verify we can actually generate an OpenAPI specification that works with this approach.\nOpenAPI does have support for switching the `content-type` on the UI, we would need to specify each separate version of a schema as `application/json+<whatever_version>`.\nCurrently, APIFlask defaults the body `content-type` to `application/json`.\n\nOther miscellaneous problems to solve:\n* How would we manage multiple schemas for a single endpoint?\n* If we want to version something other than the body (header, query param, form), would this approach work?"}
{"path":"infra/frontend/app-config/dev.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/dev.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/authentication.md\nLanguage: md\nType: code\nDirectory: documentation/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/authentication.md\nSize: 7.21 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/frontend/app-config/env-config/database.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/env-config/database.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"## Login Flow\n```mermaid\nsequenceDiagram\n    autonumber\n    participant frontend\n    participant api\n    participant login.gov\n\n    frontend->>+api: user clicks login\n    Note over api: /users/login\n\n    api->>-login.gov: redirect\n    activate login.gov\n    Note over login.gov: /authorize\n    login.gov->>+api: redirect\n    deactivate login.gov\n\n    Note over api: /users/login/callback\n    break if an error occurs\n        api->>frontend: redirect\n    end\n    api-->>login.gov: POST\n    login.gov-->>api: id_token\n    Note over login.gov: /token\n\n    api->>-frontend: redirect\n    Note over frontend: /auth/callback\n  ```\n\nAt a high level, a user goes to our login endpoint, gets redirected to login.gov, logs in there,\nand then gets redirected back to our API where we then create a session attached to the user\nin our DB and return a key that can be used in other endpoints.\n\n### Login\nFrom the frontend, a user selects login, which sends them to the API's /users/login endpoint.\n\nThis endpoint sets up the parameters to call the [login.gov /authorize](https://developers.login.gov/oidc/authorization/) endpoint\nwhich consists of several static configuration values (including where they should redirect when finished) plus two that change every request:\n* state - A UUID that we will receive back from login.gov after the redirect\n* [nonce](https://openid.net/specs/openid-connect-core-1_0.html#NonceNotes) - A UUID that we will receive back after requesting a token, helps prevent replay attacks\n\nWe store the state and nonce value in our `login_gov_state` DB table.\n\n### Login.gov Authorize\nA user logs into their account or makes a new account. After successfully completing\nthis process they are redirected back to our /users/login/callback endpoint where\nthe rest of the login process will occur.\n\n### Callback Validation\nWe [receive a redirect back](https://developers.login.gov/oidc/authorization/#authorization-response) from login.gov with a code, state, and optionally errors if there was an issue.\n\nIf any error occurred, error.\n\nWe use the state value to fetch the state we stored previously in our database.\nIf the state does not exist, error.\n\nIf there was a state in our DB, before any further processing we delete the record.\nEven if we later error, we do not want a state value to ever be re-usable to avoid\nthe potential for [replay attacks](https://en.wikipedia.org/wiki/Replay_attack).\n\n### Getting the Login.gov Token\nUsing the code we received back from login.gov, we call their [token endpoint](https://developers.login.gov/oidc/token/).\nTo call the endpoint we also need a [client_assertion jwt](https://developers.login.gov/oidc/token/#client_assertion)\nwhich is created from a private key we generated, and gave Login.gov the public key certificate.\n\nIf there is any error with [the response](https://developers.login.gov/oidc/token/#token-response) from login.gov, error.\n\nWe parse the JWT, validating it against the [public keys](https://developers.login.gov/oidc/certificates/)\nprovided by login.gov. We refresh these in the event we don't find a matching key based on the KID (key ID).\n\nWe additionally validate that the token is valid across several other fields (expired, audience, issuer, etc), but\nmost importantly we check that the `nonce` value in the key matches the one we originally sent to login.gov.\nIf any of these validations fail, we error.\n\n### Creating our own JWT\n\nIf all validation passed, we'll then check if the `sub` value from the token matches\nany user in `link_external_user` table. If not, we'll create a record + create a user.\nIn either case, we'll update the email from login.gov in this table.\n\nWe create a `user_token_session` object in our database which has a `token_id`, `expires_at` timestamp, and `is_valid` boolean.\n\nThe expiration of the token we create is not managed in the token itself, but instead in our database, and can be managed\nby our logout and refresh endpoints.\n\nWith this session token we generate a JWT containing the `token_id`, `user_id` and a few miscellaneous JWT attributes.\n\n### Redirect to final destination\nWith the token we redirect to the configured final destination URL with a message, token, and whether the user was new.\n\nIn future iterations we may adjust this logic to allow for the final destination to be set by the caller in the /users/login endpoint, however\nfor security reasons we'll want to limit where we can send to.\n\n## Error handling\nBecause these aren't standard GET/POST/etc. endpoints and instead handle\nall responses with redirects, we redirect to the same final destination in the event\nan error occurs. We send an `error` and `error_description` parameter with rough\ninformation about what issue occurred.\n\nIn both our login and callback endpoints we wrap the entire endpoint in a try/catch\nblock that handles catching errors and doing this redirect to the final destination.\n\nMost errors fall into two categories:\n* Misconfiguration in the API\n* Someone is calling the callback URL directly with random data\n\nBecause we need to protect ourselves against both of these, many error scenarios\nare actually not something we would ever expect a user to hit.\n\n## Logout\nThe logout endpoint will log out the session associated with the passed-in token.\n\nIt does this by setting the `is_valid` boolean to False.\n\n## Refresh\nThe refresh endpoint will extend how long the token is valid for of the passed-in token.\n\nIt does this by setting the `expires_at` timestamp to 30 minutes from now.\n\nIf a token is already expired, it cannot be refreshed.\n\n## Using the token\nIn endpoints that support user authentication, you can pass a header like `X-SGG-Token <your token>` in order to authenticate.\n\nYou can see which endpoints currently support this by going to our [OpenAPI docs](https://api.simpler.grants.gov/docs)\nand finding which endpoints can consume the `X-SGG-Token` header.\n\nWhen an API endpoint that is configured to use JWT auth receives a token, it parses and validates the fields.\nWe use the public key we generated to verify that we were the ones who originally generated the token which\nis largely where the security comes from.\n\nIf a token doesn't exist, is no longer valid, or has expired, we reject the request with a 401.\n\n### Example\nTo enable this auth for a given endpoint, add `api_jwt_auth` to the auth required parameter\nof your endpoint. Additionally, you can fetch the user token session object from the auth.\n\n```py\nfrom src.adapters import db\nfrom src.adapters.db import flask_db\nfrom src.api import response\nfrom src.auth.api_jwt_auth import api_jwt_auth\n\n@example_blueprint.post(\"/my-example\")\n@example_blueprint.auth_required(api_jwt_auth)\n@flask_db.with_db_session()\ndef user_token_logout(db_session: db.Session) -> response.ApiResponse:\n\n    user_token_session: UserTokenSession = api_jwt_auth.current_user  # type: ignore\n    with db_session.begin():\n        do_something(user_token_session.user)\n\n    return response.ApiResponse(\"success\")\n```\n\n# System-to-system Auth\nOur system-to-system (S2S) authentication approach right now is in development, while a basic key auth\napproach currently exists, further improvements are being scoped out."}
{"path":"infra/frontend/app-config/env-config/environment-variables.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/env-config/environment-variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/database/database-access-management.md\nLanguage: md\nType: code\nDirectory: documentation/api/database\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/database/database-access-management.md\nSize: 4.53 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/frontend/app-config/env-config/file_upload_jobs.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/env-config/file_upload_jobs.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"This document describes the best practices and patterns for how the application accesses and updates data in the database.\n\n## Client Initialization and Configuration\n\nThe database client is initialized when the application starts (see [src/app.py](../../../api/src/app.py). The database engine that is used to create acquire connections to the database is initialized using the database configuration defined in [db_config.py](../../../app/src/db/db_config.py), which is configured through environment variables. The initialized database client is then stored on the Flask app's [\\`extensions\\` dictionary](https://flask.palletsprojects.com/en/2.2.x/src/#flask.Flask.extensions) to be used throughout the lifetime of the application.\n\n## Session Management\n\nA new database session should be created for each web request and passed into service methods. As a general rule, create the session outside of functions and objects that access and/or manipulate database data. This will greatly help with achieving a predictable and consistent transactional scope. For example:\n\nFor example, **do this**\n\n```python\n### right way ###\n\nfrom flask import current_app\nimport src.adapters.db as db\n\ndef some_service_func(session: db.Session)\n    with db_session.begin(): # start transaction\n        session.query(FooBar).update({\"x\": 5})\n\n@app.post(\"/some-service\")\ndef some_service_post():\n    db_client = db.get_db(current_app)\n    with db.get_session() as db_session: # create session\n        some_service_func(db_session)\n```\n\nand **don't do this**\n\n```python\n### wrong way ###\n\nfrom flask import current_app\nimport src.adapters.db as db\n\ndef some_service_func()\n    db_client = db.get_db(current_app)\n    with db.get_session() as db_session:\n        with db_session.begin():\n            session.query(FooBar).update({\"x\": 5})\n\n@app.post(\"/some-service\")\ndef some_service_post():\n    some_service_func()\n```\n\nFor more information, see [SQLAlchemy FAQ: When do I construct a \\`Session\\`, when do I commit it, and when do I close it?](https://docs.sqlalchemy.org/en/14/orm/session_basics.html#session-faq-whentocreate)\n\n## Transactions\n\nMake sure to understand the concept of transactions and the [ACID properties of transactions](https://en.wikipedia.org/wiki/ACID).\n\nTo start a new transaction, use `session.begin()`. For example\n\n```python\ndef service_method(session: db.Session):\n    with session.begin(): # start database transaction\n        session.query(...)\n        session.add(...)\n        session.delete(...)\n```\n\nIt is important that any database write operations occur in the same transaction as database read operations that the writes are based on. This is especially important since the application has `session.expire_on_commit` set to `False`, so unintentionally accessing model objects that were read from the database in a closed transaction will not produce an obvious error.\n\nFor example, **don't do this**:\n\n```python\n# incorrect\n\ndef withdraw(session: db.Session, account_id, amount):\n    with session.begin():\n        bank_account = session.query(BankAccount).get(account_id)\n    with session.begin():\n        if bank_account.balance >= amount: # the balance could be out of date!\n            bank_account.balance -= amount\n```\n\n### Code Smells\n\nIt is generally not necessary to [refresh or expire the session](https://docs.sqlalchemy.org/en/14/orm/session_state_management.html#refreshing-expiring). Doing so could lead to additional database queries being triggered.\n\nManually calls to refresh or expire a session could potentially indicate the need for separate transactions.\n\n## Database Sessions vs Web Sessions vs Test Sessions\n\nNote that a database session is not the same thing as a web session or a test session.\n\nA database session refers to the period of time during which a user is connected to a database and can execute database queries and transactions. In a web application, this is typically the lifetime of a single web request. Or for a batch job, this is typically the lifetime of the job. A web session, on the other hand, refers to the period of time during which a user interacts with a website. Finally, a test session is the period of time of a single run of the test suite, and is what is referred to in fixtures that have `scope=\"session\"`.\n\n## References\n\n* [SQLAlchemy FAQ: When do I construct a \\`Session\\`, when do I commit it, and when do I close it?](https://docs.sqlalchemy.org/en/14/orm/session_basics.html#session-faq-whentocreate)\n* [Refreshing / Expiring the Session](https://docs.sqlalchemy.org/en/14/orm/session_state_management.html#refreshing-expiring)"}
{"path":"infra/frontend/app-config/env-config/main.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/env-config/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/database/database-local-usage.md\nLanguage: md\nType: code\nDirectory: documentation/api/database\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/database/database-local-usage.md\nSize: 1.41 KB\nLast Modified: 2025-02-14T17:08:26.465Z"}
{"path":"infra/frontend/app-config/env-config/outputs.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/env-config/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"This document describes how you can access and use your local DB.\n\n## Connecting to the DB\n\n### UI\n\nThere are many different programs you can use to connect to your DB, for example: [pgAdmin](https://www.pgadmin.org/).\n\nSteps:\n1. Make sure your local DB is running, you can start it up by doing `make init-db` which will handle starting + running initial DB migrations\n2. Open your program for connecting to the DB, and add the connection parameters as shown below\n\n![Local DB Connection Parameters](../images/local-db-connection.png)\n\nConnection parameters for the database can be found in [local.env](../../../api/local.env) including the password.\n\n### CLI\n\nIf you have `psql` installed locally, you can also access the DB by running `psql -h localhost -d app -U app` which will require you enter the password afterwards.\n\nFor example:\n![Local DB CLI](../images/local-db-cli.png)\n\n## Seeding the DB\n\nYou can populate our opportunity data by running: `make db-seed-local` when you have the DB running.\n\nThis script currently creates a few dozen opportunities each time you run it in various scenarios (differing opportunity statuses, titles, etc.).\n\nIf you require a large amount of data locally, you can run `make db-seed-local args=\"--iterations 5\"` setting the iterations count to your desired number.\nThis effectively behaves like running the script multiple times in succession, and creates that many batches of opportunities."}
{"path":"infra/frontend/app-config/env-config/variables.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config/env-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/env-config/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/database/database-management.md\nLanguage: md\nType: code\nDirectory: documentation/api/database\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/database/database-management.md\nSize: 4.67 KB\nLast Modified: 2025-02-14T17:08:26.466Z"}
{"path":"infra/frontend/app-config/feature-flags.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/feature-flags.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"- [Database Management](#database-management)\n  - [Basic operations](#basic-operations)\n    - [Initialize](#initialize)\n    - [Start](#start)\n    - [Destroy and reinitialize](#destroy-and-reinitialize)\n  - [Running migrations](#running-migrations)\n  - [Creating new migrations](#creating-new-migrations)\n  - [Multi-head situations](#multi-head-situations)\n\n## Basic operations\n### Initialize\n\nTo start a local Postgres database container in a detached state and run any\npending migrations, run `make init-db`. During initial setup, `init-db` is called\nautomatically when running `make init`.\n\n### Start\n\nTo only start the database container, run the following command:\n\n```sh\nmake start-db\n```\nThis command is not needed when starting the application with `make start`\n\n### Destroy and reinitialize\n\nTo clean the database, use the following command:\n\n```sh\nmake volume-recreate\n```\n\nThis will remove _all_ docker project volumes, rebuild the database volume, and\nrun all pending migrations. Once completed, only the database container will be\nrunning. Simply run `make start` to bring up all other project containers.\n\n## Running migrations\n\nWhen you're first setting up your environment, ensure that migrations are run\nagainst your db so it has all the required tables. `make init` does this, but if\nneeding to work with the migrations directly, some common commands:\n\n```sh\nmake db-migrate       # Apply pending migrations to db\nmake db-migrate-down     # Rollback last migration to db\nmake db-migrate-down-all # Rollback all migrations\n```\n\n## Creating new migrations\n\nIf you've changed a python object model, auto-generate a migration file for the database and run it:\n\n```sh\n$ make db-migrate-create MIGRATE_MSG=\"<brief description of change>\"\n$ make db-migrate\n```\n\n<details>\n    <summary>Example: Adding a new column to an existing table:</summary>\n\n1. Manually update the database models with the changes ([example_models.py](/api/src/db/models/example_models.py) in this example)\n```python\nclass ExampleTable(Base):\n    ...\n    my_new_timestamp: Mapped[datetime] = mapped_column(TIMESTAMP(timezone=True)) # Newly added line\n```\n\n2. Automatically generate a migration file with `make db-migrate-create MIGRATE_MSG=\"Add created_at timestamp to address table\"`\n```python\n...\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"example_table\", sa.Column(\"my_new_timestamp\", sa.TIMESTAMP(timezone=True), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"example_table\", \"my_new_timestamp\")\n    # ### end Alembic commands ###\n```\n\n3. Manually adjust the migration file as needed. Some changes will not fully auto-generate (like foreign keys), so make sure that all desired changes are included.\n</details>\n\n## Multi-head situations\n\nAlembic migrations form an ordered history, with each migration having at least\none parent migration as specified by the `down_revision` variable. This can be\nvisualized by:\n\n```sh\nmake db-migrate-history\n```\n\nWhen multiple migrations are created that point to the same `down_revision` a\nbranch is created, with the tip of each branch being a \"head\". The above history\ncommand will show this, but a list of just the heads can been retrieved with:\n\n```sh\nmake db-migrate-heads\n```\n\nCI/CD runs migrations to reach the \"head\". When there are multiple, Alembic\ncan't resolve which migrations need to be run. If you run into this error,\nyou'll need to fix the migration branches/heads before merging to `main`.\n\nIf the migrations don't depend on each other, which is likely if they've\nbranched, then you can just run:\n\n``` sh\nmake db-migrate-merge-heads\n```\n\nWhich will create a new migration pointing to all current \"head\"s, effectively\npulling them all together.\n\nOr, if you wish to avoid creating extra migrations, you can manually adjust\nthe `down_revision` of one of the migrations to point to the other one. This\nis also the necessary approach if the migrations need to happen in a defined\norder.\n\n## Alembic Check\n\nTo make sure we haven't forgotten to generate migrations after modifying the database models,\nwe rely on [alembic check](https://alembic.sqlalchemy.org/en/latest/autogenerate.html#running-alembic-check-to-test-for-new-upgrade-operations)\nwhich can be run by `make db-check-migrations`. What this command does is compare the current local\ndatabase to your migration files, and see if anything would be generated if you attempted to create\na new migration. If there would be anything created, it errors.\n\nThis check also runs as part of our CI/CD, so even if you forget to run it yourself during development,\nit will be caught when you send a pull request."}
{"path":"infra/frontend/app-config/main.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/database/database-testing.md\nLanguage: md\nType: code\nDirectory: documentation/api/database\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/database/database-testing.md\nSize: 1.23 KB\nLast Modified: 2025-02-14T17:08:26.466Z"}
{"path":"infra/frontend/app-config/outputs.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"This document describes how the database is managed in the test suite.\n\n## Test Schema\n\nThe test suite creates a new PostgreSQL database schema separate from the `public` schema that is used by the application outside of testing. This schema persists throughout the testing session is dropped at the end of the test run. The schema is created by the `db` fixture in [conftest.py](../../../api/tests/conftest.py). The fixture also creates and returns an initialized instance of the [db.DBClient](../../../api/src/db/__init__.py) that can be used to connect to the created schema.\n\nNote that [PostgreSQL schemas](https://www.postgresql.org/docs/current/ddl-schemas.html) are entirely different concepts from [Schema objects in OpenAPI specification](https://swagger.io/docs/specification/data-models/).\n\n## Test Factories\n\nThe application uses [Factory Boy](https://factoryboy.readthedocs.io/en/stable/) to generate test data for the application. This can be used to create models `Factory.build` that can be used in any test, or to prepopulate the database with persisted models using `Factory.create`. In order to use `Factory.create` to create persisted database models, include the `enable_factory_create` fixture in the test."}
{"path":"infra/frontend/app-config/prod.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/prod.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/database/erds/README.md\nLanguage: md\nType: code\nDirectory: documentation/api/database/erds\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/database/erds/README.md\nSize: 0.77 KB\nLast Modified: 2025-02-14T17:08:26.466Z"}
{"path":"infra/frontend/app-config/staging.tf","language":"unknown","type":"code","directory":"infra/frontend/app-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/staging.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"Diagrams can be manually generated by running `make create-erds` from the api folder.\n\n# Dependencies\nIf running outside of Docker, you must install `graphviz` (`brew install graphviz`) for this to work, this should be automatically installed as part of the Dockerfile inside Docker.\n\n# Caveats\nThe diagrams generated are based on our SQLAlchemy models metadata, and not the database itself, so there are a few differences.\n\n* Property fields are SQLAlchemy only and generally represent relationships (ie. values fetched via a foreign key `join`)\n\n# Files\n\n## API Schema\n![API Table ERD](api-schema.png)\n\n## Staging Schema\n![Staging Table ERD](staging-schema.png)"}
{"path":"infra/frontend/build-repository/main.tf","language":"unknown","type":"code","directory":"infra/frontend/build-repository","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/build-repository/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/development.md\nLanguage: md\nType: code\nDirectory: documentation/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/development.md\nSize: 6.40 KB\nLast Modified: 2025-02-14T17:08:26.479Z"}
{"path":"infra/frontend/build-repository/shared.s3.tfbackend","language":"unknown","type":"code","directory":"infra/frontend/build-repository","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/build-repository/shared.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"This application is dockerized. Take a look at [Dockerfile](../../api/Dockerfile) to see how it works.\n\nA very simple [docker-compose.yml](../../docker-compose.yml) has been included to support local development and deployment.\n\nSeveral components like tests, linting, and scripts can be run either inside of the Docker container, or outside on your  machine.\n\n**Running in Docker is the default**, but on some machines like the M1 Mac, running natively may be desirable for performance reasons.\n\n## Docker\n\nThis section covers development using Docker. There are a number of Docker commands included in the [Makefile](../../api/Makefile) which are helpful for local development. Run `make help` for a list of commands.\n\n### Setup\n\nRun `make init && make run-logs` to start the local containers. The application will be available at `http://localhost:8080` and API documentation at `http://localhost:8080/docs`.\n\nThis stands up the following services:\n\n* Flask API (http://localhost:8080)\n* Postgres database\n* OpenSearch node\n* OpenSearch Dashboard (http://localhost:5601)\n* [localstack](https://www.localstack.cloud) for mocking s3 actions locally\n* [mock oauth2 server](https://github.com/navikt/mock-oauth2-server) (http://localhost:5001)\n\n### Seed data\n\nRun `make db-seed-local && make populate-search-opportunities` to create local data in the database and make it available in the API.\n\n### API Authenticaion\n\nThis API uses a very simple [ApiKey authentication approach](https://apiflask.com/authentication/#use-external-authentication-library) which requires the caller to provide a static key. This is specified with the `API_AUTH_TOKEN` environment variable.\n\n### User Authentication\n\nRun `make setup-env-override-file` to create the `override.env` file which will include the necessary JWT keys for running user authentication within the app.\n\n#### Mock Oauth2 Server\n\nA mock Oauth2 server is defined and managed in the API's [docker-compose.yml](../../api/docker-compose.yml) file. It creates a mock endpoint that is configured to work with the API to stand in for login.gov for local development, and is available at `http://localhost:5001` when running the API containers.\n\n### Environment Variables\n\nMost configuration options are managed by environment variables.\n\nEnvironment variables for local development are stored in the [local.env](../../api/local.env) file. This file is automatically loaded when running. If running within Docker, this file is specified as an `env_file` in the [docker-compose](../../docker-compose.yml) file, and loaded [by a script](../../api/src/util/local.py) automatically when running most other components outside the container.\n\nAny environment variables specified directly in the [docker-compose](../../docker-compose.yml) file will take precedent over those specified in the [local.env](../../api/local.env) file.\n\n### Troubleshooting\n\nErrors in standing up the API can originate from an out of date container, database syncronization, or other issues with previously created services. Helper functions are available to rebuild:\n\n* **db-check-migrations** - check if migrations are out of sync\n* **volume-recreate** - delete all existing volumes and data\n* **remake-backend** - delete all data (`volume-recreate`) and load data (`db-seed-local` and `populate-search-opportunities`)\n\n### VSCode Remote Attach Container Debugging\n\nThe API can be run in debug mode that allows for remote attach debugging (currently only supported from VSCode) to the container.\n\n- Requirements:\n\n  - VSCode Python extension\n  - Updated Poetry with the `debugpy` dev package in `pyproject.toml`\n\n- See `./vscode/launch.json` which has the debug config. (Named `API Remote Attach`)\n\n- Start the server in debug mode via `make start-debug` or `make start-debug run-logs`.\n\n  - This will start the `main-app` service with port 5678 exposed.\n\n- The server will start in waiting mode, waiting for you to attach the debugger (see `/src/app.py`) before continuing to run.\n\n- Go to your VSCode debugger window and run the `API Remote Attach` option\n\n- You should now be able to hit set breakpoints throughout the API\n\n## Local (non-Docker)\n\nRun `export PY_RUN_APPROACH=local` to run API and test functions locally when running commands in the Makefile. For example, `make test` or `make format` will run outside of Docker.\n\n**Note:** even with the native mode, many components like the DB and API will only ever run in Docker, and you should always make sure that any implementations work within Docker.\n\nRunning in the native/local approach may require additional packages to be installed on your machine to get working.\n\n### Prerequisites\n\n1. Install the version of Python specified in [pyproject.toml](../../api/pyproject.toml)\n   [pyenv](https://github.com/pyenv/pyenv#installation) is one popular option for installing Python,\n   or [asdf](https://asdf-vm.com/).\n   - If using pyenv run `pyenv local <version>` to ensure that version will be used in subsequent steps\n2. Ensure that `python -V` and `python3 -V` are picking up that version.\n   - If not, run `pyenv init -` and/or restart your shell to ensure it was run automatically\n3. After installing and activating the right version of Python, install\n   [poetry](https://python-poetry.org/docs/#installation) and follow the instructions to add poetry to your path if necessary.\n\n   ```bash\n   curl -sSL https://install.python-poetry.org | python3 -\n   ```\n\n4. You'll also need [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n\n**Note:** All the following commands should be run from the `/api` directory.\n\n### Database setup: Run Migrations/Seeds\n\nIf you haven't done local development before you'll need to execute the migrations and seed the DB with data using the steps in [database-local-usage.md](database/database-local-usage.md)\n\n### Services\n\nIndividual services can be run through Docker, which can be useful in concert with non-Docker application development:\n\n* **OpenSearch**\n  * Run `make init-opensearch` setup the OpenSearch Container\n  * Run `make populate-search-opportunities` to push data previously seeded in the DB into the search index\n\n  If your DB or OpenSearch end up in an odd place, you can reset all the persistent storage using `make volume-recreate`.\n\n* **Localstack (local s3)**\n   * Run `make init-localstack`\n* **Mock OAuth server**\n   * Run `make init-mock-oauth2`\n\n## Next steps\n\nNow that you're up and running, read the [application docs](../../api/README.md) to familiarize yourself with the application."}
{"path":"infra/frontend/service/.terraform.lock.hcl","language":"unknown","type":"code","directory":"infra/frontend/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/service/.terraform.lock.hcl","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/error-handling.md\nLanguage: md\nType: code\nDirectory: documentation/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/error-handling.md\nSize: 3.43 KB\nLast Modified: 2025-02-14T17:08:26.479Z"}
{"path":"infra/frontend/service/dev.s3.tfbackend","language":"unknown","type":"code","directory":"infra/frontend/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/service/dev.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"When an error occurs in an API endpoint, we want to return something formatted like:\n\n```json\n{\n  \"data\": {},\n  \"errors\": [\n    {\n      \"field\": \"path.to.field_name\",\n      \"message\": \"field is required\",\n      \"type\": \"required\",\n      \"value\": null\n    }\n  ],\n  \"message\": \"There was a validation error\",\n  \"status_code\": 422\n}\n```\n\nThis structure allows users of the API the ability to determine what the error was\nand for which field in the request programatically. While not every error will\nspecify a field, by having a consistent structure users are able to reliably\nimplement error handling.\n\nHowever, the various libraries we use don't structure errors this way, and so\nwe have several pieces of code to handle making everything fit together nicely.\n\nAPIFlask supports providing your own [error processor](https://apiflask.com/error-handling/)\nwhich we have implemented in [restructure_error_response](../../api/src/api/response.py)\n\n# Error Handling\n\n## Throwing an exception\n\nWhen an exception is thrown during an API call, this will result in a 500 error\nunless it is an exception that APIFlask has configured to return a different status code\nlike the ones from the `werkzeug` library. However, if you throw one of these\nexceptions, any message or other information is lost. To avoid this, instead\nuse our `raise_flask_error` function which handles wrapping exceptions in\na format that APIFlask will keep the context we add.\n\n```py\nfrom src.api.response import ValidationErrorDetail\nfrom src.api.route_utils import raise_flask_error\nfrom src.validation.validation_constants import ValidationErrorType\n\n\nfrom werkzeug.exceptions import NotFound\n\n\nraise_flask_error(NotFound.code, \"Unable to find the record requested\", validation_issues=[ValidationErrorDetail(message=\"could not find\", type=ValidationErrorType.UNKNOWN)])\n```\n\nThis would result in an error that looks like:\n```json\n{\n  \"data\": {},\n  \"errors\": [\n    {\n      \"field\": null,\n      \"message\": \"could not find\",\n      \"type\": \"unknown\"\n    }\n  ],\n  \"message\": \"Unable to find the record requested\",\n  \"status_code\": 404\n}\n```\n\nNote that the validation error detail list is optional, and generally only used\nfor validation errors.\n\n## Marshmallow Errors\n\nBy default, Marshmallow constructs its errors like:\n```json\n{\n  \"path\": {\n    \"to\": {\n      \"field\": [\"error message1\", \"error message2\"]\n    }\n  }\n}\n```\n\nThere are two issues with this, the path structure, and error message. Fixing the path\nstructure is simple and just requires flattening the dictionary, but the error messages\nunfortunately only provide a message, when we want a message AND code for the particular error.\n\nTo work around this challenge, we created our own derived versions of the Marshmallow schema,\nfield, and validator classes in the [extensions folder](../../api/src/api/schemas/extension).\n\nThese extend the Marshmallow classes to instead output their errors as a [MarshmallowErrorContainer](../../api/src/api/schemas/extension/schema_common.py)\n\nThis is done by modifying the default error message that each validation rule has to instead\nbe a `MarshmallowErrorContainer` object. For most of the fields, this is just a bit of configuration,\nbut the validators required re-implementing them as they handled errors directly in validation.\n\nWhen Marshmallow throws its errors, our [process_marshmallow_issues](../../api/src/api/response.py) function\nwill get called which handles flattening the errors, and then restructuring them into\nproper format."}
{"path":"infra/frontend/service/image_tag.tf","language":"unknown","type":"code","directory":"infra/frontend/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/service/image_tag.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/feature-flags.md\nLanguage: md\nType: code\nDirectory: documentation/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/feature-flags.md\nSize: 4.93 KB\nLast Modified: 2025-02-14T17:08:26.479Z"}
{"path":"infra/frontend/service/main.tf","language":"unknown","type":"code","directory":"infra/frontend/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/service/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"Feature flags are setup in the API to allow for configuring behavior of the endpoints.\n\nThe flags have a default value which can be adjusted by an environment variable per environment,\nand also allow for overriding it via headers in the API endpoints.\n\n## Naming Convention\n\nNaming of feature flags has the following convention:\n* Environment variables are always snake-case all-caps like `ENABLE_OPPORTUNITY_LOG_MSG`\n* Header fields are always prefixed with `X-FF` and are capitalized-kebab-cased like `X-FF-Enable-Opportunity-Log-Msg`\n\nThe configuration internally within the API helps setup these values and maintain consistency.\n\n\n# Adding a New Feature Flag\n\nAdd the flag to the `FeatureFlag` enum. The value of the enum will be used\nto generate the environment variable name as well as the header field name as described above.\n\n```py\nclass FeatureFlag(StrEnum):\n    # ... Existing flags\n\n    # This will be used as:\n    # Header: X-FF-Enable-Opportunity-Log-Msg\n    # EnvVar: ENABLE_OPPORTUNITY_LOG_MSG\n    ENABLE_OPPORTUNITY_LOG_MSG = \"enable_opportunity_log_msg\"\n```\n\nAdd the field to the `FeatureFlagConfig` class. Use the `get_env_var_name` function\nto tell Pydantic what environment variable name to look for. The first value passed\ninto the `Field` function is the default in case it cannot find the environment variable.\n\nNOTE: While this example uses a boolean, strings, numbers, and other fields would also\n      work in this same way, so long as you can parse the text of the header/environment variable.\n\n```py\nclass FeatureFlagConfig(PydanticBaseEnvConfig):\n    # ... Existing config\n\n    # ENABLE_OPPORTUNITY_LOG_MSG\n    enable_opportunity_log_msg: bool = Field(\n        False, alias=FeatureFlag.ENABLE_OPPORTUNITY_LOG_MSG.get_env_var_name()\n    )\n```\n\nYou can now load the environment variables by calling `get_feature_flag_config()`\nwhile in the API. Note that `initialize()` needs to be called before this works,\nwhich is done in `app.py`, so any API routes will have access to this.\n\nIf you wish to make this a header field that your route can take in, first define\na Marshmallow schema like so:\n\n```py\nclass OpportunitySearchHeaderSchema(request_schema.OrderedSchema):\n    # Header field: X-FF-Enable-Opportunity-Log-Msg\n    # data_key is what the field will be set as in the request\n    enable_opportunity_log_msg = fields.Boolean(\n        data_key=FeatureFlag.ENABLE_OPPORTUNITY_LOG_MSG.get_header_name(),\n        metadata={\"description\": \"Whether to log a message in the opportunity endpoint\"},\n    )\n\n    @post_load\n    def post_load(self, data: dict, **kwargs: Any) -> FeatureFlagConfig:\n        # This approach loads the feature flag config, merges in any overrides\n        # (the enable_opportunity_log_msg field from the header) and returns the feature\n        # flag config.\n        # post_load is called after all validations are done on a Marshmallow\n        # schema when loading from JSON and is a convenient place to convert to something\n        # other than a dictionary.\n        feature_flag_config = get_feature_flag_config()\n\n        enable_opportunity_log_msg = data.get(\"enable_opportunity_log_msg\", None)\n        if enable_opportunity_log_msg is not None:\n            feature_flag_config.enable_opportunity_log_msg = enable_opportunity_log_msg\n\n        return feature_flag_config\n```\n\nThen you can use your new schema like so in a route, specifying it as an additional input.\nBecause we added the post_load implementation, instead of receiving a dictionary, we have a properly typed object.\n\n```py\n@opportunity_blueprint.post(\"/v1/opportunities/search\")\n@opportunity_blueprint.input(opportunity_schemas.OpportunitySearchV0Schema, arg_name=\"search_params\")\n@opportunity_blueprint.input(\n      opportunity_schemas.OpportunitySearchHeaderV0Schema,\n      location=\"headers\",\n      arg_name=\"feature_flag_config\",\n)\n# many=True allows us to return a list of opportunity objects\n@opportunity_blueprint.output(opportunity_schemas.OpportunityV0Schema(many=True))\n@opportunity_blueprint.auth_required(api_key_auth)\n@flask_db.with_db_session()\ndef opportunity_search(\n        db_session: db.Session, search_params: dict, feature_flag_config: FeatureFlagConfig\n) -> response.ApiResponse:\n      if feature_flag_config.enable_opportunity_log_msg:\n            logger.info(\"Feature flag enabled\")\n\n      # ... the rest of the route implementation\n```\n\n# Current Feature Flags\n\n| Environment Variable       | Header Field | Description |\n|----------------------------| ------------ |-------------|\n| ENABLE_OPPORTUNITY_LOG_MSG | X-FF-Enable-Opportunity-Log-Msg | Placeholder for the implementation of the feature flag logic, just causes a small log message. |\n\n\n# Future Enhancements\n\nA few rough ideas for how we might expand feature flags in the future:\n* Make it possible to update the configuration of a running API (ie. loading feature flags from another configurable location periodically)\n* Setup a way for an api caller (eg. front-end website) to be able to fetch feature flag values from the API"}
{"path":"infra/frontend/service/outputs.tf","language":"unknown","type":"code","directory":"infra/frontend/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/service/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/formatting-and-linting.md\nLanguage: md\nType: code\nDirectory: documentation/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/formatting-and-linting.md\nSize: 1.17 KB\nLast Modified: 2025-02-14T17:08:26.479Z"}
{"path":"infra/frontend/service/prod.s3.tfbackend","language":"unknown","type":"code","directory":"infra/frontend/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/service/prod.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"## Formatting\n\nRun `make format` to run all of the formatters.\n\nWhen we run migrations via alembic, we autorun the formatters on the generated files. See [alembic.ini](../../api/src/db/migrations/alembic.ini) for configuration.\n\n### Isort\n[isort](https://pycqa.github.io/isort/) is used to sort our Python imports. Configuration options can be found in [pyproject.toml - tool.isort](../../api/pyproject.toml)\n\n### Black\n[black](https://black.readthedocs.io/en/stable/) is used to format our Python code. Configuration options can be found in [pyproject.toml - tool.black](../../api/pyproject.toml)\n\n## Linting\n\nRun `make lint` to run all of the linters. It's recommended you run the formatters first as they fix several linting issues automatically.\n\n### Flake\n[ruff](https://flake8.pycqa.org/en/latest/) is used to enforce a set of best practices for our Python code. Configuration options can be found in [pyproject.toml - tool.ruff](../../api/pyproject.toml).\n\n### Mypy\n[mypy](https://mypy.readthedocs.io/en/stable/) is used to validate and enforce typechecking in python. Configuration options can be found in [pyproject.toml - tool.mypy](../../api/pyproject.toml)"}
{"path":"infra/frontend/service/secrets.tf","language":"unknown","type":"code","directory":"infra/frontend/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/service/secrets.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/lookup-values.md\nLanguage: md\nType: code\nDirectory: documentation/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/lookup-values.md\nSize: 4.91 KB\nLast Modified: 2025-02-14T17:08:26.481Z"}
{"path":"infra/frontend/service/staging.s3.tfbackend","language":"unknown","type":"code","directory":"infra/frontend/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/service/staging.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"A lookup value is a set of allowed values for a given field. This includes:\n* Opportunity Categories (eg. discretionary, mandatory, continuation, earmark, other)\n* Opportunity Status (eg. forecasted, posted, closed, archived)\n* Funding Instrument (eg. cooperative agreement, grant, procurement contract, other)\n* and many more\n\nThese lookup values are used throughout the system serving as both the allowed values in requests,\nas well as the database. Each lookup value has a corresponding database table that dictates what\nvalues are allowed in the DB by using foreign keys.\n\n# Setting up a lookup value\n\nLookup values are defined in [lookup_constants.py](/api/src/constants/lookup_constants.py) and are\njust simple Python enums.\n\nFor example, if we wanted to add a new lookup value, we could add:\n\n```py\nfrom enum import StrEnum\n\nclass Example(StrEnum):\n    A = \"a\"\n    B = \"b\"\n    C = \"c\"\n    D = \"d\"\n```\n\nThis would allow any values a,b,c,d - note that the value, and not the name of the\nlookup variable is used. `A` would not be considered valid while `a` would.\n\n## Using a lookup value in a Marshmallow schema\n\nSee [routes](api-details.md#routes) for details on how the Marshmallow schemas work.\n\nEnums are directly supported as a field type, and only require specifying the field like:\n\n```py\nfrom src.api.schemas.extension import Schema, fields\n\nclass ExampleSchema(Schema):\n\n    my_example_field = fields.Enum(\n        Example, metadata={\"description\": \"The example field\", \"example\": Example.A}\n    )\n```\n\nThe OpenAPI schema generated will automatically handle specifying which fields are allowed\nand error if any non-specified values are submitted. No additional configuration necessary.\n\n## Lookup values in our Postgres DB\n\nIn order to provide additional validation at the DB layer, as well as make it clear what values are allowed\nwe have setup a lookup table for any lookup value that gets stored in the database.\n\nThese tables are always named as `lk_<lookup>`, and have two columns `<lookup>_id` and `description`.\n\nIn this case, we would want to make a table named `lk_example`, with `example_id` as a primary key.\n\nThe `<lookup>_id` column is an integer that we define, while the description is the value of the enum.\n\n### Setting up a lookup value in Postgres\n\nFirst, we need to define the `<lookup>_id` column value for each enum value. This is simply defined\nas a simple mapping next to the enum. For example, this would be defined as:\n\n```py\nfrom src.db.models.lookup  import LookupConfig, LookupStr\n\nEXAMPLE_CONFIG = LookupConfig([\n        LookupStr(Example.A, 1),\n        LookupStr(Example.B, 2),\n        LookupStr(Example.C, 3),\n        LookupStr(Example.D, 4),\n    ]\n)\n```\n\nThen we want to define the lookup table in [lookup_models.py](/api/src/db/models/lookup_models.py) as:\n\n```py\nfrom sqlalchemy.orm import Mapped, mapped_column\n\nimport src.constants.lookup_constants as lookup_constants\nfrom src.db.models.lookup import Lookup, LookupRegistry, LookupTable\nfrom src.db.models.base import TimestampMixin\n\n@LookupRegistry.register_lookup(lookup_constants.EXAMPLE_CONFIG)\nclass LkExample(LookupTable, TimestampMixin):\n    __tablename__ = \"lk_example\"\n\n    example_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str]\n\n    @classmethod\n    def from_lookup(cls, lookup: Lookup) -> \"LkExample\":\n        return LkExample(example_id=lookup.lookup_val, description=lookup.get_description())\n```\n\nThe table definition itself is pretty straightforward, and follows our usual approach to any SQLAlchemy model.\n\nThe [LookupRegistry](/api/src/db/models/lookup/lookup_registry.py) is a global registry that defines the table for each lookup value. This is used in two ways:\n1. When running DB migrations, the enum values will be merged into the corresponding Lookup table automatically\n2. When defining a database table with a foreign key to these lookup values, handles converting to/from an enum.\n\nIn order to reference these lookup values on another table, you just need to define it like so:\n\n```py\nimport uuid\nfrom typing import Optional\n\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy.orm import Mapped, mapped_column\n\nfrom src.db.models.base import TimestampMixin\nfrom src.db.models.lookup_models import LkExample\nfrom src.db.models.base import Base\nfrom src.adapters.db.type_decorators.postgres_type_decorators import LookupColumn\nfrom src.constants.lookup_constants import Example\n\nclass ExampleTable(Base, TimestampMixin):\n    __tablename__ = \"example_table\"\n\n    example_table_id: Mapped[int] = mapped_column(primary_key=True)\n\n    example: Optional[Example] = mapped_column(\n        \"example_id\", LookupColumn(LkExample), ForeignKey(LkExample.example_id)\n    )\n```\n\nWhen working with this DB model, you don't need to worry about the fact that the value in the DB\nis actually an integer, by having registered the lookup table, the `LookupColumn` type will automatically\nhandle converting to and from the DB and you only ever need to work with the enum value."}
{"path":"infra/frontend/service/variables.tf","language":"unknown","type":"code","directory":"infra/frontend/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/service/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/monitoring-and-observability/logging-configuration.md\nLanguage: md\nType: code\nDirectory: documentation/api/monitoring-and-observability\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/monitoring-and-observability/logging-configuration.md\nSize: 5.84 KB\nLast Modified: 2025-02-14T17:08:26.483Z"}
{"path":"infra/modules/auth-github-actions/README.md","language":"markdown","type":"code","directory":"infra/modules/auth-github-actions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/auth-github-actions/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.142Z","content":"## Overview\n\nThis document describes how logging is configured in the application. The logging functionality is defined in the [src.logging](../../../api/src/logging/) package and leverages Python's built-in [logging](https://docs.python.org/3/library/logging.html) framework.\n\n## Usage\nOur logging approach uses a context manager to start up and tear down logging. This is mostly done to prevent tests from\nsetting up multiple loggers each time we instantiate the logger.\n\nIf you wanted to write a simple script that could log, something like this would work\n\n```py\nimport logging\n\nimport src.logging\n\nlogger = logging.getLogger(__name__)\n\ndef main():\n    with src.logging.init(__package__):\n        logger.info(\"Hello\")\n```\n\nInitializing logging is only necessary once when running an application.\nLogging is already initialized automatically for the entire API, so any new\nendpoints or features built into the API don't require the above configuration.\n\nIf you just want to be able to writes logs, you usually only need to do:\n\n```py\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef my_function():\n    logger.info(\"hello\")\n```\n\n## Formatting\n\nWe have two separate ways of formatting the logs which are controlled by the `LOG_FORMAT` environment variable.\n\n`json` (default) -> Produces JSON formatted logs which are machine-readable.\n\n```json\n{\n    \"name\": \"src.api.healthcheck\",\n    \"levelname\": \"INFO\",\n    \"funcName\": \"healthcheck_get\",\n    \"created\": \"1663261542.0465896\",\n    \"thread\": \"275144058624\",\n    \"threadName\": \"Thread-2 (process_request_thread)\",\n    \"process\": \"16\",\n    \"message\": \"GET /v1/healthcheck\",\n    \"request.method\": \"GET\",\n    \"request.path\": \"/v1/healthcheck\",\n    \"request.url_rule\": \"/v1/healthcheck\",\n    \"request_id\": \"\"\n}\n```\n\n`human-readable` (set by default in `local.env`) -> Produces color coded logs for local development or for troubleshooting.\n\n![Human readable logs](human-readable-logs.png)\n\n## Logging Extra Data in a Request\n\nThe [src.logging.flask_logger](../../../api/src/logging/flask_logger.py) module adds logging functionality to Flask applications. It automatically adds useful data from the Flask request object to logs, logs the start and end of requests, and provides a mechanism for developers to dynamically add extra data to all subsequent logs for the current request.\n\nFor example, if you would like to add the `opportunity_id` to every log message during the lifecycle of a request, then you can do:\n```py\nimport logging\n\nfrom src.logging.flask_logger import add_extra_data_to_current_request_logs\n\nlogger = logging.getLogger(__name__)\n\n# Assume API decorators here\ndef opportunity_get(opportunity_id: int):\n    \n    add_extra_data_to_current_request_logs({\"request.path.opportunity_id\": opportunity_id})\n    \n    logger.info(\"Example log 1\")\n    \n    # ... some code\n    \n    logger.info(\"Example log 2\")\n```\n\nThis would produce two log messages, which would both have `request.path.opportunity_id=<the opportunity_id>` in the extra\ninformation of the log message. This function can be called multiple times, adding more information each time.\n\nNOTE: Be careful where you place the call to this method as it requires you to be in a Flask request context (ie. reached via calling Flask)\nIf you want to test a piece of code that calls `add_extra_data_to_current_request_logs` without going via Flask, you'll either need to restructure\nthe code, or \n\n## Automatic request log details\n\nSeveral fields are automatically attached to every log message of a request regardless\nof what you configure in the [flask_logger](../../../api/src/logging/flask_logger.py). This includes:\n* Request ID\n* Request method (eg. `POST` or `PATCH`)\n* Request path\n* Request URL rule\n* Cognito ID from the `Cognito-Id` header\n\nAdditionally when a request ends, the following will always be logged:\n* Status code\n* Content length\n* Content type\n* Mimetype\n* Request duration in milliseconds\n\n## PII Masking\n\nThe [src.logging.pii](../../../api/src/logging/pii.py) module defines a filter that applies to all logs that automatically masks data fields that look like social security numbers.\n\n## Audit Logging\n\nWe have configured [audit logging](../../../api/src/logging/audit.py) which logs\nvarious events related to the process making network, file system and\nsubprocess calls. This is largely done in the event we need to debug an\nissue or to detect if a library we are relying on is potentially doing\nsomething malicious (eg. a string parsing library shouldn't be making network requests).\n\nFor more information, please review the [audit events](https://docs.python.org/3/library/audit_events.html)\nPython docs, as well as [PEP-578](https://peps.python.org/pep-0578/) which details\nthe implementation details and thought process behind this feature in Python.\n\nAudit logging can be turned on/off by the `LOG_ENABLE_AUDIT` environment variable\nwhich is defaulted off for local development as audit logs are very verbose\n\nWe don't log 100% of audit events as they are very verbose. The first 10 events\nof a particular type will all log, but after than only every 10th will log, and\nonce it has reached 100 occurrences, only every 100th. This count will only\nconsider the 128 most recently logged scenarios where a scenario is based on the\ncall + arguments (ie. calls to the request library for different URLs are treated separately)\n\n### Audit Logging Tests\n\nThe audit logging unit tests that we have written are run separately\nfrom the rest of the tests as enabling audit logging causes several hundred\nlog messages to appear when run with everything else, and makes it very difficult\nto debug and fix your tests.\n\n## Additional Reading\n\n* [Python's Logging HOWTO](https://docs.python.org/3/howto/logging.html#logging-basic-tutorial)\n* [Python's logging module API docs](https://docs.python.org/3/library/logging.html)\n* [Formatter objects](https://docs.python.org/3/library/logging.html#formatter-objects)"}
{"path":"infra/modules/auth-github-actions/main.tf","language":"unknown","type":"code","directory":"infra/modules/auth-github-actions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/auth-github-actions/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/monitoring-and-observability/logging-conventions.md\nLanguage: md\nType: code\nDirectory: documentation/api/monitoring-and-observability\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/monitoring-and-observability/logging-conventions.md\nSize: 5.79 KB\nLast Modified: 2025-02-14T17:08:26.483Z"}
{"path":"infra/modules/auth-github-actions/variables.tf","language":"unknown","type":"code","directory":"infra/modules/auth-github-actions","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/auth-github-actions/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"## Purpose\n\nLogging is a valuable tool for engineering teams to support products in production. It enables debugging, real-time alerts, and real-time business metrics. This document is for engineers to have a set of guidelines and rationale for how to approach logging events in our system.\n\n## Principles\n\n### Make code observability a primary tool for debugging and reasoning about production code\n\nWhen a user runs into an issue in production, logs offer one of the primary ways of understanding what happened. This is especially important for situations where we canâ€™t or donâ€™t know how to reproduce the issue. In general it is not feasible to attach a debugger to production systems, or to set breakpoints and inspect the state of the application in production, so logs offer a way to debug through â€œprint statementsâ€.\n\n### Make it easy for on-call engineers to search for logs in the codebase\n\nWhile it is important to use logs to reason about the code, it is often equally important to be able to understand what code is being run from the logs. This is especially true for on-call engineers who may encounter production issues that are in areas of the codebase that they are not directly familiar with. Thus it is important to make it easy to find the associated place in the code that triggered logs.\n\n### Optimize for ease, flexibility, and speed of queries over DRY principles\n\nLog querying systems are often limited in their querying abilities. Most log databases are not relational databases, and as such to enable effective querying of logs, we should think about how we design the structure of our log messages and metadata.\n\n## Guidelines\n\n### General\n\n- **Minimize logic or dynamic behavior** â€“ Logs are often used for troubleshooting, so minimize dynamic behavior in logs to reduce the chance that the logging system itself could be contributing to adding complexity to an issue.\n- **Log all key points throughout the application** â€“ Include logs for:\n  - Important branches in business logic (so you can reason from the logs which branch the code took in production without the aid of a debugger)\n  - Before and after actions that are important or actions that have a nontrivial chance of failing (e.g. external service calls, or calls to legacy systems)\n- **Log both successes and failures** â€“ Successes can be useful for debugging what steps were completed successfully to narrow down what failed. In addition, logging successes can be useful for powering real-time business metrics.\n\n### Log event type\n\n- **INFO** â€“ Use INFO events to log something informational. This can be information that's useful for investigations, debugging, or tracking metrics. Note that events such as a user or client error (such as validation errors or 4XX bad request errors) should use INFO, since those are expected to occur as part of normal operation and do not necessarily indicate anything wrong with the system. Do not use ERROR or WARNING for user or client errors to avoid cluttering error logs.\n- **ERROR** â€“ Use ERROR events if the the system is failed to complete some business operation. This can happen if there is an unexpected exception or failed assertion. Error logs can be used to trigger an alert to on-call engineers to look into a potential issue.\n- **WARNING** â€“ Use WARNING to indicate that there *may* be something wrong with the system but that we have not yet detected any immediate impact on the system's ability to successfully complete the business operation. For example, you can warn on failed soft assumptions and soft constraints. Warning logs can be used to trigger notifications that engineers need to look into during business hours.\n\n### Log messages\n\n- **Standardized log messages** â€“ Consistently formatted and worded log messages easier to read when viewing many logs at a time, which reduces the chance for human error when interpreting logs. It also makes it easier to write queries by enabling engineers to guess queries and allow New Relic autocomplete to show available log message options to filter by.\n- **Statically defined log messages** â€“ Avoid putting dynamic data in log messages. Static messages are easier to search for in the codebase. Static messages are also easier to query for those specific log events without needing to resort to RLIKE queries with regular expressions or LIKE queries.\n\n### Attributes\n\n- **Log primitives not objects** â€“ Explicitly list which attributes you are logging to avoid unintentionally logging PII. This also makes it easier for engineers to know what attributes are available for querying, or for engineers to search for parts of the codebase that logs these attributes.\n- **Structured metadata in custom attributes** â€“ Put metadata in custom attributes (not in the log message) so that it can be used in queries more easily. This is especially helpful when the attributes are used in \"group by\" clauses to avoid needing to use more complicated queries.\n  - **system identifiers** â€“ Log all relevant system identifiers (uuids, foreign keys)\n  - **correlation ids** â€“ Log ids that can be shared between frontend events, backend logs, and ideally even sent to external services\n  - **discrete or discretized attributes** â€“ Log all useful non-PII discrete attributes (enums, flags) and discretized versions of continuous attributes (e.g. comment â†’ has_comment, household â†’ is_married, has_dependents)\n- **Denormalized data** â€“ Include relevant metadata from related entities. Including denormalized (i.e. redundant) data makes queries easier and faster, and removes the need to join or self-join between datasets, which is not always feasible.\n- **Fully-qualified globally consistent attribute names** â€“ Using consistent attribute names everywhere. Use fully qualified attribute names (e.g. application.application_id instead of application_id) to avoid naming conflicts."}
{"path":"infra/modules/canary/.terraform.lock.hcl","language":"unknown","type":"code","directory":"infra/modules/canary","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/canary/.terraform.lock.hcl","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/package-dependency-management.md\nLanguage: md\nType: code\nDirectory: documentation/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/package-dependency-management.md\nSize: 6.99 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/canary/canary_script.py","language":"python","type":"code","directory":"infra/modules/canary","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/canary/canary_script.py","size":1631931,"lastModified":"2025-02-14T17:08:31.142Z","content":"We use [Poetry](https://python-poetry.org/docs/) for managing our APIs dependencies.\n\nThe [pyproject.toml](../../api/pyproject.toml) file is where we configure the minimum or exact version of a dependency. These versions also get modified\nby the poetry commands (several described below) which can help generate / find the proper versions for a dependency.\n\nFrom this configuration, a [poetry.lock](../../api/poetry.lock) is generated which has the specific references to dependencies to be downloaded. The lock\nfile should never be manually modified and should always be generated by Poetry.\n\n# General Information\n\n## Dependency Versions\n\nIn our dependencies we could specify a package version as `1.2.0` or `^1.2.0` which each have specific semantic meaning (in this case, exactly `1.2.0` or `>=1.2.0 and <2.0.0`).\n\nSee [version constraints](https://python-poetry.org/docs/dependency-specification/#version-constraints) for further details.\n\n## Package Extras\n\nSome dependencies have \"extra\" packages that can be downloaded in addition to the main package for additional optional features.\nThese can be specified in the `pyproject.toml` file like so: `marshmallow-dataclass = {extras = [\"enum\", \"union\"], version = \"^8.5.8\"}` where\neach \"extra\" is a string in the extras list.\n\n## Dev Dependencies\n\nWe split dependencies into dev and non-dev dependencies. Dev dependencies are ONLY installed locally and in some CI/CD commands as they are only necessary for local development (including tests, linting, formatting, and dev utilities).\n\nMany of our dev tools are used for testing, including generating data which we want to explicitly avoid doing non-locally.\nDoing this also reduces the image size, and improves security as there are fewer dependencies that could potentially run on a production server.\n\nDependencies are grouped under `tool.poetry.dependencies` and `tool.poetry.group.dev.dependencies`. When adding a dependency,\nmake sure to put it in the proper group as when the API runs non-locally it will only have  packages from the first group.\n\n# Common Commands\n\nSee the Poetry docs for more details, but here are a few useful commands for managing package dependencies in common scenarios.\n\n## Installing\n\nInstalling is only necessary if you're running outside of Docker (eg. you've set the `PY_RUN_APPROACH=local` env variable)\n\n```shell\npoetry install --no-root --all-extras --with dev\n```\n\nIf you want to make sure any extra packages are cleaned up that you may have previously installed:\n\n```shell\npoetry install --no-root --all-extras --with dev --sync\n```\n\n## Adding a dependency or updating minimum version\n\nAdding a dependency of at least a specific version or changing the minimum version can be done with the [add](https://python-poetry.org/docs/cli/#add) command which will update the `pyproject.toml` file for you.\n\n```shell\npoetry add alembic@^1.12.0\n\n# Depending on your shell, you may need to escape ^ like so\npoetry add alembic@\\^1.12.0\n```\n\nAdding a dev dependency\n```shell\npoetry add mypy --group dev\n```\n\nUpdating the minimum version to the latest minor (major.minor.patch semver) version\n```shell\npoetry add mypy@latest --group dev\n\n# If it was currently ~1.6.1 and the latest was 1.8.0, then it would change to ~1.8.0\n```\n\n## Update dependencies to latest / update poetry.lock\n\n[poetry update](https://python-poetry.org/docs/cli/#update) will update the poetry.lock file following the rules from the pyproject.toml file.\n\n```shell\npoetry update\n\n# Or for a specific package\npoetry update mypy\n```\n\nNote that if this isn't updating your package (Poetry is saying `No dependencies to install or update`) make sure\nthe package version you want to upgrade to is allowed by the configuration in the pyproject.toml file.\n\n## Update Poetry itself\n\nAs I always lose [the command](https://python-poetry.org/docs/cli/#self-update), for your convenience:\n```shell\npoetry self update\n```\n\n# Verification\n\nAt a minimum, all of our formatting, linting, and tests should continue working when upgrading. If a dependency upgrade causes failures\nit will need to be investigated and fixed either by adjusting the code using it (eg. a method signature in a dependency changed) or adjusting\nthe version we use.\n\nLocally it is suggested you run the following commands after upgrading to verify the API still runs:\n```shell\n# Stop and delete DB volumes so it will be made fresh\nmake clean-volumes\n\n# Rebuild\nmake init\n\n# Optionally if running outside of the docker container\npoetry install --no-root --all-extras --with dev --sync\n\n# format / lint / test\nmake check\n\n# Run the API and verify it locally at http://localhost:8080/docs\nmake run-logs\n```\n\n## Specific dependency details\n\nOur primary dependencies that we should take note of when they are upgraded (with links to their change logs).\n\n* [black](https://black.readthedocs.io/en/stable/change_log.html) / [flake8](https://flake8.pycqa.org/en/latest/release-notes/index.html) / [isort](https://pycqa.github.io/isort/CHANGELOG.html) / [mypy](https://mypy-lang.org/news.html) - I suggest running `make format lint` after upgrading these and seeing if any files change / any issues are flagged as rules may have changed.\n* [APIFlask](https://apiflask.com/changelog/) - Running tests should verify everything is still connected properly, but you should also verify the OpenAPI docs are the same by running `make run-logs` and going to http://localhost:8080/docs\n* [SQLAlchemy](https://docs.sqlalchemy.org/en/latest/changelog/)\n* [Alembic](https://alembic.sqlalchemy.org/en/latest/changelog.html)\n* [Pydantic](https://docs.pydantic.dev/latest/changelog/)\n* [Marshmallow](https://marshmallow.readthedocs.io/en/stable/changelog.html)\n* [psycopg](https://www.psycopg.org/psycopg3/docs/news.html)\n\n# Upgrading Python\nPython does [yearly releases](https://devguide.python.org/versions/) for their minor versions (eg. 3.12 -> 3.13). They do not\nuse semvar versioning, and their [minor releases](https://devguide.python.org/developer-workflow/development-cycle/#devcycle) contain\nbreaking changes.\n\nPin to a specific minor version of python just in case anything would break when the yearly release does occur.\nOnly pin the minor versions of Python (eg. 3.12), and not the patch versions (eg. 3.12.1) as those contain bug and security fixes.\n\nAlong with any version upgrades, remember to:\n- Test the system functionality before deploying to production\n- Review the [changelog](https://docs.python.org/3/whatsnew/changelog.html)\nfor any breaking changes of features you may use\n\n## Upgrade Steps\nTo upgrade the Python version, make changes in the following places:\n1. Local Python version (see more about managing local Python versions in [development](./development.md))\n2. [Dockerfile](/api/Dockerfile)\n    search for the line `FROM python:3.12-slim as base` - supported versions can be found on [Dockerhub](https://hub.docker.com/_/python)\n3. [pyproject.toml](/api/pyproject.toml)\n    search for the line\n    ```toml\n    [tool.poetry.dependencies]\n    python = \"~3.12\"\n    ```\n   Then run `poetry lock --no-update` to update the [poetry.lock](/api/poetry.lock) file."}
{"path":"infra/modules/canary/main.tf","language":"unknown","type":"code","directory":"infra/modules/canary","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/canary/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/technical-overview.md\nLanguage: md\nType: code\nDirectory: documentation/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/technical-overview.md\nSize: 2.12 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/container-image-repository/main.tf","language":"unknown","type":"code","directory":"infra/modules/container-image-repository","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/container-image-repository/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"- [Key Technologies](#key-technologies)\n- [Request operations](#request-operations)\n- [Authentication](#authentication)\n- [Authorization](#authorization)\n\n## Key Technologies\n\nThe API is written in Python, utilizing APIFlask as the web application\nframework (with Flask serving as the backend for APIFlask). The API is\ndescribed in the code via [Marshmallow Dataclasses](https://apiflask.com/schema/#use-dataclass-as-data-schema)\n\nSQLAlchemy is the ORM, with migrations driven by Alembic. pydantic is used in\nmany spots for parsing data (and often serializing it to json or plain\ndictionaries). Where pydantic is not used, plain Python dataclasses are\ngenerally preferred.\n\n- [OpenAPI Specification][oas-docs]\n- [API Flask][apiflask-home] ([source code][apiflask-src])\n- [SQLAlchemy][sqlalchemy-home] ([source code][sqlalchemy-src])\n- [Alembic][alembic-home] ([source code](alembic-src))\n- [pydantic][pydantic-home] ([source code][pydantic-src])\n- [poetry](https://python-poetry.org/docs/) - Python dependency management\n\n[oas-docs]: http://spec.openapis.org/oas/v3.0.3\n[oas-swagger-docs]: https://swagger.io/docs/specification/about/\n\n[apiflask-home]: https://apiflask.com/\n[apiflask-src]: https://github.com/apiflask/apiflask\n\n[pydantic-home]:https://pydantic-docs.helpmanual.io/\n[pydantic-src]: https://github.com/samuelcolvin/pydantic/\n\n[sqlalchemy-home]: https://www.sqlalchemy.org/\n[sqlalchemy-src]: https://github.com/sqlalchemy/sqlalchemy\n\n[alembic-home]: https://alembic.sqlalchemy.org/en/latest/\n\n## Request operations\n\n- TODO - redo this\n\n## Authentication\n\nAuthentication methods are defined in the `security_scheme` config in\n`app.py`. A particular security scheme is enabled for a route via a\n`security` block on that route.\n\nFlask runs the authentication method specified in `api_key_auth.py`\nbefore passing the request to the route handler. \nIn the `api_key` security scheme, the `X-Auth` points to the\nfunction that is run to do the authentication.\n\n## Authorization\nn/a - Specific user authorization is not yet implemented for this API.\n\n### Database diagram\nn/a - Database diagrams are not yet available for this application."}
{"path":"infra/modules/container-image-repository/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/container-image-repository","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/container-image-repository/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/api/writing-tests.md\nLanguage: md\nType: code\nDirectory: documentation/api\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/api/writing-tests.md\nSize: 4.15 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/container-image-repository/variables.tf","language":"unknown","type":"code","directory":"infra/modules/container-image-repository","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/container-image-repository/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"[pytest](https://docs.pytest.org) is our test runner, which is simple but\npowerful. If you are new to pytest, reading up on how [fixtures\nwork](https://docs.pytest.org/en/latest/explanation/fixtures.html) in particular might be\nhelpful as it's one area that is a bit different than is common with other\nrunners (and languages).\n\n## Naming\n\npytest automatically discovers tests by [following a number of\nconventions](https://docs.pytest.org/en/stable/goodpractices.html#conventions-for-python-test-discovery)\n(what it calls \"collection\").\n\nFor this project specifically:\n\n- All tests live under `api/tests/`\n- Under `tests/`, the organization mirrors the source code structure\n  - The tests for `api/src/route/` are found at `api/test/api/route/`\n- Create `__init__.py` files for each directory. This helps [avoid name\n  conflicts when pytest is resolving\n  tests](https://docs.pytest.org/en/stable/goodpractices.html#tests-outside-application-code).\n- Test files should begin with the `test_` prefix, followed by the module the\n  tests cover, for example, a file `foo.py` will have tests in a file\n  `test_foo.py`.\n- Test cases should begin with the `test_` prefix, followed by the function it's\n  testing and some description of what about the function it is testing.\n  - In `tests/api/route/test_healthcheck.py`, the `test_get_healthcheck_200` function is a test\n    (because it begins with `test_`), that covers the `healthcheck_get` function's\n    behavior around 201 responses.\n  - Tests can be grouped in classes starting with `Test`, methods that start\n    with `test_` will be picked up as tests, for example\n    `TestFeature::test_scenario`.\n\nThere are occasions where tests may not line up exactly with a single source\nfile, function, or otherwise may need to deviate from this exact structure, but\nthis is the setup in general.\n\n## conftest files\n\n`conftest.py` files are automatically loaded by pytest, making their contents\navailable to tests without needing to be imported. They are an easy place to put\nshared test fixtures as well as define other pytest configuration (define hooks,\nload plugins, define new/override assert behavior, etc.).\n\nThey should never be imported directly.\n\nThe main `tests/conftest.py` holds widely useful fixtures included for all\ntests. Scoped `conftest.py` files can be created that apply only to the tests\nbelow them in the directory hierarchy, for example, the `tests/db/conftest.py`\nfile would only be loaded for tests under `tests/db/`.\n\n[More info](https://docs.pytest.org/en/latest/how-to/fixtures.html?highlight=conftest#scope-sharing-fixtures-across-classes-modules-packages-or-session)\n\n\n## Helpers\n\nIf there is useful functionality that needs to be shared between tests, but is\nonly applicable to testing and is not a fixture, create modules under\n`tests/helpers/`.\n\nThey can be imported into tests from the path `tests.helpers`, for example,\n`from tests.helpers.foo import helper_func`.\n\n## Using Factories\n\nTo facilitate easier setup of test data, most database models have factories via\n[factory_boy](https://factoryboy.readthedocs.io/) in\n`api/src/db/models/factories.py`.\n\nThere are a few different ways of [using the\nfactories](https://factoryboy.readthedocs.io/en/stable/#using-factories), termed\n\"strategies\": build, create, and stub. Most notably for this project:\n\n- The build strategy via `FooFactory.build()` populates a model class with the\n  generated data, but does not attempt to write it to the database\n- The create strategy via `FooFactory.create()` writes a generated model to the\n  database (can think of it like `FooFactory.build()` then `db_session.add()`\n  and `db_session.commit()`)\n\nThe build strategy is useful if the code under test just needs the data on the\nmodel and doesn't actually perform any database interactions.\n\nIn order to use the create strategy, pull in the `initialize_factories_session`\nfixture.\n\nRegardless of the strategy, you can override the values for attributes on the\ngenerated models by passing them into the factory call, for example:\n\n```python\nFooFactory.build(foo_id=5, name=\"Bar\")\n```\n\nwould set `foo_id=5` and `name=\"Bar\"` on the generated model, while all other\nattributes would use what's configured on the factory class."}
{"path":"infra/modules/database/authentication.tf","language":"unknown","type":"code","directory":"infra/modules/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/authentication.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/architecture/README.md\nLanguage: md\nType: code\nDirectory: documentation/architecture\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/architecture/README.md\nSize: 4.66 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/database/backups.tf","language":"unknown","type":"code","directory":"infra/modules/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/backups.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"This document is meant to be a living record of the architecture for the simpler.grants.gov system. This includes the application, network, and infrastructure architecture, as well as the CI/CD pipeline, and other services and integrations used to support the applications.\n\nAt a high level, this system uses Github to maintain the codebase repository and run the CI/CD pipeline, and AWS to host the applications and its supporting services.\n\n## Architecture\n\nThis is a general architecture diagram of the simpler.grants.gov system.\n\n```mermaid\n%%{init: {'theme': 'neutral' } }%%\nflowchart TB\n    %% CI/CD Pipeline\n    eng[\"Developers fas:fa-laptop-code\"] --\"Push to main branch fas:fa-code-branch\"--> GH\n    subgraph GH [\"Github fab:fa-github\"]\n        repo[Simpler Grants Repo]\n        click repo href \"https://github.com/HHS/simpler-grants-gov\" _blank\n    end\n    GH --Build and Deploys Image--> iam --> ecr\n    GH --Restarts task with new Image--> iam --> ECS\n```\n\n## AWS Hosted Infrastructure\n\nThis is an architecture diagram focusing on the AWS shared infrastructure managed by simpler.grants.gov\n\n![Grants gov system architecture](https://github.com/HHS/simpler-grants-gov/assets/5768468/15ac8a1e-f68c-4cdb-9a1b-e1e6c7302539)\n\nIt was generated via the LucidChart. You can find it inside the Nava LucidChart by viewing \"Shared With Me\" > \"Nava\" > \"Grants.gov\". You can also find it at [this shareable link](https://lucid.app/lucidchart/8d0fb4b2-fe85-4460-8df9-1255a506c5b6/edit?viewport_loc=-622%2C-233%2C5673%2C3098%2C0_0&invitationId=inv_a5fd77d9-d546-4b02-925a-6c3e254ccce7), if you already have access.\n\n## AWS Shared Services\n\nThe simpler.grants.gov is using the following non infrastructure shared services in AWS:\n\n- [ECS: Elastic Container Service](https://aws.amazon.com/ecs/)\n- [ECR: Elastic Container Registry](https://aws.amazon.com/ecr/)\n- [SSM: System Manager](https://docs.aws.amazon.com/systems-manager/latest/userguide/what-is-systems-manager.html)\n- [IAM: Identity and Access Management](https://aws.amazon.com/iam/)\n- [Cloudwatch](https://aws.amazon.com/cloudwatch/)\n- [Key Management Service](https://aws.amazon.com/kms/)\n\n## CI/CD Pipeline\n\nThis is a diagram focusing on the CI/CD pipeline\n\n```mermaid\n%%{init: {'theme': 'neutral' } }%%\n    flowchart TD\n        %% CI/CD Pipeline\n        eng[\"Developers fas:fa-laptop-code\"] --\"Push to main branch fas:fa-code-branch\"--> GH\n        subgraph GH [\"Github fab:fa-github\"]\n            repo[Simpler Grants Repo]\n            click repo href \"https://github.com/HHS/simpler-grants-gov\" _blank\n        end\n        subgraph AWS[HHS AWS Tenant]\n            ECR[\"AWS\n            Elastic Container Repository\"]:::ecs\n            ECSS[\"AWS\n            Elastic Container Service\"]:::ecs\n            ECR --> ECSS\n        end\n        GH --Build and deploys image--> ECR\n        GH --Restarts task with new image--> ECSS\n\n        classDef ecs fill:#FF9900,color:black\n```\n\n## Analytics Architecture\n\nThe \"analytics\" component of the application is the parts composed of the analytics service, the analytics PostgreSQL database, and Metabase. The analytics service is an ELT service that runs on AWS Step Functions via a cron trigger. At time of writing (May 2024), it collects its analytics data from GitHub. The analytics service is entirely composed of one-off tasks and does not deploy a load balancer. The analytics service extracts its data and loads it into the analytics database. The analytics database its an AWS RDS PostgreSQL database that is similar to, but distinct from, the main database that we use for our application. Metabase is a Business Intelligence dashboarding solution that we deploy. We deploy it behind a load balancer. That load balancer connects to the Metabase container. That Metabase container then connects to the analytics database, the same analytics database where the analytics ELT service is storing its data. Metabase is then configured to display charts and graphs of the data inside of the analytics database.\n\n## Relevant ADRs\n\n- [CI/CD Task Runner](../wiki/decisions/adr/2023-06-29-ci-cd-task-runner.md)\n- [Database Choices](../wiki/decisions/adr/2023-07-05-db-choices.md)\n- [Front-End Language](../wiki/decisions/adr/2023-07-10-front-end-language.md)\n- [Front-end Framework](../wiki/decisions/adr/2023-07-14-front-end-framework.md)\n- [Back-end Language](../wiki/decisions/adr/2023-06-30-api-language.md)\n- [Back-End Framework](../wiki/decisions/adr/2023-07-07-api-framework.md)\n- [Application Infrastructure Service](../wiki/decisions/adr/2023-07-20-deployment-strategy.md)\n- [Analytics Data Storage](../wiki/decisions/adr/2024-03-19-dashboard-storage.md)\n- [Analytics Dashboard Tool](../wiki/decisions/adr/2024-04-10-dashboard-tool.md)"}
{"path":"infra/modules/database/data/main.tf","language":"unknown","type":"code","directory":"infra/modules/database/data","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/data/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/frontend/README.md\nLanguage: md\nType: code\nDirectory: documentation/frontend\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/frontend/README.md\nSize: 0.16 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/database/data/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/database/data","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/data/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"Documentation for the Next.JS-based front-end can be found in this directory and the [frontend README.md](../../frontend/README.md)."}
{"path":"infra/modules/database/data/variables.tf","language":"unknown","type":"code","directory":"infra/modules/database/data","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/data/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/frontend/analytics.md\nLanguage: md\nType: code\nDirectory: documentation/frontend\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/frontend/analytics.md\nSize: 0.70 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/database/interface/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/database/interface","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/interface/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"The Next app uses the Next third party Google library to add the necessary scripts for instantiating Google Analytics (GA) on the site. To control reporting to different Google Analytics properties, we point the site at a Google Tag Manager (GTM) account ID, which manages the creation of the correct GA tags based on hostname.\n\n- When the hostname matches PROD (simpler.grants.gov), data will be reported to the production GA account\n- If the hostname matches Staging or DEV hostnames, data will be reported to the dev GA account\n- Otherwise a placeholder ID is used, and data will effectively be routed into the abyss\n\nSee the \"GA IDs By Hostname\" variable in our GTM account for details."}
{"path":"infra/modules/database/interface/variables.tf","language":"unknown","type":"code","directory":"infra/modules/database/interface","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/interface/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/frontend/development.md\nLanguage: md\nType: code\nDirectory: documentation/frontend\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/frontend/development.md\nSize: 11.76 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/database/kms.tf","language":"unknown","type":"code","directory":"infra/modules/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/kms.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"This [Next.js](https://nextjs.org) application can be run natively (or locally)\n\n**Running locally is the default**, but it can be useful to run the built Docker images in order to more closely simulate our deployed environment, troubleshoot production issues, and to connect directly with the local API application for development.\n\n## Local (non-Docker)\n\n### ðŸ—ï¸ Development version\n\nRun `npm install && npm run dev` to install and start the application.\n\n### ðŸ›ï¸ \"Built\" version\n\nThe Next.js frontend application is exported for production using [next build](https://nextjs.org/docs/app/api-reference/cli/next#next-build-options). To recreate this locally, outside of the container, run the following:\n\n- `npm run build` - Builds the production Next.js bundle\n- `npm start` - Runs the Next.js server, after building the production bundle\n\n## Docker\n\n### ðŸ—ï¸ Development version\n\nAlternatively, you can run the application in a Docker container.\n\n**Note**: If you are running docker locally for the first time, you need to run the API locally through Docker as well, in order to create the required `api_grants_backend` network.\n\nFrom the `/frontend` directory:\n\n1. Run the local development server\n   ```bash\n   make dev\n   ```\n1. Navigate to [localhost:3000](http://localhost:3000) to view the application\n\n- If installing new packages locally with npm and using `make dev` with docker to run locally, you may need to run `make build` first to bring the new packages into the container\n\n### ðŸš€ Production version\n\nThe `make dev` command runs the `docker-compose.yml` which runs the `dev` target in the [Dockerfile](./Dockerfile). To run a production version in docker, run `docker compose up -d -f docker-compose-realease.yml` which targest the `release` stage in the docker build. This runs the production version, while still creating a network connection to the local API.\n\n### Testing Release Target Locally\n\nTo test the release target locally, run:\n\n- `make release-build OPTS=\"--tag [IMAGE_NAME]\"` or\n- `docker buildx build --target release --tag [IMAGE_NAME]` for a faster build on OSX\n\nto build a local image. To view the site at `localhost:3000`, run: `docker run -e \"HOSTNAME=0.0.0.0\" -p 3000:3000 [IMAGE_NAME]`.\n\n## ðŸŽ¯ Testing\n\n### :atom: Unit Testing\n\n[Jest](https://jestjs.io/docs/getting-started) is used as the test runner and [React Testing Library](https://testing-library.com/docs/react-testing-library/intro) provides React testing utilities.\n\nTests are manged as `.test.ts` (or `.tsx`) files in the the `tests/` directory.\n\nTo run tests:\n\n- `npm test` - Runs all tests and outputs test coverage report\n- `npm run test-update` - Updates test snapshots\n- `npm run test-watch` - Runs tests in [watch](https://jestjs.io/docs/cli#--watch) mode. Tests will re-run when files are changed, and an interactive prompt will allow you to run specific tests or update snapshots.\n\nA subset of tests can be run by passing a pattern to the script. For example, to only run tests in `tests/pages/`:\n\n```sh\nnpm run test-watch -- pages\n```\n\n### ðŸš¦ End-to-end (E2E) testing\n\n[Playwright](https://playwright.dev/) is a framework for web testing and its test runner is called [Playwright Test](https://playwright.dev/docs/api/class-test), which can be used to run E2E or integration tests across chromium, firefox, and webkit browsers.\n\nE2E test filenames end with `.spec.ts` and are found in the `tests/e2e` directory.\n\nTo run E2E tests via CLI:\n\n- `cd ../api && make init db-seed-local start` (prerequisite to start the API)\n- `npx playwright install --with-deps` â€” Downloads playwright browsers required to run tests\n- `npm run test:e2e` â€” Runs all E2E tests using the playwright config found at `tests/playwright.config.ts`\n- `npm run test:e2e:ui` â€” Run specific or all E2E tests using Playwright's [UI mode](https://playwright.dev/docs/test-ui-mode), which is useful for debugging full traces of each test\n\nTo run E2E tests using VS Code:\n\n1. Download the VS Code extension described in these [Playwright docs](https://playwright.dev/docs/running-tests#run-tests-in-vs-code)\n2. Follow the [instructions](https://playwright.dev/docs/getting-started-vscode#running-tests) Playwright provides\n\nPlaywright E2E tests run \"local-to-local\", requiring both the frontend and the API to be running for the tests to pass - and for the database to be seeded with data.\n\nIn CI, the \"Frontend Checks\" workflow (`.github/workflows/ci-frontend-e2e.yml`) runs Playwright tests, and will include a summary when complete, with an \"Artifacts\" section where there is an attached \"playwright-report\". [Playwright docs](https://playwright.dev/docs/ci-intro#html-report) describe how to view the HTML Report in more detail.\n\n### ðŸ¤– Type checking, linting, and formatting\n\n#### Tools\n\n- [TypeScript](https://www.typescriptlang.org/) is used for type checking.\n- [ESLint](https://eslint.org/) is used for linting. This helps catch common mistakes and encourage best practices.\n- [Prettier](https://prettier.io/) is used for code formatting. This reduces the need for manual formatting or nitpicking and enforces a consistent style.PRs in Github Actions, other than e2e tests\n\nIt's recommended that developers configure their code editor to auto run these tools on file save. Most code editors have plugins for these tools or provide native support.\n\n<details>\n  <summary>VSCode instructions</summary>\n\n1. Install the [Prettier](https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode) and [ESLint](https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslint) extensions.\n2. Add the following to a `.vscode/settings.json` Workspace Settings file:\n\n   ```json\n   {\n     \"editor.codeActionsOnSave\": {\n       \"source.fixAll.eslint\": true\n     },\n     \"editor.formatOnSave\": true,\n     \"editor.defaultFormatter\": \"esbenp.prettier-vscode\",\n     \"eslint.workingDirectories\": [\"./frontend\"],\n     \"typescript.validate.enable\": true\n   }\n   ```\n\n   For these tools to auto run, the settings must be located in the root of your current VSCode workspace. For example, if you open the `frontend/` directory in VSCode, the settings should be located at `frontend/.vscode/settings.json`. If you then open then root repository directory in VSCode as your workspace, these tools will not auto run. (Note that adding the settings to the root repository directory may affect other parts of a monorepo.)\n\n   You can alternatively add the settings to your User Settings, however they will apply globally to any workspace you open. See [User and Workspace Settings](https://code.visualstudio.com/docs/getstarted/settings) for more guidance.\n\n</details>\n\n#### NPM Scripts for type checking, linting, and formatting\n\n- `npm run ts:check` - Type checks all files\n- `npm run lint` - Lints all files and reports any errors\n- `npm run lint-fix` - Lints all files and fixes any auto-fixable errors\n- `npm run format`: Formats all files based on prettier rules\n- `npm run format-check`: Check files for prettier formatting violations without fixing them\n- `npm run all-checks`: Runs linting, typescript check, unit testing, and creates a build - simulating locally tests that are run on PRs in Github Actions, other than e2e tests\n\n### ðŸ–¼ï¸ Storybook\n\nStorybook is a [frontend workshop](https://bradfrost.com/blog/post/a-frontend-workshop-environment/) for developing and documenting pages and components in isolation. It allows you to render the same React components and files in the `src/` directory in a browser, without the need for a server or database. This allows you to develop and manually test components without having to run the entire Next.js application.\n\nSee the [Storybook Next.js documentation](https://github.com/storybookjs/storybook/tree/next/code/frameworks/nextjs) for more information about using Storybook with Next.js\n\nSimilar to the Next.js application, Storybook can be ran natively or in a Docker container.\n\n#### Native\n\nFrom the `frontend/` directory:\n\n1. `npm run storybook`\n2. Navigate to [localhost:6006](http://localhost:6006) to view\n\n#### Static\n\n- `npm run storybook-build` - Exports a static site to `storybook-static/`\n\n#### Docker\n\nAlternatively, you can run Storybook in a Docker container.\n\nFrom the `frontend/` directory:\n\n1. `make storybook`\n2. Navigate to [localhost:6006](http://localhost:6006) to view\n\n### ðŸ› Debugging the Next App in VSCode\n\n- See the debug config: `./.vscode/launch.json`\n  - There are several debug config targets defined there depending on if you want to debug just client components (client-side), just server components (server-side), or both (with the Full Stack option). You can also debug the built server (launched from `npm start` instead of `npm run dev`).\n- Run one of these launch targets from the VSCode debug menu\n- Place breakpoints in VSCode\n- Visit the relevant routes in the browser and confirm you can hit these breakpoints\n\n**Note** that debugging the server-side or full-stack here doesn't debug the API. [See the API readme for more information](../documentation/api/development.md)\n\n## Feature setup and development\n\nThe following features require additional local setup to use.\n\n### Search and Opportunity Pages\n\nThe `/search` and opportunity pages rely on the application API. The API endpoint and authentication token are defined in `.env.development` and can be overwritten in an `.env.local` file.\n\nThe `API_URL` environment variable can be set to connect to prod (`https://api.simpler.grants.gov`) or lower environment URLs to quickly develop using production or development data. To successfully connect to a deployed API, the `API_AUTH_TOKEN` variable must be set correctly for the environment.\n\nTo start a local development version of the API, run `make init && db-seed-local && populate-search-opportunities` in the `/api` folder.\n\nSee [documentation/api/development.md](../api/development.md) for more details.\n\n### Authentication\n\nRunning authentication locally requires running the API, directing the API redirect to the frontend, and sharing the correct JWT keys.\n\n1. Run `make setup-env-override-file` to create the `override.env` file in the `/api` folder\n2. Copy the `API_JWT_PUBLIC_KEY` value from `/api/override.env` file to `/frontend/.env.local` file which creates the necessary keys\n3. Add `LOGIN_FINAL_DESTINATION=http://localhost:3000/api/auth/callback` to the `api/override.env` so the API redirects to the correct callback route\n4. Start the API (`make make db-seed-local && make populate-search-opportunities && make start`) and frontend (`npm run dev`) for development\n\n#### Login flow\nThe [documentation/api/authentication.md](../api/authentication.md) details the login flow from the frontend â†’ API â†’ login.gov â†’ API â†’ frontend.\n\nThe `/api/auth/callback` route handler receives a JSON web token as query parameter, uses the `API_JWT_PUBLIC_KEY` env variable to verify that it was created by the API, sets a cookie with the token, then later uses that token to verify the user identity in `/api/auth/session` and other routes.\n\n#### Mock Oauth2 Server\n\nWhen clicking \"Sign in\" or other buttons that simulate the login flow locally, shoule be redirected to the mock Oauth2 server at `http://localhost:5001`. Enter any text string in the screen provided to continue the login flow.\n\n### New Relic and Sendy (email)\n\nSome functionality will not work locally without supplying the application environment variables containing secrets.\n\n- New Relic\n  - `NEW_RELIC_APP_NAME`\n  - `NEW_RELIC_LICENSE_KEY`\n- Email subscription form (Sendy)\n  - `SENDY_API_KEY`\n  - `SENDY_API_URL`\n  - `SENDY_LIST_ID`\n\nIf you need to access this functionality locally, contact an engineer on the team to get access to the necessary secrets.\n\n## Other topics\n\n- [Internationalization](../documentation/frontend/internationalization.md)\n- [Feature Flags](../documentation/frontend/featureFlags.md)\n- Refer to the [architecture decision records](../documentation/decisions) for more context on technical decisions."}
{"path":"infra/modules/database/main.tf","language":"unknown","type":"code","directory":"infra/modules/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/frontend/environments.md\nLanguage: md\nType: code\nDirectory: documentation/frontend\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/frontend/environments.md\nSize: 5.47 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/database/monitoring.tf","language":"unknown","type":"code","directory":"infra/modules/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/monitoring.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"## General Things\n\nNote that Next applications follow a set heirarchy for evaluating environment variable precedence, as noted [in documentation here](https://nextjs.org/docs/pages/building-your-application/configuring/environment-variables#environment-variable-load-order).\n\nSecret environment variables, and others that should be controlled specifically based on deployed environment, are specified in terraform, which pulls them from SSM and sets them on the service definition, which passes them from the ECS task definition to the Next container. See [code referenced here](https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/env-config/environment-variables.tf).\n\nAll environment variables referenced in the app should be handled in the [environments file here](https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/constants/environments.ts)].\n\n## NODE_ENV\n\nNextJS will set the Node `NODE_ENV` variable automatically depending on how you start the server.\n\n- `next dev` - \"development\"\n- `next build && next start` - \"production\"\n\nNext hasn't documented this behavior super well - the best reference I can find is in [this discussion thread](https://github.com/vercel/next.js/discussions/13410#discussioncomment-18760).\n\nThis behavior means that, since we use `build & start` in all of our deployed environments and E2E test runs, we cannot use rely on `NODE_ENV` to give us a meaningful value about which environment the process is running in. To work around this, any environment variables that should be dependent on the deployed environment (ie \"dev\", \"staging\", \"prod\"), should likely be referenced as a secret from SSM as described above.\n\n## Local\n\nEnvironment variables are gathered from the .env.development file.\n\nWhen running Next in a non-dockerized situation, variables are also pulled directly from your command line or environment. Ex. `API_URL=http://google.com next run dev`.\n\nSince the .env.development file is tracked in git, any sensitive environment variables used for local development or testing - for example to connect to a non-dev API instance - **SHOULD NOT** be entered into this file, even temporarily. Instead, pass any necessary env vars directly from the command line or look into something like [direnv](https://direnv.net/).\n\n## Test (and testing)\n\nUnit testing by Jest is always done in the \"test\" environment by default. See [Jest's docs](https://jestjs.io/docs/environment-variables#node_env). As a result, all Jest runs will reference the .env.test or .env.local (which is used in CI).\n\nE2E tests are run against a running Next server, so the environment used there is determined by the environment used on by whatever type of Next server is running.\n\nIn CI E2E tests use `npx playwright test`, which will run `next start` pointing at a production build of the application. To work aroudn this our CI code copies .env.development values into a .env.local file that will take precedence over .env.production. Note that NODE_ENV will still be set to \"production\".\n\nSee [our CI code](https://github.com/HHS/simpler-grants-gov/blob/1b85220c7369d40ab2f690050ece41be91c91b7f/.github/workflows/ci-frontend-e2e.yml#L58) for more details.\n\n## Development / Staging / Production\n\nNote that, as mentioned above, NODE_ENV will be set to \"production\" here due to use of `npm run build && npm start`.\n\nAs a result, environment variables are gathered from the .env.production file.\n\n## Deployment\n\n[Check out this diagram](https://lucid.app/lucidchart/107dcf47-46e7-4088-a90a-1ef3b0ca3744/edit?viewport_loc=42%2C439%2C2295%2C1182%2C0_0&invitationId=inv_3559eb81-f735-4b22-9365-49920268e061). This should explain most of what the next section explains, and more but in visual form.\n\nWill add image directly to document later.\n\n### Tricky Stuff\n\nDocker and Next (and ECS) combine to create an interesting situation when trying to figure out how to provide environment variables to the deployed application. Here are a few interesting facts:\n\n- any environment variables that we want to be directly available in client code in a Next app need to be prefixed with the `NEXT_PUBLIC_` string. See [Next docs here](https://nextjs.org/docs/pages/building-your-application/configuring/environment-variables#bundling-environment-variables-for-the-browser).\n- any `NEXT_PUBLIC_` variables must be initially referenced in the code directly from process.env (`const yrVar = process.env.NEXT_PUBLIC_YR_VAR`), rather than destructured (`const { NEXT_PUBLIC_YR_VAR } = process.env`)), but can be exported from one file to another after definition\n- these variables must be available at build time (ie `next build` rather than `next start`), and can be referenced in any components when exported from a server side file, directly referenced from process.env, or passed as a prop\n- however, currently, all environment variables are passed from the ECS task definition to the application at Docker run time. This means that the `NEXT_PUBLIC_` prefix and usage as described above is not effective in our application\n- since variables defined at Docker runtime cannot be not made available directly to client components via an import or process.env reference, to make any environment variables available to client components, we will need to\n  - import the variable into a server component\n  - pass it as a prop to client component child, or implement it within a context provider"}
{"path":"infra/modules/database/networking.tf","language":"unknown","type":"code","directory":"infra/modules/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/networking.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/frontend/featureFlags.md\nLanguage: md\nType: code\nDirectory: documentation/frontend\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/frontend/featureFlags.md\nSize: 8.62 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/database/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"Simpler Grants maintains a feature flag system within its NextJS app that currently allows for custom behavior for top level pages. Each feature flag is a simple boolean, and when turned on (or turned to `true`), the page that is set up to respond to this flag can opt out of the standard render. For example, when a \"searchOff\" feature flag is set to \"true\", the search page is configured to redirect to a maintenance page.\n\nOur feature flags implementation can read feature flag values from environment variables, but can also read them from the frontend, and stores user settings on client side cookies. The intent is for these feature flags to be user configurable, so that\n\n1. we can gate incomplete features to make it easier to coordinate development work, and\n2. links can be sent to individuals with customized query parameters to customize the user's experience.\n\nThis means that any codebase/deployment level behavior that isn't meant for users to be able to configure themselves via the methods mentioned above should not use feature flags.\n\n# Current Feature Flags\n\nYou can find and update the feature flags currently in use by the application in the [feature flags typescript file](/frontend/src/constants/featureFlags.ts).\n\n# Usage\n\n## Conventions and limitations\n\nFeature flags will follow these conventions!\n\n- feature flags should be named and conceived such that their default value is `false` or `off`. This allows us to easily implement custom behavior across the board when flags are turned on. For example, a feature flag to toggle the opportunity page should be something like `opportunityOff` or `disableOpportunity` and with a default value of `false`.\n- feature flag names will use simple camel case naming, for example `searchOff`, `disableApplicationForm`\n- names of environment variables for controlling feature flag values should match the name of the flags within the code, except\n  - using snake case\n  - all caps\n  - with `FEATURE_` at the front\n    - ex: flag `doSomethingSpecial` in code would be controlled by env var called `FEATURE_DO_SOMETHING_SPECIAL`\n\nFeature flags have the following limitations!\n\n- feature flags cannot control behavior of non page level components\n- feature flags cannot pass props to components\n- feature flags cannot affect build time behavior\n\nGenerally this means that feature flags can only currently be used to gate particular pages by redirecting them to other places, though slightly more creative usages are possible.\n\n## Setting Flags\n\nThere are three ways to manage feature flags!\n\n### Via environment variables\n\nEach feature flag can be set by a corresponding environment variable based on the following the convention discussed above.\n\n### Via the user's web browser\n\nHave the user visit `/dev/feature-flags` in their browser; they can configure feature flags directly via the GUI.\n\n### Via query parameters set on urls\n\nTo do so, add a query param of `_ff` to the site's url and use JSON notation for its value. A feature flag's value can\nbe `true` or `false`.\n\nFor example:\n\n- To **enable** a feature flag called `v2`, you would use:\n  `{url}?_ff=v2:true`\n- To **disable** a feature flag called `v2`, you would use:\n  `{url}?_ff=v2:false`\n\nTo set multiple feature flags on a single url, separate their key/value pairs with a `;`. For example:\n`{url}?_ff=v2:true;another_flag:true`. If the same flag is included multiple times, the last one takes precedence.\n\nNote, setting feature flags via the url will persist these values in the user's cookies. This way, users can send links to pages with feature flagged behavior. If a feature is released behind a feature flag a link could be sent to interested parties for feedback using the feature flagged url that would load the feature before it goes public. For example, to allow a particular stakeholder to try out a `v2` feature, but not necessarily enable it\nfor everyone else, you could simply send them the url `{url}?_ff=v2:true`.\n\n## Resetting feature flags\n\nTo reset all feature flags to the default values in your browser cookie, add a query of `?_ff=reset` to the url.\n\n## Precedence\n\nEach feature flag will be set with a default value within the code. This default can be overridden in the following order:\n\n- environment variables\n- cookies\n- query params\n\nThis means that if an flag is set to \"false\" by default, an environment variable set to \"true\" would override this, but a cookie value set to \"false\" would override the environment variable, and a query param set to \"true\" would take precedence over everything else.\n\n# Development usage\n\nFeature flags are implemented via two interfaces:\n\n- The `useFeatureFlags` hook\n- The `FeatureFlagsManager` class\n\n## Frontend\n\nWhen writing frontend code, we recommend using the `useFeatureFlag` hook since it is simpler to reference and also updates React state when updating feature flag values. You get the same interface as if you used the class directly.\n\n```tsx\nfunction MyComponent() {\n  const {\n    featureFlagsManager,  // An instance of FeatureFlagsManager\n    mounted,  // Useful for hydration\n    setFeatureFlag,  // Proxy for featureFlagsManager.setFeatureFlag that handles updating state\n  } = useFeatureFlags()\n\n  if (featureFlagsManager.isFeatureEnabled(\"someFeatureFlag\")) {\n    // Do something\n  }\n\n  if (!mounted) {\n    // To allow hydration\n    return null\n  }\n\n  return (\n    ...\n  )\n}\n```\n\n## Backend\n\nWhen writing backend code, you should use the manager class directly.\n\n```typescript\nexport default async function handler(request, response) {\n  const featureFlagsManager = new FeatureFlagsManager(request.cookies)\n  if (featureFlagsManager.isFeatureEnabled(\"someFeatureFlag\")) {\n    // Do something\n  }\n  ...\n}\n```\n\n## Adding feature flags\n\nTo add a new feature flag, you must:\n\n1. Add it and a default value to the object exported from [the FeatureFlags constants file](https://github.com/HHS/simpler-grants-gov/blob/main/frontend/src/constants/featureFlags.ts)\n2. If you want to control the feature flag with an environment variable add it to the list of environment variables [in the terraform](https://github.com/HHS/simpler-grants-gov/blob/main/infra/frontend/app-config/env-config/environment-variables.tf), and add variables for each environment in SSM\n3. That's it! Everything else is handled for you!\n\n## Testing\n\nThere are a few utility methods for helping with tests that involve feature flags.\n\n```typescript\nimport {\n  mockDefaultFeatureFlags,\n  mockFeatureFlagsCookie,\n} from \"utils/tests/featureFlagsTestUtils\";\n```\n\n`mockDefaultFeatureFlags` allows you to mock the value of `FeatureFlagsManager._defaultFeatureFlags`.\n`mockFeatureFlagsCookie` allows you to mock the stored cookie value for feature flags.\n\nYou can also mock the `FeatureFlagsManager` directly to control whether a feature is enabled. For example:\n\n```typescript\njest\n  .spyOn(FeatureFlagsManager.prototype, \"isFeatureEnabled\")\n  .mockReturnValue(true);\n```\n\nIf you run into an error like \"ReferenceError: Response is not defined\", you should add the following to the top of the\ntest file that is affected\n\n```typescript\n/**\n * @jest-environment ./tests/utils/jsdomNodeEnvironment.ts\n */\n```\n\nThis is because currently Jest is configured to run in the `jsdom` environment (enabling features like `window`), but\nthe feature flags integration also depends on some `node` environment features (middleware). The custom\n`jsdomNodeEnvironment` jest environment polyfills node fetch globals to `jsdom` so that your tests can both have access\nto `window` and use feature flag and feature flag mocking functionality.\n\n## How it works\n\n1. The next.js middleware hooks into `FeatureFlagsManager.middleware` which will take query params from the url and save the parsed values into the feature flags cookie (before a backend view function runs).\n1. The backend view function can determine if a feature flag is enabled by using `FeatureFlagsManager`. See above for usage details.\n1. The frontend components can determine if a feature flag is enabled by using `useFeatureFlags` (or `FeatureFlagsManager` directly). See above for usage details. Note, the feature flags query params in the url are meant for the middleware only and are ignored with normal frontend usage.\n\n# Graceful user cookie handling\n\nThe `FeatureFlagsManager` and middleware integration intelligently and gracefully handles cookie value errors such that if the cookie were ever to get corrupted, the site will not break.\n\n- Invalid query param formats, invalid feature flag names, and invalid feature flag values are ignored\n- If the cookie value is corrupted, a new cookie value set to the default feature flag values will be set\n- The cookie is read every time we check if a feature flag is enabled, so it also handles changes to the cookie made by another page or request"}
{"path":"infra/modules/database/role-manager.tf","language":"unknown","type":"code","directory":"infra/modules/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/role-manager.tf","size":0,"lastModified":"2025-02-14T17:08:31.142Z","content":"File: documentation/frontend/internationalization.md\nLanguage: md\nType: code\nDirectory: documentation/frontend\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/frontend/internationalization.md\nSize: 4.92 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/database/role_manager/check.py","language":"python","type":"code","directory":"infra/modules/database/role_manager","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/role_manager/check.py","size":1631931,"lastModified":"2025-02-14T17:08:31.142Z","content":"- [next-intl](https://next-intl-docs.vercel.app) is used for internationalization. Toggling between languages is done by changing the URL's path prefix (e.g. `/about` âž¡ï¸ `/es-US/about`).\n- Configuration is located in [`i18n/config.ts`](../frontend/src/i18n/config.ts). For the most part, you shouldn't need to edit this file unless adding a new formatter or new language.\n\n## Managing translations\n\n- Translations are managed as files in the [`i18n/messages`](../frontend/src/i18n/messages/) directory, where each language has its own directory (e.g. `en-US` and `es-US`).\n- How you organize translations is up to you, but here are some suggestions:\n  - Group your messages. It's recommended to use component/page names as namespaces and embrace them as the primary unit of organization in your app.\n  - By default, all messages are in a single file, but you can split them into multiple files if you prefer. Continue to export all messages from `i18n/messages/{locale}/index.ts` so that they can be imported from a single location, and so files that depend on the messages don't need to be updated.\n- There are a number of built-in formatters based on [JS's `Intl` API](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Intl) that can be used in locale strings, and custom formatters can be added as well. [See the formatting docs for details](https://next-intl-docs.vercel.app/docs/usage/numbers).\n- If a string's translation is missing in another language, the default language (English) will be used as a fallback.\n\n### Type-safe translations\n\nThe app is configured to report errors if you attempt to reference an i18n key path that doesn't exist in a locale file.\n\n[Learn more about using TypeScript with next-intl](https://next-intl-docs.vercel.app/docs/workflows/typescript).\n\n## Load translations\n\nLocale messages should only ever be loaded on the server-side, to avoid bloating the client-side bundle. If a client component needs to access translations, only the messages required by that component should be passed into it.\n\n[See the Internationalization of Server & Client Components docs](https://next-intl-docs.vercel.app/docs/environments/server-client-components) for more details.\n\n## Add a new language\n\n1. Add a language folder, using the same BCP47 language tag: `mkdir -p src/i18n/messages/<lang>`\n1. Add a language file: `touch src/i18n/messages/<lang>/index.ts` and add the translated content. The JSON structure should be the same across languages. However, non-default languages can omit keys, in which case the default language will be used as a fallback.\n1. Update [`i18n/config.ts`](../../app/src/i18n/config.ts) to include the new language in the `locales` array.\n\n## Structuring your messages\n\nIn terms of best practices, [it is recommended](https://next-intl-docs.vercel.app/docs/usage/messages#structuring-messages) to structure your messages such that they correspond to the component that will be using them. You can nest these definitions arbitrarily deep if you have a particularly complex need.\n\nIt is always preferable to structure messages per their usage rather than due to some side effect of a technological implementation. The idea is to group them semantically but also preserve maximum flexibility for a translator. For instance, splitting up a paragraph in order to separate out a link might lead to awkward translation, so it is best to keep it as a single message. The info below shows techniques for common needs that prevent unnecessary splits of content.\n\n### Variables\n\nMessages do not need to be split in order to incorporate dynamic data. Instead, these can be inserted via the [interpolation functionality](https://next-intl-docs.vercel.app/docs/usage/messages#interpolation-of-dynamic-values):\n\n```json\n\"message\": \"Hello {name}!\"\n```\n\n```tsx\nt(\"message\", { name: \"Jane\" }); // \"Hello Jane!\"\n```\n\n### Rich text messages\n\nIf your app needs a particular chunk of content to contain something other than plain text (such as links, formatting, or a custom component), you can utilize the \"rich text\" functionality ([see docs](https://next-intl-docs.vercel.app/docs/usage/messages#rich-text)). This allows one to embed arbitrary custom tags into the translation content strings and specify how each of those should be handled.\n\nExample from their docs:\n\n```json\n{\n  \"message\": \"Please refer to <guidelines>the guidelines</guidelines>.\"\n}\n```\n\n```tsx\n// Returns `<>Please refer to <a href=\"/guidelines\">the guidelines</a>.</>`\nt.rich(\"message\", {\n  guidelines: (chunks) => <a href=\"/guidelines\">{chunks}</a>,\n});\n```\n\nIf you have something that you are going to use repeatedly throughout your app, you can specify it in the [`defaultTranslationValues` config](https://next-intl-docs.vercel.app/docs/usage/configuration#default-translation-values).\n\n### Other needs\n\nFor examples of other functionality such as pluralization, arrays of content, etc, please [see the docs](https://next-intl-docs.vercel.app/docs/usage/messages)."}
{"path":"infra/modules/database/role_manager/db.py","language":"python","type":"code","directory":"infra/modules/database/role_manager","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/role_manager/db.py","size":1631931,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/goals.md\nLanguage: md\nType: code\nDirectory: documentation\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/goals.md\nSize: 2.30 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/database/role_manager/manage.py","language":"python","type":"code","directory":"infra/modules/database/role_manager","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/role_manager/manage.py","size":1631931,"lastModified":"2025-02-14T17:08:31.143Z","content":"The following are currently very draft and not finalized goals for the grants.gov modernization effort. Expect these to undergo substantial revision as we continue to refine our goals.\n\n## Vision\n\nOur vision is for the following to become true:\n\nWe want Grants.gov to be an extremely simple, accessible, and easy-to-use tool for posting, finding, sharing, and applying for federal financial assistance. Our mission is to increase access to grants and improve the grants experience for everyone.\n\n## Mission\n\nTo support you as a grants.gov user and stakeholder, we on the grants.gov team make a commitment to the following:\n\n### Make grants.gov easy\n* Make it as easy as possible for federal grantors to post opportunity listings to grants.gov\n* Provide a one-stop shop for all federal discretionary opportunities, and make it as easy as possible for potential applicants to discover opportunities relevant to their needs\n* Make it as easy as possible for applicants to apply for opportunities, through all aspects of the application including forms and attachments\n* Provide frictionless functionality and help users when they get stuck\n\n### Make grants.gov accessible & collaborative\n* Ensure that all communities have access to the grantmaking process\n* Ensure that communities with limited financial resources are better able to discover and apply for the opportunities for which they're qualified\n* Connect federal opportunities to more eligible applicants from all communities\n* Promote collaboration and build trust between the federal government and the public\n\n### Make grants.gov transparent & participatory\n* Make every step of the grants cycle transparent to understand and track\n* Provide the public with all available data about the grantmaking process, except data that can't be shared due to privacy or security concerns\n* Provide a transparent roadmap of what we're currently and planning to work on and why, and solicit, engage with, and act on stakeholder feedback on that roadmap\n* Work in the open, with fully open-source code[^1]\n\n## Metrics\n\nThe following metrics serve as key performance indicators of whether we're achieving our mission as described above.\n\n[To be added.]\n\n[^1]: In compliance with the federal [open source code policy](https://obamawhitehouse.archives.gov/sites/default/files/omb/memoranda/2016/m_16_21.pdf)."}
{"path":"infra/modules/database/role_manager/requirements.txt","language":"unknown","type":"code","directory":"infra/modules/database/role_manager","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/role_manager/requirements.txt","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/README.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/README.md\nSize: 0.67 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/database/role_manager/role_manager.py","language":"python","type":"code","directory":"infra/modules/database/role_manager","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/role_manager/role_manager.py","size":1631931,"lastModified":"2025-02-14T17:08:31.143Z","content":"## Setup\n\n* [Configure the project](../../infra/project-config/main.tf) (These values will be used in subsequent infra setup steps to namespace resources and add infrastructure tags.)\n* [Set up infrastructure developer tools](./set-up-infrastructure-tools.md)\n* [Set up AWS account](./set-up-aws-account.md)\n* [Set up application build repository](./set-up-app-build-repository.md)\n* [Set up application environment](./set-up-app-env.md)\n\n## Technical Design\n\n* [Module architecture](./module-architecture.md)\n\n## Learning\n\n* [Introduction to Terraform](./intro-to-terraform.md)\n* [Introduction to Terraform workspaces](./intro-to-terraform-workspaces.md)"}
{"path":"infra/modules/database/variables.tf","language":"unknown","type":"code","directory":"infra/modules/database","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/database/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/destroy-infrastructure.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/destroy-infrastructure.md\nSize: 2.06 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/dms-networking/main.tf","language":"unknown","type":"code","directory":"infra/modules/dms-networking","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/dms-networking/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"To destroy everything you'll need to undeploy all the infrastructure in reverse order that they were created. In particular, the account root module(s) need to be destroyed last.\n\n## Instructions\n\n1. First destroy all your environments. Within `/infra/app/service` run the following, replacing `dev` with the environment you're destroying.\n\n   ```bash\n   $ terraform init --backend-config=dev.s3.tfbackend\n   $ terraform destroy -var-file=dev.tfvars\n   ```\n\n2. Then to destroy the backends, first you'll need to add `force_destroy = true` to the S3 buckets, and update the lifecycle block to set `prevent_destroy = false`. Then run `terraform apply` from within the `infra/accounts` directory. The reason we need to do this is because S3 buckets by default are protected from destruction to avoid loss of data. See [Terraform: Destroy/Replace Buckets](https://medium.com/interleap/terraform-destroy-replace-buckets-cf9d63d0029d) for a more in depth explanation.\n\n   ```terraform\n   # infra/modules/modules/terraform-backend-s3/main.tf\n\n   resource \"aws_s3_bucket\" \"tf_state\" {\n     bucket = var.state_bucket_name\n\n     force_destroy = true\n\n     # Prevent accidental destruction a developer executing terraform destory in the wrong directory. Contains terraform state files.\n     lifecycle {\n       prevent_destroy = false\n     }\n   }\n\n   ...\n\n   resource \"aws_s3_bucket\" \"tf_log\" {\n     bucket = var.tf_logging_bucket_name\n     force_destroy = true\n   }\n   ```\n\n3. Then since we're going to be destroying the tfstate buckets, you'll want to move the tfstate file out of S3 and back to your local system. Comment out or delete the s3 backend configuration:\n\n   ```terraform\n   # infra/accounts/main.tf\n\n   # Comment out or delete the backend block\n   backend \"s3\" {\n     ...\n   }2\n   ```\n\n4. Then run the following from within the `infra/accounts` directory to copy the tfstate back to a local tfstate file:\n\n   ```bash\n   terraform init -force-copy\n   ```\n\n5. Finally, you can run `terraform destroy` within the `infra/accounts` directory.\n\n   ```bash\n   terraform destroy\n   ```"}
{"path":"infra/modules/dms-networking/networking.tf","language":"unknown","type":"code","directory":"infra/modules/dms-networking","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/dms-networking/networking.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/environment-variables-and-secrets.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/environment-variables-and-secrets.md\nSize: 5.01 KB\nLast Modified: 2025-02-14T17:08:26.484Z"}
{"path":"infra/modules/dms-networking/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/dms-networking","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/dms-networking/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"Applications follow [12-factor app](https://12factor.net/) principles to [store configuration as environment variables](https://12factor.net/config). The infrastructure provides some of these environment variables automatically, such as environment variables to authenticate as the ECS task role, environment variables for database access, and environment variables for accessing document storage. However, many applications require extra custom environment variables for application configuration and for access to secrets. This document describes how to configure application-specific environment variables and secrets. It also describes how to override those environment variables for a specific environment.\n\n## Application-specific extra environment variables\n\nApplications may need application specific configuration as environment variables. Examples may includes things like `WORKER_THREADS_COUNT`, `LOG_LEVEL`, `DB_CONNECTION_POOL_SIZE`, or `SERVER_TIMEOUT`. This section describes how to define extra environment variables for your application that are then made accessible to the ECS task by defining the environment variables in the task definition (see AWS docs on [using task definition parameters to pass environment variables to a container](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/taskdef-envfiles.html)).\n\n> âš ï¸ Note: Do not put sensitive information such as credentials as regular environment variables. The method described in this section will embed the environment variables and their values in the ECS task definition's container definitions, so anyone with access to view the task definition will be able to see the values of the environment variables. For configuring secrets, see the section below on [Secrets](#secrets)\n\nEnvironment variables are defined in the `app-config` module in the [environment_variables.tf file](/infra/app/app-config/env-config/environment_variables.tf). Modify the `default_extra_environment_variables` map to define extra environment variables specific to the application. Map keys define the environment variable name, and values define the default value for the variable across application environments. For example:\n\n```terraform\n# environment_variables.tf\n\nlocals {\n  default_extra_environment_variables = {\n    WORKER_THREADS_COUNT = 4\n    LOG_LEVEL            = \"info\"\n  }\n}\n```\n\nTo override the default values for a particular environment, modify the `app-config/[environment].tf file` for the environment, and pass overrides to the `env-config` module using the `service_override_extra_environment_variables` variable. For example:\n\n```terraform\n# dev.tf\n\nmodule \"dev_config\" {\n  source                                       = \"./env-config\"\n  service_override_extra_environment_variables = {\n    WORKER_THREADS_COUNT = 1\n    LOG_LEVEL            = \"debug\"\n  }\n  ...\n}\n```\n\n## Secrets\n\nSecrets are a specific category of environment variables that need to be handled sensitively. Examples of secrets are authentication credentials such as API keys for external services. Secrets first need to be stored in AWS SSM Parameter Store as a `SecureString`. This section then describes how to make those secrets accessible to the ECS task as environment variables through the `secrets` configuration in the container definition (see AWS documentation on [retrieving Secrets Manager secrets through environment variables](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/secrets-envvar-secrets-manager.html)).\n\nSecrets are defined in the same file that non-sensitive environment variables are defined, in the `app-config` module in the [environment_variables.tf file](/infra/app/app-config/env-config/environment_variables.tf). Modify the `secrets` map to define the secrets that the application will have access to. For each secret, the map key defines the environment variable name. The `manage_method` property, which can be set to `\"generated\"` or `\"manual\"`, defines whether or not to generate a random secret or to reference an existing secret that was manually created and stored into AWS SSM. The `secret_store_name` property defines the SSM parameter name that stores the secret value. If `manage_method = \"generated\"`, then `secret_store_name` is where terraform will store the secret. If `manage_method = \"manual\"`, then `secret_store_name` is where terraform will look for the existing secret. For example:\n\n```terraform\n# environment_variables.tf\n\nlocals {\n  secrets = {\n    GENERATED_SECRET = {\n      manage_method     = \"generated\"\n      secret_store_name = \"/${var.app_name}-${var.environment}/generated-secret\"\n    }\n    MANUALLY_CREATED_SECRET = {\n      manage_method     = \"manual\"\n      secret_store_name = \"/${var.app_name}-${var.environment}/manually-created-secret\"\n    }\n  }\n}\n```\n\n> âš ï¸ For secrets with `manage_method = \"manual\"`, make sure you store the secret in SSM Parameter Store _before_ you try to add configure your application service with the secrets, or else the service won't be able to start since the ECS Task Executor won't be able to fetch the configured secret."}
{"path":"infra/modules/dms-networking/variables.tf","language":"unknown","type":"code","directory":"infra/modules/dms-networking","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/dms-networking/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/intro-to-terraform-workspaces.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/intro-to-terraform-workspaces.md\nSize: 2.22 KB\nLast Modified: 2025-02-14T17:08:26.485Z"}
{"path":"infra/modules/domain/certificates.tf","language":"unknown","type":"code","directory":"infra/modules/domain","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/domain/certificates.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"Terraform workspaces are created by default, the default workspace is named \"default.\" Workspaces are used to allow multiple engineers to deploy their own stacks for development and testing. This allows multiple engineers to develop new features in parallel using a single environment without destroying each others infrastructure. Separate resources will be created for each engineer when using the prefix variable.\n\n## Terraform workspace commands\n\n`terraform workspace show [Name]`   - This command will show the workspace you working in.\n\n`terraform workspace list [Name]`   - This command will list all workspaces.\n\n`terraform workspace new [Name]`    - This command will create a new workspace.\n\n`terraform workspace select [Name]` - This command will switch your workspace to the workspace you select.\n\n`terraform workspace delete [Name]` - This command will delete the specified workspace. (does not delete infrastructure, that step will done first)\n\n## Workspaces and prefix - A How To\n\n Workspaces are used to allow multiple developers to deploy their own stacks for development and testing. By default \"prefix~ is set to `terraform.workspace` in the envs/dev environment, it is `staging` and `prod` in those respective environments.\n\n### envs/dev/main.tf\n\n``` tf\nlocals {\n  prefix = terraform.workspace\n}\n\nmodule \"example\" {\n  source  = \"../../modules/example\"\n  prefix  = local.prefix\n}\n\n```\n\n### modules/example/variables.tf - When creating a new module create the variable \"prefix\" in your variables.tf\n\n``` tf\n\nvariable \"prefix\" {\n  type        = string\n  description = \"prefix used to uniquely identify resources, allows parallel development\"\n\n}\n\n```\n\n### modules/example/main.tf - Use var.prefix to uniquely name resources for parallel development\n\n``` tf\n\n# Create the S3 bucket with a unique prefix from terraform.workspace.\nresource \"aws_s3_bucket\" \"example\" {\n  bucket = \"${var.prefix}-bucket\"\n\n}\n\n```\n\nWhen in the workspace \"shawn\", the resulting bucket name created in the aws account will be `shawn-bucket`. This prevents the following undesirable situation: If resources are not actively prefixed and two developers deploy the same resource, the developer who runs their deployment second will overwrite the deployment of the first."}
{"path":"infra/modules/domain/main.tf","language":"unknown","type":"code","directory":"infra/modules/domain","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/domain/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/intro-to-terraform.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/intro-to-terraform.md\nSize: 3.14 KB\nLast Modified: 2025-02-14T17:08:26.485Z"}
{"path":"infra/modules/domain/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/domain","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/domain/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"## Basic Terraform Commands\n\nThe `terraform init` command is used to initialize a working directory containing Terraform configuration files. This is the first command that should be run after writing a new Terraform configuration or cloning an existing one from version control.\n\nThe `terraform plan` command creates an execution plan, which lets you preview the changes that Terraform plans to make to your infrastructure. By default, when Terraform creates a plan it:\n\n- Reads the current state of any already-existing remote objects to make sure that the Terraform state is up-to-date.\n- Compares the current configuration to the prior state and noting any differences.\n- Proposes a set of change actions that should, if applied, make the remote objects match the configuration.\n\nThe `terraform apply` command executes the actions proposed in a Terraform plan deploying the infrastructure specified in the configuration. Use with caution. The configuration becomes idempotent once a subsequent apply returns 0 changes.\n\nThe `terraform destroy` command is a convenient way to destroy all remote objects managed by a particular Terraform configuration. Use `terraform plan -destroy` to preview what remote objects will be destroyed if you run `terraform destroy`.\n\nâš ï¸ WARNING! âš ï¸ This is a destructive command! As a best practice, it's recommended that you comment out resources in non-development environments rather than running this command. `terraform destroy` should only be used as a way to cleanup a development environment. e.g. a developers workspace after they are done with it.\n\nFor more information about terraform commands follow the link below:\n\n- [Basic CLI Features](https://www.terraform.io/cli/commands)\n\n## Terraform Dependency Lock File\n\nThe [dependency lock file](https://www.terraform.io/language/files/dependency-lock) tracks provider dependencies. It belongs to the configuration as a whole and is created when running `terraform ini`. The lock file is always named `.terraform.lock.hcl`, and this name is intended to signify that it is a lock file for various items that Terraform caches in the `.terraform` subdirectory of your working directory. You should include this file in your version control repository so that you can discuss potential changes to your external dependencies via code review, just as you would discuss potential changes to your configuration itself.\n\n## Modules\n\nA module is a container for multiple resources that are used together. Modules can be used to create lightweight abstractions, so that you can describe your infrastructure in terms of its architecture, rather than directly in terms of physical objects. The .tf files in your working directory when you run `terraform plan` or `terraform apply` together form the root module. In this root module you will call modules that you create from the module directory to build the infrastructure required to provide any functionality needed for the application.\n\n## Terraform Workspaces\n\nWorkspaces are used to allow multiple engineers to deploy their own stacks for development and testing. Read more about it in [Terraform Workspaces](./intro-to-terraform-workspaces.md)"}
{"path":"infra/modules/domain/query-logs.tf","language":"unknown","type":"code","directory":"infra/modules/domain","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/domain/query-logs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/making-infra-changes.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/making-infra-changes.md\nSize: 2.39 KB\nLast Modified: 2025-02-14T17:08:26.485Z"}
{"path":"infra/modules/domain/variables.tf","language":"unknown","type":"code","directory":"infra/modules/domain","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/domain/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"## Requirements\n\nFirst read [Module Architecture](./module-architecture.md) to understand how the terraform modules are structured.\n\n## Using make targets (recommended)\n\nFor most changes you can use the Make targets provided in the root level Makefile, and can all be run from the project root.\n\nMake changes to the account:\n\n```bash\nmake infra-update-current-account\n```\n\nMake changes to the application service in the dev environment:\n\n```bash\nmake infra-update-app-service APP_NAME=app ENVIRONMENT=dev\n```\n\nMake changes to the application build repository (Note that the build repository is shared across environments, so there is no ENVIRONMENT parameter):\n\n```bash\nmake infra-update-app-build-repository APP_NAME=app\n```\n\nYou can also pass in extra arguments to `terraform apply` by using the `TF_CLI_ARGS` or `TF_CLI_ARGS_apply` parameter (see [Terraform's docs on TF_CLI_ARGS and TF_CLI_ARGS_name](https://developer.hashicorp.com/terraform/cli/config/environment-variables#tf_cli_args-and-tf_cli_args_name)):\n\n```bash\n# Example\nTF_CLI_ARGS_apply='-input=false -auto-approve' make infra-update-app-service APP_NAME=app ENVIRONMENT=dev\nTF_CLI_ARGS_apply='-var=image_tag=abcdef1' make infra-update-app-service APP_NAME=app ENVIRONMENT=dev\n```\n\n## Using terraform CLI wrapper scripts\n\nAn alternative to using the Makefile is to directly use the terraform wrapper scripts that the Makefile uses:\n\n```bash\nproject-root$ ./bin/terraform-init.sh app/service dev\nproject-root$ ./bin/terraform-apply.sh app/service dev\nproject-root$ ./bin/terraform-init-and-apply.sh app/service dev  # calls init and apply in the same script\n```\n\nLook in the script files for more details on usage.\n\n## Using terraform CLI directly\n\nFinally, if the wrapper scripts don't meet your needs, you can always run terraform directly from the root module directory. You may need to do this if you are running terraform commands other than `terraform plan` and `terraform apply`, such as `terraform import`, `terraform taint`, etc. To do this, you'll need to pass in the appropriate `tfvars` and `tfbackend` files to `terraform init` and `terraform apply`. For example, to make changes to the application's service resources in the dev environment, cd to the `infra/app/service` directory and run:\n\n```bash\ninfra/app/service$ terraform init -backend-config=dev.s3.tfbackend\ninfra/app/service$ terraform apply -var-file=dev.tfvars\n```"}
{"path":"infra/modules/monitoring/main.tf","language":"unknown","type":"code","directory":"infra/modules/monitoring","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/monitoring/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/module-architecture.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/module-architecture.md\nSize: 3.55 KB\nLast Modified: 2025-02-14T17:08:26.485Z"}
{"path":"infra/modules/monitoring/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/monitoring","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/monitoring/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"This doc describes how Terraform modules are structured.\n\n## Code structure\n\nThe infrastructure code is organized as follows. [Root modules](https://www.terraform.io/language/modules#the-root-module) are modules that are deployed separately from each other, whereas child modules are reusable modules that are called from root modules.\n\n```text\ninfra/                  Infrastructure code\n  accounts/             Root module for IaC and IAM resources\n  app/                  Application-specific infrastructure\n    build-repository/   Root module for resources storing built release candidates used for deploys\n    network/            (In development) Root module for virtual network resources\n    database/           (In development) Root module for database resources\n    service/            Root module for application service resources (load balancer, application service)\n  modules/              Reusable child modules\n```\n\n## Module calling structure\n\nThe following diagram describes the relationship between modules and their child modules. Arrows go from the caller module to the child module.\n\n```mermaid\nflowchart TB\n\n  classDef default fill:#FFF,stroke:#000\n  classDef root-module fill:#F37100,stroke-width:3,font-family:Arial\n  classDef child-module fill:#F8E21A,font-family:Arial\n\n  subgraph infra\n    account:::root-module\n\n    subgraph app\n      app/build-repository[build-repository]:::root-module\n      app/network[network]:::root-module\n      app/database[database]:::root-module\n      app/service[service]:::root-module\n    end\n\n    subgraph modules\n      terraform-backend-s3:::child-module\n      auth-github-actions:::child-module\n      container-image-repository:::child-module\n      network:::child-module\n      database:::child-module\n      web-app:::child-module\n    end\n\n    account --> terraform-backend-s3\n    account --> auth-github-actions\n    app/build-repository --> container-image-repository\n    app/network --> network\n    app/database --> database\n    app/service --> web-app\n\n  end\n```\n\n## Application environments\n\nAn application may have multiple environments (e.g. dev, staging, prod). The environments share the same root modules but will have different configurations. The configurations are saved as separate `.tfvars` and `.tfbackend` files named after the environment. For example, the `app/service` infrastructure resources for the `dev` environment will be configured via `dev.tfvars` and `dev.s3.tfbackend` files in the `infra/app/service` module directory.\n\n## Module dependencies\n\nThe following diagram illustrates the dependency structure of the root modules.\n\n1. Account root modules need to be deployed first to create the S3 bucket and DynamoDB tables needed to configure the Terraform backends in the rest of the root modules.\n2. The application's build repository needs to be deployed next to create the resources needed to store the built release candidates that are deployed to the application environments.\n3. The individual application environment root modules are deployed last once everything else is set up. These root modules are the ones that are deployed regularly as part of application deployments.\n\n```mermaid\nflowchart RL\n\nclassDef default fill:#F8E21A,stroke:#000,font-family:Arial\n\napp/service --> app/build-repository --> accounts\napp/service --> accounts\napp/service --> app/network\napp/service --> app/database --> app/network --> accounts\napp/database --> accounts\n```\n\n## Making changes to infrastructure\n\nNow that you understand how the modules are structured, see [making changes to infrastructure](./making-infra-changes.md)."}
{"path":"infra/modules/monitoring/variables.tf","language":"unknown","type":"code","directory":"infra/modules/monitoring","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/monitoring/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/module-dependencies.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/module-dependencies.md\nSize: 4.04 KB\nLast Modified: 2025-02-14T17:08:26.485Z"}
{"path":"infra/modules/network/flow-logs.tf","language":"unknown","type":"code","directory":"infra/modules/network","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/network/flow-logs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"These are the principles that guide the design of the infrastructure template.\n\n## Use explicit outputs and variables to connect resources across child modules in the same root module\n\nIf a resource in module B depends on a resource in module A, and both modules are called from the same root module, then create an output in module A with the information that is needed by module B, and pass that into module B as an input variable.\n\n```terraform\n# root-module/main.tf\n\nmodule \"a\" {\n  ...\n}\n\nmodule \"b\" {\n  input = module.a.output\n}\n```\n\nThis makes the dependencies between the resources explicit:\n\n```mermaid\nflowchart LR\n\nsubgraph A[module A]\n  output\nend\n\nsubgraph B[module B]\n  input\nend\n\ninput -- depends on --> output\n```\n\n**Do not** use [data sources](https://developer.hashicorp.com/terraform/language/data-sources) to reference resource dependencies in the same root module. A data source does not represent a dependency in [terraform's dependency graph](https://developer.hashicorp.com/terraform/internals/graph), and therefore there will potentially be a race condition, as Terraform will not know that it needs to create/update the resource in module A before it creates/updates the resource in module B that depends on it.\n\n## Use config modules and data resources to manage dependencies between root modules\n\nIf a resource in root module S depends on a resource in root module R, it is not possible to specify the dependency directly since the resources are managed in separate state files. In this situation, use a [data source](https://developer.hashicorp.com/terraform/language/data-sources) in module S to reference the resource in module R, and use a shared configuration module that specifies identifying information that is used both to create the resource in R and to query for the resource in S.\n\n```terraform\n# root module R\n\nmodule \"config\" {\n  ...\n}\n\nresource \"parent\" \"p\" {\n  identifier = module.config.parent_identifier\n}\n```\n\n```terraform\n# root module S\n\nmodule \"config\" {\n  ...\n}\n\ndata \"parent\" \"p\" {\n  identifier = module.config.parent_identifier\n}\n\nresource \"child\" \"c\" {\n  input = data.parent.p.some_attribute\n}\n```\n\nThis makes the dependency explicit, but indirect. Instead of one resource directly depending on the other, both resources depend on a shared config value(s) that uniquely identifies the parent resource. If the parent resource changes, the data source will also change, triggering the appropriate change in the child resource. If identifying information about the parent resource changes, it must be done through the shared configuration module so that the data source's query remains in sync.\n\n```mermaid\nflowchart RL\n\nsubgraph config[config module]\n  config_value[config value]\nend\n\nsubgraph R[root module R]\n  parent[parent resource]\nend\n\nsubgraph S[root module S]\n  data.parent[parent data source]\n  child[child resource]\nend\n\nparent -- depends on --> config_value\ndata.parent -- depends on --> config_value\nchild -- depends on --> data.parent\n```\n\n## When it is not feasible to create resources using static configuration values, use root module outputs and configuration scripts to manage dependencies between root modules\n\nIn rare cases, it is not feasible to use configuration values to create a resource. In this situation, if a resource in root module S depends on a resource in root module R, create an output in R with the information that is needed by module S. Then create a configuration script for S that reads from R's output and saves the relevant information in a `.tfvars` file that S can use to specify input variables.\n\nOne example of this is the terraform bucket name that is used by the `data.terraform_remote_state.current_image_tag` data source in the `service` module. The bucket name is generated dynamically using the current AWS user's account ID, and is therefore is not specified statically via configuration.\n\nThis method should be used minimally as it is the least explicit and most brittle of all the methods. The dependency between modules remains implicit, and there is additional logic in shell scripts to maintain."}
{"path":"infra/modules/network/main.tf","language":"unknown","type":"code","directory":"infra/modules/network","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/network/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/set-up-app-build-repository.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-app-build-repository.md\nSize: 1.02 KB\nLast Modified: 2025-02-14T17:08:26.485Z"}
{"path":"infra/modules/network/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/network","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/network/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"The application build repository setup process will create infrastructure resources needed to store built release candidate artifacts used to deploy the application to an environment.\n\n## Requirements\n\nBefore setting up the application's build repository you'll need to have:\n\n1. [Set up the AWS account](./set-up-aws-account.md)\n\n## 1. Configure backend\n\nTo create the tfbackend file for the build repository using the backend configuration values from your current AWS account, run\n\n```bash\nmake infra-configure-app-build-repository APP_NAME=app\n```\n\nPass in the name of the app folder within `infra`. By default this is `app`.\n\n## 2. Create build repository resources\n\nNow run the following commands to create the resources, making sure to verify the plan before confirming the apply.\n\n```bash\nmake infra-update-app-build-repository APP_NAME=app\n```\n\n## Set up application environments\n\nOnce you set up the deployment process, you can proceed to [set up application environments](./set-up-app-env.md)"}
{"path":"infra/modules/network/variables.tf","language":"unknown","type":"code","directory":"infra/modules/network","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/network/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/set-up-app-env.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-app-env.md\nSize: 3.01 KB\nLast Modified: 2025-02-14T17:08:26.486Z"}
{"path":"infra/modules/network/vpc-endpoints.tf","language":"unknown","type":"code","directory":"infra/modules/network","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/network/vpc-endpoints.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"The application environment setup process will:\n\n1. Configure a new application environment and create the infrastructure resources for the application in that environment\n\n## Requirements\n\nBefore setting up the application's environments you'll need to have:\n\n1. [A compatible application in the app folder](https://github.com/navapbc/template-infra/blob/main/template-only-docs/application-requirements.md)\n2. [Configure the app](../../infra/frontend/app-config/main.tf). Make sure you update `has_database` to `true` or `false` depending on whether or not your application has a database to integrate with.\n3. (If the application has a database) [Set up the network with VPC endpoints required for the database](./set-up-network.md)\n4. (If the application has a database) [Set up the database for the application](./set-up-database.md)\n5. (If you have an incident management service) [Set up monitoring](./set-up-monitoring-alerts.md)\n6. [Set up the application build repository](./set-up-app-build-repository.md)\n\n## 1. Configure backend\n\nTo create the tfbackend and tfvars files for the new application environment, run\n\n```bash\nmake infra-configure-app-service APP_NAME=app ENVIRONMENT=<ENVIRONMENT>\n```\n\n`APP_NAME` needs to be the name of the application folder within the `infra` folder. It defaults to `app`.\n`ENVIRONMENT` needs to be the name of the environment you are creating. This will create a file called `<ENVIRONMENT>.s3.tfbackend` in the `infra/app/service` module directory.\n\nDepending on the value of `has_database` in the [app-config module](/infra/frontend/app-config/main.tf), the application will be configured with or without database access.\n\n## 2. Build and publish the application to the application build repository\n\nBefore creating the application resources, you'll need to first build and publish at least one image to the application build repository.\n\nThere are two ways to do this:\n\n1. Trigger the \"Build and Publish\" workflow from your repo's GitHub Actions tab. This option requires that the `role-to-assume` GitHub workflow variable has already been setup as part of the overall infra account setup process.\n1. Alternatively, run the following from the root directory. This option can take much longer than the GitHub workflow, depending on your machine's architecture.\n\n   ```bash\n   make release-build\n   make release-publish\n   ```\n\nCopy the image tag name that was published. You'll need this in the next step.\n\n## 3. Create application resources with the image tag that was published\n\nNow run the following commands to create the resources, using the image tag that was published from the previous step. Review the terraform before confirming \"yes\" to apply the changes.\n\n```bash\nTF_CLI_ARGS_apply=\"-var=image_tag=<IMAGE_TAG>\" make infra-update-app-service APP_NAME=app ENVIRONMENT=<ENVIRONMENT>\n```\n\n## 4. Configure monitoring alerts\n\nConfigure email alerts, external incident management service integration and additional Cloudwatch Alerts.\n[Configure monitoring module](./set-up-monitoring-alerts.md)"}
{"path":"infra/modules/search/authentication.tf","language":"unknown","type":"code","directory":"infra/modules/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/search/authentication.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/set-up-aws-account.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-aws-account.md\nSize: 3.14 KB\nLast Modified: 2025-02-14T17:08:26.486Z"}
{"path":"infra/modules/search/main.tf","language":"unknown","type":"code","directory":"infra/modules/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/search/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"The AWS account setup process will:\n\n1. Create the [Terraform backend](https://www.terraform.io/language/settings/backends/configuration) resources needed to store Terraform's infrastructure state files. The project uses an [S3 backend](https://www.terraform.io/language/settings/backends/s3).\n2. Create the [OpenID connect provider in AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html) to allow GitHub actions to access AWS account resources.\n\n## Prerequisites\n\n- You'll need to have [set up infrastructure tools](./set-up-infrastructure-tools.md), like Terraform, AWS CLI, and AWS authentication.\n- You'll also need to make sure the [project is configured](../..//infra/project-config/main.tf).\n\n## Overview of Terraform backend management\n\nThe approach to backend management allows Terraform to both create the resources needed for a remote backend as well as allow terraform to store that configuration state in that newly created backend. This also allows us to seperate infrastructure required to support terraform from infrastructure required to support the application. Because each backend, bootstrap or environment, stores their own terraform.tfstate in these buckets, ensure that any backends that are shared use a unique key. When using a non-default workspace, the state path will be `/workspace_key_prefix/workspace_name/key`, `workspace_key_prefix` default is `env:`\n\n## Instructions\n\n### 1. Make sure you're authenticated into the AWS account you want to configure\n\nThe account set up sets up whatever account you're authenticated into. To see which account that is, run\n\n```bash\naws sts get-caller-identity\n```\n\nTo see a more human readable account alias instead of the account, run\n\n```bash\naws iam list-account-aliases\n```\n\n### 2. Create backend resources and tfbackend config file\n\nRun the following command, replacing `<ACCOUNT_NAME>` with a human readable name for the AWS account that you're authenticated into. The account name will be used to prefix the created tfbackend file so that it's easier to visually identify as opposed to identifying the file using the account id. For example, you have an account per environment, the account name can be the name of the environment (e.g. \"prod\" or \"staging\"). Or if you are setting up an account for all lower environments, account name can be \"lowers\". If your AWS account has an account alias, you can also use that.\n\n```bash\nmake infra-set-up-account ACCOUNT_NAME=<ACCOUNT_NAME>\n```\n\nThis command will create the S3 tfstate bucket and the GitHub OIDC provider. It will also create a `[account name].[account id].s3.tfbackend` file in the `infra/accounts` directory.\n\n### 3. Update the account names map in app-config\n\nIn [app-config/main.tf](/infra/frontend/app-config/main.tf), update the `account_names_by_environment` config to reflect the account name you chose.\n\n## Making changes to the account\n\nIf you make changes to the account terraform and want to apply those changes, run\n\n```bash\nmake infra-update-current-account\n```\n\n## Destroying infrastructure\n\nTo undeploy and destroy infrastructure, see [instructions on destroying infrastructure](./destroy-infrastructure.md)."}
{"path":"infra/modules/search/networking.tf","language":"unknown","type":"code","directory":"infra/modules/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/search/networking.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/set-up-database.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-database.md\nSize: 4.16 KB\nLast Modified: 2025-02-14T17:08:26.486Z"}
{"path":"infra/modules/search/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/search/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"The database setup process will:\n\n1. Configure and deploy an application database cluster using [Amazon Aurora Serverless V2](https://aws.amazon.com/rds/aurora/serverless/)\n2. Create a [PostgreSQL schema](https://www.postgresql.org/docs/current/ddl-schemas.html) `app` to contain tables used by the application.\n3. Create an IAM policy that allows IAM roles with that policy attached to [connect to the database using IAM authentication](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.Connecting.html)\n4. Create an [AWS Lambda function](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html), the \"role manager\", for provisioning the [PostgreSQL database users](https://www.postgresql.org/docs/8.0/user-manag.html) that will be used by the application service and by the migrations task.\n5. Invoke the role manager function to create the `app` and `migrator` Postgres users.\n\n## Requirements\n\nBefore setting up the database you'll need to have:\n\n1. [Set up the AWS account](./set-up-aws-account.md)\n2. pip installed (pip is needed to download dependencies for the role manager Lambda function)\n\n## 1. Configure backend\n\nTo create the tfbackend file for the new application environment, run\n\n```bash\nmake infra-configure-app-database APP_NAME=<APP_NAME> ENVIRONMENT=<ENVIRONMENT>\n```\n\n`APP_NAME` needs to be the name of the application folder within the `infra` folder. By default, this is `app`.\n`ENVIRONMENT` needs to be the name of the environment you are creating. This will create a file called `<ENVIRONMENT>.s3.tfbackend` in the `infra/app/service` module directory.\n\n## 2. Create database resources\n\nNow run the following commands to create the resources. Review the terraform before confirming \"yes\" to apply the changes. This can take over 5 minutes.\n\n```bash\nmake infra-update-app-database APP_NAME=app ENVIRONMENT=<ENVIRONMENT>\n```\n\n## 3. Create Postgres users\n\nTrigger the role manager Lambda function that was created in the previous step in order to create the application and migrator Postgres users.\n\n```bash\nmake infra-update-app-database-roles APP_NAME=app ENVIRONMENT=<ENVIRONMENT>\n```\n\nThe Lambda function's response should describe the resulting PostgreSQL roles and groups that are configured in the database. It should look like a minified version of the following:\n\n```json\n{\n  \"roles\": [\"postgres\", \"migrator\", \"app\"],\n  \"roles_with_groups\": {\n    \"rds_superuser\": \"rds_password\",\n    \"pg_monitor\": \"pg_read_all_settings,pg_read_all_stats,pg_stat_scan_tables\",\n    \"postgres\": \"rds_superuser\",\n    \"app\": \"rds_iam\",\n    \"migrator\": \"rds_iam\"\n  },\n  \"schema_privileges\": {\n    \"public\": \"{postgres=UC/postgres,=UC/postgres}\",\n    \"app\": \"{migrator=UC/migrator,app=U/migrator}\"\n  }\n}\n```\n\n### Important note on Postgres table permissions\n\nBefore creating migrations that create tables, first create a migration that includes the following SQL command (or equivalent if your migrations are written in a general purpose programming language):\n\n```sql\nALTER DEFAULT PRIVILEGES GRANT ALL ON TABLES TO app\n```\n\nThis will cause all future tables created by the `migrator` user to automatically be accessible by the `app` user. See the [Postgres docs on ALTER DEFAULT PRIVILEGES](https://www.postgresql.org/docs/current/sql-alterdefaultprivileges.html) for more info.\n\nWhy is this needed? The reason is because the `migrator` role will be used by the migration task to run database migrations (creating tables, altering tables, etc.), while the `app` role will be used by the web service to access the database. Moreover, in Postgres, new tables won't automatically be accessible by roles other than the creator unless specifically granted, even if those other roles have usage access to the schema that the tables are created in. In other words if the `migrator` user created a new table `foo` in the `app` schema, the `app` user will not have automatically be able to access it by default.\n\n## 4. Check that database roles have been configured properly\n\n```bash\nmake infra-check-app-database-roles APP_NAME=app ENVIRONMENT=<ENVIRONMENT>\n```\n\n## Set up application environments\n\nOnce you set up the deployment process, you can proceed to [set up the application service](./set-up-app-env.md)"}
{"path":"infra/modules/search/variables.tf","language":"unknown","type":"code","directory":"infra/modules/search","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/search/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/set-up-infrastructure-tools.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-infrastructure-tools.md\nSize: 3.96 KB\nLast Modified: 2025-02-14T17:08:26.486Z"}
{"path":"infra/modules/secret/main.tf","language":"unknown","type":"code","directory":"infra/modules/secret","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/secret/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"If you are contributing to infrastructure, you will need to complete these setup steps.\n\n## Prerequisites\n\n### Install Terraform\n\n[Terraform](https://www.terraform.io/) is an infrastructure as code (IaC) tool that allows you to build, change, and version infrastructure safely and efficiently. This includes both low-level components like compute instances, storage, and networking, as well as high-level components like DNS entries and SaaS features.\n\nYou may need different versions of Terraform since different projects may require different versions of Terraform. The best way to manage Terraform versions is with [Terraform Version Manager](https://github.com/tfutils/tfenv).\n\nTo install via [Homebrew](https://brew.sh/)\n\n```bash\nbrew install tfenv\n```\n\nThen install the version of Terraform you need.\n\n```bash\ntfenv install 1.4.6\n```\n\nIf you are unfamiliar with Terraform, check out this [basic introduction to Terraform](./intro-to-terraform.md).\n\n### Install AWS CLI\n\nThe [AWS Command Line Interface (AWS CLI)](https://aws.amazon.com/cli/) is a unified tool to manage your AWS services. With just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts. Install the AWS commmand line tool by following the instructions found here:\n\n- [Install AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)\n\n### Install Go\n\nThe [Go programming language](https://go.dev/dl/) is required to run [Terratest](https://terratest.gruntwork.io/), the unit test framework for Terraform.\n\n### Install GitHub CLI\n\nThe [GitHub CLI](https://cli.github.com/) is useful for automating certain operations for GitHub such as with GitHub actions. This is needed to run [check-github-actions-auth.sh](/bin/check-github-actions-auth.sh)\n\n```bash\nbrew install gh\n```\n\n### Install linters\n\n[Shellcheck](https://github.com/koalaman/shellcheck) and [actionlint](https://github.com/rhysd/actionlint) are optional utilites for running infrastructure linters locally.\n\n```bash\nbrew install shellcheck\nbrew install actionlint\n```\n\n## AWS Authentication\n\nIn order for Terraform to authenticate with your accounts you will need to configure your aws credentials using the AWS CLI or manually create your config and credentials file. If you need to manage multiple credentials or create named profiles for use with different environments you can add the `--profile` option.\n\nThere are multiple ways to authenticate, but we recommend creating a separate profile for your project in your AWS credentials file, and setting your local environment variable `AWS_PROFILE` to the profile name. We recommend using [direnv](https://direnv.net/) to manage local environment variables.\n**Credentials should be located in ~/.aws/credentials** (Linux & Mac) or **%USERPROFILE%\\.aws\\credentials** (Windows)\n\n### Examples\n\n```bash\n$ aws configure\nAWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE\nAWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nDefault region name [None]: us-east-2\nDefault output format [None]: json\n```\n\n**Using the above command will create a [default] profile.**\n\n```bash\n$ aws configure --profile dev\nAWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE\nAWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nDefault region name [None]: us-east-2\nDefault output format [None]: json\n```\n\n**Using the above command will create a [dev] profile.**\n\nOnce you're done, verify access by running the following command to print out information about the AWS IAM user you authenticated as.\n\n```bash\naws sts get-caller-identity\n```\n\n### References\n\n- [Configuration basics][1]\n- [Named profiles for the AWS CLI][2]\n- [Configuration and credential file settings][3]\n\n[1]: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html\n[2]: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html\n[3]: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html"}
{"path":"infra/modules/secret/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/secret","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/secret/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/set-up-monitoring-alerts.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-monitoring-alerts.md\nSize: 1.40 KB\nLast Modified: 2025-02-14T17:08:26.486Z"}
{"path":"infra/modules/secret/variables.tf","language":"unknown","type":"code","directory":"infra/modules/secret","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/secret/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"## Overview\n\nThe monitoring module defines metric-based alerting policies that provides awareness into issues with the cloud application. The module supports integration with external incident management tools like Splunk-On-Call or Pagerduty. It also supports email alerts.\n\n### Set up email alerts.\n\n1. Add the `email_alerts_subscription_list` variable to the monitoring module call in the service layer\n\nFor example:\n\n```\nmodule \"monitoring\" {\n  source = \"../../modules/monitoring\"\n  email_alerts_subscription_list = [\"email1@email.com\", \"email2@email.com\"]\n  ...\n}\n```\n\n2. Run `make infra-update-app-service APP_NAME=<APP_NAME> ENVIRONMENT=<ENVIRONMENT>` to apply the changes to each environment.\n   When any of the alerts described by the module are triggered notification will be send to all email specified in the `email_alerts_subscription_list`\n\n### Set up External incident management service integration.\n\n1. Set setting `has_incident_management_service = true` in app-config/main.tf\n2. Get the integration URL for the incident management service and store it in AWS SSM Parameter Store by running the following command for each environment:\n\n```\nmake infra-configure-monitoring-secrets APP_NAME=<APP_NAME> ENVIRONMENT=<ENVIRONMENT> URL=<WEBHOOK_URL>\n```\n\n3. Run `make infra-update-app-service APP_NAME=<APP_NAME> ENVIRONMENT=<ENVIRONMENT>` to apply the changes to each environment."}
{"path":"infra/modules/service/access-control.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/access-control.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/set-up-network.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/set-up-network.md\nSize: 0.73 KB\nLast Modified: 2025-02-14T17:08:26.486Z"}
{"path":"infra/modules/service/access-logs.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/access-logs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"The network setup process will:\n\n1. Configure and deploy network resources needed by other modules. If your application has a database, it will create VPC endpoints for the AWS services needed by the database layer and a security group to contain those VPC endpoints.\n\n## Requirements\n\nBefore setting up the database you'll need to have:\n\n1. [Set up the AWS account](./set-up-aws-account.md)\n\n## 1. Configure backend\n\nTo create the tfbackend file for the new application environment, run\n\n```bash\nmake infra-configure-network\n```\n\n## 2. Create network resources\n\nNow run the following commands to create the resources. Review the terraform before confirming \"yes\" to apply the changes.\n\n```bash\nmake infra-update-network\n```"}
{"path":"infra/modules/service/application-logs.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/application-logs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/infra/vulnerability-management.md\nLanguage: md\nType: code\nDirectory: documentation/infra\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/infra/vulnerability-management.md\nSize: 3.11 KB\nLast Modified: 2025-02-14T17:08:26.486Z"}
{"path":"infra/modules/service/autoscaling.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/autoscaling.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"This repository contains a GitHub workflow that allows you to scan Docker images for vulnerabilities. The workflow, named `ci-vulnerability-scans` is located in the directory `.github/workflows`. The goal in scanning the image before pushing it to the repository is so that you can catch any vulnerabilities before deploying the image, ECR scanning takes time and the image can still be used even with vulnerabilities found by Inspector. Also, if you use `scratch` as a base image, ECR is unable to scan the image when it is pushed, which is a known issue.\n\nA way to ensure that there are smaller surface areas for vulnerabilities, follow this method of building images\n\n- Build base image with required packages, name it something like `build`\n- Configure app build from the image in the previous step, name it something like `app-build`\n- Create a final image from `scratch` named `release` (ie `from scratch as release`), and copy any needed directories from the `app-build` image\n\n```\nFROM ... AS build\n# Do base installs for dev and app-build here\nFROM build AS dev\n# Local dev installs only\nFROM build AS app-build\n# All installs for the release image\n# Any tweaks needed for the release image\nFROM scratch AS release\n# Copy over the files from app-build\n# COPY --from=app-build /app-build/paths/to/files /release/paths/to/files\n```\n\nBy following this method, your deployment image will have the minimum required directories and files, it will shrink the overall image size, and reduce findings\n\n## How to use Workflow\n\nThe workflow will run whenever there is a push to a PR or when merged to main if there are changes in the `app` direcotry. It is scanning in both cases to ensure there is no issues if a PR is approved on a Friday, but isn't merged till Monday - a CVE could have been found in the time between the last run and the merge.\n\n## Notes about Scanners\n\n### Hadolint\n\nThe hadolint scanner allows you to ignore or safelist certain findings, which can be specified in the [.hadolint.yaml](../../.hadolint.yaml) file. There is a template file here that you can use in your repo.\n\n### Trivy\n\nThe trivy scanner allows you to ignore or safelist certain findings, which can be specified in the [.trivyignore](../../.trivyignore) file. There is a template file here that you can use in your repo.\n\n### Anchore\n\nThe anchore scanner allows you to ignore or safelist certain findings, which can be specified in the [.grype.yml](../../.grype.yml) file. There is a template file here that you can use in your repo. There are flags set to ignore findings that are in the state `not-fixed`, `wont-fix`, and `unknown`.\n\n### Dockle\n\nThe dockle scanner action does not have the ability to use an ignore or safelist findings file, but is able to by specifying an allow file, or `DOCKLE_ACCEPT_FILES`, environmental variable. To get around this, there is a step before the dockle scan is ran to check for a file named [.dockleconfig](../../.dockleconfig), and pipe it to the environmental variable if it exists. Note that this will not ignore finding types like the other scanner's ignore file, but ignore the file specified in the list"}
{"path":"infra/modules/service/cdn-logs.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/cdn-logs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/notice_of_funding_opportunity_prototypes/README.md\nLanguage: md\nType: code\nDirectory: documentation/notice_of_funding_opportunity_prototypes\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/notice_of_funding_opportunity_prototypes/README.md\nSize: 0.30 KB\nLast Modified: 2025-02-14T17:08:26.486Z"}
{"path":"infra/modules/service/cdn.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/cdn.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"This directory contains Notice of Funding Opportunties with simplified language and visual and user-centered design enhancements to increase their readability and usability. These prototypes follow the principle that funding opportunities should be easy to read and understand."}
{"path":"infra/modules/service/command-execution.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/command-execution.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/user-research/README.md\nLanguage: md\nType: code\nDirectory: documentation/user-research\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/user-research/README.md\nSize: 0.68 KB\nLast Modified: 2025-02-14T17:08:26.543Z"}
{"path":"infra/modules/service/database-access.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/database-access.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"This folder contains the outputs of previous user research efforts on the project that have been used to guide our strategy and vision for the Simpler.Grants.gov initiative.\n\n### Disclaimer\n\nThe documents in this folder are meant for user research purposes only. While this research has informed the product roadmap for Simpler.Grants.gov, any references to specific recommendations, designs, or features **does not** mean that the functionality described will be delivered as part of our roadmap.\n\nFor the most up-to-date list of planned deliverables for the Simpler.Grants.gov project, please reference our [provisional roadmap](https://github.com/orgs/HHS/projects/12)."}
{"path":"infra/modules/service/dns.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/dns.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/README.md\nLanguage: md\nType: code\nDirectory: documentation/wiki\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/README.md\nSize: 1.20 KB\nLast Modified: 2025-02-14T17:08:26.596Z"}
{"path":"infra/modules/service/jobs.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/jobs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"# ðŸ‘‹ Welcome\n\n{% hint style=\"warning\" %}\n**In Progress** :construction:\n\nPardon the construction dust! We're still setting up our public wiki, so pages may be changing names or getting moved.&#x20;\n{% endhint %}\n\nLooking to join the Simpler Grants community? Start here:\n\n{% content-ref url=\"get-involved/get-involved.md\" %}\n[get-involved.md](get-involved/get-involved.md)\n{% endcontent-ref %}\n\n## Find the information you need\n\n* [I want to meet the team that is working on the Simpler.Grants.gov initiative](about/team.md)\n* [I want to learn more about the features the team is planning to build for simpler.grants.gov](product/product-roadmap.md)\n* [I want to learn more about the tools the team is using to collaborate](get-involved/communication-channels/)\n* [I want to figure out how I can get more involved with the project](get-involved/get-involved.md)"}
{"path":"infra/modules/service/load-balancer.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/load-balancer.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/SUMMARY.md\nLanguage: md\nType: code\nDirectory: documentation/wiki\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/SUMMARY.md\nSize: 9.22 KB\nLast Modified: 2025-02-14T17:08:26.596Z"}
{"path":"infra/modules/service/main.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"* [ðŸ‘‹ Welcome](README.md)\n\n## About\n\n* [Team](about/team.md)\n* [Terminology](about/terminology.md)\n\n## GET INVOLVED\n\n* [Why open source?](get-involved/get-involved.md)\n* [How to contribute code](get-involved/github-code.md)\n* [How to contribute issues](get-involved/github-planning/README.md)\n  * [Report a bug](get-involved/github-planning/filling-out-a-bug-report.md)\n  * [Request a feature](get-involved/github-planning/filling-out-a-feature-request.md)\n  * [Report a security vulnerability](get-involved/github-planning/reporting-a-security-vulnerability.md)\n* [Community guidelines](get-involved/policies-and-guidelines/README.md)\n  * [Code of Conduct](get-involved/policies-and-guidelines/code-of-conduct.md)\n  * [Content privacy](get-involved/policies-and-guidelines/content-privacy.md)\n  * [Reporting and removing content](get-involved/policies-and-guidelines/reporting-and-removing-content.md)\n  * [Incident response protocol](get-involved/policies-and-guidelines/incident-response-protocol.md)\n* [Community events](get-involved/community-events/README.md)\n  * [Fall 2024 Coding Challenge](get-involved/community-events/fall-2024-coding-challenge.md)\n* [Communication channels](get-involved/communication-channels/README.md)\n  * [Slack - Community chat](get-involved/communication-channels/slack-community-chat/README.md)\n    * [Naming Conventions](get-involved/communication-channels/slack-community-chat/naming-conventions.md)\n    * [Recommended Channels](get-involved/communication-channels/slack-community-chat/recommended-channels.md)\n    * [Installing Slack](get-involved/communication-channels/slack-community-chat/installing-slack.md)\n  * [Zoom - Public Meetings](get-involved/communication-channels/zoom-public-meetings.md)\n\n## Product\n\n* [Roadmap](product/product-roadmap.md)\n* [Deliverables](product/deliverables/README.md)\n  * [ðŸ Static site soft launch](product/deliverables/static-site-soft-launch.md)\n  * [ðŸ Static site public launch](product/deliverables/static-site-public-launch.md)\n  * [ðŸ GET Opportunities](product/deliverables/get-opportunities.md)\n  * [ðŸ Open source onboarding](product/deliverables/open-source-onboarding.md)\n  * [ðŸ Co-Design Group planning](product/deliverables/co-design-group.md)\n* [Decisions](product/decisions/README.md)\n  * [ADR Template](product/decisions/template.md)\n  * [ADRs](product/decisions/adr/README.md)\n    * [Recording Architecture Decisions](product/decisions/adr/2023-06-26-recording-architecture-decisions.md)\n    * [Task Runner for the CI / CD Pipeline](product/decisions/adr/2023-06-29-ci-cd-task-runner.md)\n    * [API Language](product/decisions/adr/2023-06-30-api-language.md)\n    * [Use Figma for design prototyping](product/decisions/adr/2023-07-03-design-prototyping-tool.md)\n    * [ADR: Chat](product/decisions/adr/2023-07-05-chat-adr.md)\n    * [DB Choices](product/decisions/adr/2023-07-05-db-choices.md)\n    * [API Framework and Libraries](product/decisions/adr/2023-07-07-api-framework.md)\n    * [Back-end Code Quality Tools](product/decisions/adr/2023-07-07-backend-tooling.md)\n    * [Front-end Language](product/decisions/adr/2023-07-10-front-end-language.md)\n    * [Communications Tooling: Wiki Platform](product/decisions/adr/2023-07-10-wiki-platform.md)\n    * [Use Mural for design diagrams and whiteboarding](product/decisions/adr/2023-07-11-design-diagramming-tool.md)\n    * [Ticket Tracking](product/decisions/adr/2023-07-11-ticket-tracking.md)\n    * [Front-end Framework](product/decisions/adr/2023-07-14-front-end-framework.md)\n    * [Front-end Code Quality Tools](product/decisions/adr/2023-07-17-frontend-tooling.md)\n    * [Front-end Testing & Coverage](product/decisions/adr/2023-07-18-frontend-testing.md)\n    * [Backend API Type](product/decisions/adr/2023-07-19-backend-api-type.md)\n    * [Front-end Testing & Coverage](product/decisions/adr/2023-07-19-backend-testing.md)\n    * [Deployment Strategy](product/decisions/adr/2023-07-20-deployment-strategy.md)\n    * [Use U.S. Web Design System for components and utility classes](product/decisions/adr/2023-07-20-fe-design-system.md)\n    * [FE server rendering](product/decisions/adr/2023-07-20-fe-server-rendering.md)\n    * [Use NPM over Yarn Architectural Decision Records](product/decisions/adr/2023-07-20-fe-use-npm.md)\n    * [U.S. Web Design System in React](product/decisions/adr/2023-07-20-fe-uswds-in-react.md)\n    * [Communications Tooling: Video Conferencing](product/decisions/adr/2023-07-24-video-conferencing.md)\n    * [Back-end Production Server](product/decisions/adr/2023-07-26-backend-prod-server.md)\n    * [Communications Tooling: Analytics Platform](product/decisions/adr/2023-08-01-analytics-platform.md)\n    * [Commit and Branch Conventions and Release Workflow](product/decisions/adr/2023-08-21-branch-conv-release-workflow.md)\n    * [Cloud Platform to Host the Project](product/decisions/adr/2023-08-21-cloud-platform.md)\n    * [Infrastructure as Code Tool](product/decisions/adr/2023-08-21-infrastructure-as-code-tool.md)\n    * [Data Replication Strategy & Tool](product/decisions/adr/2023-09-07-data-replication-tool.md)\n    * [HHS Communications Site](product/decisions/adr/2023-09-22-hhs-comms-site.md)\n    * [Communications Tooling: Email Marketing](product/decisions/adr/2023-10-16-email-marketing.md)\n    * [Communications Tooling: Listserv](product/decisions/adr/2023-10-16-listserv.md)\n    * [Use Ethnio for design research](product/decisions/adr/use-ethnio-for-design-research.md)\n    * [Uptime Monitoring](product/decisions/adr/2023-11-22-uptime-monitoring.md)\n    * [Database Migrations](product/decisions/adr/2023-12-06-database-migrations.md)\n    * [30k ft deliverable reporting strategy](product/decisions/adr/2023-12-15-deliverable-reporting-strategy.md)\n    * [Public measurement dashboard architecture](product/decisions/adr/2023-12-18-measurement-dashboard-architecture.md)\n    * [Method and technology for \"Contact Us\" CTA](product/decisions/adr/2023-12-20-contact-us-email.md)\n    * [E2E / Integration Testing Framework](product/decisions/adr/2024-02-26-e2e-integration-testing-framework.md)\n    * [Logging and Monitoring Platform](product/decisions/adr/2024-03-04-logging-monitoring.md)\n    * [Dashboard Data Storage](product/decisions/adr/2024-03-19-dashboard-storage.md)\n    * [Dashboard Data Tool](product/decisions/adr/2024-04-10-dashboard-tool.md)\n    * [Search Engine](product/decisions/adr/2024-10-02-search-engine.md)\n    * [Document Storage](product/decisions/adr/2024-10-18-document-storage.md)\n    * [Document Sharing](product/decisions/adr/2024-11-14-document-sharing.md)\n    * [Internal Wiki ADR](product/decisions/adr/2024-11-20-internal-wiki.md)\n    * [Shared Team Calendar Platform](product/decisions/adr/2024-12-05-shared-team-calendar-platform.md)\n    * [Cross-Program Team Health Survey Tool](product/decisions/adr/2024-12-06-team-health-survey-tool.md)\n    * [Adding Slack Users to SimplerGrants Slack Workspace](product/decisions/adr/2024-12-17-adding-slack-users.md)\n    * [Repo organization](product/decisions/adr/2025-01-02-repo-organization.md)\n  * [Infra](product/decisions/infra/README.md)\n    * [Use markdown architectural decision records](product/decisions/infra/0000-use-markdown-architectural-decision-records.md)\n    * [CI/CD interface](product/decisions/infra/0001-ci-cd-interface.md)\n    * [Use custom implementation of GitHub OIDC](product/decisions/infra/0002-use-custom-implementation-of-github-oidc.md)\n    * [Manage ECR in prod account module](product/decisions/infra/0003-manage-ecr-in-prod-account-module.md)\n    * [Separate terraform backend configs into separate config files](product/decisions/infra/0004-separate-terraform-backend-configs-into-separate-config-files.md)\n    * [Database module design](product/decisions/infra/0005-database-module-design.md)\n    * [Provision database users with serverless function](product/decisions/infra/0006-provision-database-users-with-serverless-function.md)\n    * [Database migration architecture](product/decisions/infra/0007-database-migration-architecture.md)\n    * [Consolidate infra config from tfvars files into config module](product/decisions/infra/0008-consolidate-infra-config-from-tfvars-files-into-config-module.md)\n    * [Environment use cases](product/decisions/infra/0009-environment-use-cases.md)\n    * [Production networking long term state](product/decisions/infra/0010-production-networking-long-term-state.md)\n* [Analytics](product/simpler-grants.gov-analytics/README.md)\n  * [Open source community metrics](product/simpler-grants.gov-analytics/open-source-community-metrics.md)\n  * [API metrics](product/simpler-grants.gov-analytics/api-metrics.md)\n\n## DESIGN & RESEARCH\n\n* [Brand guidelines](design-and-research/brand-guidelines/README.md)\n  * [Logo](design-and-research/brand-guidelines/logo.md)\n  * [Colors](design-and-research/brand-guidelines/colors.md)\n  * [Grid and composition](design-and-research/brand-guidelines/grid-and-composition.md)\n  * [Typography](design-and-research/brand-guidelines/typography.md)\n  * [Iconography](design-and-research/brand-guidelines/iconography.md)\n  * [Photos and illustrations](design-and-research/brand-guidelines/photos-and-illustrations.md)\n* [Voice and tone guidelines](design-and-research/voice-and-tone-guide.md)\n* [User research](design-and-research/user-research/README.md)\n  * [Grants.gov archetypes](design-and-research/user-research/grants.gov-archetypes.md)"}
{"path":"infra/modules/service/networking.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/networking.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/about/team.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/about\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/about/team.md\nSize: 2.69 KB\nLast Modified: 2025-02-14T17:08:26.596Z"}
{"path":"infra/modules/service/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"# Team\n\n## Product and delivery\n\n<table data-card-size=\"large\" data-view=\"cards\"><thead><tr><th></th><th></th><th data-hidden></th></tr></thead><tbody><tr><td><strong>Lucas Brown</strong></td><td>HHS | Grants.gov Modernization Lead</td><td></td></tr><tr><td><strong>Julius Chang</strong></td><td>HHS | Grants.gov Program Manager</td><td></td></tr><tr><td><strong>Billy Daly</strong></td><td>agile six | Chief Technology Officer</td><td></td></tr><tr><td><strong>Sarah Knopp</strong></td><td>agile six | Senior Delivery Manager</td><td></td></tr></tbody></table>\n\n## Engineering and design\n\n<table data-card-size=\"large\" data-column-title-hidden data-view=\"cards\"><thead><tr><th>Name</th><th>Organization and role</th></tr></thead><tbody><tr><td><strong>Margaret Spring</strong></td><td>Nava | Program Manager</td></tr><tr><td><strong>Aaron Couch</strong></td><td>Nava | Engineering Lead</td></tr><tr><td><strong>Andy Cochran</strong></td><td>Nava | Design Lead</td></tr><tr><td><strong>Brandon Tabaska</strong></td><td>Nava | Open Source Developer Evangelist</td></tr><tr><td><p><strong>James Bursa</strong> </p><p> Nava | Principal Software Engineer</p></td><td></td></tr><tr><td><strong>Michael Chouinard</strong></td><td>Nava | Senior Software Engineer</td></tr><tr><td><strong>Risha Lee</strong></td><td>Nava | Research &#x26; Co-Design</td></tr><tr><td><strong>Crystabel Rangel</strong></td><td>Nava | Designer &#x26; Researcher</td></tr><tr><td><strong>Kai Siren</strong></td><td>Nava | Infrastructure Engineer</td></tr></tbody></table>\n\n## Communications\n\n<table data-card-size=\"large\" data-column-title-hidden data-view=\"cards\"><thead><tr><th>Name</th><th>Organization and role</th></tr></thead><tbody><tr><td><strong>Adriana Weitzman, PMP, CSM</strong></td><td>HHS/Office of Grants/Grants.gov | IT Specialist, Content Manager &#x26; Communications Lead</td></tr><tr><td><strong>Senongo Akpem</strong></td><td>Nava | Marketing and Branding</td></tr><tr><td><strong>Meghan Casey</strong></td><td>Nava | Communications &#x26; Content Strategy</td></tr><tr><td><strong>Carley Kimball</strong></td><td>MicroHealth | ITS Communications Lead</td></tr><tr><td><strong>Alexis Buncich</strong></td><td>MicroHealth | ITS Digital Communications Specialist</td></tr></tbody></table>\n\n## Change log\n\nMajor updates to the content of this page will be added here.\n\n<table><thead><tr><th>Date</th><th width=\"246\">Update</th><th>Notes</th></tr></thead><tbody><tr><td>2/12/2024</td><td>Initial Content</td><td>Updated with Initial content</td></tr><tr><td>10/7/2024</td><td>Updated P&#x26;D Roster to reflect new contract update</td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table>"}
{"path":"infra/modules/service/s3.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/s3.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/about/terminology.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/about\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/about/terminology.md\nSize: 4.40 KB\nLast Modified: 2025-02-14T17:08:26.596Z"}
{"path":"infra/modules/service/s3_buckets.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/s3_buckets.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"# Terminology\n\n## Federal abbreviations\n\nThe following acronyms represent abbreviations of federal stakeholder groups and concepts related to the grants process.\n\n<table><thead><tr><th width=\"215.5\">Term</th><th>Definition</th></tr></thead><tbody><tr><td>FACE</td><td><strong>F</strong>inancial <strong>A</strong>ssistance <strong>C</strong>ommittee for <strong>E</strong>-Governance â€” various federal agencies who all do grantmaking</td></tr><tr><td>ECGAP</td><td><strong>E</strong>xecutive <strong>C</strong>ommittee on <strong>G</strong>rants <strong>A</strong>dministration <strong>P</strong>olicy</td></tr><tr><td>CGMOs</td><td><strong>C</strong>ouncil of <strong>G</strong>rants <strong>M</strong>anagement <strong>O</strong>fficers â€” various federal agencies who all do grantmaking</td></tr><tr><td>NOFO</td><td><strong>N</strong>otice <strong>o</strong>f <strong>F</strong>unding <strong>O</strong>pportunity â€” also called \"grant announcements\" </td></tr><tr><td>S2S (Grantors and Applicants)</td><td><p><strong>S</strong>ystem to <strong>S</strong>ystem - Grants.gov provides an extensive list of web services for Applicants and Agencies to perform the Find and Apply functions using our System-to-System (S2S) interface.</p><p></p><p>Applicants and grantor agencies typically integrate the Grants.gov web services into their existing grant management systems to provide a seamless user interface to their grants staff.</p></td></tr><tr><td>FDP</td><td><a href=\"https://www.nationalacademies.org/our-work/federal-demonstration-partnership#sectionContact\">Federal Demonstration Partnership</a></td></tr><tr><td>ISSO</td><td><strong>I</strong>nformation <strong>S</strong>ystem <strong>S</strong>ecurity <strong>O</strong>fficer</td></tr><tr><td>SSP</td><td><strong>S</strong>ystem <strong>S</strong>ecurity <strong>P</strong>lan â€“ a series of documentation that describe the security controls we have or plan to implement to secure our production software system.</td></tr><tr><td>ATO</td><td><strong>A</strong>uthority <strong>t</strong>o <strong>O</strong>perate - a status that approves an IT system for use in a particular organization.</td></tr><tr><td>PB</td><td><strong>P</strong>articipatory <strong>B</strong>udgeting</td></tr></tbody></table>\n\n## Technical terms\n\nThe following terms we commonly use to describe our work on the codebase and other technical aspects of the project.\n\n<table><thead><tr><th width=\"187\">Term</th><th>Definition</th></tr></thead><tbody><tr><td>Code repository</td><td>A set of folders that contain all of the code needed to build, run, and host a piece of software (or other technical tools). Sometimes used interchangeably with \"codebase\".</td></tr><tr><td>Version control</td><td>A mechanism for storing current version of a codebase as well as the history of changes made to that code.</td></tr><tr><td>Git</td><td>The most commonly used system for version control.</td></tr><tr><td>GitHub</td><td>A software platform that lets users store and host code repositories that are version controlled with git. It also offers a series of other tools that help teams plan and manage work related to their codebase.</td></tr><tr><td>Issue</td><td>GitHub's way of tracking tasks or units of work that need to be completed within a project. <strong>Note:</strong> An \"issue\" doesn't necessarily represent something that is \"wrong\" with the codebase. It could be a new feature or piece of functionality.</td></tr><tr><td>Bug</td><td>Something wrong with codebase that causes unexpected behavior, and requires \"debugging\" to identify and fix the issue.</td></tr><tr><td>Pull request</td><td>Sometimes abbreviated to \"PR\", a pull request is a way to ask project maintainers to review and accept changes to the codebase. These changes might include new features or bug fixes.</td></tr><tr><td>Sprint</td><td>A specific period of time (commonly two weeks) during which a team works on a defined set of tasks and aims to deliver new functionality.</td></tr></tbody></table>\n\n## Change log\n\nMajor updates to the content of this page will be added here.\n\n<table><thead><tr><th>Date</th><th width=\"246\">Update</th><th>Notes</th></tr></thead><tbody><tr><td>2/12/2024</td><td>Initial Content</td><td>Updated with Initial content</td></tr><tr><td>4/23/2024</td><td>Acronym change</td><td>Updated acronym from FDG to FDP</td></tr><tr><td></td><td></td><td></td></tr></tbody></table>"}
{"path":"infra/modules/service/scheduled_jobs.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/scheduled_jobs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/design-and-research/brand-guidelines/README.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/design-and-research/brand-guidelines\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/README.md\nSize: 2.09 KB\nLast Modified: 2025-02-14T17:08:26.596Z"}
{"path":"infra/modules/service/scheduler_role.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/scheduler_role.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"# Brand Guidelines\n\n## Overview\n\nThe Grants.gov brand presents a unified and recognizable identity that influences how audiences understand and perceive Grants.gov.\n\nOur brand guidelines are publicly available to help contributors maintain consistency across communication channels.\n\n## Mission\n\nGrants.gov exists to increase access to grants and improve the grants experience for everyone.\n\n## Vision\n\nGrants.gov is an extremely simple, accessible, and easy-to-use tool for posting, finding, sharing, and applying for federal financial assistance.\n\n## Brand characters\n\nWe are:\n\n* **Approachable**\\\n  We are responsive, warm, and open with our users and ourselves.\n* **Trustworthy**\\\n  We are an indispensable, credible source of knowledge, information, and expertise.\n* **Accessible**\\\n  We put our users first, ensuring our services are accessible to people with different abilities, contexts, needs, and resources.\n* **Straightforward**\\\n  We prioritize efficient, streamlined interactions.\n* **Innovative**\\\n  We are data-driven and always open to new ideas and research.\n* **Purposeful**\\\n  We constantly seek new opportunities to bring value to our users and those we serve.\n\n## Brand voice\n\n* We are responsive, warm, and open\n* We are transparent and audience-centered\n* We get straight to the point\n* We bring clarity to complex topics\n\n## Change Log\n\nMajor updates to the content of this page will be added here.\n\n<table><thead><tr><th>Date</th><th width=\"246\">Update</th><th>Notes</th></tr></thead><tbody><tr><td>12/13/2024</td><td>Initial Content</td><td>Updated with Initial content</td></tr><tr><td>01/13/2025</td><td>Content updates</td><td>Logo, Grid &#x26; Comp, and Photos &#x26; Illustrations pages: Updated with new brand colors, larger images, and clearer written guidelines.</td></tr><tr><td></td><td></td><td></td></tr></tbody></table>"}
{"path":"infra/modules/service/task-scheduler-role.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/task-scheduler-role.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/design-and-research/brand-guidelines/colors.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/design-and-research/brand-guidelines\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/colors.md\nSize: 2.46 KB\nLast Modified: 2025-02-14T17:08:26.596Z"}
{"path":"infra/modules/service/variables.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"# Colors\n\n## Overview\n\nColor plays a significant role in our brand. The following values represent our core palette. These are used most often in the system and should be relied on when the color palette must be limited. Each of our colors is named for ideas that represent trust and accessibility.&#x20;\n\nPatina Green, Forest Green, and Sky Blue should be used most frequently as the primary brand colors, with the remaining values being used as support.\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Color Overview.jpg\" alt=\"\"><figcaption></figcaption></figure>\n\n## Color Families\n\nThe Grants color system is based on the design token system in USWDS. Each color family has 7-8 grades.\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Color Families.png\" alt=\"\"><figcaption></figcaption></figure>\n\n## Color & Accessibility\n\nPairing colors together can be complex. Always consider the application, information, and audience when selecting values. Higher contrast combinations make information clearer, while lower contrast combinations can be helpful for graphic elements.\n\nColor contrast is crucial to legibility and accessibility in type. Always be sure there is enough contrast between the type and background. In most situations, type should be used in Gray or White, with color being used to create emphasis.\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Color Accessibility.png\" alt=\"\"><figcaption></figcaption></figure>\n\n## Additional donâ€™ts\n\nDo not diminish the value of color in the brand. Avoid the following uses.\n\n\\\nDo not use unapproved colors.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants color dont use unapproved colors.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\nDo not use gradients.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants color dont use gradients.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\nDo not use low-contrast combinations.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants color dont use low contrast combinations.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\nDo not use only accent color combinations.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants do not use only accent color combinations.png\" alt=\"\"><figcaption></figcaption></figure></div>"}
{"path":"infra/modules/service/waf.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/waf.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/design-and-research/brand-guidelines/grid-and-composition.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/design-and-research/brand-guidelines\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/grid-and-composition.md\nSize: 0.75 KB\nLast Modified: 2025-02-14T17:08:26.596Z"}
{"path":"infra/modules/service/workflow_orchestrator_role.tf","language":"unknown","type":"code","directory":"infra/modules/service","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/service/workflow_orchestrator_role.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"# Grid & Composition\n\n## Overview\n\nEach element in our visual system is designed to work together and flex up or down to meet any communication need or brand application.\n\n<div><figure><img src=\"../../.gitbook/assets/Letter - 1 (1).png\" alt=\"\"><figcaption></figcaption></figure> <figure><img src=\"../../.gitbook/assets/Letter - 2 (1).png\" alt=\"\"><figcaption></figcaption></figure> <figure><img src=\"../../.gitbook/assets/Letter - 3 (2).png\" alt=\"\"><figcaption></figcaption></figure> <figure><img src=\"../../.gitbook/assets/Letter - 4 (2).png\" alt=\"\"><figcaption></figcaption></figure></div>"}
{"path":"infra/modules/storage/access_control.tf","language":"unknown","type":"code","directory":"infra/modules/storage","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/storage/access_control.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/design-and-research/brand-guidelines/iconography.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/design-and-research/brand-guidelines\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/iconography.md\nSize: 1.01 KB\nLast Modified: 2025-02-14T17:08:26.596Z"}
{"path":"infra/modules/storage/encryption.tf","language":"unknown","type":"code","directory":"infra/modules/storage","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/storage/encryption.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"# Iconography\n\n## Overview\n\nIcons are simple symbols that attract attention, aid navigation, signal an action, add emphasis, or provide feedback. Icons serve as recognizable cues that users can understand easily â€” typically without thinking too hard.\n\n## USWDS Icon Component\n\nIcons must be used consistently across our site and communications applications to accomplish their purpose well. For example, using an envelope icon for both email and a mailing address would likely confuse people. Viewers should be able to trust that a certain icon always means the same thing, no matter where itâ€™s used.\n\nAll USWDS icons have passed WCAG 2.1 AA.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants uswds icons example.png\" alt=\"\"><figcaption></figcaption></figure>\n\n[View icon components from USWDS](https://designsystem.digital.gov/components/icon/)"}
{"path":"infra/modules/storage/events.tf","language":"unknown","type":"code","directory":"infra/modules/storage","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/storage/events.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/design-and-research/brand-guidelines/logo.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/design-and-research/brand-guidelines\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/logo.md\nSize: 3.47 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"infra/modules/storage/lifecycle.tf","language":"unknown","type":"code","directory":"infra/modules/storage","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/storage/lifecycle.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"# Logo\n\n## Overview\n\nThe logo is our primary identifier. It contains both the star symbol and our name in the wordmark. It should be used most often to represent our brand, positioning Grants.gov as a trusted and innovative organization dedicated to increasing access to grants and improving the grants experience for everyone.\n\nThe logo is used most often as it captures more of our brand characters. The symbol and wordmark are next frequently used, while the favicon is reserved only for specific digital applications.\n\nThe symbol and wordmark have been carefully scaled, spaced, and aligned to create the logo. Do not alter its construction.\n\n\n\n\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Logo Overview (1).jpg\" alt=\"\"><figcaption></figcaption></figure>\n\n## Star Symbol\n\nThe logo features a unique five-pointed star composed of double lines. The star, a timeless symbol of trust and guidance, speaks directly to Grants.govâ€™s purpose and focus as a federal organization. Each double line is a pathway radiating out from the central star, symbolizing openness, accessibility, and helping others achieve their goals.\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Star Symbol.png\" alt=\"\"><figcaption></figcaption></figure>\n\n## Wordmark\n\nThe wordmark features our name only. Use it in supportive applications, especially when the audience is already very familiar with our brand and identity.\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants WordMark.png\" alt=\"\"><figcaption></figcaption></figure>\n\n## Favicon\n\nThe favicon uses our star symbol in a circle container shape to help with legibility in small or complex environments. Only use it when necessary; never pair it with the wordmark or logo.\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Favicon.png\" alt=\"\"><figcaption></figcaption></figure>\n\n## Examples\n\nBelow are some examples of the logo in use.\n\n<figure><img src=\"../../.gitbook/assets/Logo examples.png\" alt=\"\"><figcaption></figcaption></figure>\n\n## Additional Don'ts\n\nDo not diminish the value of our identity. Avoid the following treatments.\n\n\\\nDo not use unapproved colors.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants logo unapproved colors.png\" alt=\"\"><figcaption></figcaption></figure>\n\n\\\nDo not apply effects.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants logo dont apply effects.png\" alt=\"\"><figcaption></figcaption></figure>\n\n\\\nDo not adjust the scale.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants logo dont adjust scale.png\" alt=\"\"><figcaption></figcaption></figure>\n\n\\\nDo not adjust the spacing.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants logo dont adjust the spacing.png\" alt=\"\"><figcaption></figcaption></figure>\n\n\\\nDo not stretch or distort.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants logo dont stretch or distort.png\" alt=\"\"><figcaption></figcaption></figure>\n\n\\\nDo not outline.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants logo dont outline.png\" alt=\"\"><figcaption></figcaption></figure>\n\n\\\nDo not use low-contrast combinations.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants logo dont use low contrast combinations.png\" alt=\"\"><figcaption></figcaption></figure>\n\n\\\nDo not change the typeface.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants logo dont change the typeface.png\" alt=\"\"><figcaption></figcaption></figure>"}
{"path":"infra/modules/storage/main.tf","language":"unknown","type":"code","directory":"infra/modules/storage","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/storage/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/design-and-research/brand-guidelines/photos-and-illustrations.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/design-and-research/brand-guidelines\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/photos-and-illustrations.md\nSize: 2.24 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"infra/modules/storage/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/storage","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/storage/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"# Photos & Illustrations\n\n## Overview\n\nThe impact of Grants.gov's purposeful investments comes alive in our compelling photo library.\n\nEach image paints a vivid picture of how Grants.gov is successfully empowering individuals and organizations across the world and delivering real-world results. The photos highlight the multifaceted nature of the grants ecosystem â€“ from students pursuing innovative research, to community initiatives tackling the lack of clean water, to organizations improving outcomes for seniors. Photos are organized around the following categories:\n\n* Agriculture\n* Arts\n* Business\n* Community\n* Family\n* HHS Grants Team\n* Infrastructure\n* Justice\n* Nature\n* Portraits\n* School\n* Science & Technology\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Photography Overview.jpg\" alt=\"\"><figcaption></figcaption></figure>\n\n## Color & Photography\n\nWhen choosing a photo, look for things in the image that closely match a color in our palette. This will be the most harmonious way of situating a photo in our brand identity. Make sure thereâ€™s enough contrast between the image and background color.\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Color &#x26; Photography.jpg\" alt=\"\"><figcaption></figcaption></figure>\n\n## Additional Donâ€™ts\n\nDon't crop inappropriately.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants photos do not crop inappropriately.png\" alt=\"\"><figcaption></figcaption></figure>\n\n\n\nDon't use low-res photos.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants photos do not use low res photos.png\" alt=\"\"><figcaption></figcaption></figure>\n\n\n\nDon't use AI-generated imagery.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants photos do not use AI-generated imagery.png\" alt=\"\"><figcaption></figcaption></figure>\n\n\n\nDon't use distracting or off-topic imagery.\n\n<figure><img src=\"../../.gitbook/assets/simpler grants photos do not use distracting or off-topic imagery.png\" alt=\"\"><figcaption></figcaption></figure>"}
{"path":"infra/modules/storage/variables.tf","language":"unknown","type":"code","directory":"infra/modules/storage","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/storage/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/design-and-research/brand-guidelines/typography.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/design-and-research/brand-guidelines\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/brand-guidelines/typography.md\nSize: 4.88 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"infra/modules/terraform-backend-s3/README.md","language":"markdown","type":"code","directory":"infra/modules/terraform-backend-s3","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/terraform-backend-s3/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.143Z","content":"# Typography\n\n## Overview\n\nTypography is one of the most, if not the most, important communication tools for our brand. It carries our brand voice; with it, we establish our tone, personality, and message.&#x20;\n\n## Primary Typeface\n\n### Public Sans\n\nPublic Sans is a strong, neutral, open-source typeface for interfaces, text, and headings developed by USWDS. It is seen across US government websites and communications, giving Grants an additional source of credibility.\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Public Sans Sample.png\" alt=\"\"><figcaption></figcaption></figure>\n\n[Download Public Sans](https://fonts.google.com/specimen/Public+Sans). It is already included in USWDS but should be added to HHS staff computers to maintain brand alignment in Simpler Grants material.\n\n\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Public Sans Weights Sample.png\" alt=\"\"><figcaption><p>Public Sans weights</p></figcaption></figure>\n\n## Secondary Typeface\n\n### Newsreader\n\nNewsreader is an open-source font from Production Type designed for on-screen, longer-form reading. NewsReaderâ€™s shapes are open and familiar, fostering engaged reading, with features like optical sizes and large display cuts that maximize the typeâ€™s ability to be expressive. In general, use NewsReader for content such as blog posts and informational page content that is longer than 2-3 paragraphs.\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants NewsReader Sample.png\" alt=\"\"><figcaption></figcaption></figure>\n\n[Download NewsReader](https://fonts.google.com/specimen/Newsreader). It should be added to HHS staff computers to maintain brand alignment.\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants NewsReader Weights Sample.png\" alt=\"\"><figcaption><p>NewsReader weights</p></figcaption></figure>\n\n## Hierarchy\n\nDifferent type sizes should always have a relationship to one another. There is no exact rule, but select a base type size in a composition and make all other sizes proportionate to this foundation. Our brand defaults to a 1.35 type scale.\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Type Scale Sample.png\" alt=\"\"><figcaption></figcaption></figure>\n\n## Alternates\n\nUse these alternates when there are program limitations for typeface use, such as fallback fonts on the web or PowerPoint decks.\n\n### Arial\n\n* Public Sans â†’ Arial\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Arial Sample.png\" alt=\"\"><figcaption></figcaption></figure>\n\nArial is installed on your computer.\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Arial Weights Sample.png\" alt=\"\"><figcaption><p>Arial weights</p></figcaption></figure>\n\n### Georgia\n\n* NewsReader â†’ Georgia\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Georgia Sample.png\" alt=\"\"><figcaption></figcaption></figure>\n\n<figure><img src=\"../../.gitbook/assets/Simpler Grants Georgia Weights Sample.png\" alt=\"\"><figcaption><p>Georgia weights</p></figcaption></figure>\n\n## Donâ€™ts\n\nDon't apply effects to type.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants dont apply effects to type.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\nDon't center align type.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants dont center align type.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\nDon't justify text.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants dont justify text.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\nDon't outline the type.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants dont outline type.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\nDon't right align type.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants dont right alight type.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\nDon't set type in all caps.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants dont set type in all caps.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\nDon't set type in all lowercase.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants dont set type in all lower case.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\nDon't set type too small.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants dont set type too small.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\nDon't use low contrast on type.\n\n<div align=\"left\"><figure><img src=\"../../.gitbook/assets/simpler grants dont use low contrast on type.png\" alt=\"\"><figcaption></figcaption></figure></div>\n\nDon't use unapproved typefaces.\n\n<div align=\"left\" data-full-width=\"false\"><figure><img src=\"../../.gitbook/assets/simpler grants dont use unapproved typefaces.png\" alt=\"\"><figcaption></figcaption></figure></div>"}
{"path":"infra/modules/terraform-backend-s3/main.tf","language":"unknown","type":"code","directory":"infra/modules/terraform-backend-s3","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/terraform-backend-s3/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/design-and-research/user-research/README.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/design-and-research/user-research\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/user-research/README.md\nSize: 0.02 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"infra/modules/terraform-backend-s3/outputs.tf","language":"unknown","type":"code","directory":"infra/modules/terraform-backend-s3","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/terraform-backend-s3/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":""}
{"path":"infra/modules/terraform-backend-s3/variables.tf","language":"unknown","type":"code","directory":"infra/modules/terraform-backend-s3","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/modules/terraform-backend-s3/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/design-and-research/user-research/grants.gov-archetypes.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/design-and-research/user-research\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/user-research/grants.gov-archetypes.md\nSize: 0.02 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"infra/networks/.terraform.lock.hcl","language":"unknown","type":"code","directory":"infra/networks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/networks/.terraform.lock.hcl","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":""}
{"path":"infra/networks/dev.s3.tfbackend","language":"unknown","type":"code","directory":"infra/networks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/networks/dev.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/design-and-research/voice-and-tone-guide.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/design-and-research\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/design-and-research/voice-and-tone-guide.md\nSize: 12.38 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"infra/networks/main.tf","language":"unknown","type":"code","directory":"infra/networks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/networks/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"# Voice and tone guidelines\n\n## I. Introduction and Definitions\n\nThe purpose of this voice and tone guide is to help you write for Grants.gov so that we sound like the same organization everywhere people encounter our content. Consider these definitions to understand the difference between voice and tone:\n\n* Our **voice** is our unique personality. Itâ€™s grounded in our brand identity, particularly our brand characters. It should be a consistent thread through everything we write and say.\n* Our **tone** adapts to the situation. For example, we may adjust our tone for the audience or the context in which our audiences need our information.\n\n## II. Our Voice\n\nOur brand characters serve as the foundation of our voice (and everything else about Grants.gov, from our visual identity to the product experience). This section details how we bring the four brand characters most relevant to our voice and tone to life in what we write and say:\n\n* Approachable\n* Trustworthy\n* Straightforward\n* Purposeful\n\n### Approachable\n\nWe are responsive, warm, and open.\n\n#### **How we achieve this:**&#x20;\n\n* Our writing positions us as a helpful concierge:\n\n{% hint style=\"warning\" %}\nThe Grants Learning Center is your gateway to the federal grants world.\n{% endhint %}\n\n{% hint style=\"success\" %}\nGet your federal grants questions answered in the Grants Learning Center.\n{% endhint %}\n\n* We focus on the positive and specify a next step when appropriate.\n\n{% hint style=\"warning\" %}\nGrantors may register and log on to Grants.gov, but they will not be able to perform agency-related actions until they are affiliated with the agency and assigned role(s).\n{% endhint %}\n\n{% hint style=\"success\" %}\nOnce your Point of Contact (POC) links you to the agency and assigns you the appropriate roles, youâ€™ll be able to complete tasks in Grants.gov. Let your agency POC know after you register.\n{% endhint %}\n\n### Trustworthy\n\nWe are an indispensable, credible source of knowledge, information, and expertise.\n\n#### How we achieve this:\n\n* We are transparent and clear about our role in the federal grantmaking process.\n\n{% hint style=\"warning\" %}\nThe Grants.gov program management office was established, in 2002, as a part of the President's Management Agenda. Managed by the Department of Health and Human Services, Grants.gov is an E-Government initiative operating under the governance of the Office of Management and Budget.\n\nUnder the President's Management Agenda, the office was chartered to deliver a system that provides a centralized location for grant seekers to find and apply for federal funding opportunities. Today, the Grants.gov system houses information on over 1,000 grant programs and vets grant applications for federal grant-making agencies.\n{% endhint %}\n\n{% hint style=\"success\" %}\nGrants.gov is a website from the U.S. government that pulls together grants offered by federal agencies in one place. Your organization can apply for many federal grants using the Grants.gov Workspace.\n\nFor grants you can apply for using Grants.gov, youâ€™ll be able to track when it is delivered to the granting agency. After that hand-off, the granting agency will take over.\n{% endhint %}\n\n* We anticipate what our audiences want or need to know and pull out the information that impacts them most.\n\n{% hint style=\"warning\" %}\n_In this example from the Grants.Gov Release Notes page, a user has to open a PDF to understand the changes and whether they affect them._ <img src=\"../.gitbook/assets/Grants.gov Release Notes Example (1).png\" alt=\"Screenshot of a release notes table entry that includes the release number, a short description of the release, the release date, and a link to the full release details.\" data-size=\"original\">\n{% endhint %}\n\n{% hint style=\"success\" %}\n_Adding a short summary of the key changes and why they are important suggests transparency. For example:_ \\\n\\\n**What you should know**\n\nThese changes were made to ensure Grants.gov data â€“ yours and ours â€“ is as secure as possible:&#x20;\n\n* Youâ€™ll need to create a login.gov account or link your login.gov account to use Grants.gov.\n* Itâ€™s now a requirement to add another way to verify your account (multi-factor authentication or MFA).\n{% endhint %}\n\n### Straightforward\n\nWe prioritize efficient, streamlined interactions.\n\n#### **How we achieve this:**&#x20;\n\n* We get straight to the point.\n\n{% hint style=\"warning\" %}\nDetermining whether you are eligible to apply for and receive a federal grant is very important. If you are not legally eligible for a specific funding opportunity, you would waste a lot of time and money completing the application process when you cannot actually receive the grant.\n\nWhen considering eligibility, the first step is to know what type of organization you represent (or whether you are applying as an individual). If you already know whether you will apply on behalf of your organization or as an individual, then you are ready to check your eligibility.\n\nThere are many types of organizations generally eligible to apply for funding opportunities on Grants.gov. Each type of organization listed in the categories below is a specific search criterion in Search Grants. Individual applicants are welcome too!\n{% endhint %}\n\n{% hint style=\"success\" %}\nOrganizations who are eligible to apply for federal grants tend to fall into one of these categories:\n\n* Local government\n* Schools, colleges, and universities\n* Public housing organizations\n* Nonprofit organizations\n* Businesses\n\nSome grants are also open to individuals (like fellowships) and foreign organizations. Be sure to read the eligibility requirements carefully to make sure youâ€™re eligible before you apply.\n{% endhint %}\n\n* We write like people talk and use plain language. For example, we use the second-person you, embrace contractions, avoid jargon, and choose the simplest, most precise words.\n\n{% hint style=\"warning\" %}\n**How many grantors can register under an agency?**\n\nAn unlimited number of grantors can be registered under an agency. An unlimited number of grantors can be registered under an agency.\n{% endhint %}\n\n{% hint style=\"success\" %}\n**How many grantors can register under an agency?**\n\nYou can register as many grantors as you need to manage your agencyâ€™s grants.\n{% endhint %}\n\n### Purposeful\n\nWe constantly seek new opportunities to bring value to our users and those we serve.\n\n#### **How we achieve this:**&#x20;\n\n* We prioritize what our audiences want and need to know over self-promotion or lengthy background information.\n\n{% hint style=\"warning\" %}\nCustomize your Grants.gov opportunity subscriptions, including expanded criteria for saved searches and the ability to view and manage existing subscriptions. To subscribe to Grants.gov email notifications, users must first create a Grants.gov account.\n{% endhint %}\n\n{% hint style=\"success\" %}\nYouâ€™re in charge of what information you get from Grants.gov. Log in or create your account to choose which email notifications you want in your inbox.\n{% endhint %}\n\n* We bring clarity to complex topics.\n\n{% hint style=\"warning\" %}\n**Understanding the Reporting and Oversight Process In 2006, the Federal Funding** Accountability and Transparency Act (FFATA) set in motion a government-wide reporting procedure that has continued to evolve.\n\nThe law requires that information about entities and organizations receiving federal funds be disclosed to the public via a central website, USAspending.gov. This information currently includes the entity's name, amount of the grant, funding agency, and location â€“ among other requirements â€“ and is published by the grant-making agency on USASpending.gov.\n{% endhint %}\n\n{% hint style=\"success\" %}\n**Grant Reporting and Oversight**&#x20;\n\nGrant reporting and oversight accomplishes two things:\n\n1. Gives the public access to information about who gets federal grant money on USASpending.com \\[as required by the Federal Funding Accountability and Transparency Act (FFATA)]\n2. Helps granting agencies understand the impact of their grants\n{% endhint %}\n\n## III. Tone\n\nAlong with a reminder of how we define tone, we can talk about the contexts for varying the tone, such as by audience and archetype, by channel, by content type, and situation. The following tone matrix provides some considerations for tone for five content purposes. Keep in mind that:\n\n* Choosing a tone and writing in that tone is not an exact science.\n* Our content doesnâ€™t always fit neatly in these categories, but they provide a container for thinking through tone.\n\n### Tone Matrix\n\n<table data-full-width=\"true\"><thead><tr><th> </th><th>Inform</th><th>Educate</th><th>Facilitate</th><th>Support</th><th>Promote</th></tr></thead><tbody><tr><td><strong>Description</strong></td><td>Content that provides matter-of-fact details about Grants.gov</td><td>Content that helps people understand how to use Grants.gov to publish grant opportunities or find grants to apply for</td><td>Content that helps people complete their tasks related to Grants.gov</td><td>Content that helps people troubleshoot an issue with Grants.gov or administer their Grants.gov account</td><td>Content that shares new information or drives people to content in other content purpose categories</td></tr><tr><td><strong>Examples</strong></td><td><ul><li>Planned maintenance alert (distributed in various channels)</li><li>About Grants.gov content</li><li>Release notes</li></ul><p></p></td><td><ul><li>Grants 101 content</li><li>How-to content</li><li>Tips and Did you Know content</li><li>Eligibility details</li></ul></td><td><ul><li>User interface copy (including success messages)</li><li>Confirmation emails</li><li>Content for media professionals (e.g., press release)</li><li>Content for elected officials (e.g., constituent presentation)</li></ul></td><td><ul><li>User interface copy (including error messages)</li><li>How-to content for people responsible for agency administration and technical implementation (there is likely overlap with Educate content)</li></ul></td><td><ul><li>Tweet promoting a blog post on a how-to topic</li><li>Information about a new feature in the newsletter or release note summary/overview</li><li>Email introducing Grants.gov to community organizations</li></ul></td></tr><tr><td><p><strong>Tone</strong> </p><p><strong>Attributes</strong></p></td><td><ul><li>Matter-of-fact</li><li>Precise</li><li>Transparent</li></ul></td><td><ul><li>Helpful</li><li>Encouraging</li><li>Instructive</li></ul></td><td><ul><li>Reassuring</li><li>Empathetic</li><li>Anticipatory</li></ul></td><td><ul><li>Constructive</li><li>Instructive</li><li>Actionable</li></ul></td><td><ul><li>Value-focused</li><li>Upbeat</li><li>Inviting</li></ul></td></tr><tr><td><strong>Considerations</strong></td><td><ul><li>Be as clear and concise as possible while maintaining accuracy.</li><li>Use neutral language to avoid ascribing emotion to the content.</li><li>Only include historical information that your audiences need to know to understand the information.</li></ul></td><td><ul><li>Use the language your users are most likely to recognize and understand, even if it may be considered jargon to people outside your audience.</li><li>Use a guiding tone without being patronizing.</li><li>Write applicant content for the novice archetype.</li></ul></td><td><ul><li>Write in a way that suggests you are attuned to the needs of the audience, e.g., knowing what a reporter needs to write an unbiased, factual story.</li><li>Anticipate what questions the audience may have and answer them in language that makes sense to them.</li><li>Be realistic about the effort and time a task may involve, while decomplicating the information.</li></ul></td><td><ul><li>Use the language your users are most likely to recognize and understand, even if it may be considered jargon to people outside your audience.</li><li>Guide people to the next logical step or steps for their situation; e.g. error messages should tell people how to fix the error.</li></ul></td><td><p></p><ul><li>Focus on the benefit of what youâ€™re promoting to the intended audience.</li><li>Avoid crossing the line between upbeat and cheesy.</li><li>Keep an eye out for alienating language, e.g., phrases like â€œItâ€™s simple and fastâ€ could alienate someone who processes information more slowly or uses accessibility tools to do their work.</li></ul></td></tr></tbody></table>\n\n## IV. In Practice\n\nThis section will be continually updated to show examples of content that is written in accordance with our voice and tone guidance.&#x20;"}
{"path":"infra/networks/outputs.tf","language":"unknown","type":"code","directory":"infra/networks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/networks/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/get-involved/communication-channels/README.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/get-involved/communication-channels\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/communication-channels/README.md\nSize: 3.80 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"infra/networks/prod.s3.tfbackend","language":"unknown","type":"code","directory":"infra/networks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/networks/prod.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"# Communication channels\n\n{% hint style=\"danger\" %}\n**Warning**\n\nProhibited activity (e.g. violations of our code of conduct or the Federal Hatch Act) are _**never**_ allowed in _**any**_ channel. Project maintainers have the right and responsibility to moderate and remove prohibited content from all communication channels.\n{% endhint %}\n\n## Public channels\n\nThe following table links to the main channels we use to collaborate on the project and describes the kind of information you'll find in each channel.\n\n<table><thead><tr><th width=\"219\">Channel</th><th width=\"137\">Platform</th><th>Description</th></tr></thead><tbody><tr><td><a href=\"https://www.grants.gov\">grants.gov</a></td><td>Website</td><td>The official platform that users can visit to discover and apply for federal funding opportunities.</td></tr><tr><td><a href=\"https://simpler.grants.gov\">simpler.grants.gov</a></td><td>Website</td><td>A <strong>new</strong> website where you can test out the features we're building to make it easier to discover and apply for federal funding opportunities.</td></tr><tr><td><a href=\"https://app.gitbook.com/o/cFcvhi6d0nlLyH2VzVgn/s/Pm7UEzeiS1tbLCV1SFRu/\">Public wiki</a></td><td>GitBook</td><td>Public-facing knowledge base with information about how we're building simpler.grants.gov.</td></tr><tr><td><a href=\"https://github.com/HHS/simpler-grants-gov\">Code repository</a></td><td>GitHub</td><td>Open source repository that stores the source code and technical documentation for the API, static site, and other tools/services related to the initiative.</td></tr><tr><td><a href=\"https://github.com/HHS/simpler-grants-gov/issues\">Project tickets</a></td><td>GitHub</td><td>GitHub issues that describe the scope and content of work tasks related to the Simpler Grants initiative.</td></tr><tr><td><a href=\"https://github.com/orgs/HHS/projects/12\">Product roadmap</a></td><td>GitHub</td><td>Provisional public roadmap that describes the features and functionality we plan to deliver as a part of this initiative.</td></tr><tr><td><a href=\"https://github.com/orgs/HHS/projects/13\">Sprint board</a></td><td>GitHub</td><td>Planning tool that helps the team organize project tickets into sprints that deliver pieces of functionality described in the product roadmap.</td></tr><tr><td><a href=\"https://join.slack.com/t/betagrantsgov/shared_invite/zt-2ckveruk4-MaemUdO1st6C6FVrJBuIoA\">Community forum</a></td><td>Slack</td><td>Slack workspace that facilitates communication amongst internal and external stakeholders about the work happening within the initiative.</td></tr><tr><td>Listserv</td><td>Google group</td><td>Email-based forum that provides another forum for internal and external stakeholders to share updates or ask questions about the initiative.</td></tr></tbody></table>\n\n## Channel-specific guides\n\nVisit the following pages to learn more about individual communication channels\n\n{% content-ref url=\"broken-reference\" %}\n[Broken link](broken-reference)\n{% endcontent-ref %}\n\n{% content-ref url=\"slack-community-chat/\" %}\n[slack-community-chat](slack-community-chat/)\n{% endcontent-ref %}\n\n{% content-ref url=\"broken-reference\" %}\n[Broken link](broken-reference)\n{% endcontent-ref %}\n\n{% content-ref url=\"../github-planning/\" %}\n[github-planning](../github-planning/)\n{% endcontent-ref %}\n\n{% content-ref url=\"zoom-public-meetings.md\" %}\n[zoom-public-meetings.md](zoom-public-meetings.md)\n{% endcontent-ref %}\n\n## Change log\n\nMajor updates to the content of this page will be added here.\n\n<table><thead><tr><th>Date</th><th width=\"246\">Update</th><th>Notes</th></tr></thead><tbody><tr><td>2/12/2024</td><td>Initial Content</td><td>Updated with Initial content</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table>"}
{"path":"infra/networks/staging.s3.tfbackend","language":"unknown","type":"code","directory":"infra/networks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/networks/staging.s3.tfbackend","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"File: documentation/wiki/get-involved/communication-channels/slack-community-chat/README.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/get-involved/communication-channels/slack-community-chat\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/communication-channels/slack-community-chat/README.md\nSize: 5.14 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"infra/networks/variables.tf","language":"unknown","type":"code","directory":"infra/networks","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/networks/variables.tf","size":0,"lastModified":"2025-02-14T17:08:31.143Z","content":"# Slack - Community chat\n\n{% hint style=\"success\" %}\n**Welcome to our Slack workspace!**&#x20;\n\nSlack is our chat platform for communication and collaboration within the Simpler Grants community. This guide will help you get started and make the most out of your Slack experience.\\\n\\\nPlease [use this invitation link](https://join.slack.com/t/betagrantsgov/shared_invite/zt-2ckveruk4-MaemUdO1st6C6FVrJBuIoA) to join.\n{% endhint %}\n\n## Getting Started\n\n### 1. Joining Slack\n\n* If you need help installing Slack, please use the link below to our step-by-step guide.\n* If you haven't already, sign up for Slack using the provided invitation link above.\n* Once signed up, download the Slack desktop or mobile app for easy access to the workspace.\n* Post a short introduction in [#all-introductions](https://betagrantsgov.slack.com/archives/C05TDLG3M51) please include:\n  * Name\n  * Pronouns\n  * What brings you to the Simpler Grants community\n\n{% content-ref url=\"installing-slack.md\" %}\n[installing-slack.md](installing-slack.md)\n{% endcontent-ref %}\n\n### 2. Navigating Channels\n\n* Our channels use a prefix naming convention to explain what the channel is used for.&#x20;\n\n{% content-ref url=\"naming-conventions.md\" %}\n[naming-conventions.md](naming-conventions.md)\n{% endcontent-ref %}\n\n* Familiarize yourself with the different channels in our workspace. Channels are organized by topics, projects, or teams.&#x20;\n\n{% content-ref url=\"recommended-channels.md\" %}\n[recommended-channels.md](recommended-channels.md)\n{% endcontent-ref %}\n\n### 3. Understanding Direct Messages\n\n* As an open source community, we try to keep conversations in public channels to make sure that the conversation is open and viewable to all participants. We recommend that you err on the side of posting in a public channel if you can.\n* Use direct messages to communicate privately with individual team members if the&#x20;\n* You can also create group messages for small team discussions.\n\n## Using Slack for Project Contributions\n\n### 1. Communication Etiquette\n\n* Follow the[ code of conduct](../../policies-and-guidelines/) that has been set by the Simpler Grants community\n* When possible please use public channels instead of private or group DMs to make sure that we are creating a resource for everyone in the community.&#x20;\n* Avoid posting personally identifiable information on slack. This is a public chat platform and anything posted can be seen by the general public.\n\n### 2. Sharing Updates\n\n* Use appropriate channels to share project updates, announcements, or important information.\n* Utilize threads to keep conversations organized and focused.\n\n### 3. Asking Questions\n\n* Don't hesitate to ask questions in relevant channels or through direct messages.\n* Prefix your question with \"@channel\" or \"@here\" if it's urgent or requires immediate attention.\n  * This will notify everyone in the channel, so only use in case extreme urgency.&#x20;\n\n### 4. Collaborating on Code\n\n* Share code snippets or GitHub links directly in Slack channels for quick feedback.\n* Use [#topic-code-review](https://betagrantsgov.slack.com/archives/C06JS9Y6249) for requesting code reviews and discussions.\n\n### 5. Meeting Coordination\n\n* Use Slack to schedule meetings, share agendas, and coordinate with team members.\n* Utilize integrations like Google Calendar or Zoom to sync meeting schedules.\n\n## Best Practices and Conventions\n\n### 1. Channel Guidelines\n\n* Each channel may have specific guidelines or rules outlined in the channel description. Familiarize yourself with these guidelines before participating.\n\n### 2. Notifications Management\n\n* Customize your notification settings to ensure you receive relevant updates without being overwhelmed.\n* Consider muting channels or threads that are less relevant to your role or interests.\n\n### 3. Use threads to continue a conversation instead of posting to the main channel\n\n* When you see a message that you would like to respond to in the main channel, please make use of the threads feature to keep the main channels clear and concise.&#x20;\n  * A thread can be created on a new post in the main channel by hovering over the message and clicking \"reply in thread\" on the toolbar that appears.\n  * You can join an existing thread by clicking on the Reply underneath a message that already has an ongoing thread.\n* When in a thread, only the people who are currently involved in that thread will get notifications, which helps to keep notification spam down for all members of the main channel.\n  * If you need to get the attention of someone not in the thread, you can use the @\\<username> command, which will notify them that they were mentioned in the thread.\n  * You can also choose to send a response to both the thread and main channel if you belive that a threaded message should be seen by everyone.&#x20;\n\n## Change log\n\nMajor updates to the content of this page will be added here.\n\n<table><thead><tr><th>Date</th><th width=\"246\">Update</th><th>Notes</th></tr></thead><tbody><tr><td>2/12/2024</td><td>Initial Content</td><td>Updated with Initial content</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table>"}
{"path":"infra/project-config/README.md","language":"markdown","type":"code","directory":"infra/project-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/project-config/README.md","size":894508,"lastModified":"2025-02-14T17:08:31.144Z","content":"File: documentation/wiki/get-involved/communication-channels/slack-community-chat/installing-slack.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/get-involved/communication-channels/slack-community-chat\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/communication-channels/slack-community-chat/installing-slack.md\nSize: 5.15 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"infra/project-config/aws-services.tf","language":"unknown","type":"code","directory":"infra/project-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/project-config/aws-services.tf","size":0,"lastModified":"2025-02-14T17:08:31.144Z","content":"# Installing Slack\n\n## **Overview**\n\nSlack is[ a messaging application for business](https://www.youtube.com/watch?v=6wjmH5qL3Ms\\&t=13s), and is a vital tool for how the Simpler Grants.gov team communicates. If you're new to this tool, please take some time to follow the onboarding steps below.\n\n### **Step 1 - Sign up for Slack**\n\n* Please [use this invitation link](https://join.slack.com/t/betagrantsgov/shared\\_invite/zt-2ckveruk4-MaemUdO1st6C6FVrJBuIoA) to join.\n  * Your invitation will say the following:\n  * â€œLetâ€™s work together on Slack. Use this link to start a direct message with me: \\[Link to be included]â€\n  * Click the link within the invitation and it should direct you to a â€œLetâ€™s get you set upâ€ page similar to the below:&#x20;\n\n\\\n![](https://lh6.googleusercontent.com/4\\_HXxmdT21mdjFqAoJ52cC2QFTRl2eI9yorwIuA10xz0Sd6d3YThZU99seoRHyDBiGrO3WUeo9P5fTNBLTpnDrNJ0m5Z8AEf2MVEnYv-GtnhGbPvHuL9qdzjVjcVcA8rwfXsBnDE6XWFhaMxe4Da69Y)\\\n\n\n### **Step 2 - Get Started with Slack**\n\n* Please follow the instructions on your â€œGet Startedâ€ page. If you receive a prompt similar to the below, select â€œContinueâ€&#x20;\n\n![](https://lh5.googleusercontent.com/fdtfJawoJ\\_Mq-qWRI2PV6XnZkeIFYzRzFPeuNOks1Kp3qaASE3vTQDoGxVI8WkkwjxkMqOp9aYMg7He0yShtvsUcSCChxK92W2uYFXxQ5vB\\_7IrhXfWzIv6-A3tCWyt3BOPwsl-XpHMPEqUIR9mK8i0)![](https://lh6.googleusercontent.com/\\_tIuQc2aZ8wim7Ggp4MUyKlPFmgg-eTq7R90Fjxqteb43iJQeQR5C6RheqGEfGULwoqQ-Fjh4-7JC7tR6TS46EI0Jlaskk8\\_XkUJbteGLKhW\\_LEjg7pitvRsShT2jJD1p6RRgaLjgN34DoPkPoquubA)\n\n* Please enter your email address and select â€œContinueâ€\n* Authentication Time! Upon arriving at the next page, a 6 character code should have been sent to your email address. Please go view your HHS inbox to view the 6 character code and enter it in the screen below. If your inbox is categorized into various sections, please remember to select â€œview allâ€ within your inbox to find the code. If you still donâ€™t see the email, check your spam filter.\n\n![](https://lh3.googleusercontent.com/cnVybR1Qg2431UtS1PQamPIshgfM1PSIBOXX-jkTInATZi7PtXSTFX6inrRulTe6\\_V-Fwy58Sr43\\_SO2G9PaMbJovG86w1660OirNBZ8buUqilUd2H0\\_ThwF7lTRJDzIo1e4FkQ7hou0JH49X5OHNpM)\n\n* Letâ€™s start a conversation!\n  * Select â€œStart a Conversationâ€ and move forward to the next screen![](https://lh5.googleusercontent.com/QnPahYqYbYZF9zXX4UiLyscl50FTss4ZcE1AOEdHhaZqaAA1a-vQe6pby0YbCmG\\_xz2DXofc9XaqvMNUPD5vb-D0ZHdt2zayGd8q8ka8BELgT29yrnIm7oe7KunaRV0F\\_IMmru-0ISvXFejlTIvcSkI)\\\n\n* Welcome to the Simpler Grants Slack Channel! At this point, youâ€™ve automatically joined our #general Channel. Once youâ€™ve completed your onboarding process, I will send you a message and proceed to add you to our internal and public channels.&#x20;\n\n### **Step 3 - Letâ€™s get to know you better!**&#x20;\n\n* Create your profile.\n  * At the top right of your slack screen, select the white background icon with a human figure within it. Then select â€œProfile.â€![](https://lh6.googleusercontent.com/AAzDboo17XfgbTYNxes8wcIepmh2eKBIeWcz2GG9vXOljb0Uj-xGdxpyGJAySRtENTFc7FThQOcTioCCZobXoehHRzaoun25LfBLxMdk-ocsiad1GWkVWRivwRDNIk8Ol87VHOcCmd-ScD-knfzWik8)\n* Select â€œEditâ€&#x20;\n* Fill in the displayed fields and upload a professional image. ![](https://lh6.googleusercontent.com/SsLQ7gUxgs2fbxPw0nxDWplEMEB3d5VaAIDPMOdwPQveCYhX-q91aBxR0kJupWgs79L2wEtKP8lrk6NoOFoU05v0KCNV36\\_IofDcK8XrRh4zdz-Eh\\_mqtuNdWFk1qbzdK99I9mtZou0v7m6EaO0MnSc)![](https://lh6.googleusercontent.com/Y\\_E5CoWduHmsddV\\_msGpbeNknMnc3qBntPwJX5f8VI-o9Fc2hIsu29ja2AG4mNq5e1xGOmKtamdri-\\_Y8wHYg21UZ2sgMqNgUJ2jABNCMIBLI3EXFpAAvZNfpwC\\_bcdGAPrRp9rNi6TD9sriab4xcQU)\n\n## **Download Slack on your desktop**\n\n**Step 1**: Visit the Slack Downloads website and choose to â€œDownload (64 - BIT)â€ the Windows version. Download link is [here](https://slack.com/downloads/windows).\n\n**Step 2**: Go to your local â€œDownloadsâ€ folder and select â€œSlackSetup.exeâ€. Continue with your typical local process to install software on your computer.\n\n**Step 3:** Launch Slack! Select â€œSign in to Slackâ€ using your previously created credentials.\n\n&#x20;![](https://lh5.googleusercontent.com/tXBWrwkDUm9xNOyQj2uiWEaYkSjFCmP49GRzuW9C-9\\_P7B8zSgjIx8Y8S6m8IxLo8O8LXeR9KFdvCX88fv\\_vqEVnbuQZui3NBMAceBuoyu\\_uBWwC2H5fByHxQJYkEle-izIF5s9rhyG\\_el-vMBRqcKA)\n\n**Step 4**: Choose a Workspace. Click on â€œSimpler Grants.govâ€ and select â€œOpen Slackâ€\n\n**Step 5**: At this step, you might receive a â€œWe donâ€™t recognize this deviceâ€ prompt. Please follow the instructions to retrieve the six digit code.\n\n![](https://lh5.googleusercontent.com/OX3B6K8f\\_KMzoGmi0eULNOQEvDR5uT8GRxuFVj\\_ynAw0Cvwm-OUch8tZzqXyX0j3fz5PF7-SWvjsEFqAXC-3tjWz2qfWyxOLkDYx0m9cUqSDLp-SaskfxFRdYmVosIpQ4\\_SXt9wn9Nn3GusKd1QZqJ0)\n\n**Step 6**: The Slack application should now be available on your desktop.\n\n\n\n## Change log\n\nMajor updates to the content of this page will be added here.\n\n<table><thead><tr><th>Date</th><th width=\"246\">Update</th><th>Notes</th></tr></thead><tbody><tr><td>2/12/2024</td><td>Initial Content</td><td>Updated with Initial content</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table>"}
{"path":"infra/project-config/main.tf","language":"unknown","type":"code","directory":"infra/project-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/project-config/main.tf","size":0,"lastModified":"2025-02-14T17:08:31.144Z","content":"File: documentation/wiki/get-involved/communication-channels/slack-community-chat/naming-conventions.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/get-involved/communication-channels/slack-community-chat\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/communication-channels/slack-community-chat/naming-conventions.md\nSize: 1.73 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"infra/project-config/networks.tf","language":"unknown","type":"code","directory":"infra/project-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/project-config/networks.tf","size":0,"lastModified":"2025-02-14T17:08:31.144Z","content":"# Naming Conventions\n\n\n\n<table><thead><tr><th width=\"144.33333333333331\">Prefix</th><th width=\"336\">Description</th><th>Examples</th></tr></thead><tbody><tr><td><code>all-</code></td><td>Public groups that are open to everyone</td><td><ul><li>all-grants-gov</li><li>all-nofo</li><li>all-welcome</li></ul></td></tr><tr><td><code>internal-</code></td><td>Intentionally private groups for internal team communication</td><td><ul><li>internal-grants-gov</li><li>internal-nofo</li></ul></td></tr><tr><td><code>topic-</code></td><td>Public groups that relate to work in a specific practice or subject area with long-running communication</td><td><ul><li>topic-design</li><li>topic-engineering</li><li>topic-product</li></ul></td></tr><tr><td><code>temp-</code></td><td>Time-limited private or public groups that are organized around a particular topic or working effort. These could be noisy or time sensitive and are planned to be archived</td><td><ul><li>temp-slack-channel-setup</li><li>temp-prod-incident-13</li></ul></td></tr><tr><td><code>zbot-</code></td><td>Public or private groups with slack automations </td><td><ul><li>zbot-aws-alerts</li></ul></td></tr><tr><td><code>z_</code></td><td>Public groups for miscellaneous or fun topics to discuss</td><td><ul><li>z_grants-pets</li><li>z_random</li></ul></td></tr></tbody></table>\n\n## Change log\n\nMajor updates to the content of this page will be added here.\n\n<table><thead><tr><th>Date</th><th width=\"246\">Update</th><th>Notes</th></tr></thead><tbody><tr><td>2/12/2024</td><td>Initial Content</td><td>Updated with Initial content</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table>"}
{"path":"infra/project-config/outputs.tf","language":"unknown","type":"code","directory":"infra/project-config","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/project-config/outputs.tf","size":0,"lastModified":"2025-02-14T17:08:31.144Z","content":"File: documentation/wiki/get-involved/communication-channels/slack-community-chat/recommended-channels.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/get-involved/communication-channels/slack-community-chat\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/communication-channels/slack-community-chat/recommended-channels.md\nSize: 1.85 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"infra/test/go.mod","language":"unknown","type":"code","directory":"infra/test","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/test/go.mod","size":0,"lastModified":"2025-02-14T17:08:31.144Z","content":"# Recommended Channels\n\n\n\n<table><thead><tr><th width=\"174.33333333333331\">Type</th><th width=\"336\">Description</th><th>Discipline</th></tr></thead><tbody><tr><td>General</td><td>Groups that are targeted at anyone.</td><td><ul><li><a href=\"https://betagrantsgov.slack.com/archives/C061F5B6QTF\">#topic-help</a></li><li><a href=\"https://betagrantsgov.slack.com/archives/C05TXR02EDS\">#topic-reference-shelf</a></li><li><a href=\"https://betagrantsgov.slack.com/archives/C05PNFH383W\">#all-grants-gov</a></li></ul></td></tr><tr><td>Engineering</td><td>Groups that are focused on those who would like to contribute code or are of the engineering discipline.</td><td><ul><li><a href=\"https://betagrantsgov.slack.com/archives/C05TSL64VUH\">#topic-engineering</a></li></ul></td></tr><tr><td>Design</td><td>Groups that are focused on those who would like to contribute designs or are of the design discipline.</td><td><ul><li><a href=\"https://betagrantsgov.slack.com/archives/C05TGEL3C6Q\">#topic-design</a></li></ul></td></tr><tr><td>Product</td><td>Groups that focus on the Simpler Grants product work. </td><td><ul><li><a href=\"https://betagrantsgov.slack.com/archives/C05TGEN1J7N\">#topic-product</a></li></ul></td></tr><tr><td>Communications</td><td>Groups that focus on Simpler Grants comms work.</td><td><ul><li><a href=\"https://betagrantsgov.slack.com/archives/C061HLPAR4K\">#topic-communications</a></li></ul></td></tr></tbody></table>\n\n## Change log\n\nMajor updates to the content of this page will be added here.\n\n<table><thead><tr><th>Date</th><th width=\"246\">Update</th><th>Notes</th></tr></thead><tbody><tr><td>2/12/2024</td><td>Initial Content</td><td>Updated with Initial content</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table>"}
{"path":"infra/test/go.sum","language":"unknown","type":"code","directory":"infra/test","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/test/go.sum","size":0,"lastModified":"2025-02-14T17:08:31.144Z","content":"File: documentation/wiki/get-involved/communication-channels/zoom-public-meetings.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/get-involved/communication-channels\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/communication-channels/zoom-public-meetings.md\nSize: 1.74 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"infra/test/helpers.go","language":"unknown","type":"code","directory":"infra/test","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/test/helpers.go","size":0,"lastModified":"2025-02-14T17:08:31.144Z","content":"# Zoom - Public Meetings\n\nWelcome to our Zoom meetings platform! Zoom is our primary tool for virtual meetings within the Simpler Grants community. This guide will help you get started and make the most out of your Zoom experience.\n\n## Getting Started\n\n### 1. Joining Zoom Meetings\n\n* Click on the Zoom meeting link provided by the meeting organizer.\n* If it's your first time using Zoom, you may be prompted to download and install the Zoom application. Follow the on-screen instructions to do so.\n\n### 2. Testing Audio and Video\n\n* Before joining a meeting, test your audio and video settings to ensure they're working correctly.\n* Click on \"Test Speaker & Microphone\" or \"Test Video\" in the Zoom settings menu to check your audio and video devices.\n\n## Using Zoom for Meetings\n\n### 1. Meeting Etiquette\n\n* Follow the[ code of conduct](../policies-and-guidelines/) that has been set by the Simpler Grants community\n* Keep discussions relevant to the meeting agenda and avoid disruptive behavior.\n* Mute your microphone when not speaking to minimize background noise.\n* Use the \"Raise Hand\" feature to indicate when you want to speak or have a question.\n\n### 2. Participating in Discussions\n\n* Contribute actively to discussions by sharing your insights and ideas.\n* Use the chat feature to ask questions or provide feedback in text format during the meeting.\n\n## Change log\n\nMajor updates to the content of this page will be added here.\n\n<table><thead><tr><th>Date</th><th width=\"246\">Update</th><th>Notes</th></tr></thead><tbody><tr><td>2/12/2024</td><td>Initial Content</td><td>Updated with Initial content</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table>"}
{"path":"infra/test/infra_test.go","language":"unknown","type":"code","directory":"infra/test","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/infra/test/infra_test.go","size":0,"lastModified":"2025-02-14T17:08:31.144Z","content":"File: documentation/wiki/get-involved/community-events/README.md\nLanguage: md\nType: code\nDirectory: documentation/wiki/get-involved/community-events\nGitHub URL: https://github.com/HHS/simpler-grants-gov/blob/main/documentation/wiki/get-involved/community-events/README.md\nSize: 0.02 KB\nLast Modified: 2025-02-14T17:08:26.597Z"}
{"path":"repolinter.json","language":"json","type":"code","directory":"root","github_url":"https://github.com/HHS/simpler-grants-gov/blob/main/repolinter.json","size":0,"lastModified":"2025-02-14T17:08:31.144Z","content":""}